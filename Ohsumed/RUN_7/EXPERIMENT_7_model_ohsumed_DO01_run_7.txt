(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.01059 val_loss= 3.11328 val_acc= 0.24776 time= 0.58487
Epoch: 0002 train_loss= 3.11288 train_acc= 0.22171 val_loss= 3.06613 val_acc= 0.24478 time= 0.29300
Epoch: 0003 train_loss= 3.06538 train_acc= 0.21112 val_loss= 2.99468 val_acc= 0.24179 time= 0.28503
Epoch: 0004 train_loss= 2.99362 train_acc= 0.20649 val_loss= 2.90645 val_acc= 0.24179 time= 0.28500
Epoch: 0005 train_loss= 2.90601 train_acc= 0.20251 val_loss= 2.81747 val_acc= 0.23881 time= 0.29252
Epoch: 0006 train_loss= 2.81881 train_acc= 0.20285 val_loss= 2.74508 val_acc= 0.23881 time= 0.29200
Epoch: 0007 train_loss= 2.74917 train_acc= 0.20615 val_loss= 2.70328 val_acc= 0.24179 time= 0.28600
Epoch: 0008 train_loss= 2.71042 train_acc= 0.20847 val_loss= 2.69249 val_acc= 0.22985 time= 0.29000
Epoch: 0009 train_loss= 2.70128 train_acc= 0.19523 val_loss= 2.68971 val_acc= 0.20597 time= 0.29504
Epoch: 0010 train_loss= 2.69836 train_acc= 0.17803 val_loss= 2.67577 val_acc= 0.20597 time= 0.29300
Epoch: 0011 train_loss= 2.67897 train_acc= 0.17439 val_loss= 2.64826 val_acc= 0.20597 time= 0.28900
Epoch: 0012 train_loss= 2.64407 train_acc= 0.17637 val_loss= 2.61622 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.60198 train_acc= 0.18332 val_loss= 2.58716 val_acc= 0.22985 time= 0.29333
Epoch: 0014 train_loss= 2.56329 train_acc= 0.19623 val_loss= 2.56207 val_acc= 0.24776 time= 0.29300
Epoch: 0015 train_loss= 2.52979 train_acc= 0.21873 val_loss= 2.53773 val_acc= 0.26567 time= 0.28900
Epoch: 0016 train_loss= 2.49821 train_acc= 0.24785 val_loss= 2.51050 val_acc= 0.28955 time= 0.29300
Epoch: 0017 train_loss= 2.46461 train_acc= 0.28061 val_loss= 2.47829 val_acc= 0.30448 time= 0.29000
Epoch: 0018 train_loss= 2.42718 train_acc= 0.30940 val_loss= 2.44069 val_acc= 0.31940 time= 0.29500
Epoch: 0019 train_loss= 2.38536 train_acc= 0.33422 val_loss= 2.39854 val_acc= 0.32537 time= 0.28700
Epoch: 0020 train_loss= 2.33885 train_acc= 0.35473 val_loss= 2.35332 val_acc= 0.32836 time= 0.28600
Epoch: 0021 train_loss= 2.28876 train_acc= 0.36896 val_loss= 2.30643 val_acc= 0.34030 time= 0.28600
Epoch: 0022 train_loss= 2.23631 train_acc= 0.37922 val_loss= 2.25877 val_acc= 0.34925 time= 0.29900
Epoch: 0023 train_loss= 2.18147 train_acc= 0.38451 val_loss= 2.21101 val_acc= 0.35224 time= 0.28800
Epoch: 0024 train_loss= 2.12568 train_acc= 0.39378 val_loss= 2.16362 val_acc= 0.36716 time= 0.28700
Epoch: 0025 train_loss= 2.06900 train_acc= 0.41363 val_loss= 2.11671 val_acc= 0.38806 time= 0.28719
Epoch: 0026 train_loss= 2.01040 train_acc= 0.44209 val_loss= 2.07052 val_acc= 0.41493 time= 0.29423
Epoch: 0027 train_loss= 1.94952 train_acc= 0.48478 val_loss= 2.02572 val_acc= 0.46269 time= 0.28700
Epoch: 0028 train_loss= 1.89144 train_acc= 0.52283 val_loss= 1.98258 val_acc= 0.48955 time= 0.28656
Epoch: 0029 train_loss= 1.83060 train_acc= 0.55725 val_loss= 1.94022 val_acc= 0.50149 time= 0.29000
Epoch: 0030 train_loss= 1.77223 train_acc= 0.58968 val_loss= 1.89678 val_acc= 0.52239 time= 0.29300
Epoch: 0031 train_loss= 1.71041 train_acc= 0.60721 val_loss= 1.85152 val_acc= 0.53433 time= 0.29000
Epoch: 0032 train_loss= 1.64771 train_acc= 0.61615 val_loss= 1.80535 val_acc= 0.52537 time= 0.28700
Epoch: 0033 train_loss= 1.58504 train_acc= 0.62541 val_loss= 1.75989 val_acc= 0.52537 time= 0.29000
Epoch: 0034 train_loss= 1.52696 train_acc= 0.63369 val_loss= 1.71658 val_acc= 0.53731 time= 0.29600
Epoch: 0035 train_loss= 1.46916 train_acc= 0.64990 val_loss= 1.67607 val_acc= 0.54328 time= 0.29000
Epoch: 0036 train_loss= 1.41372 train_acc= 0.66115 val_loss= 1.63766 val_acc= 0.55224 time= 0.28913
Epoch: 0037 train_loss= 1.35728 train_acc= 0.67704 val_loss= 1.60145 val_acc= 0.56119 time= 0.28606
Epoch: 0038 train_loss= 1.30439 train_acc= 0.68961 val_loss= 1.56777 val_acc= 0.57612 time= 0.29288
Epoch: 0039 train_loss= 1.25157 train_acc= 0.69921 val_loss= 1.53688 val_acc= 0.58209 time= 0.29000
Epoch: 0040 train_loss= 1.19896 train_acc= 0.70781 val_loss= 1.50800 val_acc= 0.58209 time= 0.28697
Epoch: 0041 train_loss= 1.14813 train_acc= 0.71707 val_loss= 1.47997 val_acc= 0.58209 time= 0.28800
Epoch: 0042 train_loss= 1.10007 train_acc= 0.73031 val_loss= 1.45253 val_acc= 0.58806 time= 0.29201
Epoch: 0043 train_loss= 1.05442 train_acc= 0.74289 val_loss= 1.42633 val_acc= 0.60000 time= 0.29703
Epoch: 0044 train_loss= 1.00943 train_acc= 0.76109 val_loss= 1.40175 val_acc= 0.60597 time= 0.28797
Epoch: 0045 train_loss= 0.96405 train_acc= 0.77598 val_loss= 1.37846 val_acc= 0.60896 time= 0.28703
Epoch: 0046 train_loss= 0.92137 train_acc= 0.78392 val_loss= 1.35699 val_acc= 0.60597 time= 0.29000
Epoch: 0047 train_loss= 0.87973 train_acc= 0.79451 val_loss= 1.33758 val_acc= 0.61493 time= 0.29397
Epoch: 0048 train_loss= 0.84006 train_acc= 0.80741 val_loss= 1.32018 val_acc= 0.62090 time= 0.29004
Epoch: 0049 train_loss= 0.79984 train_acc= 0.81866 val_loss= 1.30435 val_acc= 0.61493 time= 0.28800
Epoch: 0050 train_loss= 0.76602 train_acc= 0.82363 val_loss= 1.28920 val_acc= 0.62090 time= 0.28898
Epoch: 0051 train_loss= 0.72914 train_acc= 0.83388 val_loss= 1.27463 val_acc= 0.62388 time= 0.29403
Epoch: 0052 train_loss= 0.69419 train_acc= 0.84547 val_loss= 1.26190 val_acc= 0.62985 time= 0.28701
Epoch: 0053 train_loss= 0.66101 train_acc= 0.85275 val_loss= 1.25062 val_acc= 0.63284 time= 0.28700
Epoch: 0054 train_loss= 0.63119 train_acc= 0.85738 val_loss= 1.24034 val_acc= 0.63284 time= 0.28899
Epoch: 0055 train_loss= 0.59868 train_acc= 0.86400 val_loss= 1.23107 val_acc= 0.64478 time= 0.29601
Epoch: 0056 train_loss= 0.57120 train_acc= 0.87260 val_loss= 1.22307 val_acc= 0.63881 time= 0.28700
Epoch: 0057 train_loss= 0.54379 train_acc= 0.88418 val_loss= 1.21602 val_acc= 0.64179 time= 0.29000
Epoch: 0058 train_loss= 0.51698 train_acc= 0.88848 val_loss= 1.20933 val_acc= 0.64776 time= 0.28899
Epoch: 0059 train_loss= 0.49180 train_acc= 0.89510 val_loss= 1.20273 val_acc= 0.64776 time= 0.29503
Epoch: 0060 train_loss= 0.46698 train_acc= 0.90536 val_loss= 1.19621 val_acc= 0.65075 time= 0.28998
Epoch: 0061 train_loss= 0.44269 train_acc= 0.90735 val_loss= 1.19041 val_acc= 0.65672 time= 0.29004
Epoch: 0062 train_loss= 0.42209 train_acc= 0.91363 val_loss= 1.18574 val_acc= 0.66269 time= 0.28600
Epoch: 0063 train_loss= 0.40078 train_acc= 0.91794 val_loss= 1.18274 val_acc= 0.66567 time= 0.29400
Epoch: 0064 train_loss= 0.38137 train_acc= 0.92455 val_loss= 1.18099 val_acc= 0.66269 time= 0.29003
Epoch: 0065 train_loss= 0.36139 train_acc= 0.92985 val_loss= 1.17993 val_acc= 0.65970 time= 0.28700
Epoch: 0066 train_loss= 0.34280 train_acc= 0.93613 val_loss= 1.17887 val_acc= 0.65672 time= 0.28700
Epoch: 0067 train_loss= 0.32502 train_acc= 0.94077 val_loss= 1.17788 val_acc= 0.65672 time= 0.29925
Epoch: 0068 train_loss= 0.31118 train_acc= 0.94672 val_loss= 1.17764 val_acc= 0.66269 time= 0.29403
Epoch: 0069 train_loss= 0.29439 train_acc= 0.95103 val_loss= 1.17753 val_acc= 0.67463 time= 0.28668
Epoch: 0070 train_loss= 0.28008 train_acc= 0.95235 val_loss= 1.17696 val_acc= 0.67761 time= 0.28701
Epoch: 0071 train_loss= 0.26594 train_acc= 0.95566 val_loss= 1.17715 val_acc= 0.67761 time= 0.29500
Epoch: 0072 train_loss= 0.25460 train_acc= 0.95996 val_loss= 1.17814 val_acc= 0.67761 time= 0.29100
Epoch: 0073 train_loss= 0.24115 train_acc= 0.96062 val_loss= 1.17984 val_acc= 0.67463 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.15857 accuracy= 0.68959 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7219    0.7135    0.7176       342
           1     0.7103    0.7379    0.7238       103
           2     0.7167    0.6143    0.6615       140
           3     0.6538    0.4304    0.5191        79
           4     0.6456    0.7727    0.7034       132
           5     0.6804    0.7891    0.7308       313
           6     0.6759    0.7157    0.6952       102
           7     0.6765    0.3286    0.4423        70
           8     0.5667    0.3400    0.4250        50
           9     0.6304    0.7484    0.6844       155
          10     0.8417    0.6257    0.7178       187
          11     0.6132    0.6450    0.6287       231
          12     0.7485    0.7191    0.7335       178
          13     0.7681    0.8117    0.7893       600
          14     0.7792    0.8492    0.8127       590
          15     0.7606    0.7105    0.7347        76
          16     0.6875    0.3235    0.4400        34
          17     0.5000    0.1000    0.1667        10
          18     0.4408    0.4797    0.4594       419
          19     0.6634    0.5194    0.5826       129
          20     0.6296    0.6071    0.6182        28
          21     1.0000    0.7241    0.8400        29
          22     0.6400    0.3478    0.4507        46

    accuracy                         0.6896      4043
   macro avg     0.6848    0.5936    0.6208      4043
weighted avg     0.6921    0.6896    0.6851      4043

Macro average Test Precision, Recall and F1-Score...
(0.6848163566654548, 0.5936241635776334, 0.6207589061463803, None)
Micro average Test Precision, Recall and F1-Score...
(0.6895869403907989, 0.6895869403907989, 0.6895869403907989, None)
embeddings:
14157 3357 4043
[[ 0.3460002   0.4755151   0.5649893  ...  0.3203567   0.27648342
   0.45120555]
 [ 0.07504476  0.11130293  0.27642775 ... -0.02617161 -0.01677509
   0.12077054]
 [ 0.32692906  0.36913916  0.38920888 ...  0.21239711  0.27209792
   0.23799834]
 ...
 [ 0.15236157  0.21812999  0.3061368  ...  0.19143972  0.16163784
   0.07303724]
 [-0.02026217  0.14027853  0.1008627  ... -0.08006909  0.05426463
   0.13616903]
 [ 0.15165475  0.09667853  0.40937918 ...  0.14009781  0.04582777
   0.21580592]]

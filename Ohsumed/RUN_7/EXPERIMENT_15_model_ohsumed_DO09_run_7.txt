(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13555 train_acc= 0.02912 val_loss= 3.11926 val_acc= 0.26269 time= 0.59188
Epoch: 0002 train_loss= 3.11957 train_acc= 0.23891 val_loss= 3.08160 val_acc= 0.26866 time= 0.29600
Epoch: 0003 train_loss= 3.08162 train_acc= 0.24355 val_loss= 3.02214 val_acc= 0.26866 time= 0.28800
Epoch: 0004 train_loss= 3.02233 train_acc= 0.23660 val_loss= 2.94465 val_acc= 0.26866 time= 0.29110
Epoch: 0005 train_loss= 2.94694 train_acc= 0.23858 val_loss= 2.85982 val_acc= 0.26866 time= 0.29007
Epoch: 0006 train_loss= 2.86453 train_acc= 0.25480 val_loss= 2.78363 val_acc= 0.28060 time= 0.29200
Epoch: 0007 train_loss= 2.79208 train_acc= 0.24024 val_loss= 2.73000 val_acc= 0.28060 time= 0.28925
Epoch: 0008 train_loss= 2.74236 train_acc= 0.24553 val_loss= 2.70609 val_acc= 0.25373 time= 0.28900
Epoch: 0009 train_loss= 2.72014 train_acc= 0.24752 val_loss= 2.70128 val_acc= 0.20896 time= 0.29097
Epoch: 0010 train_loss= 2.72412 train_acc= 0.19259 val_loss= 2.69717 val_acc= 0.20000 time= 0.29800
Epoch: 0011 train_loss= 2.71182 train_acc= 0.17306 val_loss= 2.68416 val_acc= 0.20000 time= 0.28909
Epoch: 0012 train_loss= 2.69829 train_acc= 0.17141 val_loss= 2.66296 val_acc= 0.20000 time= 0.28500
Epoch: 0013 train_loss= 2.67409 train_acc= 0.17141 val_loss= 2.64012 val_acc= 0.20000 time= 0.28800
Epoch: 0014 train_loss= 2.63824 train_acc= 0.17240 val_loss= 2.62016 val_acc= 0.20597 time= 0.29500
Epoch: 0015 train_loss= 2.61088 train_acc= 0.17604 val_loss= 2.60260 val_acc= 0.20896 time= 0.28800
Epoch: 0016 train_loss= 2.58464 train_acc= 0.18696 val_loss= 2.58486 val_acc= 0.23881 time= 0.29000
Epoch: 0017 train_loss= 2.56048 train_acc= 0.21013 val_loss= 2.56441 val_acc= 0.26269 time= 0.28900
Epoch: 0018 train_loss= 2.53825 train_acc= 0.23494 val_loss= 2.53998 val_acc= 0.28358 time= 0.29600
Epoch: 0019 train_loss= 2.50432 train_acc= 0.26142 val_loss= 2.51126 val_acc= 0.29552 time= 0.28700
Epoch: 0020 train_loss= 2.47847 train_acc= 0.28557 val_loss= 2.47891 val_acc= 0.30746 time= 0.29200
Epoch: 0021 train_loss= 2.44895 train_acc= 0.29186 val_loss= 2.44401 val_acc= 0.31045 time= 0.29100
Epoch: 0022 train_loss= 2.41247 train_acc= 0.30410 val_loss= 2.40740 val_acc= 0.31045 time= 0.30000
Epoch: 0023 train_loss= 2.37288 train_acc= 0.30609 val_loss= 2.36997 val_acc= 0.31343 time= 0.28800
Epoch: 0024 train_loss= 2.32613 train_acc= 0.31602 val_loss= 2.33219 val_acc= 0.31343 time= 0.29000
Epoch: 0025 train_loss= 2.29257 train_acc= 0.32065 val_loss= 2.29410 val_acc= 0.31343 time= 0.28800
Epoch: 0026 train_loss= 2.23920 train_acc= 0.33620 val_loss= 2.25627 val_acc= 0.32836 time= 0.29200
Epoch: 0027 train_loss= 2.20222 train_acc= 0.34050 val_loss= 2.21869 val_acc= 0.34328 time= 0.29100
Epoch: 0028 train_loss= 2.15728 train_acc= 0.36565 val_loss= 2.18140 val_acc= 0.36716 time= 0.29500
Epoch: 0029 train_loss= 2.09978 train_acc= 0.40139 val_loss= 2.14471 val_acc= 0.39403 time= 0.28800
Epoch: 0030 train_loss= 2.07222 train_acc= 0.44540 val_loss= 2.10766 val_acc= 0.42687 time= 0.29167
Epoch: 0031 train_loss= 2.02062 train_acc= 0.47022 val_loss= 2.07042 val_acc= 0.45970 time= 0.29100
Epoch: 0032 train_loss= 1.97867 train_acc= 0.49636 val_loss= 2.03254 val_acc= 0.47761 time= 0.29000
Epoch: 0033 train_loss= 1.92469 train_acc= 0.51886 val_loss= 1.99371 val_acc= 0.49851 time= 0.28500
Epoch: 0034 train_loss= 1.89147 train_acc= 0.52581 val_loss= 1.95427 val_acc= 0.49851 time= 0.29400
Epoch: 0035 train_loss= 1.83814 train_acc= 0.54666 val_loss= 1.91506 val_acc= 0.50149 time= 0.29200
Epoch: 0036 train_loss= 1.78886 train_acc= 0.54798 val_loss= 1.87672 val_acc= 0.50149 time= 0.28800
Epoch: 0037 train_loss= 1.74761 train_acc= 0.54897 val_loss= 1.83944 val_acc= 0.51642 time= 0.28700
Epoch: 0038 train_loss= 1.69082 train_acc= 0.56982 val_loss= 1.80411 val_acc= 0.52239 time= 0.29500
Epoch: 0039 train_loss= 1.65671 train_acc= 0.58339 val_loss= 1.77016 val_acc= 0.53731 time= 0.29233
Epoch: 0040 train_loss= 1.61285 train_acc= 0.58438 val_loss= 1.73736 val_acc= 0.53731 time= 0.28700
Epoch: 0041 train_loss= 1.55822 train_acc= 0.60920 val_loss= 1.70529 val_acc= 0.53731 time= 0.28600
Epoch: 0042 train_loss= 1.52930 train_acc= 0.60688 val_loss= 1.67396 val_acc= 0.54030 time= 0.29529
Epoch: 0043 train_loss= 1.48021 train_acc= 0.62277 val_loss= 1.64409 val_acc= 0.53731 time= 0.29617
Epoch: 0044 train_loss= 1.44301 train_acc= 0.63402 val_loss= 1.61603 val_acc= 0.54030 time= 0.28600
Epoch: 0045 train_loss= 1.39414 train_acc= 0.63733 val_loss= 1.59017 val_acc= 0.54627 time= 0.28700
Epoch: 0046 train_loss= 1.35821 train_acc= 0.63931 val_loss= 1.56539 val_acc= 0.55522 time= 0.28700
Epoch: 0047 train_loss= 1.32563 train_acc= 0.64692 val_loss= 1.54027 val_acc= 0.55522 time= 0.29400
Epoch: 0048 train_loss= 1.29840 train_acc= 0.65652 val_loss= 1.51641 val_acc= 0.57015 time= 0.29200
Epoch: 0049 train_loss= 1.25935 train_acc= 0.67637 val_loss= 1.49334 val_acc= 0.57910 time= 0.28800
Epoch: 0050 train_loss= 1.21256 train_acc= 0.68365 val_loss= 1.47107 val_acc= 0.58209 time= 0.28500
Epoch: 0051 train_loss= 1.19113 train_acc= 0.68696 val_loss= 1.44921 val_acc= 0.58209 time= 0.29600
Epoch: 0052 train_loss= 1.14775 train_acc= 0.70251 val_loss= 1.42922 val_acc= 0.57910 time= 0.29200
Epoch: 0053 train_loss= 1.12759 train_acc= 0.71046 val_loss= 1.41076 val_acc= 0.59403 time= 0.28525
Epoch: 0054 train_loss= 1.09978 train_acc= 0.70913 val_loss= 1.39367 val_acc= 0.60000 time= 0.29100
Epoch: 0055 train_loss= 1.05638 train_acc= 0.71939 val_loss= 1.37697 val_acc= 0.60000 time= 0.30300
Epoch: 0056 train_loss= 1.05006 train_acc= 0.72568 val_loss= 1.35970 val_acc= 0.60896 time= 0.28897
Epoch: 0057 train_loss= 1.00896 train_acc= 0.73561 val_loss= 1.34349 val_acc= 0.60299 time= 0.28903
Epoch: 0058 train_loss= 0.96866 train_acc= 0.74090 val_loss= 1.32874 val_acc= 0.60896 time= 0.28697
Epoch: 0059 train_loss= 0.94587 train_acc= 0.75347 val_loss= 1.31558 val_acc= 0.61493 time= 0.29900
Epoch: 0060 train_loss= 0.91892 train_acc= 0.76340 val_loss= 1.30370 val_acc= 0.62388 time= 0.29003
Epoch: 0061 train_loss= 0.89143 train_acc= 0.76969 val_loss= 1.29167 val_acc= 0.62687 time= 0.28611
Epoch: 0062 train_loss= 0.87527 train_acc= 0.78160 val_loss= 1.27996 val_acc= 0.63284 time= 0.29700
Epoch: 0063 train_loss= 0.86840 train_acc= 0.77002 val_loss= 1.26987 val_acc= 0.64179 time= 0.29400
Epoch: 0064 train_loss= 0.82787 train_acc= 0.78293 val_loss= 1.25935 val_acc= 0.62687 time= 0.29100
Epoch: 0065 train_loss= 0.81164 train_acc= 0.79021 val_loss= 1.25056 val_acc= 0.62090 time= 0.28800
Epoch: 0066 train_loss= 0.77284 train_acc= 0.80311 val_loss= 1.24177 val_acc= 0.62687 time= 0.28800
Epoch: 0067 train_loss= 0.75029 train_acc= 0.80741 val_loss= 1.23247 val_acc= 0.62687 time= 0.29367
Epoch: 0068 train_loss= 0.74162 train_acc= 0.80510 val_loss= 1.22470 val_acc= 0.63582 time= 0.29200
Epoch: 0069 train_loss= 0.72224 train_acc= 0.81668 val_loss= 1.22064 val_acc= 0.62985 time= 0.28878
Epoch: 0070 train_loss= 0.69564 train_acc= 0.81502 val_loss= 1.21597 val_acc= 0.62985 time= 0.28800
Epoch: 0071 train_loss= 0.67291 train_acc= 0.83190 val_loss= 1.20834 val_acc= 0.62388 time= 0.29500
Epoch: 0072 train_loss= 0.65897 train_acc= 0.83918 val_loss= 1.19989 val_acc= 0.64478 time= 0.29626
Epoch: 0073 train_loss= 0.63893 train_acc= 0.83984 val_loss= 1.19338 val_acc= 0.65075 time= 0.28797
Epoch: 0074 train_loss= 0.62734 train_acc= 0.84183 val_loss= 1.18711 val_acc= 0.65373 time= 0.28608
Epoch: 0075 train_loss= 0.59396 train_acc= 0.85539 val_loss= 1.18034 val_acc= 0.65075 time= 0.28800
Epoch: 0076 train_loss= 0.58671 train_acc= 0.85672 val_loss= 1.17417 val_acc= 0.64776 time= 0.29300
Epoch: 0077 train_loss= 0.57647 train_acc= 0.85407 val_loss= 1.16841 val_acc= 0.64776 time= 0.28600
Epoch: 0078 train_loss= 0.56613 train_acc= 0.85606 val_loss= 1.16324 val_acc= 0.65373 time= 0.28520
Epoch: 0079 train_loss= 0.55334 train_acc= 0.86003 val_loss= 1.16155 val_acc= 0.65672 time= 0.29320
Epoch: 0080 train_loss= 0.53087 train_acc= 0.86731 val_loss= 1.15986 val_acc= 0.65672 time= 0.29800
Epoch: 0081 train_loss= 0.52058 train_acc= 0.87260 val_loss= 1.15707 val_acc= 0.65672 time= 0.28800
Epoch: 0082 train_loss= 0.50922 train_acc= 0.87525 val_loss= 1.15362 val_acc= 0.66567 time= 0.28600
Epoch: 0083 train_loss= 0.49214 train_acc= 0.88848 val_loss= 1.15266 val_acc= 0.65672 time= 0.29008
Epoch: 0084 train_loss= 0.48171 train_acc= 0.88484 val_loss= 1.15140 val_acc= 0.66269 time= 0.29597
Epoch: 0085 train_loss= 0.47733 train_acc= 0.87988 val_loss= 1.14782 val_acc= 0.65970 time= 0.28803
Epoch: 0086 train_loss= 0.45585 train_acc= 0.89080 val_loss= 1.14650 val_acc= 0.66269 time= 0.28801
Epoch: 0087 train_loss= 0.44331 train_acc= 0.88915 val_loss= 1.14632 val_acc= 0.65075 time= 0.28805
Epoch: 0088 train_loss= 0.43908 train_acc= 0.89610 val_loss= 1.14545 val_acc= 0.65075 time= 0.29397
Epoch: 0089 train_loss= 0.43883 train_acc= 0.89676 val_loss= 1.14105 val_acc= 0.65075 time= 0.28804
Epoch: 0090 train_loss= 0.42702 train_acc= 0.90007 val_loss= 1.13839 val_acc= 0.65373 time= 0.29299
Epoch: 0091 train_loss= 0.40836 train_acc= 0.90900 val_loss= 1.13796 val_acc= 0.65075 time= 0.28900
Epoch: 0092 train_loss= 0.40691 train_acc= 0.90668 val_loss= 1.13607 val_acc= 0.65672 time= 0.29265
Epoch: 0093 train_loss= 0.38821 train_acc= 0.90867 val_loss= 1.13703 val_acc= 0.66866 time= 0.29203
Epoch: 0094 train_loss= 0.36981 train_acc= 0.91628 val_loss= 1.14005 val_acc= 0.67164 time= 0.29104
Epoch: 0095 train_loss= 0.36865 train_acc= 0.91529 val_loss= 1.14195 val_acc= 0.67761 time= 0.28600
Early stopping...
Optimization Finished!
Test set results: cost= 1.15851 accuracy= 0.68217 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7316    0.6696    0.6992       342
           1     0.7130    0.7476    0.7299       103
           2     0.7885    0.5857    0.6721       140
           3     0.7105    0.3418    0.4615        79
           4     0.6554    0.7348    0.6929       132
           5     0.6596    0.7923    0.7199       313
           6     0.6667    0.7059    0.6857       102
           7     0.6087    0.2000    0.3011        70
           8     0.6957    0.3200    0.4384        50
           9     0.6550    0.7226    0.6871       155
          10     0.8311    0.6578    0.7343       187
          11     0.6120    0.6623    0.6362       231
          12     0.7750    0.6966    0.7337       178
          13     0.7596    0.8267    0.7917       600
          14     0.7583    0.8559    0.8041       590
          15     0.7500    0.6316    0.6857        76
          16     0.7692    0.2941    0.4255        34
          17     0.5000    0.1000    0.1667        10
          18     0.4075    0.4940    0.4466       419
          19     0.6598    0.4961    0.5664       129
          20     0.6957    0.5714    0.6275        28
          21     1.0000    0.7241    0.8400        29
          22     0.6154    0.3478    0.4444        46

    accuracy                         0.6822      4043
   macro avg     0.6964    0.5730    0.6083      4043
weighted avg     0.6898    0.6822    0.6768      4043

Macro average Test Precision, Recall and F1-Score...
(0.6964346771185035, 0.5729939144373176, 0.6082878327817322, None)
Micro average Test Precision, Recall and F1-Score...
(0.6821667078901805, 0.6821667078901805, 0.6821667078901805, None)
embeddings:
14157 3357 4043
[[ 0.3990478   0.3008853   0.28179026 ...  0.26670736  0.14100082
   0.13470693]
 [ 0.07094138 -0.12670127 -0.05255085 ...  0.13407767  0.03822868
  -0.01549403]
 [ 0.26763567 -0.03935147  0.02748385 ...  0.1964123   0.05036071
   0.35086262]
 ...
 [ 0.20369679  0.19590037  0.12949243 ...  0.20130225  0.20217425
   0.06482892]
 [ 0.0273199  -0.1315535  -0.02162039 ...  0.05148533 -0.0040743
   0.09264567]
 [ 0.30463868  0.0911017   0.26012766 ...  0.30389065  0.14730245
   0.05852854]]

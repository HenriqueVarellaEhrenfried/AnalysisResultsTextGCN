(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13558 train_acc= 0.00927 val_loss= 2.94333 val_acc= 0.20896 time= 0.58093
Epoch: 0002 train_loss= 2.94408 train_acc= 0.18531 val_loss= 2.73345 val_acc= 0.20299 time= 0.29229
Epoch: 0003 train_loss= 2.75087 train_acc= 0.17273 val_loss= 2.69674 val_acc= 0.20000 time= 0.28703
Epoch: 0004 train_loss= 2.71683 train_acc= 0.17141 val_loss= 2.55605 val_acc= 0.26567 time= 0.29124
Epoch: 0005 train_loss= 2.54188 train_acc= 0.25017 val_loss= 2.50109 val_acc= 0.33731 time= 0.28500
Epoch: 0006 train_loss= 2.46112 train_acc= 0.34977 val_loss= 2.39702 val_acc= 0.38209 time= 0.29200
Epoch: 0007 train_loss= 2.33796 train_acc= 0.39808 val_loss= 2.23825 val_acc= 0.40597 time= 0.29200
Epoch: 0008 train_loss= 2.16980 train_acc= 0.42555 val_loss= 2.10650 val_acc= 0.38507 time= 0.28700
Epoch: 0009 train_loss= 2.02345 train_acc= 0.42455 val_loss= 1.99120 val_acc= 0.44776 time= 0.28900
Epoch: 0010 train_loss= 1.88012 train_acc= 0.48974 val_loss= 1.87049 val_acc= 0.49552 time= 0.29797
Epoch: 0011 train_loss= 1.72543 train_acc= 0.56883 val_loss= 1.77963 val_acc= 0.51940 time= 0.29603
Epoch: 0012 train_loss= 1.58215 train_acc= 0.60556 val_loss= 1.68996 val_acc= 0.54328 time= 0.29197
Epoch: 0013 train_loss= 1.43919 train_acc= 0.63369 val_loss= 1.62000 val_acc= 0.56119 time= 0.29300
Epoch: 0014 train_loss= 1.31523 train_acc= 0.68001 val_loss= 1.55162 val_acc= 0.57015 time= 0.29303
Epoch: 0015 train_loss= 1.20936 train_acc= 0.69854 val_loss= 1.48311 val_acc= 0.57910 time= 0.29400
Epoch: 0016 train_loss= 1.09640 train_acc= 0.71211 val_loss= 1.42081 val_acc= 0.59104 time= 0.28800
Epoch: 0017 train_loss= 0.98300 train_acc= 0.74553 val_loss= 1.38472 val_acc= 0.60896 time= 0.29000
Epoch: 0018 train_loss= 0.88666 train_acc= 0.76406 val_loss= 1.35375 val_acc= 0.60896 time= 0.29232
Epoch: 0019 train_loss= 0.79786 train_acc= 0.78590 val_loss= 1.33348 val_acc= 0.60597 time= 0.29568
Epoch: 0020 train_loss= 0.71242 train_acc= 0.81701 val_loss= 1.31660 val_acc= 0.62985 time= 0.28800
Epoch: 0021 train_loss= 0.62762 train_acc= 0.83918 val_loss= 1.29779 val_acc= 0.62687 time= 0.29200
Epoch: 0022 train_loss= 0.55562 train_acc= 0.85407 val_loss= 1.28778 val_acc= 0.62687 time= 0.29000
Epoch: 0023 train_loss= 0.49174 train_acc= 0.86532 val_loss= 1.27675 val_acc= 0.63881 time= 0.29500
Epoch: 0024 train_loss= 0.42861 train_acc= 0.88815 val_loss= 1.27877 val_acc= 0.64776 time= 0.29097
Epoch: 0025 train_loss= 0.37440 train_acc= 0.90735 val_loss= 1.28881 val_acc= 0.65075 time= 0.28903
Epoch: 0026 train_loss= 0.32039 train_acc= 0.92555 val_loss= 1.30240 val_acc= 0.66866 time= 0.28700
Epoch: 0027 train_loss= 0.28402 train_acc= 0.93183 val_loss= 1.29951 val_acc= 0.66269 time= 0.29500
Epoch: 0028 train_loss= 0.24695 train_acc= 0.94044 val_loss= 1.30463 val_acc= 0.68358 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.31290 accuracy= 0.68439 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7293    0.6696    0.6982       342
           1     0.7130    0.7476    0.7299       103
           2     0.6531    0.6857    0.6690       140
           3     0.5286    0.4684    0.4966        79
           4     0.6242    0.7424    0.6782       132
           5     0.7072    0.7796    0.7416       313
           6     0.6281    0.7451    0.6816       102
           7     0.6250    0.3571    0.4545        70
           8     0.6957    0.3200    0.4384        50
           9     0.6126    0.7548    0.6763       155
          10     0.8170    0.6684    0.7353       187
          11     0.5719    0.7056    0.6318       231
          12     0.7558    0.7303    0.7429       178
          13     0.7706    0.7950    0.7826       600
          14     0.7746    0.8390    0.8055       590
          15     0.7432    0.7237    0.7333        76
          16     0.7222    0.3824    0.5000        34
          17     0.3333    0.1000    0.1538        10
          18     0.4585    0.4224    0.4398       419
          19     0.6250    0.5039    0.5579       129
          20     0.6667    0.5714    0.6154        28
          21     0.9524    0.6897    0.8000        29
          22     0.5172    0.3261    0.4000        46

    accuracy                         0.6844      4043
   macro avg     0.6620    0.5969    0.6158      4043
weighted avg     0.6835    0.6844    0.6793      4043

Macro average Test Precision, Recall and F1-Score...
(0.6619687764221893, 0.5968784330698582, 0.6157674954415546, None)
Micro average Test Precision, Recall and F1-Score...
(0.6843927776403661, 0.6843927776403661, 0.6843927776403661, None)
embeddings:
14157 3357 4043
[[ 0.38111275  0.08972345 -0.5601882  ...  0.3803241   0.11835955
   0.37175116]
 [ 0.3729304  -0.09678619 -0.14411955 ...  0.45746163 -0.0161745
   0.09374545]
 [ 0.2565957   0.03065592 -0.35512996 ...  0.57725596  0.22344014
   0.14978988]
 ...
 [ 0.12946962  0.07142808 -0.16592884 ...  0.14786421  0.09001229
   0.18629947]
 [ 0.36582494 -0.25951853 -0.00941464 ...  0.46295908 -0.1345068
  -0.04730353]
 [ 0.2668972   0.1855845  -0.16129884 ...  0.09898014  0.16053474
   0.30298287]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13543 train_acc= 0.07346 val_loss= 3.11069 val_acc= 0.20299 time= 0.58867
Epoch: 0002 train_loss= 3.11067 train_acc= 0.17240 val_loss= 3.05881 val_acc= 0.20000 time= 0.28908
Epoch: 0003 train_loss= 3.05834 train_acc= 0.17141 val_loss= 2.98199 val_acc= 0.20000 time= 0.28703
Epoch: 0004 train_loss= 2.98117 train_acc= 0.17141 val_loss= 2.88912 val_acc= 0.20000 time= 0.29897
Epoch: 0005 train_loss= 2.88870 train_acc= 0.17141 val_loss= 2.79860 val_acc= 0.20000 time= 0.28705
Epoch: 0006 train_loss= 2.79980 train_acc= 0.17141 val_loss= 2.72872 val_acc= 0.20000 time= 0.28513
Epoch: 0007 train_loss= 2.73348 train_acc= 0.17141 val_loss= 2.69501 val_acc= 0.20299 time= 0.28900
Epoch: 0008 train_loss= 2.70321 train_acc= 0.17174 val_loss= 2.69219 val_acc= 0.20299 time= 0.29700
Epoch: 0009 train_loss= 2.70263 train_acc= 0.17207 val_loss= 2.69101 val_acc= 0.20597 time= 0.28701
Epoch: 0010 train_loss= 2.70006 train_acc= 0.17306 val_loss= 2.67268 val_acc= 0.20597 time= 0.29199
Epoch: 0011 train_loss= 2.67603 train_acc= 0.17637 val_loss= 2.64233 val_acc= 0.21194 time= 0.28800
Epoch: 0012 train_loss= 2.63579 train_acc= 0.18498 val_loss= 2.61144 val_acc= 0.22985 time= 0.29299
Epoch: 0013 train_loss= 2.59427 train_acc= 0.19821 val_loss= 2.58497 val_acc= 0.24776 time= 0.29208
Epoch: 0014 train_loss= 2.55897 train_acc= 0.22138 val_loss= 2.56146 val_acc= 0.26866 time= 0.29100
Epoch: 0015 train_loss= 2.52749 train_acc= 0.24388 val_loss= 2.53726 val_acc= 0.28358 time= 0.28801
Epoch: 0016 train_loss= 2.49746 train_acc= 0.27002 val_loss= 2.50921 val_acc= 0.29552 time= 0.29096
Epoch: 0017 train_loss= 2.46468 train_acc= 0.29153 val_loss= 2.47571 val_acc= 0.30746 time= 0.29403
Epoch: 0018 train_loss= 2.42799 train_acc= 0.30874 val_loss= 2.43671 val_acc= 0.31045 time= 0.28800
Epoch: 0019 train_loss= 2.38579 train_acc= 0.32330 val_loss= 2.39339 val_acc= 0.31343 time= 0.29100
Epoch: 0020 train_loss= 2.33884 train_acc= 0.34150 val_loss= 2.34743 val_acc= 0.31940 time= 0.28997
Epoch: 0021 train_loss= 2.28933 train_acc= 0.35341 val_loss= 2.30056 val_acc= 0.33134 time= 0.29400
Epoch: 0022 train_loss= 2.23723 train_acc= 0.36267 val_loss= 2.25386 val_acc= 0.34627 time= 0.28803
Epoch: 0023 train_loss= 2.18631 train_acc= 0.37227 val_loss= 2.20790 val_acc= 0.35522 time= 0.28500
Epoch: 0024 train_loss= 2.13251 train_acc= 0.38915 val_loss= 2.16273 val_acc= 0.37015 time= 0.29397
Epoch: 0025 train_loss= 2.07828 train_acc= 0.40966 val_loss= 2.11822 val_acc= 0.39403 time= 0.29282
Epoch: 0026 train_loss= 2.02302 train_acc= 0.44441 val_loss= 2.07441 val_acc= 0.41194 time= 0.28803
Epoch: 0027 train_loss= 1.96558 train_acc= 0.48709 val_loss= 2.03148 val_acc= 0.45075 time= 0.29000
Epoch: 0028 train_loss= 1.90785 train_acc= 0.52250 val_loss= 1.98941 val_acc= 0.49552 time= 0.28903
Epoch: 0029 train_loss= 1.85102 train_acc= 0.56287 val_loss= 1.94753 val_acc= 0.50149 time= 0.29432
Epoch: 0030 train_loss= 1.79214 train_acc= 0.58570 val_loss= 1.90495 val_acc= 0.51343 time= 0.28692
Epoch: 0031 train_loss= 1.73542 train_acc= 0.59993 val_loss= 1.86129 val_acc= 0.53134 time= 0.29700
Epoch: 0032 train_loss= 1.67641 train_acc= 0.60589 val_loss= 1.81739 val_acc= 0.53433 time= 0.28701
Epoch: 0033 train_loss= 1.61842 train_acc= 0.61251 val_loss= 1.77456 val_acc= 0.53433 time= 0.29278
Epoch: 0034 train_loss= 1.56082 train_acc= 0.62541 val_loss= 1.73359 val_acc= 0.54030 time= 0.28897
Epoch: 0035 train_loss= 1.50388 train_acc= 0.63964 val_loss= 1.69453 val_acc= 0.55522 time= 0.29003
Epoch: 0036 train_loss= 1.45063 train_acc= 0.65288 val_loss= 1.65715 val_acc= 0.55821 time= 0.28900
Epoch: 0037 train_loss= 1.39410 train_acc= 0.66678 val_loss= 1.62133 val_acc= 0.55821 time= 0.29129
Epoch: 0038 train_loss= 1.34230 train_acc= 0.67836 val_loss= 1.58742 val_acc= 0.57313 time= 0.29100
Epoch: 0039 train_loss= 1.28926 train_acc= 0.68994 val_loss= 1.55535 val_acc= 0.57910 time= 0.28600
Epoch: 0040 train_loss= 1.23723 train_acc= 0.69954 val_loss= 1.52483 val_acc= 0.57910 time= 0.28600
Epoch: 0041 train_loss= 1.18832 train_acc= 0.70549 val_loss= 1.49576 val_acc= 0.58507 time= 0.29651
Epoch: 0042 train_loss= 1.13938 train_acc= 0.71972 val_loss= 1.46772 val_acc= 0.58806 time= 0.29201
Epoch: 0043 train_loss= 1.09460 train_acc= 0.72932 val_loss= 1.44098 val_acc= 0.59701 time= 0.28399
Epoch: 0044 train_loss= 1.04614 train_acc= 0.74255 val_loss= 1.41581 val_acc= 0.59701 time= 0.29300
Epoch: 0045 train_loss= 0.99981 train_acc= 0.75811 val_loss= 1.39236 val_acc= 0.60299 time= 0.29200
Epoch: 0046 train_loss= 0.95737 train_acc= 0.77201 val_loss= 1.37057 val_acc= 0.60299 time= 0.30300
Epoch: 0047 train_loss= 0.91427 train_acc= 0.78392 val_loss= 1.35038 val_acc= 0.60597 time= 0.28600
Epoch: 0048 train_loss= 0.87418 train_acc= 0.79550 val_loss= 1.33151 val_acc= 0.60597 time= 0.29000
Epoch: 0049 train_loss= 0.83418 train_acc= 0.80741 val_loss= 1.31396 val_acc= 0.61493 time= 0.29007
Epoch: 0050 train_loss= 0.79468 train_acc= 0.81734 val_loss= 1.29713 val_acc= 0.61194 time= 0.29701
Epoch: 0051 train_loss= 0.75710 train_acc= 0.82594 val_loss= 1.28121 val_acc= 0.62687 time= 0.28600
Epoch: 0052 train_loss= 0.72140 train_acc= 0.83488 val_loss= 1.26680 val_acc= 0.62090 time= 0.29297
Epoch: 0053 train_loss= 0.68783 train_acc= 0.84017 val_loss= 1.25441 val_acc= 0.62687 time= 0.29003
Epoch: 0054 train_loss= 0.65308 train_acc= 0.85208 val_loss= 1.24393 val_acc= 0.63284 time= 0.29500
Epoch: 0055 train_loss= 0.62234 train_acc= 0.85936 val_loss= 1.23500 val_acc= 0.63284 time= 0.29200
Epoch: 0056 train_loss= 0.58979 train_acc= 0.87062 val_loss= 1.22683 val_acc= 0.63582 time= 0.28900
Epoch: 0057 train_loss= 0.56025 train_acc= 0.87955 val_loss= 1.21854 val_acc= 0.63582 time= 0.28700
Epoch: 0058 train_loss= 0.53361 train_acc= 0.88484 val_loss= 1.21027 val_acc= 0.63881 time= 0.29275
Epoch: 0059 train_loss= 0.50811 train_acc= 0.89113 val_loss= 1.20241 val_acc= 0.63881 time= 0.28903
Epoch: 0060 train_loss= 0.48037 train_acc= 0.89874 val_loss= 1.19503 val_acc= 0.63881 time= 0.29205
Epoch: 0061 train_loss= 0.45567 train_acc= 0.90304 val_loss= 1.18881 val_acc= 0.64478 time= 0.28804
Epoch: 0062 train_loss= 0.43270 train_acc= 0.91032 val_loss= 1.18420 val_acc= 0.65075 time= 0.29341
Epoch: 0063 train_loss= 0.41140 train_acc= 0.91330 val_loss= 1.18130 val_acc= 0.65672 time= 0.29303
Epoch: 0064 train_loss= 0.39021 train_acc= 0.92025 val_loss= 1.17919 val_acc= 0.65672 time= 0.28500
Epoch: 0065 train_loss= 0.37126 train_acc= 0.92522 val_loss= 1.17709 val_acc= 0.65672 time= 0.29000
Epoch: 0066 train_loss= 0.35248 train_acc= 0.93018 val_loss= 1.17569 val_acc= 0.66269 time= 0.28997
Epoch: 0067 train_loss= 0.33438 train_acc= 0.93580 val_loss= 1.17401 val_acc= 0.66866 time= 0.29503
Epoch: 0068 train_loss= 0.31750 train_acc= 0.94011 val_loss= 1.17281 val_acc= 0.67164 time= 0.28700
Epoch: 0069 train_loss= 0.30183 train_acc= 0.94507 val_loss= 1.17207 val_acc= 0.67164 time= 0.28900
Epoch: 0070 train_loss= 0.28508 train_acc= 0.95003 val_loss= 1.17190 val_acc= 0.66567 time= 0.28933
Epoch: 0071 train_loss= 0.27174 train_acc= 0.95301 val_loss= 1.17284 val_acc= 0.65970 time= 0.29351
Epoch: 0072 train_loss= 0.25740 train_acc= 0.95665 val_loss= 1.17392 val_acc= 0.66567 time= 0.28935
Epoch: 0073 train_loss= 0.24476 train_acc= 0.96029 val_loss= 1.17553 val_acc= 0.66567 time= 0.28999
Early stopping...
Optimization Finished!
Test set results: cost= 1.16457 accuracy= 0.68588 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7298    0.6871    0.7078       342
           1     0.6726    0.7379    0.7037       103
           2     0.7395    0.6286    0.6795       140
           3     0.6731    0.4430    0.5344        79
           4     0.6211    0.7576    0.6826       132
           5     0.6505    0.8147    0.7234       313
           6     0.6574    0.6961    0.6762       102
           7     0.6286    0.3143    0.4190        70
           8     0.6250    0.4000    0.4878        50
           9     0.6237    0.7484    0.6804       155
          10     0.8451    0.6417    0.7295       187
          11     0.6201    0.6147    0.6174       231
          12     0.7885    0.6910    0.7365       178
          13     0.7701    0.8150    0.7919       600
          14     0.7834    0.8458    0.8134       590
          15     0.7639    0.7237    0.7432        76
          16     0.7857    0.3235    0.4583        34
          17     0.5000    0.1000    0.1667        10
          18     0.4239    0.4654    0.4437       419
          19     0.6667    0.5116    0.5789       129
          20     0.6296    0.6071    0.6182        28
          21     0.9565    0.7586    0.8462        29
          22     0.5556    0.3261    0.4110        46

    accuracy                         0.6859      4043
   macro avg     0.6831    0.5936    0.6196      4043
weighted avg     0.6902    0.6859    0.6819      4043

Macro average Test Precision, Recall and F1-Score...
(0.683050568184477, 0.5935617864913743, 0.6195503187932057, None)
Micro average Test Precision, Recall and F1-Score...
(0.6858768241404898, 0.6858768241404898, 0.6858768241404898, None)
embeddings:
14157 3357 4043
[[ 0.29474732  0.43519053  0.47798616 ...  0.40299073  0.4560952
   0.33446622]
 [-0.02183729  0.1410343   0.12454876 ...  0.02770824 -0.02358781
   0.07855832]
 [ 0.14811622  0.16094512  0.35103586 ...  0.15475316  0.2292817
   0.223578  ]
 ...
 [ 0.1936031   0.14366311  0.21151745 ...  0.12691267  0.21392773
   0.13007204]
 [-0.05287549  0.5062024   0.4546644  ...  0.2174289   0.1278219
   0.08372229]
 [ 0.1814715   0.29534867  0.2489176  ...  0.16500938  0.30670783
   0.16136304]]

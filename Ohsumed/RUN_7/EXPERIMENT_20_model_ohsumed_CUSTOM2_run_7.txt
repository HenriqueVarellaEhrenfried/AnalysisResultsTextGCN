(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13548 train_acc= 0.09067 val_loss= 2.92188 val_acc= 0.24478 time= 0.58797
Epoch: 0002 train_loss= 2.92171 train_acc= 0.21476 val_loss= 2.73744 val_acc= 0.20597 time= 0.28803
Epoch: 0003 train_loss= 2.75691 train_acc= 0.18266 val_loss= 2.65846 val_acc= 0.20597 time= 0.29000
Epoch: 0004 train_loss= 2.67666 train_acc= 0.17273 val_loss= 2.54600 val_acc= 0.28657 time= 0.28597
Epoch: 0005 train_loss= 2.51542 train_acc= 0.26671 val_loss= 2.47032 val_acc= 0.32836 time= 0.29603
Epoch: 0006 train_loss= 2.41767 train_acc= 0.32263 val_loss= 2.33871 val_acc= 0.37313 time= 0.28800
Epoch: 0007 train_loss= 2.27102 train_acc= 0.39576 val_loss= 2.17158 val_acc= 0.41791 time= 0.29100
Epoch: 0008 train_loss= 2.09499 train_acc= 0.43746 val_loss= 2.03062 val_acc= 0.42090 time= 0.28597
Epoch: 0009 train_loss= 1.93220 train_acc= 0.47154 val_loss= 1.92336 val_acc= 0.45970 time= 0.29403
Epoch: 0010 train_loss= 1.78276 train_acc= 0.51555 val_loss= 1.81300 val_acc= 0.51642 time= 0.28800
Epoch: 0011 train_loss= 1.61946 train_acc= 0.59232 val_loss= 1.72679 val_acc= 0.53134 time= 0.28701
Epoch: 0012 train_loss= 1.47228 train_acc= 0.62409 val_loss= 1.64141 val_acc= 0.54627 time= 0.28899
Epoch: 0013 train_loss= 1.33439 train_acc= 0.66380 val_loss= 1.56283 val_acc= 0.56418 time= 0.29300
Epoch: 0014 train_loss= 1.21590 train_acc= 0.69358 val_loss= 1.49009 val_acc= 0.59104 time= 0.28697
Epoch: 0015 train_loss= 1.09844 train_acc= 0.71377 val_loss= 1.44075 val_acc= 0.58806 time= 0.28803
Epoch: 0016 train_loss= 1.00378 train_acc= 0.73230 val_loss= 1.40206 val_acc= 0.59701 time= 0.28600
Epoch: 0017 train_loss= 0.89658 train_acc= 0.75215 val_loss= 1.37990 val_acc= 0.60000 time= 0.29697
Epoch: 0018 train_loss= 0.79813 train_acc= 0.79120 val_loss= 1.36255 val_acc= 0.60299 time= 0.28703
Epoch: 0019 train_loss= 0.71181 train_acc= 0.81436 val_loss= 1.33100 val_acc= 0.62090 time= 0.28897
Epoch: 0020 train_loss= 0.62436 train_acc= 0.84249 val_loss= 1.29830 val_acc= 0.63284 time= 0.29200
Epoch: 0021 train_loss= 0.55457 train_acc= 0.85308 val_loss= 1.28309 val_acc= 0.63582 time= 0.29300
Epoch: 0022 train_loss= 0.48152 train_acc= 0.86863 val_loss= 1.28894 val_acc= 0.63284 time= 0.29403
Epoch: 0023 train_loss= 0.41655 train_acc= 0.88981 val_loss= 1.30131 val_acc= 0.65075 time= 0.28600
Epoch: 0024 train_loss= 0.36056 train_acc= 0.91363 val_loss= 1.30929 val_acc= 0.64179 time= 0.28602
Early stopping...
Optimization Finished!
Test set results: cost= 1.30338 accuracy= 0.67054 time= 0.12697
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6930    0.6930    0.6930       342
           1     0.6765    0.6699    0.6732       103
           2     0.6792    0.5143    0.5854       140
           3     0.3908    0.4304    0.4096        79
           4     0.6250    0.7576    0.6849       132
           5     0.7088    0.7700    0.7381       313
           6     0.6800    0.6667    0.6733       102
           7     0.5143    0.2571    0.3429        70
           8     0.6207    0.3600    0.4557        50
           9     0.6471    0.7097    0.6769       155
          10     0.7725    0.6898    0.7288       187
          11     0.6411    0.5801    0.6091       231
          12     0.7875    0.7079    0.7456       178
          13     0.7864    0.7733    0.7798       600
          14     0.7557    0.8492    0.7997       590
          15     0.7612    0.6711    0.7133        76
          16     0.5455    0.3529    0.4286        34
          17     0.0000    0.0000    0.0000        10
          18     0.4255    0.4773    0.4499       419
          19     0.5423    0.5969    0.5683       129
          20     0.6667    0.6429    0.6545        28
          21     1.0000    0.6207    0.7660        29
          22     0.4000    0.3043    0.3457        46

    accuracy                         0.6705      4043
   macro avg     0.6226    0.5693    0.5879      4043
weighted avg     0.6715    0.6705    0.6676      4043

Macro average Test Precision, Recall and F1-Score...
(0.6225939794447782, 0.5693466697483405, 0.5879214497492848, None)
Micro average Test Precision, Recall and F1-Score...
(0.6705416769725452, 0.6705416769725452, 0.6705416769725452, None)
embeddings:
14157 3357 4043
[[ 0.185622    0.31800586  0.44495308 ...  0.35372406 -0.29936674
   0.4433145 ]
 [-0.02008559 -0.06201999  0.01249941 ...  0.23675632 -0.03424217
   0.43875325]
 [ 0.02900614  0.19647056  0.176399   ...  0.47488078 -0.14598177
   0.62968594]
 ...
 [ 0.06411306  0.1315082   0.16676022 ...  0.07196671 -0.01327975
   0.25927028]
 [ 0.15436155 -0.02699789  0.00757998 ...  0.61450726 -0.08980041
   0.39875126]
 [ 0.21665536  0.30222127  0.09376489 ... -0.01983973 -0.07035345
   0.3073379 ]]

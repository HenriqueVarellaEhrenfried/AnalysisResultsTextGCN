(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13553 train_acc= 0.02581 val_loss= 2.92188 val_acc= 0.20597 time= 0.58677
Epoch: 0002 train_loss= 2.92257 train_acc= 0.17637 val_loss= 2.72875 val_acc= 0.20597 time= 0.29404
Epoch: 0003 train_loss= 2.74794 train_acc= 0.17439 val_loss= 2.68272 val_acc= 0.20597 time= 0.29220
Epoch: 0004 train_loss= 2.70109 train_acc= 0.17439 val_loss= 2.55947 val_acc= 0.27463 time= 0.29300
Epoch: 0005 train_loss= 2.53366 train_acc= 0.25778 val_loss= 2.50573 val_acc= 0.33433 time= 0.28803
Epoch: 0006 train_loss= 2.45765 train_acc= 0.32925 val_loss= 2.38871 val_acc= 0.37313 time= 0.29001
Epoch: 0007 train_loss= 2.32207 train_acc= 0.40371 val_loss= 2.22573 val_acc= 0.39104 time= 0.29501
Epoch: 0008 train_loss= 2.15072 train_acc= 0.42852 val_loss= 2.08219 val_acc= 0.38209 time= 0.29207
Epoch: 0009 train_loss= 1.99853 train_acc= 0.43084 val_loss= 1.96763 val_acc= 0.42090 time= 0.29000
Epoch: 0010 train_loss= 1.85609 train_acc= 0.47121 val_loss= 1.85644 val_acc= 0.49254 time= 0.28900
Epoch: 0011 train_loss= 1.69765 train_acc= 0.56254 val_loss= 1.76742 val_acc= 0.53731 time= 0.29993
Epoch: 0012 train_loss= 1.55409 train_acc= 0.61251 val_loss= 1.67350 val_acc= 0.54328 time= 0.29098
Epoch: 0013 train_loss= 1.41063 train_acc= 0.63964 val_loss= 1.59441 val_acc= 0.55821 time= 0.28800
Epoch: 0014 train_loss= 1.28828 train_acc= 0.68498 val_loss= 1.51871 val_acc= 0.57313 time= 0.29300
Epoch: 0015 train_loss= 1.17067 train_acc= 0.71377 val_loss= 1.44702 val_acc= 0.58806 time= 0.29300
Epoch: 0016 train_loss= 1.05506 train_acc= 0.72700 val_loss= 1.39853 val_acc= 0.59403 time= 0.29000
Epoch: 0017 train_loss= 0.95253 train_acc= 0.74189 val_loss= 1.36521 val_acc= 0.59403 time= 0.29000
Epoch: 0018 train_loss= 0.85280 train_acc= 0.76870 val_loss= 1.32457 val_acc= 0.61493 time= 0.29401
Epoch: 0019 train_loss= 0.75173 train_acc= 0.79914 val_loss= 1.29920 val_acc= 0.61791 time= 0.29007
Epoch: 0020 train_loss= 0.67388 train_acc= 0.83024 val_loss= 1.26604 val_acc= 0.62687 time= 0.29042
Epoch: 0021 train_loss= 0.59026 train_acc= 0.85308 val_loss= 1.24314 val_acc= 0.63284 time= 0.28600
Epoch: 0022 train_loss= 0.51376 train_acc= 0.85572 val_loss= 1.23606 val_acc= 0.62687 time= 0.29500
Epoch: 0023 train_loss= 0.45076 train_acc= 0.87260 val_loss= 1.23622 val_acc= 0.64478 time= 0.29000
Epoch: 0024 train_loss= 0.39385 train_acc= 0.89610 val_loss= 1.24183 val_acc= 0.66866 time= 0.28617
Epoch: 0025 train_loss= 0.33881 train_acc= 0.92025 val_loss= 1.24283 val_acc= 0.65672 time= 0.28909
Epoch: 0026 train_loss= 0.29138 train_acc= 0.93249 val_loss= 1.25234 val_acc= 0.65672 time= 0.29300
Epoch: 0027 train_loss= 0.26052 train_acc= 0.93613 val_loss= 1.25955 val_acc= 0.67463 time= 0.28799
Epoch: 0028 train_loss= 0.22915 train_acc= 0.94044 val_loss= 1.26921 val_acc= 0.67463 time= 0.28997
Early stopping...
Optimization Finished!
Test set results: cost= 1.31491 accuracy= 0.67227 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7039    0.6813    0.6924       342
           1     0.6583    0.7670    0.7085       103
           2     0.6716    0.6429    0.6569       140
           3     0.7143    0.3797    0.4959        79
           4     0.6835    0.7197    0.7011       132
           5     0.6932    0.7508    0.7209       313
           6     0.6410    0.7353    0.6849       102
           7     0.6000    0.3000    0.4000        70
           8     0.5200    0.2600    0.3467        50
           9     0.6032    0.7355    0.6628       155
          10     0.8276    0.6417    0.7229       187
          11     0.5688    0.6797    0.6193       231
          12     0.7636    0.7079    0.7347       178
          13     0.7551    0.7967    0.7753       600
          14     0.8055    0.8000    0.8027       590
          15     0.7761    0.6842    0.7273        76
          16     0.7143    0.2941    0.4167        34
          17     0.5000    0.1000    0.1667        10
          18     0.3964    0.4797    0.4341       419
          19     0.6117    0.4884    0.5431       129
          20     0.7895    0.5357    0.6383        28
          21     0.8800    0.7586    0.8148        29
          22     0.5333    0.3478    0.4211        46

    accuracy                         0.6723      4043
   macro avg     0.6700    0.5777    0.6038      4043
weighted avg     0.6799    0.6723    0.6700      4043

Macro average Test Precision, Recall and F1-Score...
(0.6700446808106433, 0.5776793989989164, 0.603786075206386, None)
Micro average Test Precision, Recall and F1-Score...
(0.6722730645560228, 0.6722730645560228, 0.6722730645560228, None)
embeddings:
14157 3357 4043
[[-0.6333257   0.29411575  0.57232076 ... -0.5458436   0.14991403
   0.4700664 ]
 [-0.17072573  0.0902406   0.07757552 ... -0.13993755 -0.01403952
   0.05789867]
 [-0.34265032  0.28783524  0.02734654 ... -0.33439347  0.04700868
   0.21914132]
 ...
 [-0.20502111  0.12821285  0.2317589  ... -0.12957013  0.177666
   0.13086261]
 [-0.20932361  0.3187763  -0.03714619 ... -0.1847999   0.08028129
   0.21652138]
 [-0.19666643  0.1336856   0.49867144 ... -0.16665798  0.17875534
   0.17763764]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13557 train_acc= 0.01985 val_loss= 3.11734 val_acc= 0.20299 time= 0.58096
Epoch: 0002 train_loss= 3.11762 train_acc= 0.17306 val_loss= 3.07624 val_acc= 0.20000 time= 0.29000
Epoch: 0003 train_loss= 3.07699 train_acc= 0.17174 val_loss= 3.01157 val_acc= 0.20000 time= 0.29119
Epoch: 0004 train_loss= 3.01290 train_acc= 0.17174 val_loss= 2.92750 val_acc= 0.20000 time= 0.29097
Epoch: 0005 train_loss= 2.92975 train_acc= 0.17174 val_loss= 2.83596 val_acc= 0.20000 time= 0.29684
Epoch: 0006 train_loss= 2.84066 train_acc= 0.17174 val_loss= 2.75539 val_acc= 0.20000 time= 0.28900
Epoch: 0007 train_loss= 2.76266 train_acc= 0.17174 val_loss= 2.70205 val_acc= 0.20000 time= 0.29000
Epoch: 0008 train_loss= 2.71143 train_acc= 0.17207 val_loss= 2.68285 val_acc= 0.20299 time= 0.29308
Epoch: 0009 train_loss= 2.69367 train_acc= 0.17340 val_loss= 2.68142 val_acc= 0.20597 time= 0.29300
Epoch: 0010 train_loss= 2.69255 train_acc= 0.17505 val_loss= 2.67291 val_acc= 0.20896 time= 0.28999
Epoch: 0011 train_loss= 2.67934 train_acc= 0.17704 val_loss= 2.64877 val_acc= 0.21194 time= 0.29100
Epoch: 0012 train_loss= 2.64643 train_acc= 0.18597 val_loss= 2.61692 val_acc= 0.22985 time= 0.29600
Epoch: 0013 train_loss= 2.60371 train_acc= 0.19854 val_loss= 2.58657 val_acc= 0.25075 time= 0.29207
Epoch: 0014 train_loss= 2.56367 train_acc= 0.21873 val_loss= 2.56002 val_acc= 0.25970 time= 0.28800
Epoch: 0015 train_loss= 2.52814 train_acc= 0.24123 val_loss= 2.53441 val_acc= 0.27761 time= 0.29200
Epoch: 0016 train_loss= 2.49554 train_acc= 0.26340 val_loss= 2.50590 val_acc= 0.29851 time= 0.29500
Epoch: 0017 train_loss= 2.46039 train_acc= 0.29219 val_loss= 2.47227 val_acc= 0.31343 time= 0.29000
Epoch: 0018 train_loss= 2.42438 train_acc= 0.32263 val_loss= 2.43332 val_acc= 0.32537 time= 0.28700
Epoch: 0019 train_loss= 2.38203 train_acc= 0.35341 val_loss= 2.39014 val_acc= 0.33731 time= 0.29265
Epoch: 0020 train_loss= 2.33626 train_acc= 0.36896 val_loss= 2.34438 val_acc= 0.34030 time= 0.29100
Epoch: 0021 train_loss= 2.28615 train_acc= 0.37889 val_loss= 2.29756 val_acc= 0.34627 time= 0.29000
Epoch: 0022 train_loss= 2.23359 train_acc= 0.38716 val_loss= 2.25069 val_acc= 0.35224 time= 0.29100
Epoch: 0023 train_loss= 2.17933 train_acc= 0.39610 val_loss= 2.20398 val_acc= 0.35821 time= 0.29297
Epoch: 0024 train_loss= 2.12470 train_acc= 0.40602 val_loss= 2.15711 val_acc= 0.37313 time= 0.29103
Epoch: 0025 train_loss= 2.06643 train_acc= 0.43051 val_loss= 2.11000 val_acc= 0.40000 time= 0.28900
Epoch: 0026 train_loss= 2.00603 train_acc= 0.45996 val_loss= 2.06319 val_acc= 0.43881 time= 0.29001
Epoch: 0027 train_loss= 1.94783 train_acc= 0.49901 val_loss= 2.01740 val_acc= 0.48060 time= 0.29399
Epoch: 0028 train_loss= 1.88765 train_acc= 0.52945 val_loss= 1.97262 val_acc= 0.49552 time= 0.28801
Epoch: 0029 train_loss= 1.82744 train_acc= 0.56420 val_loss= 1.92786 val_acc= 0.49851 time= 0.28797
Epoch: 0030 train_loss= 1.76743 train_acc= 0.58736 val_loss= 1.88221 val_acc= 0.51343 time= 0.29503
Epoch: 0031 train_loss= 1.70604 train_acc= 0.59762 val_loss= 1.83554 val_acc= 0.53134 time= 0.29100
Epoch: 0032 train_loss= 1.64592 train_acc= 0.60887 val_loss= 1.78902 val_acc= 0.53433 time= 0.28900
Epoch: 0033 train_loss= 1.58412 train_acc= 0.62310 val_loss= 1.74422 val_acc= 0.54030 time= 0.29300
Epoch: 0034 train_loss= 1.52281 train_acc= 0.63567 val_loss= 1.70231 val_acc= 0.54627 time= 0.28897
Epoch: 0035 train_loss= 1.46418 train_acc= 0.64957 val_loss= 1.66340 val_acc= 0.54627 time= 0.29303
Epoch: 0036 train_loss= 1.40723 train_acc= 0.66082 val_loss= 1.62643 val_acc= 0.54925 time= 0.28700
Epoch: 0037 train_loss= 1.35356 train_acc= 0.66876 val_loss= 1.59117 val_acc= 0.57015 time= 0.29308
Epoch: 0038 train_loss= 1.29959 train_acc= 0.68398 val_loss= 1.55763 val_acc= 0.57313 time= 0.29612
Epoch: 0039 train_loss= 1.24362 train_acc= 0.69557 val_loss= 1.52622 val_acc= 0.57612 time= 0.28899
Epoch: 0040 train_loss= 1.19136 train_acc= 0.70781 val_loss= 1.49605 val_acc= 0.58209 time= 0.29399
Epoch: 0041 train_loss= 1.14250 train_acc= 0.71741 val_loss= 1.46691 val_acc= 0.59403 time= 0.28900
Epoch: 0042 train_loss= 1.09371 train_acc= 0.73395 val_loss= 1.43935 val_acc= 0.59403 time= 0.29100
Epoch: 0043 train_loss= 1.04119 train_acc= 0.74983 val_loss= 1.41374 val_acc= 0.59403 time= 0.28701
Epoch: 0044 train_loss= 0.99682 train_acc= 0.76307 val_loss= 1.38942 val_acc= 0.60299 time= 0.29201
Epoch: 0045 train_loss= 0.95393 train_acc= 0.77300 val_loss= 1.36683 val_acc= 0.60000 time= 0.28877
Epoch: 0046 train_loss= 0.90917 train_acc= 0.78590 val_loss= 1.34555 val_acc= 0.60299 time= 0.29397
Epoch: 0047 train_loss= 0.86784 train_acc= 0.79616 val_loss= 1.32637 val_acc= 0.61493 time= 0.28800
Epoch: 0048 train_loss= 0.82446 train_acc= 0.80940 val_loss= 1.30904 val_acc= 0.62090 time= 0.29311
Epoch: 0049 train_loss= 0.78837 train_acc= 0.81833 val_loss= 1.29304 val_acc= 0.62985 time= 0.29100
Epoch: 0050 train_loss= 0.75060 train_acc= 0.82429 val_loss= 1.27787 val_acc= 0.64179 time= 0.28900
Epoch: 0051 train_loss= 0.71231 train_acc= 0.83885 val_loss= 1.26388 val_acc= 0.63284 time= 0.28900
Epoch: 0052 train_loss= 0.67854 train_acc= 0.84712 val_loss= 1.25200 val_acc= 0.63881 time= 0.29000
Epoch: 0053 train_loss= 0.64673 train_acc= 0.85606 val_loss= 1.24156 val_acc= 0.63881 time= 0.29000
Epoch: 0054 train_loss= 0.61352 train_acc= 0.86499 val_loss= 1.23230 val_acc= 0.63582 time= 0.28700
Epoch: 0055 train_loss= 0.57891 train_acc= 0.87161 val_loss= 1.22383 val_acc= 0.64776 time= 0.29300
Epoch: 0056 train_loss= 0.55371 train_acc= 0.87723 val_loss= 1.21609 val_acc= 0.64776 time= 0.28722
Epoch: 0057 train_loss= 0.52579 train_acc= 0.88319 val_loss= 1.20900 val_acc= 0.64776 time= 0.29600
Epoch: 0058 train_loss= 0.50066 train_acc= 0.89246 val_loss= 1.20196 val_acc= 0.65075 time= 0.29719
Epoch: 0059 train_loss= 0.47184 train_acc= 0.90371 val_loss= 1.19444 val_acc= 0.65672 time= 0.29297
Epoch: 0060 train_loss= 0.44806 train_acc= 0.90602 val_loss= 1.18754 val_acc= 0.66269 time= 0.29203
Epoch: 0061 train_loss= 0.42667 train_acc= 0.91330 val_loss= 1.18249 val_acc= 0.66269 time= 0.29100
Epoch: 0062 train_loss= 0.40573 train_acc= 0.92025 val_loss= 1.17930 val_acc= 0.66567 time= 0.29300
Epoch: 0063 train_loss= 0.38409 train_acc= 0.92124 val_loss= 1.17759 val_acc= 0.66567 time= 0.28797
Epoch: 0064 train_loss= 0.36381 train_acc= 0.92687 val_loss= 1.17600 val_acc= 0.66866 time= 0.29403
Epoch: 0065 train_loss= 0.34721 train_acc= 0.93547 val_loss= 1.17447 val_acc= 0.66866 time= 0.29197
Epoch: 0066 train_loss= 0.32828 train_acc= 0.94044 val_loss= 1.17362 val_acc= 0.67164 time= 0.28803
Epoch: 0067 train_loss= 0.31136 train_acc= 0.94408 val_loss= 1.17367 val_acc= 0.68060 time= 0.29301
Epoch: 0068 train_loss= 0.29468 train_acc= 0.95069 val_loss= 1.17439 val_acc= 0.67463 time= 0.29409
Epoch: 0069 train_loss= 0.28152 train_acc= 0.95400 val_loss= 1.17277 val_acc= 0.68358 time= 0.29100
Epoch: 0070 train_loss= 0.26771 train_acc= 0.95599 val_loss= 1.17201 val_acc= 0.67761 time= 0.28800
Epoch: 0071 train_loss= 0.25566 train_acc= 0.95864 val_loss= 1.17231 val_acc= 0.67761 time= 0.29300
Epoch: 0072 train_loss= 0.24037 train_acc= 0.95963 val_loss= 1.17437 val_acc= 0.68060 time= 0.28997
Epoch: 0073 train_loss= 0.22883 train_acc= 0.96426 val_loss= 1.17741 val_acc= 0.67761 time= 0.29103
Early stopping...
Optimization Finished!
Test set results: cost= 1.16291 accuracy= 0.69206 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7262    0.7135    0.7198       342
           1     0.7027    0.7573    0.7290       103
           2     0.7568    0.6000    0.6693       140
           3     0.6471    0.4177    0.5077        79
           4     0.6689    0.7652    0.7138       132
           5     0.6851    0.7923    0.7348       313
           6     0.6759    0.7157    0.6952       102
           7     0.6053    0.3286    0.4259        70
           8     0.6111    0.4400    0.5116        50
           9     0.6284    0.7419    0.6805       155
          10     0.8489    0.6310    0.7239       187
          11     0.6266    0.6320    0.6293       231
          12     0.7644    0.7472    0.7557       178
          13     0.7755    0.8117    0.7932       600
          14     0.7783    0.8390    0.8075       590
          15     0.7857    0.7237    0.7534        76
          16     0.7500    0.3529    0.4800        34
          17     0.6667    0.2000    0.3077        10
          18     0.4366    0.5012    0.4667       419
          19     0.6471    0.5116    0.5714       129
          20     0.6296    0.6071    0.6182        28
          21     0.9545    0.7241    0.8235        29
          22     0.6000    0.3261    0.4225        46

    accuracy                         0.6921      4043
   macro avg     0.6944    0.6035    0.6322      4043
weighted avg     0.6963    0.6921    0.6889      4043

Macro average Test Precision, Recall and F1-Score...
(0.6944027961611859, 0.6034711345616524, 0.6322023000466879, None)
Micro average Test Precision, Recall and F1-Score...
(0.6920603512243384, 0.6920603512243384, 0.6920603512243384, None)
embeddings:
14157 3357 4043
[[ 0.379668    0.3575648   0.29340646 ...  0.46976128  0.32910192
   0.59768933]
 [-0.0284944   0.2638557   0.32912523 ...  0.24215505  0.1420092
   0.3795808 ]
 [ 0.30822274  0.33310428  0.2987868  ...  0.38839474  0.2758633
   0.52873725]
 ...
 [ 0.2540263   0.28678855  0.16800234 ...  0.12954283  0.20691961
   0.14955539]
 [ 0.12708323  0.05243034  0.24203444 ...  0.28335056 -0.02245014
   0.45595855]
 [ 0.14361508  0.10334782  0.02564273 ...  0.201629    0.24984013
   0.3180642 ]]

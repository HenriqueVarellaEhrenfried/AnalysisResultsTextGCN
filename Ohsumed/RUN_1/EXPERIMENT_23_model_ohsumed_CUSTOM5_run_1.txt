(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13552 train_acc= 0.02416 val_loss= 3.11203 val_acc= 0.20299 time= 0.58342
Epoch: 0002 train_loss= 3.11237 train_acc= 0.17273 val_loss= 3.06388 val_acc= 0.20299 time= 0.29411
Epoch: 0003 train_loss= 3.06477 train_acc= 0.17340 val_loss= 2.99163 val_acc= 0.20299 time= 0.29202
Epoch: 0004 train_loss= 2.99353 train_acc= 0.17373 val_loss= 2.90200 val_acc= 0.20299 time= 0.28799
Epoch: 0005 train_loss= 2.90588 train_acc= 0.17306 val_loss= 2.81171 val_acc= 0.20299 time= 0.29097
Epoch: 0006 train_loss= 2.81760 train_acc= 0.17240 val_loss= 2.73998 val_acc= 0.20299 time= 0.29123
Epoch: 0007 train_loss= 2.74807 train_acc= 0.17439 val_loss= 2.70050 val_acc= 0.20597 time= 0.29111
Epoch: 0008 train_loss= 2.71086 train_acc= 0.17472 val_loss= 2.69255 val_acc= 0.20597 time= 0.28603
Epoch: 0009 train_loss= 2.70385 train_acc= 0.17306 val_loss= 2.69177 val_acc= 0.20000 time= 0.29300
Epoch: 0010 train_loss= 2.70268 train_acc= 0.17207 val_loss= 2.67869 val_acc= 0.20000 time= 0.29500
Epoch: 0011 train_loss= 2.68472 train_acc= 0.17174 val_loss= 2.65216 val_acc= 0.20299 time= 0.29000
Epoch: 0012 train_loss= 2.64902 train_acc= 0.17306 val_loss= 2.62280 val_acc= 0.20896 time= 0.28700
Epoch: 0013 train_loss= 2.60920 train_acc= 0.17571 val_loss= 2.59720 val_acc= 0.21791 time= 0.29300
Epoch: 0014 train_loss= 2.57228 train_acc= 0.18696 val_loss= 2.57465 val_acc= 0.23881 time= 0.29144
Epoch: 0015 train_loss= 2.54096 train_acc= 0.20582 val_loss= 2.55198 val_acc= 0.25970 time= 0.28700
Epoch: 0016 train_loss= 2.51023 train_acc= 0.23825 val_loss= 2.52608 val_acc= 0.28060 time= 0.29200
Epoch: 0017 train_loss= 2.47824 train_acc= 0.27300 val_loss= 2.49518 val_acc= 0.29851 time= 0.29405
Epoch: 0018 train_loss= 2.44226 train_acc= 0.31238 val_loss= 2.45885 val_acc= 0.31642 time= 0.29200
Epoch: 0019 train_loss= 2.40100 train_acc= 0.34745 val_loss= 2.41785 val_acc= 0.32537 time= 0.28809
Epoch: 0020 train_loss= 2.35635 train_acc= 0.37095 val_loss= 2.37359 val_acc= 0.33433 time= 0.29199
Epoch: 0021 train_loss= 2.30838 train_acc= 0.37690 val_loss= 2.32768 val_acc= 0.33731 time= 0.29200
Epoch: 0022 train_loss= 2.25573 train_acc= 0.38120 val_loss= 2.28148 val_acc= 0.34030 time= 0.28733
Epoch: 0023 train_loss= 2.20415 train_acc= 0.38650 val_loss= 2.23560 val_acc= 0.35224 time= 0.29100
Epoch: 0024 train_loss= 2.15067 train_acc= 0.39345 val_loss= 2.18996 val_acc= 0.37015 time= 0.29300
Epoch: 0025 train_loss= 2.09523 train_acc= 0.41595 val_loss= 2.14425 val_acc= 0.39403 time= 0.29400
Epoch: 0026 train_loss= 2.03787 train_acc= 0.44805 val_loss= 2.09867 val_acc= 0.40597 time= 0.28997
Epoch: 0027 train_loss= 1.98064 train_acc= 0.48147 val_loss= 2.05389 val_acc= 0.44776 time= 0.29500
Epoch: 0028 train_loss= 1.92040 train_acc= 0.52019 val_loss= 2.01033 val_acc= 0.46269 time= 0.29600
Epoch: 0029 train_loss= 1.86057 train_acc= 0.55361 val_loss= 1.96746 val_acc= 0.48955 time= 0.29013
Epoch: 0030 train_loss= 1.80285 train_acc= 0.57412 val_loss= 1.92383 val_acc= 0.51045 time= 0.28900
Epoch: 0031 train_loss= 1.74698 train_acc= 0.59133 val_loss= 1.87843 val_acc= 0.53134 time= 0.29400
Epoch: 0032 train_loss= 1.68387 train_acc= 0.60324 val_loss= 1.83190 val_acc= 0.53134 time= 0.29600
Epoch: 0033 train_loss= 1.62232 train_acc= 0.62376 val_loss= 1.78604 val_acc= 0.54030 time= 0.29100
Epoch: 0034 train_loss= 1.56443 train_acc= 0.63269 val_loss= 1.74258 val_acc= 0.53731 time= 0.28700
Epoch: 0035 train_loss= 1.50608 train_acc= 0.64328 val_loss= 1.70213 val_acc= 0.54030 time= 0.29397
Epoch: 0036 train_loss= 1.44808 train_acc= 0.65420 val_loss= 1.66433 val_acc= 0.55224 time= 0.29504
Epoch: 0037 train_loss= 1.39359 train_acc= 0.66512 val_loss= 1.62870 val_acc= 0.55821 time= 0.28799
Epoch: 0038 train_loss= 1.33859 train_acc= 0.67704 val_loss= 1.59476 val_acc= 0.56418 time= 0.29413
Epoch: 0039 train_loss= 1.28696 train_acc= 0.68597 val_loss= 1.56208 val_acc= 0.56716 time= 0.29307
Epoch: 0040 train_loss= 1.23535 train_acc= 0.70053 val_loss= 1.53027 val_acc= 0.56418 time= 0.29030
Epoch: 0041 train_loss= 1.18244 train_acc= 0.71145 val_loss= 1.49959 val_acc= 0.57313 time= 0.29005
Epoch: 0042 train_loss= 1.13323 train_acc= 0.72369 val_loss= 1.47068 val_acc= 0.57612 time= 0.29100
Epoch: 0043 train_loss= 1.08283 train_acc= 0.74090 val_loss= 1.44353 val_acc= 0.58209 time= 0.29100
Epoch: 0044 train_loss= 1.03663 train_acc= 0.75381 val_loss= 1.41804 val_acc= 0.58806 time= 0.29003
Epoch: 0045 train_loss= 0.99220 train_acc= 0.76506 val_loss= 1.39392 val_acc= 0.59701 time= 0.28866
Epoch: 0046 train_loss= 0.94763 train_acc= 0.77796 val_loss= 1.37114 val_acc= 0.60597 time= 0.29500
Epoch: 0047 train_loss= 0.90484 train_acc= 0.79451 val_loss= 1.35006 val_acc= 0.61194 time= 0.29100
Epoch: 0048 train_loss= 0.86055 train_acc= 0.80278 val_loss= 1.33050 val_acc= 0.61493 time= 0.28600
Epoch: 0049 train_loss= 0.82249 train_acc= 0.81436 val_loss= 1.31231 val_acc= 0.61791 time= 0.29408
Epoch: 0050 train_loss= 0.78377 train_acc= 0.82462 val_loss= 1.29539 val_acc= 0.62687 time= 0.29507
Epoch: 0051 train_loss= 0.74643 train_acc= 0.83157 val_loss= 1.27991 val_acc= 0.62090 time= 0.28800
Epoch: 0052 train_loss= 0.70884 train_acc= 0.84216 val_loss= 1.26604 val_acc= 0.62090 time= 0.28997
Epoch: 0053 train_loss= 0.67416 train_acc= 0.85043 val_loss= 1.25390 val_acc= 0.62388 time= 0.29303
Epoch: 0054 train_loss= 0.64294 train_acc= 0.85837 val_loss= 1.24274 val_acc= 0.62985 time= 0.29700
Epoch: 0055 train_loss= 0.61025 train_acc= 0.86598 val_loss= 1.23258 val_acc= 0.63582 time= 0.29200
Epoch: 0056 train_loss= 0.58159 train_acc= 0.87558 val_loss= 1.22336 val_acc= 0.63582 time= 0.28800
Epoch: 0057 train_loss= 0.55145 train_acc= 0.87988 val_loss= 1.21497 val_acc= 0.63582 time= 0.29700
Epoch: 0058 train_loss= 0.52251 train_acc= 0.88815 val_loss= 1.20740 val_acc= 0.63881 time= 0.29200
Epoch: 0059 train_loss= 0.49733 train_acc= 0.89643 val_loss= 1.20062 val_acc= 0.64179 time= 0.29000
Epoch: 0060 train_loss= 0.47147 train_acc= 0.90238 val_loss= 1.19460 val_acc= 0.64478 time= 0.29100
Epoch: 0061 train_loss= 0.44800 train_acc= 0.90536 val_loss= 1.18931 val_acc= 0.64179 time= 0.29100
Epoch: 0062 train_loss= 0.42530 train_acc= 0.91396 val_loss= 1.18482 val_acc= 0.64478 time= 0.29314
Epoch: 0063 train_loss= 0.40365 train_acc= 0.91992 val_loss= 1.18126 val_acc= 0.64776 time= 0.29100
Epoch: 0064 train_loss= 0.38267 train_acc= 0.92124 val_loss= 1.17867 val_acc= 0.65672 time= 0.29200
Epoch: 0065 train_loss= 0.36324 train_acc= 0.92952 val_loss= 1.17697 val_acc= 0.65672 time= 0.29297
Epoch: 0066 train_loss= 0.34364 train_acc= 0.93349 val_loss= 1.17562 val_acc= 0.65672 time= 0.28903
Epoch: 0067 train_loss= 0.32676 train_acc= 0.93977 val_loss= 1.17468 val_acc= 0.66269 time= 0.29097
Epoch: 0068 train_loss= 0.31035 train_acc= 0.94209 val_loss= 1.17397 val_acc= 0.66269 time= 0.29803
Epoch: 0069 train_loss= 0.29476 train_acc= 0.94639 val_loss= 1.17338 val_acc= 0.66269 time= 0.28897
Epoch: 0070 train_loss= 0.28062 train_acc= 0.95169 val_loss= 1.17289 val_acc= 0.66567 time= 0.29203
Epoch: 0071 train_loss= 0.26643 train_acc= 0.95533 val_loss= 1.17294 val_acc= 0.66567 time= 0.29197
Epoch: 0072 train_loss= 0.25127 train_acc= 0.95797 val_loss= 1.17485 val_acc= 0.66567 time= 0.29612
Epoch: 0073 train_loss= 0.23918 train_acc= 0.96195 val_loss= 1.17866 val_acc= 0.66567 time= 0.29000
Early stopping...
Optimization Finished!
Test set results: cost= 1.16448 accuracy= 0.68439 time= 0.12903
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7187    0.6871    0.7025       342
           1     0.6695    0.7670    0.7149       103
           2     0.7304    0.6000    0.6588       140
           3     0.6400    0.4051    0.4961        79
           4     0.6369    0.7576    0.6920       132
           5     0.6958    0.7891    0.7395       313
           6     0.6792    0.7059    0.6923       102
           7     0.6286    0.3143    0.4190        70
           8     0.5882    0.4000    0.4762        50
           9     0.6141    0.7290    0.6667       155
          10     0.8592    0.6524    0.7416       187
          11     0.6092    0.6277    0.6183       231
          12     0.7771    0.7247    0.7500       178
          13     0.7692    0.8167    0.7922       600
          14     0.7841    0.8373    0.8098       590
          15     0.7333    0.7237    0.7285        76
          16     0.7059    0.3529    0.4706        34
          17     0.6667    0.2000    0.3077        10
          18     0.4126    0.4678    0.4385       419
          19     0.6321    0.5194    0.5702       129
          20     0.6400    0.5714    0.6038        28
          21     1.0000    0.7241    0.8400        29
          22     0.5185    0.3043    0.3836        46

    accuracy                         0.6844      4043
   macro avg     0.6830    0.5947    0.6223      4043
weighted avg     0.6883    0.6844    0.6812      4043

Macro average Test Precision, Recall and F1-Score...
(0.6830197061771577, 0.5946777168125238, 0.6223058005507897, None)
Micro average Test Precision, Recall and F1-Score...
(0.6843927776403661, 0.6843927776403661, 0.6843927776403661, None)
embeddings:
14157 3357 4043
[[0.450881   0.37758505 0.45510224 ... 0.41688406 0.5133574  0.58047664]
 [0.14775309 0.20112365 0.00062992 ... 0.13949409 0.38060504 0.25958043]
 [0.29795468 0.26801762 0.10783584 ... 0.26811957 0.40613976 0.29387054]
 ...
 [0.15785512 0.13905936 0.15983783 ... 0.20669606 0.28812188 0.25898835]
 [0.01068004 0.0252066  0.01124938 ... 0.5248461  0.06356931 0.03961447]
 [0.25376526 0.241615   0.35589713 ... 0.23796934 0.39983198 0.42300376]]

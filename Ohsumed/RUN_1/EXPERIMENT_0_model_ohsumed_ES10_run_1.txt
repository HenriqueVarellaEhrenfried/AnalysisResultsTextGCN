(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13558 train_acc= 0.01059 val_loss= 3.11595 val_acc= 0.20000 time= 0.58199
Epoch: 0002 train_loss= 3.11654 train_acc= 0.17174 val_loss= 3.07354 val_acc= 0.20000 time= 0.29100
Epoch: 0003 train_loss= 3.07500 train_acc= 0.17207 val_loss= 3.00791 val_acc= 0.20000 time= 0.28897
Epoch: 0004 train_loss= 3.01025 train_acc= 0.17141 val_loss= 2.92460 val_acc= 0.20000 time= 0.29299
Epoch: 0005 train_loss= 2.92870 train_acc= 0.17141 val_loss= 2.83636 val_acc= 0.20000 time= 0.29100
Epoch: 0006 train_loss= 2.84186 train_acc= 0.17141 val_loss= 2.75917 val_acc= 0.20000 time= 0.29003
Epoch: 0007 train_loss= 2.76650 train_acc= 0.17174 val_loss= 2.70683 val_acc= 0.20000 time= 0.29501
Epoch: 0008 train_loss= 2.71713 train_acc= 0.17174 val_loss= 2.68754 val_acc= 0.20000 time= 0.29100
Epoch: 0009 train_loss= 2.70126 train_acc= 0.17174 val_loss= 2.68849 val_acc= 0.20299 time= 0.28899
Epoch: 0010 train_loss= 2.70114 train_acc= 0.17207 val_loss= 2.68433 val_acc= 0.20597 time= 0.28900
Epoch: 0011 train_loss= 2.69371 train_acc= 0.17505 val_loss= 2.66393 val_acc= 0.20597 time= 0.29499
Epoch: 0012 train_loss= 2.66580 train_acc= 0.17803 val_loss= 2.63350 val_acc= 0.21791 time= 0.29005
Epoch: 0013 train_loss= 2.62673 train_acc= 0.18795 val_loss= 2.60294 val_acc= 0.23582 time= 0.29221
Epoch: 0014 train_loss= 2.58649 train_acc= 0.20119 val_loss= 2.57709 val_acc= 0.25373 time= 0.29100
Epoch: 0015 train_loss= 2.55361 train_acc= 0.22502 val_loss= 2.55435 val_acc= 0.27164 time= 0.29500
Epoch: 0016 train_loss= 2.52327 train_acc= 0.24553 val_loss= 2.53084 val_acc= 0.28060 time= 0.29515
Epoch: 0017 train_loss= 2.49122 train_acc= 0.26936 val_loss= 2.50328 val_acc= 0.29851 time= 0.29400
Epoch: 0018 train_loss= 2.45845 train_acc= 0.29682 val_loss= 2.47017 val_acc= 0.30746 time= 0.29600
Epoch: 0019 train_loss= 2.42189 train_acc= 0.31370 val_loss= 2.43210 val_acc= 0.31940 time= 0.29000
Epoch: 0020 train_loss= 2.38118 train_acc= 0.33388 val_loss= 2.39027 val_acc= 0.33134 time= 0.29142
Epoch: 0021 train_loss= 2.33275 train_acc= 0.34944 val_loss= 2.34600 val_acc= 0.34030 time= 0.28700
Epoch: 0022 train_loss= 2.28416 train_acc= 0.35738 val_loss= 2.30036 val_acc= 0.34627 time= 0.29501
Epoch: 0023 train_loss= 2.23192 train_acc= 0.36962 val_loss= 2.25421 val_acc= 0.34925 time= 0.29212
Epoch: 0024 train_loss= 2.17980 train_acc= 0.37889 val_loss= 2.20784 val_acc= 0.35224 time= 0.29201
Epoch: 0025 train_loss= 2.12329 train_acc= 0.39543 val_loss= 2.16141 val_acc= 0.36119 time= 0.28796
Epoch: 0026 train_loss= 2.06928 train_acc= 0.41396 val_loss= 2.11534 val_acc= 0.39104 time= 0.29203
Epoch: 0027 train_loss= 2.01186 train_acc= 0.44871 val_loss= 2.07014 val_acc= 0.41791 time= 0.29200
Epoch: 0028 train_loss= 1.95204 train_acc= 0.48279 val_loss= 2.02633 val_acc= 0.46567 time= 0.29100
Epoch: 0029 train_loss= 1.89011 train_acc= 0.52416 val_loss= 1.98371 val_acc= 0.48955 time= 0.29407
Epoch: 0030 train_loss= 1.83234 train_acc= 0.55460 val_loss= 1.94142 val_acc= 0.51642 time= 0.29272
Epoch: 0031 train_loss= 1.77558 train_acc= 0.58041 val_loss= 1.89860 val_acc= 0.52836 time= 0.28803
Epoch: 0032 train_loss= 1.72395 train_acc= 0.59828 val_loss= 1.85461 val_acc= 0.52537 time= 0.28901
Epoch: 0033 train_loss= 1.65742 train_acc= 0.60953 val_loss= 1.81052 val_acc= 0.52239 time= 0.29399
Epoch: 0034 train_loss= 1.60073 train_acc= 0.62508 val_loss= 1.76755 val_acc= 0.53134 time= 0.29200
Epoch: 0035 train_loss= 1.54955 train_acc= 0.62773 val_loss= 1.72643 val_acc= 0.53433 time= 0.28802
Epoch: 0036 train_loss= 1.48943 train_acc= 0.63865 val_loss= 1.68757 val_acc= 0.53433 time= 0.28996
Epoch: 0037 train_loss= 1.43318 train_acc= 0.65288 val_loss= 1.65070 val_acc= 0.54328 time= 0.29500
Epoch: 0038 train_loss= 1.38589 train_acc= 0.65751 val_loss= 1.61578 val_acc= 0.54627 time= 0.29300
Epoch: 0039 train_loss= 1.33060 train_acc= 0.67538 val_loss= 1.58307 val_acc= 0.56716 time= 0.28903
Epoch: 0040 train_loss= 1.28162 train_acc= 0.68365 val_loss= 1.55295 val_acc= 0.57910 time= 0.29601
Epoch: 0041 train_loss= 1.23140 train_acc= 0.69921 val_loss= 1.52493 val_acc= 0.57910 time= 0.29199
Epoch: 0042 train_loss= 1.18391 train_acc= 0.70781 val_loss= 1.49779 val_acc= 0.58507 time= 0.29097
Epoch: 0043 train_loss= 1.13311 train_acc= 0.72667 val_loss= 1.47036 val_acc= 0.58507 time= 0.29003
Epoch: 0044 train_loss= 1.09258 train_acc= 0.73858 val_loss= 1.44400 val_acc= 0.59104 time= 0.29397
Epoch: 0045 train_loss= 1.04636 train_acc= 0.74388 val_loss= 1.41935 val_acc= 0.59104 time= 0.29004
Epoch: 0046 train_loss= 1.00139 train_acc= 0.75877 val_loss= 1.39573 val_acc= 0.60299 time= 0.28896
Epoch: 0047 train_loss= 0.95910 train_acc= 0.76671 val_loss= 1.37340 val_acc= 0.60896 time= 0.29100
Epoch: 0048 train_loss= 0.92019 train_acc= 0.78127 val_loss= 1.35237 val_acc= 0.61493 time= 0.29304
Epoch: 0049 train_loss= 0.88432 train_acc= 0.78789 val_loss= 1.33328 val_acc= 0.62687 time= 0.29303
Epoch: 0050 train_loss= 0.84151 train_acc= 0.79782 val_loss= 1.31675 val_acc= 0.62687 time= 0.28993
Epoch: 0051 train_loss= 0.81052 train_acc= 0.80874 val_loss= 1.30186 val_acc= 0.62687 time= 0.29207
Epoch: 0052 train_loss= 0.77428 train_acc= 0.81502 val_loss= 1.28757 val_acc= 0.62985 time= 0.29000
Epoch: 0053 train_loss= 0.73407 train_acc= 0.82892 val_loss= 1.27482 val_acc= 0.63284 time= 0.28703
Epoch: 0054 train_loss= 0.70391 train_acc= 0.83422 val_loss= 1.26246 val_acc= 0.64179 time= 0.28800
Epoch: 0055 train_loss= 0.67562 train_acc= 0.84646 val_loss= 1.25143 val_acc= 0.64478 time= 0.29600
Epoch: 0056 train_loss= 0.64777 train_acc= 0.85175 val_loss= 1.24084 val_acc= 0.64179 time= 0.28808
Epoch: 0057 train_loss= 0.61475 train_acc= 0.86036 val_loss= 1.23144 val_acc= 0.64179 time= 0.28900
Epoch: 0058 train_loss= 0.58832 train_acc= 0.86830 val_loss= 1.22269 val_acc= 0.63881 time= 0.28800
Epoch: 0059 train_loss= 0.56415 train_acc= 0.87095 val_loss= 1.21381 val_acc= 0.63881 time= 0.29400
Epoch: 0060 train_loss= 0.53646 train_acc= 0.88319 val_loss= 1.20451 val_acc= 0.63582 time= 0.28900
Epoch: 0061 train_loss= 0.51676 train_acc= 0.88915 val_loss= 1.19747 val_acc= 0.65075 time= 0.29000
Epoch: 0062 train_loss= 0.49350 train_acc= 0.89246 val_loss= 1.19046 val_acc= 0.65075 time= 0.29700
Epoch: 0063 train_loss= 0.46643 train_acc= 0.90172 val_loss= 1.18614 val_acc= 0.65672 time= 0.28897
Epoch: 0064 train_loss= 0.44255 train_acc= 0.90834 val_loss= 1.18344 val_acc= 0.65672 time= 0.28800
Epoch: 0065 train_loss= 0.42301 train_acc= 0.90371 val_loss= 1.17949 val_acc= 0.65075 time= 0.28703
Epoch: 0066 train_loss= 0.40859 train_acc= 0.91661 val_loss= 1.17517 val_acc= 0.66269 time= 0.29599
Epoch: 0067 train_loss= 0.38690 train_acc= 0.92124 val_loss= 1.17136 val_acc= 0.67164 time= 0.28901
Epoch: 0068 train_loss= 0.37353 train_acc= 0.92522 val_loss= 1.16949 val_acc= 0.67164 time= 0.29005
Epoch: 0069 train_loss= 0.35652 train_acc= 0.93117 val_loss= 1.16758 val_acc= 0.66866 time= 0.28801
Epoch: 0070 train_loss= 0.33875 train_acc= 0.93514 val_loss= 1.16667 val_acc= 0.66567 time= 0.29300
Epoch: 0071 train_loss= 0.32416 train_acc= 0.93812 val_loss= 1.16821 val_acc= 0.66567 time= 0.29200
Epoch: 0072 train_loss= 0.31086 train_acc= 0.94408 val_loss= 1.16829 val_acc= 0.67463 time= 0.29098
Epoch: 0073 train_loss= 0.29694 train_acc= 0.94308 val_loss= 1.16543 val_acc= 0.67164 time= 0.29297
Epoch: 0074 train_loss= 0.28174 train_acc= 0.94639 val_loss= 1.16177 val_acc= 0.66269 time= 0.29003
Epoch: 0075 train_loss= 0.27324 train_acc= 0.95400 val_loss= 1.15976 val_acc= 0.66866 time= 0.29400
Epoch: 0076 train_loss= 0.26380 train_acc= 0.95235 val_loss= 1.16168 val_acc= 0.68060 time= 0.28797
Epoch: 0077 train_loss= 0.25028 train_acc= 0.95764 val_loss= 1.16659 val_acc= 0.66866 time= 0.29504
Early stopping...
Optimization Finished!
Test set results: cost= 1.16425 accuracy= 0.67945 time= 0.13000
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7226    0.6550    0.6871       342
           1     0.7000    0.7476    0.7230       103
           2     0.7411    0.5929    0.6587       140
           3     0.5714    0.3544    0.4375        79
           4     0.6712    0.7424    0.7050       132
           5     0.6685    0.7859    0.7225       313
           6     0.6923    0.7059    0.6990       102
           7     0.6111    0.3143    0.4151        70
           8     0.6429    0.3600    0.4615        50
           9     0.5938    0.7355    0.6571       155
          10     0.8133    0.6524    0.7240       187
          11     0.5673    0.6753    0.6166       231
          12     0.7862    0.7022    0.7418       178
          13     0.7861    0.7900    0.7880       600
          14     0.7864    0.8424    0.8134       590
          15     0.7606    0.7105    0.7347        76
          16     0.6667    0.3529    0.4615        34
          17     0.5000    0.1000    0.1667        10
          18     0.4116    0.4893    0.4471       419
          19     0.6346    0.5116    0.5665       129
          20     0.6429    0.6429    0.6429        28
          21     0.9545    0.7241    0.8235        29
          22     0.5385    0.3043    0.3889        46

    accuracy                         0.6794      4043
   macro avg     0.6723    0.5866    0.6123      4043
weighted avg     0.6857    0.6794    0.6769      4043

Macro average Test Precision, Recall and F1-Score...
(0.6723219792267657, 0.5866043207922241, 0.6122744217565603, None)
Micro average Test Precision, Recall and F1-Score...
(0.6794459559732872, 0.6794459559732872, 0.6794459559732872, None)
embeddings:
14157 3357 4043
[[ 0.3963018   0.5618261   0.46541896 ...  0.5586377   0.46700597
  -0.04870642]
 [ 0.16733864  0.27702004 -0.06768552 ...  0.00701068  0.2128064
  -0.02677898]
 [ 0.10273726  0.29105276  0.16121191 ...  0.14137092  0.34989962
  -0.04856408]
 ...
 [ 0.16835271  0.20648593  0.24884841 ...  0.33727854  0.21349566
  -0.01226981]
 [ 0.2297402   0.28357175  0.12775508 ...  0.01712669 -0.04164711
  -0.00957808]
 [ 0.35995004  0.24007607  0.07371645 ...  0.31242934  0.28641015
  -0.02766688]]

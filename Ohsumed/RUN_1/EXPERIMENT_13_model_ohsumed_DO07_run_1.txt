(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.03640 val_loss= 3.11692 val_acc= 0.22687 time= 0.58814
Epoch: 0002 train_loss= 3.11674 train_acc= 0.19325 val_loss= 3.07399 val_acc= 0.20597 time= 0.29100
Epoch: 0003 train_loss= 3.07403 train_acc= 0.17968 val_loss= 3.00648 val_acc= 0.20597 time= 0.28800
Epoch: 0004 train_loss= 3.00705 train_acc= 0.17836 val_loss= 2.92012 val_acc= 0.20597 time= 0.29600
Epoch: 0005 train_loss= 2.92084 train_acc= 0.17538 val_loss= 2.82946 val_acc= 0.20597 time= 0.29300
Epoch: 0006 train_loss= 2.83053 train_acc= 0.17439 val_loss= 2.75166 val_acc= 0.20597 time= 0.29400
Epoch: 0007 train_loss= 2.75877 train_acc= 0.17704 val_loss= 2.70213 val_acc= 0.20597 time= 0.28900
Epoch: 0008 train_loss= 2.71168 train_acc= 0.17770 val_loss= 2.68774 val_acc= 0.20597 time= 0.29400
Epoch: 0009 train_loss= 2.70021 train_acc= 0.17869 val_loss= 2.68867 val_acc= 0.20597 time= 0.29600
Epoch: 0010 train_loss= 2.70118 train_acc= 0.17935 val_loss= 2.68026 val_acc= 0.20597 time= 0.29300
Epoch: 0011 train_loss= 2.69131 train_acc= 0.17869 val_loss= 2.65667 val_acc= 0.21194 time= 0.29600
Epoch: 0012 train_loss= 2.66001 train_acc= 0.18597 val_loss= 2.62709 val_acc= 0.22985 time= 0.29000
Epoch: 0013 train_loss= 2.61859 train_acc= 0.19954 val_loss= 2.59940 val_acc= 0.25075 time= 0.29400
Epoch: 0014 train_loss= 2.58132 train_acc= 0.22204 val_loss= 2.57495 val_acc= 0.26567 time= 0.29000
Epoch: 0015 train_loss= 2.54821 train_acc= 0.24057 val_loss= 2.55135 val_acc= 0.28060 time= 0.29400
Epoch: 0016 train_loss= 2.51884 train_acc= 0.26142 val_loss= 2.52546 val_acc= 0.29254 time= 0.29086
Epoch: 0017 train_loss= 2.48699 train_acc= 0.27167 val_loss= 2.49517 val_acc= 0.30149 time= 0.28916
Epoch: 0018 train_loss= 2.45361 train_acc= 0.28491 val_loss= 2.45979 val_acc= 0.31045 time= 0.29100
Epoch: 0019 train_loss= 2.41742 train_acc= 0.30113 val_loss= 2.42035 val_acc= 0.31045 time= 0.29200
Epoch: 0020 train_loss= 2.37257 train_acc= 0.31701 val_loss= 2.37839 val_acc= 0.31343 time= 0.29323
Epoch: 0021 train_loss= 2.32728 train_acc= 0.33124 val_loss= 2.33527 val_acc= 0.31940 time= 0.28999
Epoch: 0022 train_loss= 2.28568 train_acc= 0.33388 val_loss= 2.29197 val_acc= 0.32836 time= 0.29500
Epoch: 0023 train_loss= 2.23741 train_acc= 0.34514 val_loss= 2.24887 val_acc= 0.34030 time= 0.29300
Epoch: 0024 train_loss= 2.18870 train_acc= 0.35109 val_loss= 2.20628 val_acc= 0.34328 time= 0.29200
Epoch: 0025 train_loss= 2.13513 train_acc= 0.36433 val_loss= 2.16433 val_acc= 0.36119 time= 0.28804
Epoch: 0026 train_loss= 2.08684 train_acc= 0.39576 val_loss= 2.12319 val_acc= 0.37612 time= 0.29197
Epoch: 0027 train_loss= 2.02938 train_acc= 0.43084 val_loss= 2.08269 val_acc= 0.40597 time= 0.29303
Epoch: 0028 train_loss= 1.97677 train_acc= 0.46062 val_loss= 2.04290 val_acc= 0.43881 time= 0.28900
Epoch: 0029 train_loss= 1.93098 train_acc= 0.50463 val_loss= 2.00345 val_acc= 0.48657 time= 0.28900
Epoch: 0030 train_loss= 1.88030 train_acc= 0.53276 val_loss= 1.96356 val_acc= 0.49254 time= 0.29200
Epoch: 0031 train_loss= 1.81333 train_acc= 0.56916 val_loss= 1.92271 val_acc= 0.50746 time= 0.29500
Epoch: 0032 train_loss= 1.76275 train_acc= 0.58206 val_loss= 1.88097 val_acc= 0.51343 time= 0.28897
Epoch: 0033 train_loss= 1.71781 train_acc= 0.59034 val_loss= 1.83979 val_acc= 0.51642 time= 0.29503
Epoch: 0034 train_loss= 1.66646 train_acc= 0.59861 val_loss= 1.79997 val_acc= 0.52239 time= 0.29333
Epoch: 0035 train_loss= 1.60899 train_acc= 0.60721 val_loss= 1.76263 val_acc= 0.51940 time= 0.29000
Epoch: 0036 train_loss= 1.56033 train_acc= 0.62045 val_loss= 1.72709 val_acc= 0.52836 time= 0.28897
Epoch: 0037 train_loss= 1.50344 train_acc= 0.62740 val_loss= 1.69352 val_acc= 0.53731 time= 0.29103
Epoch: 0038 train_loss= 1.45564 train_acc= 0.64229 val_loss= 1.66090 val_acc= 0.54030 time= 0.29400
Epoch: 0039 train_loss= 1.40877 train_acc= 0.65156 val_loss= 1.62889 val_acc= 0.56119 time= 0.29097
Epoch: 0040 train_loss= 1.36004 train_acc= 0.66678 val_loss= 1.59786 val_acc= 0.57015 time= 0.29103
Epoch: 0041 train_loss= 1.30930 train_acc= 0.67075 val_loss= 1.56944 val_acc= 0.58209 time= 0.29604
Epoch: 0042 train_loss= 1.27062 train_acc= 0.69457 val_loss= 1.54282 val_acc= 0.58209 time= 0.29103
Epoch: 0043 train_loss= 1.23713 train_acc= 0.69259 val_loss= 1.51649 val_acc= 0.59104 time= 0.28797
Epoch: 0044 train_loss= 1.18268 train_acc= 0.70053 val_loss= 1.49014 val_acc= 0.60000 time= 0.29302
Epoch: 0045 train_loss= 1.14946 train_acc= 0.71178 val_loss= 1.46480 val_acc= 0.60000 time= 0.29298
Epoch: 0046 train_loss= 1.11145 train_acc= 0.72071 val_loss= 1.44075 val_acc= 0.60000 time= 0.28903
Epoch: 0047 train_loss= 1.05328 train_acc= 0.73527 val_loss= 1.41750 val_acc= 0.60896 time= 0.28997
Epoch: 0048 train_loss= 1.02707 train_acc= 0.74289 val_loss= 1.39620 val_acc= 0.60299 time= 0.29503
Epoch: 0049 train_loss= 0.98458 train_acc= 0.75215 val_loss= 1.37767 val_acc= 0.61194 time= 0.29500
Epoch: 0050 train_loss= 0.95374 train_acc= 0.75943 val_loss= 1.36090 val_acc= 0.62090 time= 0.29100
Epoch: 0051 train_loss= 0.92137 train_acc= 0.77498 val_loss= 1.34557 val_acc= 0.61791 time= 0.28800
Epoch: 0052 train_loss= 0.88569 train_acc= 0.78425 val_loss= 1.33110 val_acc= 0.62687 time= 0.29830
Epoch: 0053 train_loss= 0.84977 train_acc= 0.79848 val_loss= 1.31719 val_acc= 0.62687 time= 0.29000
Epoch: 0054 train_loss= 0.80891 train_acc= 0.80874 val_loss= 1.30348 val_acc= 0.64179 time= 0.29097
Epoch: 0055 train_loss= 0.78875 train_acc= 0.81138 val_loss= 1.29076 val_acc= 0.63284 time= 0.29703
Epoch: 0056 train_loss= 0.75839 train_acc= 0.81138 val_loss= 1.27816 val_acc= 0.62687 time= 0.29180
Epoch: 0057 train_loss= 0.72811 train_acc= 0.82164 val_loss= 1.26388 val_acc= 0.64179 time= 0.29007
Epoch: 0058 train_loss= 0.69792 train_acc= 0.83322 val_loss= 1.25307 val_acc= 0.64776 time= 0.28800
Epoch: 0059 train_loss= 0.67720 train_acc= 0.83951 val_loss= 1.24579 val_acc= 0.64179 time= 0.29597
Epoch: 0060 train_loss= 0.64266 train_acc= 0.84745 val_loss= 1.23773 val_acc= 0.64478 time= 0.28803
Epoch: 0061 train_loss= 0.61935 train_acc= 0.85804 val_loss= 1.22900 val_acc= 0.65373 time= 0.28805
Epoch: 0062 train_loss= 0.60196 train_acc= 0.86168 val_loss= 1.22154 val_acc= 0.64179 time= 0.29200
Epoch: 0063 train_loss= 0.56903 train_acc= 0.86698 val_loss= 1.21515 val_acc= 0.65075 time= 0.29400
Epoch: 0064 train_loss= 0.55572 train_acc= 0.86797 val_loss= 1.21003 val_acc= 0.64478 time= 0.29100
Epoch: 0065 train_loss= 0.52578 train_acc= 0.88418 val_loss= 1.20455 val_acc= 0.65075 time= 0.28800
Epoch: 0066 train_loss= 0.51073 train_acc= 0.88021 val_loss= 1.20038 val_acc= 0.65672 time= 0.29300
Epoch: 0067 train_loss= 0.48854 train_acc= 0.88683 val_loss= 1.19585 val_acc= 0.66269 time= 0.29300
Epoch: 0068 train_loss= 0.47059 train_acc= 0.89411 val_loss= 1.19034 val_acc= 0.65970 time= 0.28900
Epoch: 0069 train_loss= 0.45542 train_acc= 0.89974 val_loss= 1.18501 val_acc= 0.65075 time= 0.29100
Epoch: 0070 train_loss= 0.44197 train_acc= 0.90371 val_loss= 1.17805 val_acc= 0.65970 time= 0.29500
Epoch: 0071 train_loss= 0.42150 train_acc= 0.90668 val_loss= 1.17078 val_acc= 0.66866 time= 0.29110
Epoch: 0072 train_loss= 0.40022 train_acc= 0.91463 val_loss= 1.16453 val_acc= 0.66567 time= 0.28804
Epoch: 0073 train_loss= 0.38623 train_acc= 0.92091 val_loss= 1.16172 val_acc= 0.66269 time= 0.29497
Epoch: 0074 train_loss= 0.38048 train_acc= 0.91562 val_loss= 1.16106 val_acc= 0.65970 time= 0.30003
Epoch: 0075 train_loss= 0.36682 train_acc= 0.92323 val_loss= 1.16063 val_acc= 0.67164 time= 0.28997
Epoch: 0076 train_loss= 0.34685 train_acc= 0.93216 val_loss= 1.16001 val_acc= 0.67164 time= 0.28771
Epoch: 0077 train_loss= 0.33459 train_acc= 0.93249 val_loss= 1.16039 val_acc= 0.67463 time= 0.29712
Epoch: 0078 train_loss= 0.31964 train_acc= 0.93911 val_loss= 1.16135 val_acc= 0.67761 time= 0.29000
Epoch: 0079 train_loss= 0.31927 train_acc= 0.93613 val_loss= 1.16273 val_acc= 0.67463 time= 0.29200
Epoch: 0080 train_loss= 0.29775 train_acc= 0.94474 val_loss= 1.16346 val_acc= 0.67463 time= 0.29000
Epoch: 0081 train_loss= 0.28780 train_acc= 0.94639 val_loss= 1.16310 val_acc= 0.67761 time= 0.29599
Early stopping...
Optimization Finished!
Test set results: cost= 1.15688 accuracy= 0.68489 time= 0.13000
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7095    0.6784    0.6936       342
           1     0.7103    0.7379    0.7238       103
           2     0.7885    0.5857    0.6721       140
           3     0.5862    0.4304    0.4964        79
           4     0.6667    0.7576    0.7092       132
           5     0.6622    0.7955    0.7228       313
           6     0.6607    0.7255    0.6916       102
           7     0.6000    0.3000    0.4000        70
           8     0.6250    0.3000    0.4054        50
           9     0.6474    0.7226    0.6829       155
          10     0.8511    0.6417    0.7317       187
          11     0.6271    0.6407    0.6338       231
          12     0.7711    0.7191    0.7442       178
          13     0.7527    0.8217    0.7857       600
          14     0.7564    0.8576    0.8038       590
          15     0.7606    0.7105    0.7347        76
          16     0.7333    0.3235    0.4490        34
          17     0.5000    0.1000    0.1667        10
          18     0.4314    0.4726    0.4510       419
          19     0.6598    0.4961    0.5664       129
          20     0.6957    0.5714    0.6275        28
          21     1.0000    0.7241    0.8400        29
          22     0.6364    0.3043    0.4118        46

    accuracy                         0.6849      4043
   macro avg     0.6883    0.5833    0.6150      4043
weighted avg     0.6874    0.6849    0.6794      4043

Macro average Test Precision, Recall and F1-Score...
(0.6883396952012726, 0.5833452610706698, 0.6149539077674634, None)
Micro average Test Precision, Recall and F1-Score...
(0.684887459807074, 0.684887459807074, 0.684887459807074, None)
embeddings:
14157 3357 4043
[[ 0.35802633  0.32861418  0.4464078  ...  0.376133    0.39895815
   0.27007484]
 [-0.03746157  0.15032102 -0.03638761 ...  0.10109577  0.24838564
  -0.03571074]
 [ 0.11223311  0.25533655  0.15174395 ...  0.23516907  0.10155883
   0.16223602]
 ...
 [ 0.2473793   0.22295438  0.20114918 ...  0.10046984  0.16159256
   0.12561962]
 [ 0.06773432  0.3771919   0.05092763 ...  0.26087382  0.4948953
   0.00052375]
 [ 0.255137    0.13984649  0.13089734 ...  0.04991302  0.23757735
   0.1434832 ]]

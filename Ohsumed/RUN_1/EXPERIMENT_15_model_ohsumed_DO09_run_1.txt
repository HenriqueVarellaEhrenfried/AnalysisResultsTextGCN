(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.04765 val_loss= 3.11618 val_acc= 0.20000 time= 0.58162
Epoch: 0002 train_loss= 3.11669 train_acc= 0.17207 val_loss= 3.07177 val_acc= 0.20000 time= 0.29600
Epoch: 0003 train_loss= 3.07195 train_acc= 0.17141 val_loss= 3.00317 val_acc= 0.20000 time= 0.29317
Epoch: 0004 train_loss= 3.00518 train_acc= 0.17141 val_loss= 2.91675 val_acc= 0.20000 time= 0.28903
Epoch: 0005 train_loss= 2.91958 train_acc= 0.17141 val_loss= 2.82679 val_acc= 0.20000 time= 0.29400
Epoch: 0006 train_loss= 2.83301 train_acc= 0.17141 val_loss= 2.75081 val_acc= 0.20000 time= 0.29300
Epoch: 0007 train_loss= 2.75752 train_acc= 0.17141 val_loss= 2.70329 val_acc= 0.20000 time= 0.29414
Epoch: 0008 train_loss= 2.72081 train_acc= 0.17141 val_loss= 2.69213 val_acc= 0.20000 time= 0.29100
Epoch: 0009 train_loss= 2.71012 train_acc= 0.17174 val_loss= 2.70093 val_acc= 0.20000 time= 0.29204
Epoch: 0010 train_loss= 2.72044 train_acc= 0.17174 val_loss= 2.69916 val_acc= 0.20000 time= 0.29496
Epoch: 0011 train_loss= 2.71551 train_acc= 0.17240 val_loss= 2.67959 val_acc= 0.20299 time= 0.29300
Epoch: 0012 train_loss= 2.69175 train_acc= 0.17207 val_loss= 2.65323 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.65572 train_acc= 0.17869 val_loss= 2.63043 val_acc= 0.21493 time= 0.29603
Epoch: 0014 train_loss= 2.62717 train_acc= 0.18332 val_loss= 2.61327 val_acc= 0.23284 time= 0.29500
Epoch: 0015 train_loss= 2.59666 train_acc= 0.20715 val_loss= 2.59865 val_acc= 0.25373 time= 0.29200
Epoch: 0016 train_loss= 2.58172 train_acc= 0.22601 val_loss= 2.58256 val_acc= 0.26269 time= 0.28900
Epoch: 0017 train_loss= 2.56518 train_acc= 0.22799 val_loss= 2.56238 val_acc= 0.27761 time= 0.29497
Epoch: 0018 train_loss= 2.53594 train_acc= 0.26042 val_loss= 2.53702 val_acc= 0.28060 time= 0.29552
Epoch: 0019 train_loss= 2.50537 train_acc= 0.27267 val_loss= 2.50711 val_acc= 0.28657 time= 0.29203
Epoch: 0020 train_loss= 2.48242 train_acc= 0.27201 val_loss= 2.47411 val_acc= 0.28955 time= 0.29232
Epoch: 0021 train_loss= 2.44833 train_acc= 0.27664 val_loss= 2.43948 val_acc= 0.30149 time= 0.29701
Epoch: 0022 train_loss= 2.41966 train_acc= 0.27565 val_loss= 2.40454 val_acc= 0.30448 time= 0.28999
Epoch: 0023 train_loss= 2.37788 train_acc= 0.27929 val_loss= 2.37009 val_acc= 0.30448 time= 0.28800
Epoch: 0024 train_loss= 2.34406 train_acc= 0.28425 val_loss= 2.33594 val_acc= 0.30746 time= 0.29602
Epoch: 0025 train_loss= 2.30898 train_acc= 0.29517 val_loss= 2.30172 val_acc= 0.30746 time= 0.29100
Epoch: 0026 train_loss= 2.27704 train_acc= 0.30907 val_loss= 2.26730 val_acc= 0.31642 time= 0.29000
Epoch: 0027 train_loss= 2.22686 train_acc= 0.32462 val_loss= 2.23282 val_acc= 0.33731 time= 0.28900
Epoch: 0028 train_loss= 2.18540 train_acc= 0.34646 val_loss= 2.19861 val_acc= 0.36119 time= 0.29497
Epoch: 0029 train_loss= 2.14333 train_acc= 0.37690 val_loss= 2.16497 val_acc= 0.38209 time= 0.29403
Epoch: 0030 train_loss= 2.10167 train_acc= 0.42124 val_loss= 2.13138 val_acc= 0.41493 time= 0.29000
Epoch: 0031 train_loss= 2.05255 train_acc= 0.45268 val_loss= 2.09732 val_acc= 0.45672 time= 0.29400
Epoch: 0032 train_loss= 2.00923 train_acc= 0.47121 val_loss= 2.06216 val_acc= 0.47164 time= 0.29400
Epoch: 0033 train_loss= 1.97411 train_acc= 0.48610 val_loss= 2.02588 val_acc= 0.48060 time= 0.29000
Epoch: 0034 train_loss= 1.92439 train_acc= 0.51522 val_loss= 1.98902 val_acc= 0.47761 time= 0.29100
Epoch: 0035 train_loss= 1.88826 train_acc= 0.51357 val_loss= 1.95226 val_acc= 0.49254 time= 0.29500
Epoch: 0036 train_loss= 1.82421 train_acc= 0.54401 val_loss= 1.91584 val_acc= 0.49552 time= 0.29700
Epoch: 0037 train_loss= 1.79168 train_acc= 0.54103 val_loss= 1.87999 val_acc= 0.50448 time= 0.29699
Epoch: 0038 train_loss= 1.74850 train_acc= 0.54897 val_loss= 1.84545 val_acc= 0.51940 time= 0.29200
Epoch: 0039 train_loss= 1.70544 train_acc= 0.55394 val_loss= 1.81179 val_acc= 0.53134 time= 0.30000
Epoch: 0040 train_loss= 1.66307 train_acc= 0.57048 val_loss= 1.77970 val_acc= 0.52836 time= 0.28996
Epoch: 0041 train_loss= 1.61879 train_acc= 0.58471 val_loss= 1.74819 val_acc= 0.53731 time= 0.28903
Epoch: 0042 train_loss= 1.57141 train_acc= 0.59431 val_loss= 1.71813 val_acc= 0.53731 time= 0.28800
Epoch: 0043 train_loss= 1.52319 train_acc= 0.61052 val_loss= 1.68974 val_acc= 0.54627 time= 0.29501
Epoch: 0044 train_loss= 1.49343 train_acc= 0.60854 val_loss= 1.66244 val_acc= 0.55224 time= 0.28999
Epoch: 0045 train_loss= 1.46823 train_acc= 0.61582 val_loss= 1.63383 val_acc= 0.55522 time= 0.29228
Epoch: 0046 train_loss= 1.41562 train_acc= 0.63005 val_loss= 1.60603 val_acc= 0.56418 time= 0.29700
Epoch: 0047 train_loss= 1.38094 train_acc= 0.64097 val_loss= 1.57886 val_acc= 0.56716 time= 0.29119
Epoch: 0048 train_loss= 1.33358 train_acc= 0.65586 val_loss= 1.55379 val_acc= 0.57015 time= 0.28754
Epoch: 0049 train_loss= 1.28245 train_acc= 0.66578 val_loss= 1.53069 val_acc= 0.56716 time= 0.28999
Epoch: 0050 train_loss= 1.27884 train_acc= 0.65850 val_loss= 1.50881 val_acc= 0.57910 time= 0.29582
Epoch: 0051 train_loss= 1.22985 train_acc= 0.68431 val_loss= 1.48789 val_acc= 0.59403 time= 0.28800
Epoch: 0052 train_loss= 1.20878 train_acc= 0.67869 val_loss= 1.46968 val_acc= 0.60000 time= 0.28708
Epoch: 0053 train_loss= 1.17154 train_acc= 0.69292 val_loss= 1.45297 val_acc= 0.61194 time= 0.29097
Epoch: 0054 train_loss= 1.13278 train_acc= 0.70615 val_loss= 1.43511 val_acc= 0.60896 time= 0.29703
Epoch: 0055 train_loss= 1.10040 train_acc= 0.71707 val_loss= 1.41636 val_acc= 0.60000 time= 0.29300
Epoch: 0056 train_loss= 1.09413 train_acc= 0.71443 val_loss= 1.39767 val_acc= 0.59701 time= 0.28897
Epoch: 0057 train_loss= 1.05092 train_acc= 0.72369 val_loss= 1.38013 val_acc= 0.59104 time= 0.30003
Epoch: 0058 train_loss= 1.00937 train_acc= 0.74487 val_loss= 1.36341 val_acc= 0.59403 time= 0.29097
Epoch: 0059 train_loss= 0.99688 train_acc= 0.73296 val_loss= 1.34861 val_acc= 0.60597 time= 0.29503
Epoch: 0060 train_loss= 0.96326 train_acc= 0.74785 val_loss= 1.33619 val_acc= 0.61493 time= 0.28903
Epoch: 0061 train_loss= 0.95260 train_acc= 0.74950 val_loss= 1.32531 val_acc= 0.62090 time= 0.29857
Epoch: 0062 train_loss= 0.91179 train_acc= 0.76539 val_loss= 1.31516 val_acc= 0.62985 time= 0.28827
Epoch: 0063 train_loss= 0.88013 train_acc= 0.77201 val_loss= 1.30434 val_acc= 0.62090 time= 0.28900
Epoch: 0064 train_loss= 0.88058 train_acc= 0.77565 val_loss= 1.29652 val_acc= 0.62090 time= 0.29200
Epoch: 0065 train_loss= 0.83578 train_acc= 0.78657 val_loss= 1.29061 val_acc= 0.62985 time= 0.29600
Epoch: 0066 train_loss= 0.81729 train_acc= 0.78590 val_loss= 1.28335 val_acc= 0.63284 time= 0.29000
Epoch: 0067 train_loss= 0.79562 train_acc= 0.79914 val_loss= 1.27186 val_acc= 0.62687 time= 0.28900
Epoch: 0068 train_loss= 0.78651 train_acc= 0.79848 val_loss= 1.25964 val_acc= 0.63284 time= 0.29800
Epoch: 0069 train_loss= 0.74499 train_acc= 0.81171 val_loss= 1.24784 val_acc= 0.62985 time= 0.28911
Epoch: 0070 train_loss= 0.74109 train_acc= 0.80311 val_loss= 1.23795 val_acc= 0.63582 time= 0.28800
Epoch: 0071 train_loss= 0.70871 train_acc= 0.82462 val_loss= 1.22828 val_acc= 0.62388 time= 0.28997
Epoch: 0072 train_loss= 0.69180 train_acc= 0.83322 val_loss= 1.22241 val_acc= 0.62985 time= 0.29603
Epoch: 0073 train_loss= 0.68069 train_acc= 0.82528 val_loss= 1.21741 val_acc= 0.63582 time= 0.28634
Epoch: 0074 train_loss= 0.67385 train_acc= 0.83024 val_loss= 1.21259 val_acc= 0.64179 time= 0.29600
Epoch: 0075 train_loss= 0.64789 train_acc= 0.83355 val_loss= 1.20638 val_acc= 0.65672 time= 0.29497
Epoch: 0076 train_loss= 0.63460 train_acc= 0.84183 val_loss= 1.20006 val_acc= 0.65075 time= 0.29300
Epoch: 0077 train_loss= 0.61650 train_acc= 0.83653 val_loss= 1.19499 val_acc= 0.64776 time= 0.29008
Epoch: 0078 train_loss= 0.58719 train_acc= 0.84878 val_loss= 1.19043 val_acc= 0.65075 time= 0.29100
Epoch: 0079 train_loss= 0.59185 train_acc= 0.85043 val_loss= 1.18563 val_acc= 0.65075 time= 0.29793
Epoch: 0080 train_loss= 0.58180 train_acc= 0.85473 val_loss= 1.18166 val_acc= 0.64478 time= 0.29099
Epoch: 0081 train_loss= 0.55585 train_acc= 0.86234 val_loss= 1.17976 val_acc= 0.64776 time= 0.28900
Epoch: 0082 train_loss= 0.55170 train_acc= 0.86300 val_loss= 1.17599 val_acc= 0.64478 time= 0.29000
Epoch: 0083 train_loss= 0.53757 train_acc= 0.87128 val_loss= 1.17193 val_acc= 0.64478 time= 0.29400
Epoch: 0084 train_loss= 0.51884 train_acc= 0.86797 val_loss= 1.16848 val_acc= 0.64179 time= 0.28900
Epoch: 0085 train_loss= 0.50752 train_acc= 0.87492 val_loss= 1.16480 val_acc= 0.64776 time= 0.29000
Epoch: 0086 train_loss= 0.47596 train_acc= 0.88087 val_loss= 1.16121 val_acc= 0.65970 time= 0.29222
Epoch: 0087 train_loss= 0.48718 train_acc= 0.87955 val_loss= 1.15709 val_acc= 0.66567 time= 0.29376
Epoch: 0088 train_loss= 0.46114 train_acc= 0.88418 val_loss= 1.15474 val_acc= 0.65970 time= 0.28705
Epoch: 0089 train_loss= 0.44820 train_acc= 0.89080 val_loss= 1.15445 val_acc= 0.66866 time= 0.28897
Epoch: 0090 train_loss= 0.44745 train_acc= 0.89047 val_loss= 1.15443 val_acc= 0.66567 time= 0.29803
Epoch: 0091 train_loss= 0.43903 train_acc= 0.89709 val_loss= 1.15397 val_acc= 0.65970 time= 0.28907
Epoch: 0092 train_loss= 0.43580 train_acc= 0.89676 val_loss= 1.15524 val_acc= 0.65970 time= 0.28928
Epoch: 0093 train_loss= 0.42380 train_acc= 0.89610 val_loss= 1.15717 val_acc= 0.65672 time= 0.29300
Epoch: 0094 train_loss= 0.41124 train_acc= 0.90271 val_loss= 1.15796 val_acc= 0.66866 time= 0.29400
Epoch: 0095 train_loss= 0.39812 train_acc= 0.90635 val_loss= 1.15751 val_acc= 0.66567 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.16130 accuracy= 0.67870 time= 0.13100
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7212    0.6959    0.7083       342
           1     0.6847    0.7379    0.7103       103
           2     0.7456    0.6071    0.6693       140
           3     0.6757    0.3165    0.4310        79
           4     0.6556    0.7500    0.6996       132
           5     0.6458    0.7923    0.7116       313
           6     0.6356    0.7353    0.6818       102
           7     0.6333    0.2714    0.3800        70
           8     0.6190    0.2600    0.3662        50
           9     0.6437    0.7226    0.6809       155
          10     0.8224    0.6684    0.7375       187
          11     0.6208    0.6450    0.6327       231
          12     0.7529    0.7191    0.7356       178
          13     0.7829    0.7933    0.7881       600
          14     0.7618    0.8458    0.8016       590
          15     0.7727    0.6711    0.7183        76
          16     0.8333    0.2941    0.4348        34
          17     1.0000    0.1000    0.1818        10
          18     0.4000    0.4916    0.4411       419
          19     0.6559    0.4729    0.5495       129
          20     0.6667    0.5000    0.5714        28
          21     1.0000    0.7241    0.8400        29
          22     0.6842    0.2826    0.4000        46

    accuracy                         0.6787      4043
   macro avg     0.7137    0.5694    0.6031      4043
weighted avg     0.6882    0.6787    0.6739      4043

Macro average Test Precision, Recall and F1-Score...
(0.7136531664787349, 0.5694393294986461, 0.6031110301106166, None)
Micro average Test Precision, Recall and F1-Score...
(0.6787039327232254, 0.6787039327232254, 0.6787039327232254, None)
embeddings:
14157 3357 4043
[[ 0.3343148   0.28808644  0.25236627 ...  0.24122228  0.3067565
   0.36750314]
 [ 0.03512685 -0.01299395  0.15964018 ... -0.06776561  0.24825251
   0.09777178]
 [ 0.2638875   0.2329425   0.50714564 ...  0.2638116   0.2191798
   0.3508359 ]
 ...
 [ 0.0587952   0.12029076  0.07425097 ...  0.04694092  0.12468825
   0.25857338]
 [ 0.0464029   0.0668859   0.28245103 ...  0.30903652  0.17872497
  -0.03735217]
 [ 0.21923797  0.07416263 -0.04543341 ...  0.08108628  0.21491432
   0.09993015]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.05228 val_loss= 3.11311 val_acc= 0.20000 time= 0.58496
Epoch: 0002 train_loss= 3.11302 train_acc= 0.17141 val_loss= 3.06429 val_acc= 0.20000 time= 0.29300
Epoch: 0003 train_loss= 3.06428 train_acc= 0.17141 val_loss= 2.98998 val_acc= 0.20000 time= 0.29200
Epoch: 0004 train_loss= 2.98998 train_acc= 0.17141 val_loss= 2.89728 val_acc= 0.20000 time= 0.29005
Epoch: 0005 train_loss= 2.89924 train_acc= 0.17141 val_loss= 2.80353 val_acc= 0.20000 time= 0.29103
Epoch: 0006 train_loss= 2.80828 train_acc= 0.17141 val_loss= 2.72859 val_acc= 0.20000 time= 0.28897
Epoch: 0007 train_loss= 2.73745 train_acc= 0.17141 val_loss= 2.68896 val_acc= 0.20000 time= 0.29503
Epoch: 0008 train_loss= 2.70197 train_acc= 0.17141 val_loss= 2.68646 val_acc= 0.20000 time= 0.28697
Epoch: 0009 train_loss= 2.70081 train_acc= 0.17141 val_loss= 2.69128 val_acc= 0.20000 time= 0.29000
Epoch: 0010 train_loss= 2.70350 train_acc= 0.17141 val_loss= 2.67741 val_acc= 0.20597 time= 0.29500
Epoch: 0011 train_loss= 2.68219 train_acc= 0.17273 val_loss= 2.64884 val_acc= 0.20896 time= 0.29003
Epoch: 0012 train_loss= 2.64289 train_acc= 0.18034 val_loss= 2.61904 val_acc= 0.22687 time= 0.29100
Epoch: 0013 train_loss= 2.60248 train_acc= 0.19490 val_loss= 2.59445 val_acc= 0.24776 time= 0.28797
Epoch: 0014 train_loss= 2.56848 train_acc= 0.21674 val_loss= 2.57361 val_acc= 0.25970 time= 0.29303
Epoch: 0015 train_loss= 2.53972 train_acc= 0.23494 val_loss= 2.55177 val_acc= 0.27463 time= 0.28817
Epoch: 0016 train_loss= 2.51199 train_acc= 0.26042 val_loss= 2.52554 val_acc= 0.28657 time= 0.29600
Epoch: 0017 train_loss= 2.48209 train_acc= 0.28193 val_loss= 2.49372 val_acc= 0.29254 time= 0.29200
Epoch: 0018 train_loss= 2.44730 train_acc= 0.29947 val_loss= 2.45690 val_acc= 0.30149 time= 0.29600
Epoch: 0019 train_loss= 2.40684 train_acc= 0.31138 val_loss= 2.41667 val_acc= 0.30746 time= 0.29300
Epoch: 0020 train_loss= 2.36449 train_acc= 0.31800 val_loss= 2.37487 val_acc= 0.30746 time= 0.28900
Epoch: 0021 train_loss= 2.31996 train_acc= 0.32263 val_loss= 2.33286 val_acc= 0.31343 time= 0.29100
Epoch: 0022 train_loss= 2.27322 train_acc= 0.32892 val_loss= 2.29116 val_acc= 0.32239 time= 0.28800
Epoch: 0023 train_loss= 2.22600 train_acc= 0.33554 val_loss= 2.24950 val_acc= 0.32836 time= 0.29500
Epoch: 0024 train_loss= 2.17625 train_acc= 0.35175 val_loss= 2.20752 val_acc= 0.34627 time= 0.28900
Epoch: 0025 train_loss= 2.12331 train_acc= 0.38054 val_loss= 2.16525 val_acc= 0.36418 time= 0.29400
Epoch: 0026 train_loss= 2.06950 train_acc= 0.41595 val_loss= 2.12326 val_acc= 0.40597 time= 0.29997
Epoch: 0027 train_loss= 2.01390 train_acc= 0.46724 val_loss= 2.08189 val_acc= 0.43284 time= 0.29104
Epoch: 0028 train_loss= 1.95911 train_acc= 0.50827 val_loss= 2.04075 val_acc= 0.48060 time= 0.28899
Epoch: 0029 train_loss= 1.90393 train_acc= 0.54203 val_loss= 1.99896 val_acc= 0.50448 time= 0.29500
Epoch: 0030 train_loss= 1.84786 train_acc= 0.56684 val_loss= 1.95573 val_acc= 0.51343 time= 0.28997
Epoch: 0031 train_loss= 1.79009 train_acc= 0.58901 val_loss= 1.91105 val_acc= 0.52537 time= 0.29303
Epoch: 0032 train_loss= 1.73041 train_acc= 0.60258 val_loss= 1.86559 val_acc= 0.52836 time= 0.29397
Epoch: 0033 train_loss= 1.66850 train_acc= 0.61085 val_loss= 1.82072 val_acc= 0.52239 time= 0.28800
Epoch: 0034 train_loss= 1.61318 train_acc= 0.61846 val_loss= 1.77771 val_acc= 0.52836 time= 0.28900
Epoch: 0035 train_loss= 1.55450 train_acc= 0.63104 val_loss= 1.73713 val_acc= 0.53731 time= 0.28803
Epoch: 0036 train_loss= 1.49561 train_acc= 0.64328 val_loss= 1.69906 val_acc= 0.53433 time= 0.29200
Epoch: 0037 train_loss= 1.44228 train_acc= 0.65553 val_loss= 1.66319 val_acc= 0.54030 time= 0.29300
Epoch: 0038 train_loss= 1.38581 train_acc= 0.67009 val_loss= 1.62905 val_acc= 0.55522 time= 0.29100
Epoch: 0039 train_loss= 1.33230 train_acc= 0.68200 val_loss= 1.59629 val_acc= 0.56119 time= 0.28900
Epoch: 0040 train_loss= 1.27927 train_acc= 0.69424 val_loss= 1.56423 val_acc= 0.57313 time= 0.29297
Epoch: 0041 train_loss= 1.22660 train_acc= 0.70351 val_loss= 1.53260 val_acc= 0.58209 time= 0.29503
Epoch: 0042 train_loss= 1.17461 train_acc= 0.71939 val_loss= 1.50176 val_acc= 0.58209 time= 0.28800
Epoch: 0043 train_loss= 1.12584 train_acc= 0.73031 val_loss= 1.47268 val_acc= 0.58507 time= 0.29500
Epoch: 0044 train_loss= 1.07692 train_acc= 0.74322 val_loss= 1.44592 val_acc= 0.58507 time= 0.29145
Epoch: 0045 train_loss= 1.02949 train_acc= 0.75778 val_loss= 1.42166 val_acc= 0.59104 time= 0.28900
Epoch: 0046 train_loss= 0.98408 train_acc= 0.76770 val_loss= 1.39945 val_acc= 0.59403 time= 0.28900
Epoch: 0047 train_loss= 0.94041 train_acc= 0.77962 val_loss= 1.37893 val_acc= 0.59403 time= 0.29300
Epoch: 0048 train_loss= 0.89690 train_acc= 0.78756 val_loss= 1.35969 val_acc= 0.59701 time= 0.29274
Epoch: 0049 train_loss= 0.85533 train_acc= 0.79881 val_loss= 1.34131 val_acc= 0.60896 time= 0.28868
Epoch: 0050 train_loss= 0.81476 train_acc= 0.81337 val_loss= 1.32323 val_acc= 0.61194 time= 0.28900
Epoch: 0051 train_loss= 0.77637 train_acc= 0.81966 val_loss= 1.30616 val_acc= 0.61493 time= 0.29900
Epoch: 0052 train_loss= 0.73650 train_acc= 0.83488 val_loss= 1.29024 val_acc= 0.61791 time= 0.29000
Epoch: 0053 train_loss= 0.70174 train_acc= 0.84183 val_loss= 1.27563 val_acc= 0.61791 time= 0.28707
Epoch: 0054 train_loss= 0.66549 train_acc= 0.84911 val_loss= 1.26286 val_acc= 0.62687 time= 0.29203
Epoch: 0055 train_loss= 0.63385 train_acc= 0.85837 val_loss= 1.25203 val_acc= 0.62388 time= 0.29097
Epoch: 0056 train_loss= 0.59988 train_acc= 0.86698 val_loss= 1.24231 val_acc= 0.63582 time= 0.28803
Epoch: 0057 train_loss= 0.57114 train_acc= 0.87426 val_loss= 1.23411 val_acc= 0.63881 time= 0.29100
Epoch: 0058 train_loss= 0.53973 train_acc= 0.88518 val_loss= 1.22641 val_acc= 0.64478 time= 0.29497
Epoch: 0059 train_loss= 0.51272 train_acc= 0.89212 val_loss= 1.21874 val_acc= 0.64478 time= 0.29403
Epoch: 0060 train_loss= 0.48606 train_acc= 0.89874 val_loss= 1.21118 val_acc= 0.64776 time= 0.29102
Epoch: 0061 train_loss= 0.46061 train_acc= 0.90900 val_loss= 1.20322 val_acc= 0.64478 time= 0.29099
Epoch: 0062 train_loss= 0.43669 train_acc= 0.91430 val_loss= 1.19593 val_acc= 0.65373 time= 0.29657
Epoch: 0063 train_loss= 0.41361 train_acc= 0.91760 val_loss= 1.19032 val_acc= 0.65970 time= 0.28797
Epoch: 0064 train_loss= 0.39160 train_acc= 0.92654 val_loss= 1.18703 val_acc= 0.65672 time= 0.28749
Epoch: 0065 train_loss= 0.37155 train_acc= 0.93084 val_loss= 1.18522 val_acc= 0.65970 time= 0.29298
Epoch: 0066 train_loss= 0.35146 train_acc= 0.93547 val_loss= 1.18326 val_acc= 0.66567 time= 0.29200
Epoch: 0067 train_loss= 0.33244 train_acc= 0.94275 val_loss= 1.18143 val_acc= 0.65970 time= 0.28900
Epoch: 0068 train_loss= 0.31410 train_acc= 0.94375 val_loss= 1.18020 val_acc= 0.65970 time= 0.28897
Epoch: 0069 train_loss= 0.29841 train_acc= 0.94805 val_loss= 1.17919 val_acc= 0.66269 time= 0.29603
Epoch: 0070 train_loss= 0.28230 train_acc= 0.95169 val_loss= 1.17878 val_acc= 0.66567 time= 0.29000
Epoch: 0071 train_loss= 0.26895 train_acc= 0.95235 val_loss= 1.17879 val_acc= 0.66269 time= 0.28597
Epoch: 0072 train_loss= 0.25390 train_acc= 0.95764 val_loss= 1.17984 val_acc= 0.65970 time= 0.28803
Epoch: 0073 train_loss= 0.23956 train_acc= 0.96062 val_loss= 1.18123 val_acc= 0.66269 time= 0.29497
Epoch: 0074 train_loss= 0.22897 train_acc= 0.96261 val_loss= 1.18323 val_acc= 0.67164 time= 0.28803
Early stopping...
Optimization Finished!
Test set results: cost= 1.16240 accuracy= 0.68810 time= 0.12697
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7270    0.6930    0.7096       342
           1     0.6875    0.7476    0.7163       103
           2     0.7632    0.6214    0.6850       140
           3     0.6207    0.4557    0.5255        79
           4     0.6471    0.7500    0.6947       132
           5     0.6739    0.7923    0.7283       313
           6     0.7059    0.7059    0.7059       102
           7     0.6216    0.3286    0.4299        70
           8     0.5676    0.4200    0.4828        50
           9     0.6053    0.7419    0.6667       155
          10     0.8500    0.6364    0.7278       187
          11     0.6203    0.6364    0.6282       231
          12     0.7647    0.7303    0.7471       178
          13     0.7714    0.8100    0.7902       600
          14     0.7741    0.8424    0.8068       590
          15     0.7857    0.7237    0.7534        76
          16     0.6875    0.3235    0.4400        34
          17     0.5000    0.1000    0.1667        10
          18     0.4386    0.4773    0.4571       419
          19     0.6296    0.5271    0.5738       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.7241    0.8400        29
          22     0.5600    0.3043    0.3944        46

    accuracy                         0.6881      4043
   macro avg     0.6792    0.5972    0.6218      4043
weighted avg     0.6907    0.6881    0.6844      4043

Macro average Test Precision, Recall and F1-Score...
(0.6792308245320116, 0.5971675689701177, 0.6218252090054325, None)
Micro average Test Precision, Recall and F1-Score...
(0.6881028938906752, 0.6881028938906752, 0.6881028938906752, None)
embeddings:
14157 3357 4043
[[ 0.45456675  0.36188784  0.53684783 ...  0.33056408  0.59874195
   0.4267706 ]
 [-0.00620962  0.04288122  0.2678746  ... -0.06198714  0.43024302
   0.06057484]
 [ 0.30539662  0.23350386  0.63425845 ...  0.42644987  0.470468
   0.34886315]
 ...
 [ 0.07490425  0.10866804  0.13323931 ...  0.15142287  0.27596834
   0.28117946]
 [ 0.08892454  0.22452645  0.38093537 ...  0.20149764  0.41923365
   0.02694316]
 [ 0.24489816  0.06278629  0.05645173 ...  0.19452257  0.35993743
   0.16487566]]

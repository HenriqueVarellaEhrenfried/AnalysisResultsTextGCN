(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13554 train_acc= 0.03839 val_loss= 3.11901 val_acc= 0.28358 time= 0.58498
Epoch: 0002 train_loss= 3.11844 train_acc= 0.26903 val_loss= 3.07999 val_acc= 0.32537 time= 0.29000
Epoch: 0003 train_loss= 3.07880 train_acc= 0.31502 val_loss= 3.01734 val_acc= 0.34627 time= 0.28900
Epoch: 0004 train_loss= 3.01520 train_acc= 0.32660 val_loss= 2.93580 val_acc= 0.33433 time= 0.28807
Epoch: 0005 train_loss= 2.93410 train_acc= 0.30377 val_loss= 2.84846 val_acc= 0.26269 time= 0.29000
Epoch: 0006 train_loss= 2.84787 train_acc= 0.25745 val_loss= 2.77150 val_acc= 0.22687 time= 0.28700
Epoch: 0007 train_loss= 2.77305 train_acc= 0.21343 val_loss= 2.71887 val_acc= 0.22687 time= 0.29300
Epoch: 0008 train_loss= 2.72354 train_acc= 0.22105 val_loss= 2.69557 val_acc= 0.30149 time= 0.28897
Epoch: 0009 train_loss= 2.70240 train_acc= 0.29285 val_loss= 2.68789 val_acc= 0.25075 time= 0.28911
Epoch: 0010 train_loss= 2.69606 train_acc= 0.22071 val_loss= 2.67923 val_acc= 0.20896 time= 0.28997
Epoch: 0011 train_loss= 2.68730 train_acc= 0.18134 val_loss= 2.66024 val_acc= 0.20597 time= 0.28885
Epoch: 0012 train_loss= 2.66294 train_acc= 0.17406 val_loss= 2.63193 val_acc= 0.20597 time= 0.29100
Epoch: 0013 train_loss= 2.62769 train_acc= 0.17538 val_loss= 2.60135 val_acc= 0.20896 time= 0.28607
Epoch: 0014 train_loss= 2.58698 train_acc= 0.18101 val_loss= 2.57364 val_acc= 0.22687 time= 0.29104
Epoch: 0015 train_loss= 2.55348 train_acc= 0.19226 val_loss= 2.54872 val_acc= 0.24776 time= 0.29000
Epoch: 0016 train_loss= 2.52220 train_acc= 0.21641 val_loss= 2.52372 val_acc= 0.26567 time= 0.28776
Epoch: 0017 train_loss= 2.49202 train_acc= 0.25347 val_loss= 2.49577 val_acc= 0.28657 time= 0.28700
Epoch: 0018 train_loss= 2.45902 train_acc= 0.28921 val_loss= 2.46353 val_acc= 0.30746 time= 0.29405
Epoch: 0019 train_loss= 2.42285 train_acc= 0.31899 val_loss= 2.42707 val_acc= 0.32836 time= 0.29000
Epoch: 0020 train_loss= 2.38359 train_acc= 0.33752 val_loss= 2.38718 val_acc= 0.33731 time= 0.28800
Epoch: 0021 train_loss= 2.34001 train_acc= 0.35308 val_loss= 2.34498 val_acc= 0.34925 time= 0.29200
Epoch: 0022 train_loss= 2.29314 train_acc= 0.36466 val_loss= 2.30159 val_acc= 0.35224 time= 0.29211
Epoch: 0023 train_loss= 2.24148 train_acc= 0.37392 val_loss= 2.25762 val_acc= 0.35821 time= 0.29000
Epoch: 0024 train_loss= 2.19424 train_acc= 0.38352 val_loss= 2.21353 val_acc= 0.36119 time= 0.28600
Epoch: 0025 train_loss= 2.14044 train_acc= 0.38749 val_loss= 2.16970 val_acc= 0.38209 time= 0.29000
Epoch: 0026 train_loss= 2.08740 train_acc= 0.40602 val_loss= 2.12636 val_acc= 0.40000 time= 0.29000
Epoch: 0027 train_loss= 2.03740 train_acc= 0.43547 val_loss= 2.08366 val_acc= 0.42687 time= 0.28700
Epoch: 0028 train_loss= 1.97926 train_acc= 0.47055 val_loss= 2.04168 val_acc= 0.45373 time= 0.28800
Epoch: 0029 train_loss= 1.92629 train_acc= 0.51026 val_loss= 2.00032 val_acc= 0.48955 time= 0.29600
Epoch: 0030 train_loss= 1.87297 train_acc= 0.54269 val_loss= 1.95915 val_acc= 0.51045 time= 0.28900
Epoch: 0031 train_loss= 1.81926 train_acc= 0.56651 val_loss= 1.91749 val_acc= 0.51940 time= 0.28500
Epoch: 0032 train_loss= 1.75931 train_acc= 0.58339 val_loss= 1.87518 val_acc= 0.51642 time= 0.29400
Epoch: 0033 train_loss= 1.69254 train_acc= 0.60424 val_loss= 1.83244 val_acc= 0.51343 time= 0.28983
Epoch: 0034 train_loss= 1.64235 train_acc= 0.61317 val_loss= 1.78977 val_acc= 0.52239 time= 0.28800
Epoch: 0035 train_loss= 1.58776 train_acc= 0.61780 val_loss= 1.74832 val_acc= 0.53433 time= 0.28900
Epoch: 0036 train_loss= 1.52362 train_acc= 0.63700 val_loss= 1.70892 val_acc= 0.54030 time= 0.29101
Epoch: 0037 train_loss= 1.47430 train_acc= 0.64758 val_loss= 1.67160 val_acc= 0.55224 time= 0.29015
Epoch: 0038 train_loss= 1.42432 train_acc= 0.65486 val_loss= 1.63631 val_acc= 0.56119 time= 0.28700
Epoch: 0039 train_loss= 1.37743 train_acc= 0.66446 val_loss= 1.60285 val_acc= 0.56418 time= 0.28907
Epoch: 0040 train_loss= 1.32134 train_acc= 0.67836 val_loss= 1.57231 val_acc= 0.57910 time= 0.29600
Epoch: 0041 train_loss= 1.27361 train_acc= 0.68762 val_loss= 1.54362 val_acc= 0.58209 time= 0.29000
Epoch: 0042 train_loss= 1.22620 train_acc= 0.69656 val_loss= 1.51679 val_acc= 0.58507 time= 0.28804
Epoch: 0043 train_loss= 1.18186 train_acc= 0.71310 val_loss= 1.49016 val_acc= 0.58507 time= 0.29307
Epoch: 0044 train_loss= 1.13749 train_acc= 0.72038 val_loss= 1.46437 val_acc= 0.59104 time= 0.29600
Epoch: 0045 train_loss= 1.09188 train_acc= 0.73428 val_loss= 1.43899 val_acc= 0.59104 time= 0.28871
Epoch: 0046 train_loss= 1.04662 train_acc= 0.74255 val_loss= 1.41447 val_acc= 0.59701 time= 0.28900
Epoch: 0047 train_loss= 1.00884 train_acc= 0.75414 val_loss= 1.39110 val_acc= 0.60299 time= 0.29400
Epoch: 0048 train_loss= 0.96675 train_acc= 0.76473 val_loss= 1.37005 val_acc= 0.60299 time= 0.28800
Epoch: 0049 train_loss= 0.92886 train_acc= 0.77565 val_loss= 1.35117 val_acc= 0.61194 time= 0.29000
Epoch: 0050 train_loss= 0.89291 train_acc= 0.78326 val_loss= 1.33461 val_acc= 0.61194 time= 0.28800
Epoch: 0051 train_loss= 0.85804 train_acc= 0.79153 val_loss= 1.31922 val_acc= 0.61791 time= 0.29600
Epoch: 0052 train_loss= 0.81922 train_acc= 0.78954 val_loss= 1.30469 val_acc= 0.62090 time= 0.28800
Epoch: 0053 train_loss= 0.78704 train_acc= 0.81238 val_loss= 1.28955 val_acc= 0.62388 time= 0.28900
Epoch: 0054 train_loss= 0.74813 train_acc= 0.82197 val_loss= 1.27549 val_acc= 0.62090 time= 0.29403
Epoch: 0055 train_loss= 0.71896 train_acc= 0.82925 val_loss= 1.26380 val_acc= 0.62985 time= 0.28900
Epoch: 0056 train_loss= 0.69313 train_acc= 0.83289 val_loss= 1.25305 val_acc= 0.63582 time= 0.28800
Epoch: 0057 train_loss= 0.66442 train_acc= 0.84447 val_loss= 1.24394 val_acc= 0.64179 time= 0.28800
Epoch: 0058 train_loss= 0.63544 train_acc= 0.85043 val_loss= 1.23522 val_acc= 0.63881 time= 0.29100
Epoch: 0059 train_loss= 0.60972 train_acc= 0.85804 val_loss= 1.22593 val_acc= 0.63284 time= 0.28700
Epoch: 0060 train_loss= 0.58174 train_acc= 0.86863 val_loss= 1.21679 val_acc= 0.65075 time= 0.28700
Epoch: 0061 train_loss= 0.55577 train_acc= 0.87062 val_loss= 1.20914 val_acc= 0.64776 time= 0.29200
Epoch: 0062 train_loss= 0.53353 train_acc= 0.88154 val_loss= 1.20116 val_acc= 0.64478 time= 0.29200
Epoch: 0063 train_loss= 0.50783 train_acc= 0.88385 val_loss= 1.19465 val_acc= 0.64776 time= 0.28700
Epoch: 0064 train_loss= 0.48595 train_acc= 0.89378 val_loss= 1.19009 val_acc= 0.65970 time= 0.29108
Epoch: 0065 train_loss= 0.46805 train_acc= 0.89610 val_loss= 1.18482 val_acc= 0.65373 time= 0.29703
Epoch: 0066 train_loss= 0.45043 train_acc= 0.90371 val_loss= 1.18005 val_acc= 0.65075 time= 0.28897
Epoch: 0067 train_loss= 0.43079 train_acc= 0.90867 val_loss= 1.17730 val_acc= 0.65373 time= 0.29103
Epoch: 0068 train_loss= 0.40997 train_acc= 0.91099 val_loss= 1.17363 val_acc= 0.65373 time= 0.29097
Epoch: 0069 train_loss= 0.39811 train_acc= 0.91529 val_loss= 1.17156 val_acc= 0.65373 time= 0.29303
Epoch: 0070 train_loss= 0.38171 train_acc= 0.92025 val_loss= 1.17039 val_acc= 0.66567 time= 0.29000
Epoch: 0071 train_loss= 0.36343 train_acc= 0.92786 val_loss= 1.16812 val_acc= 0.65970 time= 0.28800
Epoch: 0072 train_loss= 0.34667 train_acc= 0.93613 val_loss= 1.16348 val_acc= 0.66567 time= 0.28965
Epoch: 0073 train_loss= 0.33637 train_acc= 0.93283 val_loss= 1.15878 val_acc= 0.65970 time= 0.29297
Epoch: 0074 train_loss= 0.32066 train_acc= 0.93746 val_loss= 1.15642 val_acc= 0.65970 time= 0.29103
Epoch: 0075 train_loss= 0.31013 train_acc= 0.93878 val_loss= 1.15566 val_acc= 0.67761 time= 0.29097
Epoch: 0076 train_loss= 0.29718 train_acc= 0.94672 val_loss= 1.15781 val_acc= 0.68358 time= 0.29203
Epoch: 0077 train_loss= 0.28265 train_acc= 0.94507 val_loss= 1.16053 val_acc= 0.67164 time= 0.28697
Epoch: 0078 train_loss= 0.26884 train_acc= 0.95301 val_loss= 1.16341 val_acc= 0.66866 time= 0.29012
Epoch: 0079 train_loss= 0.25929 train_acc= 0.95467 val_loss= 1.16576 val_acc= 0.66866 time= 0.28904
Early stopping...
Optimization Finished!
Test set results: cost= 1.15677 accuracy= 0.68439 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7160    0.6930    0.7043       342
           1     0.7000    0.7476    0.7230       103
           2     0.8081    0.5714    0.6695       140
           3     0.6111    0.4177    0.4962        79
           4     0.6456    0.7727    0.7034       132
           5     0.6667    0.7923    0.7241       313
           6     0.6667    0.7451    0.7037       102
           7     0.6176    0.3000    0.4038        70
           8     0.5882    0.4000    0.4762        50
           9     0.6509    0.7097    0.6790       155
          10     0.8403    0.6471    0.7311       187
          11     0.6400    0.6234    0.6316       231
          12     0.7619    0.7191    0.7399       178
          13     0.7581    0.8150    0.7855       600
          14     0.7774    0.8407    0.8078       590
          15     0.7671    0.7368    0.7517        76
          16     0.7857    0.3235    0.4583        34
          17     0.5000    0.1000    0.1667        10
          18     0.4179    0.4678    0.4414       419
          19     0.6442    0.5194    0.5751       129
          20     0.6800    0.6071    0.6415        28
          21     1.0000    0.7241    0.8400        29
          22     0.4000    0.3478    0.3721        46

    accuracy                         0.6844      4043
   macro avg     0.6802    0.5922    0.6185      4043
weighted avg     0.6883    0.6844    0.6807      4043

Macro average Test Precision, Recall and F1-Score...
(0.6801568354915052, 0.5922345028295269, 0.6185247367088206, None)
Micro average Test Precision, Recall and F1-Score...
(0.6843927776403661, 0.6843927776403661, 0.6843927776403661, None)
embeddings:
14157 3357 4043
[[ 5.04799664e-01  3.71956319e-01  3.97329837e-01 ...  4.05942261e-01
   4.93725926e-01  5.83805859e-01]
 [ 1.53880149e-01 -5.72606325e-02  1.53279006e-01 ...  2.61500925e-01
   2.61914939e-01  2.94610143e-01]
 [ 1.36448011e-01  1.08541586e-01  3.01447153e-01 ...  4.87475723e-01
   2.52357274e-01  2.28769839e-01]
 ...
 [ 3.18964332e-01  2.49930739e-01  2.43587688e-01 ...  1.71523571e-01
   1.48279980e-01  2.06703305e-01]
 [ 4.91861403e-02  1.38892099e-01  8.11490193e-02 ...  8.40101764e-02
   4.63401794e-01 -2.40663923e-02]
 [ 3.16470474e-01  3.73776078e-01  1.14757568e-04 ...  2.70677954e-01
   1.13008827e-01  3.82416546e-01]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13546 train_acc= 0.05791 val_loss= 3.11118 val_acc= 0.20896 time= 0.58696
Epoch: 0002 train_loss= 3.11128 train_acc= 0.18531 val_loss= 3.06087 val_acc= 0.21194 time= 0.29400
Epoch: 0003 train_loss= 3.06105 train_acc= 0.18696 val_loss= 2.98617 val_acc= 0.21791 time= 0.29097
Epoch: 0004 train_loss= 2.98666 train_acc= 0.18928 val_loss= 2.89532 val_acc= 0.22687 time= 0.29003
Epoch: 0005 train_loss= 2.89699 train_acc= 0.19358 val_loss= 2.80484 val_acc= 0.22985 time= 0.29097
Epoch: 0006 train_loss= 2.80915 train_acc= 0.19788 val_loss= 2.73457 val_acc= 0.23582 time= 0.29500
Epoch: 0007 train_loss= 2.74100 train_acc= 0.20516 val_loss= 2.69901 val_acc= 0.24179 time= 0.29403
Epoch: 0008 train_loss= 2.70695 train_acc= 0.21046 val_loss= 2.69122 val_acc= 0.22985 time= 0.29002
Epoch: 0009 train_loss= 2.70035 train_acc= 0.19656 val_loss= 2.68773 val_acc= 0.20896 time= 0.29410
Epoch: 0010 train_loss= 2.69604 train_acc= 0.18365 val_loss= 2.67206 val_acc= 0.20896 time= 0.29300
Epoch: 0011 train_loss= 2.67497 train_acc= 0.17935 val_loss= 2.64401 val_acc= 0.20896 time= 0.29600
Epoch: 0012 train_loss= 2.63868 train_acc= 0.18498 val_loss= 2.61288 val_acc= 0.22687 time= 0.29100
Epoch: 0013 train_loss= 2.59726 train_acc= 0.19325 val_loss= 2.58446 val_acc= 0.24179 time= 0.29200
Epoch: 0014 train_loss= 2.55976 train_acc= 0.21079 val_loss= 2.55889 val_acc= 0.26269 time= 0.29300
Epoch: 0015 train_loss= 2.52550 train_acc= 0.23329 val_loss= 2.53322 val_acc= 0.27761 time= 0.28900
Epoch: 0016 train_loss= 2.49321 train_acc= 0.25910 val_loss= 2.50437 val_acc= 0.29254 time= 0.28800
Epoch: 0017 train_loss= 2.46036 train_acc= 0.28425 val_loss= 2.47070 val_acc= 0.30746 time= 0.29500
Epoch: 0018 train_loss= 2.42196 train_acc= 0.30443 val_loss= 2.43202 val_acc= 0.31343 time= 0.29300
Epoch: 0019 train_loss= 2.37823 train_acc= 0.32793 val_loss= 2.38918 val_acc= 0.31940 time= 0.28900
Epoch: 0020 train_loss= 2.33087 train_acc= 0.34679 val_loss= 2.34347 val_acc= 0.33134 time= 0.29400
Epoch: 0021 train_loss= 2.28206 train_acc= 0.36201 val_loss= 2.29613 val_acc= 0.34030 time= 0.29400
Epoch: 0022 train_loss= 2.22715 train_acc= 0.37128 val_loss= 2.24829 val_acc= 0.34328 time= 0.28800
Epoch: 0023 train_loss= 2.17438 train_acc= 0.37756 val_loss= 2.20083 val_acc= 0.35224 time= 0.28810
Epoch: 0024 train_loss= 2.11688 train_acc= 0.38948 val_loss= 2.15416 val_acc= 0.36119 time= 0.29200
Epoch: 0025 train_loss= 2.06008 train_acc= 0.41297 val_loss= 2.10832 val_acc= 0.38507 time= 0.29400
Epoch: 0026 train_loss= 2.00406 train_acc= 0.44308 val_loss= 2.06342 val_acc= 0.40896 time= 0.28807
Epoch: 0027 train_loss= 1.94383 train_acc= 0.48180 val_loss= 2.01953 val_acc= 0.46567 time= 0.29140
Epoch: 0028 train_loss= 1.88529 train_acc= 0.52680 val_loss= 1.97640 val_acc= 0.48955 time= 0.29799
Epoch: 0029 train_loss= 1.82541 train_acc= 0.56320 val_loss= 1.93315 val_acc= 0.51642 time= 0.29103
Epoch: 0030 train_loss= 1.76723 train_acc= 0.58703 val_loss= 1.88897 val_acc= 0.52537 time= 0.28900
Epoch: 0031 train_loss= 1.70627 train_acc= 0.60589 val_loss= 1.84359 val_acc= 0.52836 time= 0.29705
Epoch: 0032 train_loss= 1.64639 train_acc= 0.61317 val_loss= 1.79762 val_acc= 0.53134 time= 0.29555
Epoch: 0033 train_loss= 1.58629 train_acc= 0.62277 val_loss= 1.75265 val_acc= 0.53731 time= 0.29103
Epoch: 0034 train_loss= 1.52711 train_acc= 0.62938 val_loss= 1.71018 val_acc= 0.54627 time= 0.28769
Epoch: 0035 train_loss= 1.46873 train_acc= 0.64626 val_loss= 1.67073 val_acc= 0.54627 time= 0.29700
Epoch: 0036 train_loss= 1.41547 train_acc= 0.65884 val_loss= 1.63375 val_acc= 0.54925 time= 0.28900
Epoch: 0037 train_loss= 1.36187 train_acc= 0.66876 val_loss= 1.59892 val_acc= 0.56119 time= 0.29000
Epoch: 0038 train_loss= 1.30701 train_acc= 0.68398 val_loss= 1.56656 val_acc= 0.57313 time= 0.29205
Epoch: 0039 train_loss= 1.25523 train_acc= 0.69523 val_loss= 1.53676 val_acc= 0.57313 time= 0.29600
Epoch: 0040 train_loss= 1.20366 train_acc= 0.70384 val_loss= 1.50898 val_acc= 0.58209 time= 0.29100
Epoch: 0041 train_loss= 1.15490 train_acc= 0.71443 val_loss= 1.48211 val_acc= 0.59104 time= 0.28800
Epoch: 0042 train_loss= 1.10703 train_acc= 0.72634 val_loss= 1.45575 val_acc= 0.58507 time= 0.30000
Epoch: 0043 train_loss= 1.06116 train_acc= 0.73891 val_loss= 1.43013 val_acc= 0.58507 time= 0.28900
Epoch: 0044 train_loss= 1.01525 train_acc= 0.75612 val_loss= 1.40555 val_acc= 0.59104 time= 0.29200
Epoch: 0045 train_loss= 0.97019 train_acc= 0.76704 val_loss= 1.38225 val_acc= 0.60299 time= 0.29300
Epoch: 0046 train_loss= 0.92978 train_acc= 0.77664 val_loss= 1.36110 val_acc= 0.60597 time= 0.29600
Epoch: 0047 train_loss= 0.88996 train_acc= 0.79186 val_loss= 1.34246 val_acc= 0.60896 time= 0.28900
Epoch: 0048 train_loss= 0.84817 train_acc= 0.80212 val_loss= 1.32558 val_acc= 0.61194 time= 0.29000
Epoch: 0049 train_loss= 0.81281 train_acc= 0.81271 val_loss= 1.31032 val_acc= 0.61493 time= 0.29000
Epoch: 0050 train_loss= 0.77459 train_acc= 0.82230 val_loss= 1.29644 val_acc= 0.61791 time= 0.29200
Epoch: 0051 train_loss= 0.73684 train_acc= 0.83388 val_loss= 1.28376 val_acc= 0.62388 time= 0.28900
Epoch: 0052 train_loss= 0.70334 train_acc= 0.84381 val_loss= 1.27160 val_acc= 0.62090 time= 0.28900
Epoch: 0053 train_loss= 0.67227 train_acc= 0.85043 val_loss= 1.25969 val_acc= 0.62090 time= 0.29500
Epoch: 0054 train_loss= 0.63942 train_acc= 0.85341 val_loss= 1.24875 val_acc= 0.62388 time= 0.28800
Epoch: 0055 train_loss= 0.60902 train_acc= 0.86433 val_loss= 1.23862 val_acc= 0.62090 time= 0.29000
Epoch: 0056 train_loss= 0.57989 train_acc= 0.87260 val_loss= 1.22952 val_acc= 0.62090 time= 0.28909
Epoch: 0057 train_loss= 0.55247 train_acc= 0.88054 val_loss= 1.22195 val_acc= 0.62090 time= 0.29700
Epoch: 0058 train_loss= 0.52458 train_acc= 0.88882 val_loss= 1.21523 val_acc= 0.62985 time= 0.29007
Epoch: 0059 train_loss= 0.50043 train_acc= 0.89378 val_loss= 1.20909 val_acc= 0.64179 time= 0.28987
Epoch: 0060 train_loss= 0.47625 train_acc= 0.90139 val_loss= 1.20312 val_acc= 0.64478 time= 0.29400
Epoch: 0061 train_loss= 0.45226 train_acc= 0.90668 val_loss= 1.19717 val_acc= 0.64776 time= 0.29200
Epoch: 0062 train_loss= 0.43139 train_acc= 0.91198 val_loss= 1.19201 val_acc= 0.65373 time= 0.29100
Epoch: 0063 train_loss= 0.40803 train_acc= 0.91529 val_loss= 1.18808 val_acc= 0.65373 time= 0.29000
Epoch: 0064 train_loss= 0.38890 train_acc= 0.92224 val_loss= 1.18521 val_acc= 0.65672 time= 0.29600
Epoch: 0065 train_loss= 0.36981 train_acc= 0.93051 val_loss= 1.18360 val_acc= 0.65672 time= 0.29200
Epoch: 0066 train_loss= 0.35092 train_acc= 0.93415 val_loss= 1.18249 val_acc= 0.65970 time= 0.29000
Epoch: 0067 train_loss= 0.33371 train_acc= 0.93911 val_loss= 1.18129 val_acc= 0.65672 time= 0.29300
Epoch: 0068 train_loss= 0.31811 train_acc= 0.94242 val_loss= 1.17950 val_acc= 0.66269 time= 0.29400
Epoch: 0069 train_loss= 0.30238 train_acc= 0.94341 val_loss= 1.17779 val_acc= 0.66866 time= 0.29396
Epoch: 0070 train_loss= 0.28820 train_acc= 0.94970 val_loss= 1.17696 val_acc= 0.67463 time= 0.29000
Epoch: 0071 train_loss= 0.27506 train_acc= 0.95301 val_loss= 1.17664 val_acc= 0.67463 time= 0.29745
Epoch: 0072 train_loss= 0.26008 train_acc= 0.95831 val_loss= 1.17749 val_acc= 0.67463 time= 0.29103
Epoch: 0073 train_loss= 0.24705 train_acc= 0.95930 val_loss= 1.17866 val_acc= 0.67463 time= 0.29109
Epoch: 0074 train_loss= 0.23746 train_acc= 0.96294 val_loss= 1.18026 val_acc= 0.67164 time= 0.29203
Early stopping...
Optimization Finished!
Test set results: cost= 1.16190 accuracy= 0.68884 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7278    0.6959    0.7115       342
           1     0.7130    0.7476    0.7299       103
           2     0.7478    0.6143    0.6745       140
           3     0.6000    0.4177    0.4925        79
           4     0.6824    0.7652    0.7214       132
           5     0.6947    0.7923    0.7403       313
           6     0.7019    0.7157    0.7087       102
           7     0.5946    0.3143    0.4112        70
           8     0.5667    0.3400    0.4250        50
           9     0.6154    0.7226    0.6647       155
          10     0.8561    0.6364    0.7301       187
          11     0.6056    0.6580    0.6307       231
          12     0.7683    0.7079    0.7368       178
          13     0.7701    0.8150    0.7919       600
          14     0.7780    0.8492    0.8120       590
          15     0.7571    0.6974    0.7260        76
          16     0.7059    0.3529    0.4706        34
          17     1.0000    0.1000    0.1818        10
          18     0.4298    0.4893    0.4576       419
          19     0.6204    0.5194    0.5654       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.7241    0.8400        29
          22     0.5833    0.3043    0.4000        46

    accuracy                         0.6888      4043
   macro avg     0.7017    0.5923    0.6198      4043
weighted avg     0.6929    0.6888    0.6851      4043

Macro average Test Precision, Recall and F1-Score...
(0.701717371523496, 0.5922697995643539, 0.6197517855784193, None)
Micro average Test Precision, Recall and F1-Score...
(0.688844917140737, 0.688844917140737, 0.688844917140737, None)
embeddings:
14157 3357 4043
[[4.88233060e-01 4.29764062e-01 4.79705602e-01 ... 5.21321952e-01
  5.35963595e-01 5.98503768e-01]
 [6.37274161e-02 3.24316323e-04 6.38447106e-02 ... 2.90976703e-01
  3.08763444e-01 2.06513375e-01]
 [4.10131425e-01 1.17846407e-01 7.47374445e-02 ... 5.46820581e-01
  2.24137008e-01 3.19427490e-01]
 ...
 [2.82658607e-01 1.50700405e-01 1.14857495e-01 ... 2.44801223e-01
  2.07154736e-01 3.02577823e-01]
 [1.39156476e-01 2.70594984e-01 3.75928372e-01 ... 8.38500857e-02
  4.87811267e-01 3.47886905e-02]
 [1.26840860e-01 2.13805109e-01 2.70103127e-01 ... 7.66435787e-02
  2.56684750e-01 4.28944618e-01]]

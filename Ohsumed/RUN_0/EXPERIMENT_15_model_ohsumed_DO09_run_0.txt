(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13546 train_acc= 0.05460 val_loss= 3.11484 val_acc= 0.20000 time= 0.57992
Epoch: 0002 train_loss= 3.11537 train_acc= 0.17141 val_loss= 3.06883 val_acc= 0.20000 time= 0.28901
Epoch: 0003 train_loss= 3.06993 train_acc= 0.17141 val_loss= 2.99948 val_acc= 0.20000 time= 0.29102
Epoch: 0004 train_loss= 3.00050 train_acc= 0.17141 val_loss= 2.91353 val_acc= 0.20000 time= 0.28881
Epoch: 0005 train_loss= 2.91844 train_acc= 0.17141 val_loss= 2.82502 val_acc= 0.20000 time= 0.29105
Epoch: 0006 train_loss= 2.83497 train_acc= 0.17141 val_loss= 2.75125 val_acc= 0.20000 time= 0.29100
Epoch: 0007 train_loss= 2.76402 train_acc= 0.17141 val_loss= 2.70598 val_acc= 0.20000 time= 0.29400
Epoch: 0008 train_loss= 2.72247 train_acc= 0.17141 val_loss= 2.69504 val_acc= 0.20000 time= 0.28800
Epoch: 0009 train_loss= 2.71473 train_acc= 0.17141 val_loss= 2.70071 val_acc= 0.20000 time= 0.28800
Epoch: 0010 train_loss= 2.72757 train_acc= 0.17141 val_loss= 2.69639 val_acc= 0.20000 time= 0.29200
Epoch: 0011 train_loss= 2.71505 train_acc= 0.17174 val_loss= 2.67693 val_acc= 0.20299 time= 0.29400
Epoch: 0012 train_loss= 2.68599 train_acc= 0.17505 val_loss= 2.65202 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.65462 train_acc= 0.18299 val_loss= 2.62997 val_acc= 0.22985 time= 0.28706
Epoch: 0014 train_loss= 2.61935 train_acc= 0.20086 val_loss= 2.61283 val_acc= 0.25075 time= 0.29111
Epoch: 0015 train_loss= 2.59918 train_acc= 0.21145 val_loss= 2.59758 val_acc= 0.26567 time= 0.29100
Epoch: 0016 train_loss= 2.57389 train_acc= 0.24355 val_loss= 2.58028 val_acc= 0.27463 time= 0.28997
Epoch: 0017 train_loss= 2.55605 train_acc= 0.25381 val_loss= 2.55851 val_acc= 0.28358 time= 0.29004
Epoch: 0018 train_loss= 2.52463 train_acc= 0.26704 val_loss= 2.53165 val_acc= 0.28955 time= 0.29502
Epoch: 0019 train_loss= 2.50217 train_acc= 0.28789 val_loss= 2.50056 val_acc= 0.29851 time= 0.28688
Epoch: 0020 train_loss= 2.47157 train_acc= 0.30113 val_loss= 2.46658 val_acc= 0.30448 time= 0.28800
Epoch: 0021 train_loss= 2.43530 train_acc= 0.29484 val_loss= 2.43124 val_acc= 0.31045 time= 0.29000
Epoch: 0022 train_loss= 2.39401 train_acc= 0.30907 val_loss= 2.39541 val_acc= 0.30746 time= 0.29300
Epoch: 0023 train_loss= 2.36233 train_acc= 0.31602 val_loss= 2.35971 val_acc= 0.31045 time= 0.28800
Epoch: 0024 train_loss= 2.32057 train_acc= 0.30410 val_loss= 2.32442 val_acc= 0.32239 time= 0.28900
Epoch: 0025 train_loss= 2.27807 train_acc= 0.32528 val_loss= 2.28910 val_acc= 0.32537 time= 0.29200
Epoch: 0026 train_loss= 2.24375 train_acc= 0.33653 val_loss= 2.25345 val_acc= 0.33433 time= 0.29708
Epoch: 0027 train_loss= 2.19135 train_acc= 0.37095 val_loss= 2.21753 val_acc= 0.36716 time= 0.28904
Epoch: 0028 train_loss= 2.15591 train_acc= 0.39907 val_loss= 2.18157 val_acc= 0.37910 time= 0.29303
Epoch: 0029 train_loss= 2.10594 train_acc= 0.43183 val_loss= 2.14586 val_acc= 0.41791 time= 0.29500
Epoch: 0030 train_loss= 2.06610 train_acc= 0.45533 val_loss= 2.11021 val_acc= 0.43582 time= 0.28897
Epoch: 0031 train_loss= 2.02330 train_acc= 0.49007 val_loss= 2.07444 val_acc= 0.45075 time= 0.29400
Epoch: 0032 train_loss= 1.98330 train_acc= 0.51026 val_loss= 2.03773 val_acc= 0.47463 time= 0.28703
Epoch: 0033 train_loss= 1.93478 train_acc= 0.51886 val_loss= 2.00009 val_acc= 0.48060 time= 0.29025
Epoch: 0034 train_loss= 1.89438 train_acc= 0.52283 val_loss= 1.96247 val_acc= 0.48060 time= 0.28700
Epoch: 0035 train_loss= 1.83313 train_acc= 0.54203 val_loss= 1.92498 val_acc= 0.48955 time= 0.29141
Epoch: 0036 train_loss= 1.81431 train_acc= 0.54567 val_loss= 1.88830 val_acc= 0.48955 time= 0.28797
Epoch: 0037 train_loss= 1.75424 train_acc= 0.55526 val_loss= 1.85339 val_acc= 0.50149 time= 0.29500
Epoch: 0038 train_loss= 1.70623 train_acc= 0.56618 val_loss= 1.82018 val_acc= 0.51045 time= 0.29203
Epoch: 0039 train_loss= 1.65926 train_acc= 0.57280 val_loss= 1.78726 val_acc= 0.51343 time= 0.29300
Epoch: 0040 train_loss= 1.62153 train_acc= 0.58306 val_loss= 1.75563 val_acc= 0.52537 time= 0.29200
Epoch: 0041 train_loss= 1.58624 train_acc= 0.59431 val_loss= 1.72477 val_acc= 0.53134 time= 0.29000
Epoch: 0042 train_loss= 1.53325 train_acc= 0.60390 val_loss= 1.69414 val_acc= 0.55224 time= 0.29497
Epoch: 0043 train_loss= 1.51682 train_acc= 0.60390 val_loss= 1.66442 val_acc= 0.54030 time= 0.28914
Epoch: 0044 train_loss= 1.46950 train_acc= 0.61813 val_loss= 1.63557 val_acc= 0.54925 time= 0.29100
Epoch: 0045 train_loss= 1.41851 train_acc= 0.62905 val_loss= 1.60877 val_acc= 0.55522 time= 0.28914
Epoch: 0046 train_loss= 1.38690 train_acc= 0.64361 val_loss= 1.58264 val_acc= 0.56418 time= 0.29100
Epoch: 0047 train_loss= 1.34066 train_acc= 0.65784 val_loss= 1.55799 val_acc= 0.57015 time= 0.28904
Epoch: 0048 train_loss= 1.32444 train_acc= 0.65586 val_loss= 1.53559 val_acc= 0.57612 time= 0.29170
Epoch: 0049 train_loss= 1.28732 train_acc= 0.66612 val_loss= 1.51473 val_acc= 0.58209 time= 0.29200
Epoch: 0050 train_loss= 1.22870 train_acc= 0.68531 val_loss= 1.49463 val_acc= 0.58806 time= 0.28800
Epoch: 0051 train_loss= 1.22220 train_acc= 0.68101 val_loss= 1.47566 val_acc= 0.58806 time= 0.29200
Epoch: 0052 train_loss= 1.18938 train_acc= 0.69027 val_loss= 1.45727 val_acc= 0.60000 time= 0.29400
Epoch: 0053 train_loss= 1.15037 train_acc= 0.69887 val_loss= 1.43958 val_acc= 0.60299 time= 0.29200
Epoch: 0054 train_loss= 1.12075 train_acc= 0.70086 val_loss= 1.42243 val_acc= 0.60896 time= 0.28900
Epoch: 0055 train_loss= 1.10949 train_acc= 0.70946 val_loss= 1.40542 val_acc= 0.60896 time= 0.29200
Epoch: 0056 train_loss= 1.05567 train_acc= 0.72899 val_loss= 1.38799 val_acc= 0.60299 time= 0.29200
Epoch: 0057 train_loss= 1.04342 train_acc= 0.73064 val_loss= 1.37107 val_acc= 0.60896 time= 0.29200
Epoch: 0058 train_loss= 1.01782 train_acc= 0.73594 val_loss= 1.35516 val_acc= 0.61493 time= 0.29000
Epoch: 0059 train_loss= 0.98070 train_acc= 0.74355 val_loss= 1.34143 val_acc= 0.62388 time= 0.29610
Epoch: 0060 train_loss= 0.96035 train_acc= 0.75513 val_loss= 1.32910 val_acc= 0.62388 time= 0.29200
Epoch: 0061 train_loss= 0.94164 train_acc= 0.75017 val_loss= 1.31853 val_acc= 0.62090 time= 0.29000
Epoch: 0062 train_loss= 0.92442 train_acc= 0.75910 val_loss= 1.30932 val_acc= 0.62985 time= 0.29300
Epoch: 0063 train_loss= 0.87305 train_acc= 0.77267 val_loss= 1.29971 val_acc= 0.62090 time= 0.29400
Epoch: 0064 train_loss= 0.85971 train_acc= 0.78359 val_loss= 1.29144 val_acc= 0.62090 time= 0.28906
Epoch: 0065 train_loss= 0.85364 train_acc= 0.78061 val_loss= 1.28386 val_acc= 0.62687 time= 0.28800
Epoch: 0066 train_loss= 0.81935 train_acc= 0.79715 val_loss= 1.27426 val_acc= 0.62388 time= 0.29600
Epoch: 0067 train_loss= 0.79145 train_acc= 0.80212 val_loss= 1.26307 val_acc= 0.62687 time= 0.29200
Epoch: 0068 train_loss= 0.78628 train_acc= 0.79649 val_loss= 1.25298 val_acc= 0.62388 time= 0.29000
Epoch: 0069 train_loss= 0.75967 train_acc= 0.79947 val_loss= 1.24432 val_acc= 0.63881 time= 0.28897
Epoch: 0070 train_loss= 0.74757 train_acc= 0.80410 val_loss= 1.23600 val_acc= 0.63284 time= 0.29303
Epoch: 0071 train_loss= 0.71787 train_acc= 0.81668 val_loss= 1.22837 val_acc= 0.63284 time= 0.28800
Epoch: 0072 train_loss= 0.70372 train_acc= 0.82131 val_loss= 1.22291 val_acc= 0.62985 time= 0.28710
Epoch: 0073 train_loss= 0.68641 train_acc= 0.81767 val_loss= 1.21949 val_acc= 0.63881 time= 0.29399
Epoch: 0074 train_loss= 0.66002 train_acc= 0.83024 val_loss= 1.21469 val_acc= 0.64179 time= 0.29097
Epoch: 0075 train_loss= 0.65332 train_acc= 0.84679 val_loss= 1.20691 val_acc= 0.63284 time= 0.29103
Epoch: 0076 train_loss= 0.64155 train_acc= 0.84381 val_loss= 1.20142 val_acc= 0.64179 time= 0.29000
Epoch: 0077 train_loss= 0.61315 train_acc= 0.85242 val_loss= 1.19649 val_acc= 0.63881 time= 0.29300
Epoch: 0078 train_loss= 0.60391 train_acc= 0.85010 val_loss= 1.18994 val_acc= 0.63881 time= 0.29009
Epoch: 0079 train_loss= 0.57519 train_acc= 0.85672 val_loss= 1.18354 val_acc= 0.65075 time= 0.28997
Epoch: 0080 train_loss= 0.56714 train_acc= 0.86499 val_loss= 1.18360 val_acc= 0.65672 time= 0.29400
Epoch: 0081 train_loss= 0.55915 train_acc= 0.86036 val_loss= 1.18468 val_acc= 0.65970 time= 0.29903
Epoch: 0082 train_loss= 0.54964 train_acc= 0.85903 val_loss= 1.18183 val_acc= 0.65672 time= 0.29300
Epoch: 0083 train_loss= 0.53660 train_acc= 0.86797 val_loss= 1.17754 val_acc= 0.64776 time= 0.28700
Epoch: 0084 train_loss= 0.51897 train_acc= 0.86962 val_loss= 1.17517 val_acc= 0.65672 time= 0.29300
Epoch: 0085 train_loss= 0.50724 train_acc= 0.87525 val_loss= 1.17110 val_acc= 0.65672 time= 0.28900
Epoch: 0086 train_loss= 0.49379 train_acc= 0.88087 val_loss= 1.16665 val_acc= 0.65970 time= 0.28701
Epoch: 0087 train_loss= 0.49017 train_acc= 0.87558 val_loss= 1.16119 val_acc= 0.65672 time= 0.28911
Epoch: 0088 train_loss= 0.47411 train_acc= 0.88551 val_loss= 1.15712 val_acc= 0.65672 time= 0.28974
Epoch: 0089 train_loss= 0.46054 train_acc= 0.89179 val_loss= 1.15402 val_acc= 0.65970 time= 0.28797
Epoch: 0090 train_loss= 0.44321 train_acc= 0.89676 val_loss= 1.15394 val_acc= 0.66567 time= 0.28603
Epoch: 0091 train_loss= 0.45159 train_acc= 0.88915 val_loss= 1.15289 val_acc= 0.66866 time= 0.29200
Epoch: 0092 train_loss= 0.44329 train_acc= 0.89014 val_loss= 1.15225 val_acc= 0.66866 time= 0.29101
Epoch: 0093 train_loss= 0.43069 train_acc= 0.89510 val_loss= 1.15079 val_acc= 0.67164 time= 0.28700
Epoch: 0094 train_loss= 0.41208 train_acc= 0.90536 val_loss= 1.15039 val_acc= 0.67463 time= 0.28897
Epoch: 0095 train_loss= 0.39944 train_acc= 0.90702 val_loss= 1.15191 val_acc= 0.67164 time= 0.29303
Epoch: 0096 train_loss= 0.39868 train_acc= 0.90106 val_loss= 1.15186 val_acc= 0.67164 time= 0.29201
Epoch: 0097 train_loss= 0.38399 train_acc= 0.91297 val_loss= 1.15052 val_acc= 0.67761 time= 0.29203
Epoch: 0098 train_loss= 0.38581 train_acc= 0.91330 val_loss= 1.15262 val_acc= 0.66866 time= 0.28997
Early stopping...
Optimization Finished!
Test set results: cost= 1.16056 accuracy= 0.68019 time= 0.13100
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7164    0.7018    0.7090       342
           1     0.6870    0.7670    0.7248       103
           2     0.7549    0.5500    0.6364       140
           3     0.6842    0.3291    0.4444        79
           4     0.6415    0.7727    0.7010       132
           5     0.6974    0.7732    0.7333       313
           6     0.6759    0.7157    0.6952       102
           7     0.5897    0.3286    0.4220        70
           8     0.6667    0.2400    0.3529        50
           9     0.6627    0.7226    0.6914       155
          10     0.8440    0.6364    0.7256       187
          11     0.5977    0.6753    0.6341       231
          12     0.7799    0.6966    0.7359       178
          13     0.7655    0.8050    0.7847       600
          14     0.7692    0.8475    0.8065       590
          15     0.7619    0.6316    0.6906        76
          16     0.6875    0.3235    0.4400        34
          17     0.0000    0.0000    0.0000        10
          18     0.3978    0.5107    0.4472       419
          19     0.6559    0.4729    0.5495       129
          20     0.8235    0.5000    0.6222        28
          21     0.9545    0.7241    0.8235        29
          22     0.5909    0.2826    0.3824        46

    accuracy                         0.6802      4043
   macro avg     0.6698    0.5655    0.5980      4043
weighted avg     0.6884    0.6802    0.6760      4043

Macro average Test Precision, Recall and F1-Score...
(0.6697722787480449, 0.5655141466743765, 0.5979513967023209, None)
Micro average Test Precision, Recall and F1-Score...
(0.680187979223349, 0.680187979223349, 0.680187979223349, None)
embeddings:
14157 3357 4043
[[ 0.3829232   0.31342408  0.23984678 ...  0.27796045  0.24078077
   0.24622542]
 [ 0.05900097  0.19687188  0.00401556 ...  0.1509527   0.01182983
  -0.03363672]
 [ 0.32237816  0.06736161  0.08656419 ...  0.04347533  0.19935983
   0.08260024]
 ...
 [ 0.11329985  0.22266047  0.05986672 ...  0.06822024  0.14693542
   0.2794589 ]
 [ 0.38251695  0.20168506  0.1397415  ...  0.16915163 -0.01213541
  -0.05762137]
 [-0.03435187  0.17067705  0.15941401 ...  0.17448042  0.0637747
   0.15500794]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.04699 val_loss= 3.11561 val_acc= 0.29254 time= 0.58109
Epoch: 0002 train_loss= 3.11595 train_acc= 0.27995 val_loss= 3.07016 val_acc= 0.26866 time= 0.29503
Epoch: 0003 train_loss= 3.07141 train_acc= 0.25050 val_loss= 2.99964 val_acc= 0.25970 time= 0.29147
Epoch: 0004 train_loss= 3.00192 train_acc= 0.23395 val_loss= 2.91090 val_acc= 0.25373 time= 0.28700
Epoch: 0005 train_loss= 2.91431 train_acc= 0.22733 val_loss= 2.81955 val_acc= 0.24776 time= 0.28903
Epoch: 0006 train_loss= 2.82508 train_acc= 0.21707 val_loss= 2.74430 val_acc= 0.24776 time= 0.29500
Epoch: 0007 train_loss= 2.75401 train_acc= 0.21674 val_loss= 2.69999 val_acc= 0.24478 time= 0.29300
Epoch: 0008 train_loss= 2.71075 train_acc= 0.21509 val_loss= 2.68794 val_acc= 0.22687 time= 0.29300
Epoch: 0009 train_loss= 2.70029 train_acc= 0.19722 val_loss= 2.68586 val_acc= 0.20597 time= 0.29500
Epoch: 0010 train_loss= 2.69796 train_acc= 0.18068 val_loss= 2.67399 val_acc= 0.20597 time= 0.29200
Epoch: 0011 train_loss= 2.67996 train_acc= 0.17571 val_loss= 2.64863 val_acc= 0.20597 time= 0.28900
Epoch: 0012 train_loss= 2.64592 train_acc= 0.17604 val_loss= 2.61818 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.60661 train_acc= 0.18332 val_loss= 2.58968 val_acc= 0.22985 time= 0.29500
Epoch: 0014 train_loss= 2.56809 train_acc= 0.19788 val_loss= 2.56458 val_acc= 0.25075 time= 0.29011
Epoch: 0015 train_loss= 2.53622 train_acc= 0.22038 val_loss= 2.54038 val_acc= 0.26567 time= 0.29301
Epoch: 0016 train_loss= 2.50493 train_acc= 0.25281 val_loss= 2.51385 val_acc= 0.29254 time= 0.29404
Epoch: 0017 train_loss= 2.47538 train_acc= 0.27829 val_loss= 2.48292 val_acc= 0.29851 time= 0.29500
Epoch: 0018 train_loss= 2.43823 train_acc= 0.30278 val_loss= 2.44706 val_acc= 0.31642 time= 0.28700
Epoch: 0019 train_loss= 2.39854 train_acc= 0.32727 val_loss= 2.40693 val_acc= 0.32239 time= 0.29000
Epoch: 0020 train_loss= 2.35639 train_acc= 0.34150 val_loss= 2.36393 val_acc= 0.33433 time= 0.29600
Epoch: 0021 train_loss= 2.30943 train_acc= 0.35672 val_loss= 2.31965 val_acc= 0.34328 time= 0.29000
Epoch: 0022 train_loss= 2.26049 train_acc= 0.36565 val_loss= 2.27525 val_acc= 0.34328 time= 0.29000
Epoch: 0023 train_loss= 2.20778 train_acc= 0.37459 val_loss= 2.23120 val_acc= 0.34925 time= 0.29200
Epoch: 0024 train_loss= 2.15806 train_acc= 0.38120 val_loss= 2.18748 val_acc= 0.34925 time= 0.29500
Epoch: 0025 train_loss= 2.10165 train_acc= 0.40371 val_loss= 2.14390 val_acc= 0.38209 time= 0.29100
Epoch: 0026 train_loss= 2.05204 train_acc= 0.42786 val_loss= 2.10047 val_acc= 0.39403 time= 0.29400
Epoch: 0027 train_loss= 1.99735 train_acc= 0.45632 val_loss= 2.05751 val_acc= 0.42985 time= 0.29111
Epoch: 0028 train_loss= 1.93471 train_acc= 0.49570 val_loss= 2.01528 val_acc= 0.47463 time= 0.29505
Epoch: 0029 train_loss= 1.88379 train_acc= 0.52680 val_loss= 1.97349 val_acc= 0.48657 time= 0.29100
Epoch: 0030 train_loss= 1.82718 train_acc= 0.55824 val_loss= 1.93158 val_acc= 0.51642 time= 0.29100
Epoch: 0031 train_loss= 1.77222 train_acc= 0.58637 val_loss= 1.88906 val_acc= 0.52537 time= 0.29210
Epoch: 0032 train_loss= 1.71349 train_acc= 0.59696 val_loss= 1.84626 val_acc= 0.53134 time= 0.29097
Epoch: 0033 train_loss= 1.65471 train_acc= 0.61019 val_loss= 1.80416 val_acc= 0.53134 time= 0.29203
Epoch: 0034 train_loss= 1.59291 train_acc= 0.61449 val_loss= 1.76326 val_acc= 0.53134 time= 0.29068
Epoch: 0035 train_loss= 1.54374 train_acc= 0.62839 val_loss= 1.72413 val_acc= 0.53134 time= 0.29400
Epoch: 0036 train_loss= 1.48576 train_acc= 0.63666 val_loss= 1.68671 val_acc= 0.53433 time= 0.29100
Epoch: 0037 train_loss= 1.43680 train_acc= 0.64891 val_loss= 1.65114 val_acc= 0.54030 time= 0.29400
Epoch: 0038 train_loss= 1.38795 train_acc= 0.65950 val_loss= 1.61728 val_acc= 0.55821 time= 0.28800
Epoch: 0039 train_loss= 1.33885 train_acc= 0.66810 val_loss= 1.58526 val_acc= 0.56119 time= 0.29700
Epoch: 0040 train_loss= 1.28761 train_acc= 0.67704 val_loss= 1.55573 val_acc= 0.57313 time= 0.29000
Epoch: 0041 train_loss= 1.23293 train_acc= 0.69193 val_loss= 1.52817 val_acc= 0.57910 time= 0.29200
Epoch: 0042 train_loss= 1.19008 train_acc= 0.69987 val_loss= 1.50188 val_acc= 0.57612 time= 0.29412
Epoch: 0043 train_loss= 1.14078 train_acc= 0.71377 val_loss= 1.47620 val_acc= 0.58209 time= 0.29119
Epoch: 0044 train_loss= 1.09765 train_acc= 0.73097 val_loss= 1.45096 val_acc= 0.59403 time= 0.29096
Epoch: 0045 train_loss= 1.05133 train_acc= 0.74322 val_loss= 1.42682 val_acc= 0.60000 time= 0.28803
Epoch: 0046 train_loss= 1.01104 train_acc= 0.75844 val_loss= 1.40390 val_acc= 0.60000 time= 0.29200
Epoch: 0047 train_loss= 0.96792 train_acc= 0.77035 val_loss= 1.38201 val_acc= 0.60299 time= 0.29400
Epoch: 0048 train_loss= 0.93110 train_acc= 0.77995 val_loss= 1.36147 val_acc= 0.61791 time= 0.29200
Epoch: 0049 train_loss= 0.89482 train_acc= 0.78855 val_loss= 1.34285 val_acc= 0.61791 time= 0.29397
Epoch: 0050 train_loss= 0.85506 train_acc= 0.79715 val_loss= 1.32545 val_acc= 0.61791 time= 0.29506
Epoch: 0051 train_loss= 0.81799 train_acc= 0.80609 val_loss= 1.30982 val_acc= 0.61791 time= 0.29006
Epoch: 0052 train_loss= 0.78769 train_acc= 0.81602 val_loss= 1.29500 val_acc= 0.62090 time= 0.29063
Epoch: 0053 train_loss= 0.75204 train_acc= 0.82594 val_loss= 1.28165 val_acc= 0.62687 time= 0.29314
Epoch: 0054 train_loss= 0.71482 train_acc= 0.83587 val_loss= 1.26847 val_acc= 0.63582 time= 0.29300
Epoch: 0055 train_loss= 0.68412 train_acc= 0.84083 val_loss= 1.25642 val_acc= 0.64179 time= 0.28905
Epoch: 0056 train_loss= 0.64888 train_acc= 0.85473 val_loss= 1.24543 val_acc= 0.65075 time= 0.28900
Epoch: 0057 train_loss= 0.62566 train_acc= 0.85639 val_loss= 1.23614 val_acc= 0.65373 time= 0.29500
Epoch: 0058 train_loss= 0.59490 train_acc= 0.86830 val_loss= 1.22773 val_acc= 0.65075 time= 0.29308
Epoch: 0059 train_loss= 0.57412 train_acc= 0.87227 val_loss= 1.22029 val_acc= 0.64776 time= 0.28804
Epoch: 0060 train_loss= 0.54725 train_acc= 0.88021 val_loss= 1.21268 val_acc= 0.64776 time= 0.29200
Epoch: 0061 train_loss= 0.52386 train_acc= 0.88518 val_loss= 1.20560 val_acc= 0.65373 time= 0.29700
Epoch: 0062 train_loss= 0.49546 train_acc= 0.89014 val_loss= 1.19871 val_acc= 0.65373 time= 0.28903
Epoch: 0063 train_loss= 0.47674 train_acc= 0.90007 val_loss= 1.19261 val_acc= 0.64776 time= 0.29297
Epoch: 0064 train_loss= 0.45616 train_acc= 0.90106 val_loss= 1.18775 val_acc= 0.65672 time= 0.29638
Epoch: 0065 train_loss= 0.43645 train_acc= 0.90602 val_loss= 1.18312 val_acc= 0.65672 time= 0.29108
Epoch: 0066 train_loss= 0.41280 train_acc= 0.91231 val_loss= 1.17947 val_acc= 0.66567 time= 0.28604
Epoch: 0067 train_loss= 0.39439 train_acc= 0.91760 val_loss= 1.17502 val_acc= 0.66269 time= 0.29031
Epoch: 0068 train_loss= 0.37942 train_acc= 0.92191 val_loss= 1.17181 val_acc= 0.66866 time= 0.29500
Epoch: 0069 train_loss= 0.36562 train_acc= 0.92389 val_loss= 1.17038 val_acc= 0.65970 time= 0.29000
Epoch: 0070 train_loss= 0.34820 train_acc= 0.93283 val_loss= 1.16896 val_acc= 0.65970 time= 0.28620
Epoch: 0071 train_loss= 0.33303 train_acc= 0.93680 val_loss= 1.16829 val_acc= 0.66567 time= 0.29397
Epoch: 0072 train_loss= 0.31341 train_acc= 0.94540 val_loss= 1.16741 val_acc= 0.67463 time= 0.29110
Epoch: 0073 train_loss= 0.30486 train_acc= 0.94077 val_loss= 1.16602 val_acc= 0.66567 time= 0.28797
Epoch: 0074 train_loss= 0.28669 train_acc= 0.94970 val_loss= 1.16633 val_acc= 0.65970 time= 0.29200
Epoch: 0075 train_loss= 0.27881 train_acc= 0.95367 val_loss= 1.16733 val_acc= 0.65970 time= 0.29596
Epoch: 0076 train_loss= 0.26334 train_acc= 0.95400 val_loss= 1.16490 val_acc= 0.66567 time= 0.29000
Epoch: 0077 train_loss= 0.25618 train_acc= 0.95533 val_loss= 1.16310 val_acc= 0.66567 time= 0.29214
Epoch: 0078 train_loss= 0.24316 train_acc= 0.95698 val_loss= 1.16401 val_acc= 0.66866 time= 0.28900
Epoch: 0079 train_loss= 0.23521 train_acc= 0.95996 val_loss= 1.16575 val_acc= 0.67463 time= 0.29300
Epoch: 0080 train_loss= 0.22296 train_acc= 0.96195 val_loss= 1.16819 val_acc= 0.67463 time= 0.28997
Early stopping...
Optimization Finished!
Test set results: cost= 1.16163 accuracy= 0.68810 time= 0.12803
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7406    0.6930    0.7160       342
           1     0.6870    0.7670    0.7248       103
           2     0.7436    0.6214    0.6770       140
           3     0.6667    0.4051    0.5039        79
           4     0.6429    0.7500    0.6923       132
           5     0.6649    0.7987    0.7257       313
           6     0.6893    0.6961    0.6927       102
           7     0.6190    0.3714    0.4643        70
           8     0.5758    0.3800    0.4578        50
           9     0.6250    0.7419    0.6785       155
          10     0.8333    0.6417    0.7251       187
          11     0.6203    0.6364    0.6282       231
          12     0.7588    0.7247    0.7414       178
          13     0.7799    0.8033    0.7915       600
          14     0.7664    0.8508    0.8064       590
          15     0.7534    0.7237    0.7383        76
          16     0.7059    0.3529    0.4706        34
          17     0.6667    0.2000    0.3077        10
          18     0.4321    0.4630    0.4470       419
          19     0.6226    0.5116    0.5617       129
          20     0.6250    0.7143    0.6667        28
          21     0.9565    0.7586    0.8462        29
          22     0.6667    0.3478    0.4571        46

    accuracy                         0.6881      4043
   macro avg     0.6888    0.6067    0.6313      4043
weighted avg     0.6907    0.6881    0.6842      4043

Macro average Test Precision, Recall and F1-Score...
(0.6887976805657824, 0.6066781236756792, 0.6313381930979858, None)
Micro average Test Precision, Recall and F1-Score...
(0.6881028938906752, 0.6881028938906752, 0.6881028938906752, None)
embeddings:
14157 3357 4043
[[ 0.36658844  0.4297434   0.42668867 ...  0.49666604  0.34183222
   0.43063173]
 [ 0.20708016  0.10253352  0.16042852 ...  0.33324486  0.01111077
  -0.01697712]
 [ 0.17598282  0.48673248  0.51233715 ...  0.5248166   0.23864974
   0.22731042]
 ...
 [ 0.0635035   0.23450643  0.23518777 ...  0.27794394  0.13164467
   0.23211353]
 [ 0.31301114  0.1313112   0.23043343 ...  0.48444417  0.13480802
   0.01066611]
 [ 0.12054422  0.03570841  0.04752997 ...  0.28901875  0.32052016
   0.3857361 ]]

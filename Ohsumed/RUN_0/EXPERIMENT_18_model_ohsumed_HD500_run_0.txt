(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13552 train_acc= 0.05096 val_loss= 3.10404 val_acc= 0.25373 time= 5.95104
Epoch: 0002 train_loss= 3.10418 train_acc= 0.23561 val_loss= 3.01959 val_acc= 0.24776 time= 5.81296
Epoch: 0003 train_loss= 3.02040 train_acc= 0.21774 val_loss= 2.89421 val_acc= 0.24478 time= 5.80403
Epoch: 0004 train_loss= 2.89589 train_acc= 0.20847 val_loss= 2.77418 val_acc= 0.24478 time= 5.80400
Epoch: 0005 train_loss= 2.77715 train_acc= 0.20946 val_loss= 2.70742 val_acc= 0.24478 time= 5.80700
Epoch: 0006 train_loss= 2.71267 train_acc= 0.21509 val_loss= 2.70339 val_acc= 0.21791 time= 5.81700
Epoch: 0007 train_loss= 2.71212 train_acc= 0.18663 val_loss= 2.70012 val_acc= 0.20597 time= 5.81301
Epoch: 0008 train_loss= 2.70871 train_acc= 0.17240 val_loss= 2.66383 val_acc= 0.20597 time= 5.82400
Epoch: 0009 train_loss= 2.66007 train_acc= 0.17340 val_loss= 2.61564 val_acc= 0.20896 time= 5.83600
Epoch: 0010 train_loss= 2.59806 train_acc= 0.18365 val_loss= 2.57680 val_acc= 0.23284 time= 5.80000
Epoch: 0011 train_loss= 2.54667 train_acc= 0.20615 val_loss= 2.54488 val_acc= 0.26567 time= 5.81300
Epoch: 0012 train_loss= 2.50618 train_acc= 0.24752 val_loss= 2.51013 val_acc= 0.29552 time= 5.82401
Epoch: 0013 train_loss= 2.46263 train_acc= 0.29749 val_loss= 2.46597 val_acc= 0.32239 time= 5.81596
Epoch: 0014 train_loss= 2.41134 train_acc= 0.35341 val_loss= 2.41132 val_acc= 0.33433 time= 5.82103
Epoch: 0015 train_loss= 2.35126 train_acc= 0.39047 val_loss= 2.34924 val_acc= 0.34925 time= 5.83000
Epoch: 0016 train_loss= 2.28387 train_acc= 0.39841 val_loss= 2.28387 val_acc= 0.35224 time= 5.79600
Epoch: 0017 train_loss= 2.21272 train_acc= 0.40238 val_loss= 2.21865 val_acc= 0.35821 time= 5.81700
Epoch: 0018 train_loss= 2.13538 train_acc= 0.40834 val_loss= 2.15507 val_acc= 0.37313 time= 5.80597
Epoch: 0019 train_loss= 2.06499 train_acc= 0.42455 val_loss= 2.09317 val_acc= 0.40000 time= 5.82904
Epoch: 0020 train_loss= 1.98687 train_acc= 0.45698 val_loss= 2.03281 val_acc= 0.43284 time= 5.83799
Epoch: 0021 train_loss= 1.90756 train_acc= 0.49967 val_loss= 1.97523 val_acc= 0.47164 time= 5.87500
Epoch: 0022 train_loss= 1.83047 train_acc= 0.54434 val_loss= 1.92080 val_acc= 0.50149 time= 5.81200
Epoch: 0023 train_loss= 1.75436 train_acc= 0.58405 val_loss= 1.86763 val_acc= 0.52836 time= 5.82700
Epoch: 0024 train_loss= 1.68010 train_acc= 0.60589 val_loss= 1.81316 val_acc= 0.52836 time= 5.82400
Epoch: 0025 train_loss= 1.60427 train_acc= 0.61648 val_loss= 1.75783 val_acc= 0.53433 time= 5.81600
Epoch: 0026 train_loss= 1.52866 train_acc= 0.62806 val_loss= 1.70386 val_acc= 0.53731 time= 5.84599
Epoch: 0027 train_loss= 1.45344 train_acc= 0.64593 val_loss= 1.65442 val_acc= 0.54328 time= 5.79502
Epoch: 0028 train_loss= 1.38880 train_acc= 0.66181 val_loss= 1.61054 val_acc= 0.55821 time= 5.82199
Epoch: 0029 train_loss= 1.32187 train_acc= 0.67538 val_loss= 1.56966 val_acc= 0.57015 time= 5.82100
Epoch: 0030 train_loss= 1.26338 train_acc= 0.68762 val_loss= 1.53074 val_acc= 0.58507 time= 5.84900
Epoch: 0031 train_loss= 1.19660 train_acc= 0.70053 val_loss= 1.49595 val_acc= 0.58209 time= 5.84100
Epoch: 0032 train_loss= 1.13544 train_acc= 0.70880 val_loss= 1.46506 val_acc= 0.59701 time= 5.80799
Epoch: 0033 train_loss= 1.07759 train_acc= 0.72171 val_loss= 1.43483 val_acc= 0.59403 time= 5.80901
Epoch: 0034 train_loss= 1.02268 train_acc= 0.73660 val_loss= 1.40667 val_acc= 0.59403 time= 5.83100
Epoch: 0035 train_loss= 0.96626 train_acc= 0.75943 val_loss= 1.37990 val_acc= 0.58806 time= 5.83000
Epoch: 0036 train_loss= 0.91130 train_acc= 0.77333 val_loss= 1.35393 val_acc= 0.60299 time= 5.84199
Epoch: 0037 train_loss= 0.86561 train_acc= 0.78756 val_loss= 1.33014 val_acc= 0.61493 time= 5.82701
Epoch: 0038 train_loss= 0.81287 train_acc= 0.80046 val_loss= 1.30944 val_acc= 0.62388 time= 5.83800
Epoch: 0039 train_loss= 0.76718 train_acc= 0.81502 val_loss= 1.29156 val_acc= 0.62687 time= 5.81100
Epoch: 0040 train_loss= 0.72323 train_acc= 0.82561 val_loss= 1.27449 val_acc= 0.62687 time= 5.82499
Epoch: 0041 train_loss= 0.67638 train_acc= 0.83951 val_loss= 1.25878 val_acc= 0.62687 time= 5.83000
Epoch: 0042 train_loss= 0.63533 train_acc= 0.84745 val_loss= 1.24557 val_acc= 0.62687 time= 5.82999
Epoch: 0043 train_loss= 0.59458 train_acc= 0.85672 val_loss= 1.23370 val_acc= 0.62388 time= 5.82300
Epoch: 0044 train_loss= 0.56001 train_acc= 0.86698 val_loss= 1.22199 val_acc= 0.63881 time= 5.86300
Epoch: 0045 train_loss= 0.52060 train_acc= 0.87591 val_loss= 1.21243 val_acc= 0.63881 time= 5.80899
Epoch: 0046 train_loss= 0.48911 train_acc= 0.88948 val_loss= 1.20562 val_acc= 0.64478 time= 5.82001
Epoch: 0047 train_loss= 0.46071 train_acc= 0.89245 val_loss= 1.19849 val_acc= 0.64776 time= 5.83000
Epoch: 0048 train_loss= 0.42821 train_acc= 0.90172 val_loss= 1.19280 val_acc= 0.64179 time= 5.83500
Epoch: 0049 train_loss= 0.40013 train_acc= 0.91396 val_loss= 1.18717 val_acc= 0.65373 time= 5.83100
Epoch: 0050 train_loss= 0.37299 train_acc= 0.91694 val_loss= 1.17976 val_acc= 0.66269 time= 5.81000
Epoch: 0051 train_loss= 0.34808 train_acc= 0.92389 val_loss= 1.17412 val_acc= 0.66866 time= 5.87599
Epoch: 0052 train_loss= 0.32728 train_acc= 0.93183 val_loss= 1.17101 val_acc= 0.66269 time= 5.80697
Epoch: 0053 train_loss= 0.30594 train_acc= 0.93977 val_loss= 1.17167 val_acc= 0.65672 time= 5.82703
Epoch: 0054 train_loss= 0.28617 train_acc= 0.94540 val_loss= 1.17440 val_acc= 0.66567 time= 5.84800
Epoch: 0055 train_loss= 0.26819 train_acc= 0.94705 val_loss= 1.17712 val_acc= 0.68060 time= 5.81600
Epoch: 0056 train_loss= 0.24604 train_acc= 0.95466 val_loss= 1.17875 val_acc= 0.67761 time= 5.79800
Epoch: 0057 train_loss= 0.23284 train_acc= 0.95500 val_loss= 1.17927 val_acc= 0.67164 time= 5.80900
Epoch: 0058 train_loss= 0.21716 train_acc= 0.96062 val_loss= 1.18089 val_acc= 0.66269 time= 5.84700
Early stopping...
Optimization Finished!
Test set results: cost= 1.17423 accuracy= 0.68612 time= 2.02800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7231    0.6871    0.7046       342
           1     0.7064    0.7476    0.7264       103
           2     0.7236    0.6357    0.6768       140
           3     0.6000    0.4557    0.5180        79
           4     0.6735    0.7500    0.7097       132
           5     0.7126    0.7764    0.7431       313
           6     0.6727    0.7255    0.6981       102
           7     0.5833    0.3000    0.3962        70
           8     0.5588    0.3800    0.4524        50
           9     0.6129    0.7355    0.6686       155
          10     0.8333    0.6417    0.7251       187
          11     0.5992    0.6537    0.6253       231
          12     0.7588    0.7247    0.7414       178
          13     0.7774    0.7917    0.7845       600
          14     0.7829    0.8373    0.8092       590
          15     0.7432    0.7237    0.7333        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4222    0.4988    0.4573       419
          19     0.6505    0.5194    0.5776       129
          20     0.6071    0.6071    0.6071        28
          21     1.0000    0.7586    0.8627        29
          22     0.6522    0.3261    0.4348        46

    accuracy                         0.6861      4043
   macro avg     0.6783    0.5969    0.6213      4043
weighted avg     0.6908    0.6861    0.6838      4043

Macro average Test Precision, Recall and F1-Score...
(0.6782501965325403, 0.5969207350474451, 0.621283953745883, None)
Micro average Test Precision, Recall and F1-Score...
(0.6861241652238437, 0.6861241652238437, 0.6861241652238437, None)
embeddings:
14157 3357 4043
[[ 0.28674558  0.3021466   0.24521194 ...  0.14373335  0.38244176
   0.32931146]
 [ 0.03373682  0.15778288 -0.00261551 ... -0.01819062  0.17398925
  -0.0236878 ]
 [ 0.12682788  0.2809596   0.11043117 ...  0.02930399  0.3019392
   0.23067327]
 ...
 [ 0.11456778  0.18929504  0.10954792 ...  0.04896452  0.16479678
   0.14897278]
 [ 0.07690327 -0.0355788   0.18633579 ... -0.00563958  0.34419575
  -0.03382498]
 [ 0.11006458  0.12570316  0.14901026 ...  0.0887773   0.07936507
   0.24017097]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.01555 val_loss= 2.91305 val_acc= 0.20299 time= 0.58517
Epoch: 0002 train_loss= 2.91668 train_acc= 0.17373 val_loss= 2.75476 val_acc= 0.20299 time= 0.29944
Epoch: 0003 train_loss= 2.77757 train_acc= 0.17240 val_loss= 2.65844 val_acc= 0.20299 time= 0.29001
Epoch: 0004 train_loss= 2.66859 train_acc= 0.17306 val_loss= 2.55187 val_acc= 0.26567 time= 0.28999
Epoch: 0005 train_loss= 2.52924 train_acc= 0.24586 val_loss= 2.48035 val_acc= 0.30746 time= 0.28700
Epoch: 0006 train_loss= 2.44042 train_acc= 0.30807 val_loss= 2.34719 val_acc= 0.37015 time= 0.29197
Epoch: 0007 train_loss= 2.28830 train_acc= 0.38385 val_loss= 2.18365 val_acc= 0.40000 time= 0.28903
Epoch: 0008 train_loss= 2.11567 train_acc= 0.42919 val_loss= 2.05028 val_acc= 0.42985 time= 0.28700
Epoch: 0009 train_loss= 1.95942 train_acc= 0.47055 val_loss= 1.93193 val_acc= 0.47164 time= 0.28998
Epoch: 0010 train_loss= 1.81248 train_acc= 0.54169 val_loss= 1.82421 val_acc= 0.51940 time= 0.29502
Epoch: 0011 train_loss= 1.64835 train_acc= 0.58769 val_loss= 1.72258 val_acc= 0.54030 time= 0.29297
Epoch: 0012 train_loss= 1.49679 train_acc= 0.61913 val_loss= 1.63782 val_acc= 0.56716 time= 0.28911
Epoch: 0013 train_loss= 1.36366 train_acc= 0.65685 val_loss= 1.56416 val_acc= 0.56418 time= 0.29497
Epoch: 0014 train_loss= 1.24464 train_acc= 0.68564 val_loss= 1.49359 val_acc= 0.57910 time= 0.29003
Epoch: 0015 train_loss= 1.13076 train_acc= 0.70880 val_loss= 1.43679 val_acc= 0.58806 time= 0.28598
Epoch: 0016 train_loss= 1.03026 train_acc= 0.72998 val_loss= 1.40177 val_acc= 0.59701 time= 0.29002
Epoch: 0017 train_loss= 0.92380 train_acc= 0.75281 val_loss= 1.37526 val_acc= 0.59403 time= 0.29400
Epoch: 0018 train_loss= 0.82808 train_acc= 0.78293 val_loss= 1.35258 val_acc= 0.60896 time= 0.28900
Epoch: 0019 train_loss= 0.74011 train_acc= 0.81072 val_loss= 1.32449 val_acc= 0.61493 time= 0.29000
Epoch: 0020 train_loss= 0.66120 train_acc= 0.83289 val_loss= 1.29532 val_acc= 0.62090 time= 0.29300
Epoch: 0021 train_loss= 0.57482 train_acc= 0.85109 val_loss= 1.27862 val_acc= 0.61791 time= 0.29300
Epoch: 0022 train_loss= 0.51067 train_acc= 0.85970 val_loss= 1.27344 val_acc= 0.63582 time= 0.28800
Epoch: 0023 train_loss= 0.44369 train_acc= 0.88352 val_loss= 1.28739 val_acc= 0.64179 time= 0.28800
Epoch: 0024 train_loss= 0.39262 train_acc= 0.90437 val_loss= 1.30192 val_acc= 0.65672 time= 0.29600
Epoch: 0025 train_loss= 0.33870 train_acc= 0.91628 val_loss= 1.30546 val_acc= 0.65672 time= 0.29000
Epoch: 0026 train_loss= 0.29446 train_acc= 0.93216 val_loss= 1.29192 val_acc= 0.66269 time= 0.28600
Epoch: 0027 train_loss= 0.25739 train_acc= 0.93779 val_loss= 1.29238 val_acc= 0.67164 time= 0.29200
Epoch: 0028 train_loss= 0.22529 train_acc= 0.94772 val_loss= 1.31444 val_acc= 0.66866 time= 0.29300
Early stopping...
Optimization Finished!
Test set results: cost= 1.32870 accuracy= 0.67697 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6997    0.6813    0.6904       342
           1     0.6480    0.7864    0.7105       103
           2     0.6164    0.7000    0.6555       140
           3     0.5593    0.4177    0.4783        79
           4     0.7197    0.7197    0.7197       132
           5     0.6475    0.7923    0.7126       313
           6     0.5515    0.7353    0.6303       102
           7     0.5750    0.3286    0.4182        70
           8     0.6316    0.2400    0.3478        50
           9     0.6196    0.7355    0.6726       155
          10     0.8286    0.6203    0.7095       187
          11     0.5827    0.6710    0.6237       231
          12     0.7661    0.7360    0.7507       178
          13     0.7591    0.7983    0.7782       600
          14     0.7776    0.8475    0.8110       590
          15     0.7432    0.7237    0.7333        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4596    0.3938    0.4242       419
          19     0.6237    0.4496    0.5225       129
          20     0.6296    0.6071    0.6182        28
          21     1.0000    0.6552    0.7917        29
          22     0.5484    0.3696    0.4416        46

    accuracy                         0.6770      4043
   macro avg     0.6606    0.5853    0.6034      4043
weighted avg     0.6750    0.6770    0.6696      4043

Macro average Test Precision, Recall and F1-Score...
(0.6605517855448089, 0.585294350522162, 0.6033792688750087, None)
Micro average Test Precision, Recall and F1-Score...
(0.6769725451397477, 0.6769725451397477, 0.6769725451397477, None)
embeddings:
14157 3357 4043
[[ 0.17052838  0.5019654   0.08133984 ...  0.3489577   0.69901663
   0.6838353 ]
 [-0.04455438  0.05210369  0.00697699 ...  0.09066768  0.46991253
   0.38734803]
 [-0.01106104  0.4144433   0.07357428 ...  0.29162875  0.35709977
   0.6634443 ]
 ...
 [ 0.01022185  0.22366607  0.05129349 ...  0.12379806  0.38858294
   0.42886597]
 [ 0.1241791  -0.06845297 -0.14383662 ... -0.04780699  0.34625024
  -0.21703826]
 [ 0.08969612  0.29371738  0.13385263 ...  0.32208398  0.40411362
   0.32547712]]

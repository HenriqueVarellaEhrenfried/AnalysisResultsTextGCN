(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.01655 val_loss= 3.11412 val_acc= 0.22985 time= 0.58516
Epoch: 0002 train_loss= 3.11448 train_acc= 0.19755 val_loss= 3.06702 val_acc= 0.22388 time= 0.29122
Epoch: 0003 train_loss= 3.06812 train_acc= 0.18862 val_loss= 2.99472 val_acc= 0.20597 time= 0.29406
Epoch: 0004 train_loss= 2.99663 train_acc= 0.18299 val_loss= 2.90449 val_acc= 0.20597 time= 0.29300
Epoch: 0005 train_loss= 2.90733 train_acc= 0.17902 val_loss= 2.81219 val_acc= 0.20896 time= 0.29000
Epoch: 0006 train_loss= 2.81701 train_acc= 0.17968 val_loss= 2.73827 val_acc= 0.20896 time= 0.28900
Epoch: 0007 train_loss= 2.74444 train_acc= 0.18233 val_loss= 2.69886 val_acc= 0.20896 time= 0.29300
Epoch: 0008 train_loss= 2.70567 train_acc= 0.18564 val_loss= 2.69293 val_acc= 0.20896 time= 0.28700
Epoch: 0009 train_loss= 2.70055 train_acc= 0.17704 val_loss= 2.69322 val_acc= 0.20299 time= 0.29109
Epoch: 0010 train_loss= 2.69841 train_acc= 0.17373 val_loss= 2.67925 val_acc= 0.20299 time= 0.29000
Epoch: 0011 train_loss= 2.67925 train_acc= 0.17240 val_loss= 2.65186 val_acc= 0.20597 time= 0.29200
Epoch: 0012 train_loss= 2.64221 train_acc= 0.17306 val_loss= 2.62210 val_acc= 0.20896 time= 0.29568
Epoch: 0013 train_loss= 2.60157 train_acc= 0.17637 val_loss= 2.59612 val_acc= 0.22090 time= 0.29100
Epoch: 0014 train_loss= 2.56570 train_acc= 0.18994 val_loss= 2.57232 val_acc= 0.24179 time= 0.29017
Epoch: 0015 train_loss= 2.53375 train_acc= 0.21112 val_loss= 2.54661 val_acc= 0.26269 time= 0.29300
Epoch: 0016 train_loss= 2.50125 train_acc= 0.24090 val_loss= 2.51633 val_acc= 0.28060 time= 0.29000
Epoch: 0017 train_loss= 2.46680 train_acc= 0.27399 val_loss= 2.48098 val_acc= 0.29254 time= 0.29000
Epoch: 0018 train_loss= 2.42664 train_acc= 0.30278 val_loss= 2.44128 val_acc= 0.31642 time= 0.29100
Epoch: 0019 train_loss= 2.38452 train_acc= 0.32032 val_loss= 2.39831 val_acc= 0.31642 time= 0.28900
Epoch: 0020 train_loss= 2.33720 train_acc= 0.33852 val_loss= 2.35326 val_acc= 0.32239 time= 0.29200
Epoch: 0021 train_loss= 2.28791 train_acc= 0.35010 val_loss= 2.30701 val_acc= 0.32537 time= 0.28900
Epoch: 0022 train_loss= 2.23625 train_acc= 0.35705 val_loss= 2.26022 val_acc= 0.33731 time= 0.29100
Epoch: 0023 train_loss= 2.18160 train_acc= 0.37227 val_loss= 2.21340 val_acc= 0.35224 time= 0.29100
Epoch: 0024 train_loss= 2.12533 train_acc= 0.39510 val_loss= 2.16690 val_acc= 0.37015 time= 0.29400
Epoch: 0025 train_loss= 2.06773 train_acc= 0.41727 val_loss= 2.12095 val_acc= 0.39104 time= 0.29106
Epoch: 0026 train_loss= 2.00881 train_acc= 0.45566 val_loss= 2.07575 val_acc= 0.43582 time= 0.29300
Epoch: 0027 train_loss= 1.94785 train_acc= 0.50000 val_loss= 2.03146 val_acc= 0.46269 time= 0.28914
Epoch: 0028 train_loss= 1.88925 train_acc= 0.53805 val_loss= 1.98754 val_acc= 0.49851 time= 0.28837
Epoch: 0029 train_loss= 1.82963 train_acc= 0.57181 val_loss= 1.94291 val_acc= 0.51940 time= 0.29200
Epoch: 0030 train_loss= 1.76627 train_acc= 0.59431 val_loss= 1.89690 val_acc= 0.52239 time= 0.29297
Epoch: 0031 train_loss= 1.70405 train_acc= 0.60589 val_loss= 1.84976 val_acc= 0.52836 time= 0.29138
Epoch: 0032 train_loss= 1.64167 train_acc= 0.61482 val_loss= 1.80303 val_acc= 0.53134 time= 0.29003
Epoch: 0033 train_loss= 1.58122 train_acc= 0.62210 val_loss= 1.75819 val_acc= 0.53731 time= 0.29300
Epoch: 0034 train_loss= 1.52156 train_acc= 0.63402 val_loss= 1.71622 val_acc= 0.53731 time= 0.28897
Epoch: 0035 train_loss= 1.46689 train_acc= 0.64825 val_loss= 1.67698 val_acc= 0.54925 time= 0.28703
Epoch: 0036 train_loss= 1.40899 train_acc= 0.66082 val_loss= 1.64010 val_acc= 0.56716 time= 0.28997
Epoch: 0037 train_loss= 1.35448 train_acc= 0.67240 val_loss= 1.60541 val_acc= 0.56716 time= 0.29503
Epoch: 0038 train_loss= 1.30140 train_acc= 0.68630 val_loss= 1.57276 val_acc= 0.57910 time= 0.28797
Epoch: 0039 train_loss= 1.24833 train_acc= 0.69557 val_loss= 1.54200 val_acc= 0.58507 time= 0.28700
Epoch: 0040 train_loss= 1.19675 train_acc= 0.70814 val_loss= 1.51234 val_acc= 0.59104 time= 0.29407
Epoch: 0041 train_loss= 1.14681 train_acc= 0.71840 val_loss= 1.48401 val_acc= 0.58806 time= 0.28798
Epoch: 0042 train_loss= 1.09719 train_acc= 0.73395 val_loss= 1.45748 val_acc= 0.58806 time= 0.28700
Epoch: 0043 train_loss= 1.05281 train_acc= 0.74487 val_loss= 1.43253 val_acc= 0.59104 time= 0.28600
Epoch: 0044 train_loss= 1.00622 train_acc= 0.75910 val_loss= 1.40939 val_acc= 0.60299 time= 0.29501
Epoch: 0045 train_loss= 0.96658 train_acc= 0.76936 val_loss= 1.38809 val_acc= 0.60299 time= 0.28806
Epoch: 0046 train_loss= 0.92128 train_acc= 0.78359 val_loss= 1.36868 val_acc= 0.60299 time= 0.29199
Epoch: 0047 train_loss= 0.88099 train_acc= 0.79616 val_loss= 1.35067 val_acc= 0.60299 time= 0.29704
Epoch: 0048 train_loss= 0.84097 train_acc= 0.80609 val_loss= 1.33390 val_acc= 0.61791 time= 0.29500
Epoch: 0049 train_loss= 0.80026 train_acc= 0.81767 val_loss= 1.31817 val_acc= 0.61493 time= 0.28900
Epoch: 0050 train_loss= 0.76551 train_acc= 0.82727 val_loss= 1.30341 val_acc= 0.61791 time= 0.28900
Epoch: 0051 train_loss= 0.72989 train_acc= 0.83686 val_loss= 1.29037 val_acc= 0.62985 time= 0.29407
Epoch: 0052 train_loss= 0.69448 train_acc= 0.84348 val_loss= 1.27874 val_acc= 0.62985 time= 0.29006
Epoch: 0053 train_loss= 0.66139 train_acc= 0.85506 val_loss= 1.26860 val_acc= 0.61791 time= 0.28700
Epoch: 0054 train_loss= 0.62998 train_acc= 0.86036 val_loss= 1.25943 val_acc= 0.61791 time= 0.29009
Epoch: 0055 train_loss= 0.59896 train_acc= 0.86764 val_loss= 1.25142 val_acc= 0.61791 time= 0.29301
Epoch: 0056 train_loss= 0.57168 train_acc= 0.87690 val_loss= 1.24431 val_acc= 0.61791 time= 0.28769
Epoch: 0057 train_loss= 0.54232 train_acc= 0.88220 val_loss= 1.23787 val_acc= 0.62687 time= 0.28697
Epoch: 0058 train_loss= 0.51422 train_acc= 0.89246 val_loss= 1.23202 val_acc= 0.62687 time= 0.29314
Epoch: 0059 train_loss= 0.48995 train_acc= 0.89709 val_loss= 1.22628 val_acc= 0.62687 time= 0.29300
Epoch: 0060 train_loss= 0.46574 train_acc= 0.90602 val_loss= 1.21989 val_acc= 0.63284 time= 0.28800
Epoch: 0061 train_loss= 0.44250 train_acc= 0.91231 val_loss= 1.21371 val_acc= 0.63582 time= 0.29095
Epoch: 0062 train_loss= 0.42087 train_acc= 0.91727 val_loss= 1.20896 val_acc= 0.64179 time= 0.29600
Epoch: 0063 train_loss= 0.39925 train_acc= 0.92356 val_loss= 1.20684 val_acc= 0.64478 time= 0.28800
Epoch: 0064 train_loss= 0.37883 train_acc= 0.92720 val_loss= 1.20662 val_acc= 0.64776 time= 0.28700
Epoch: 0065 train_loss= 0.36187 train_acc= 0.93183 val_loss= 1.20595 val_acc= 0.65075 time= 0.29300
Epoch: 0066 train_loss= 0.34297 train_acc= 0.93547 val_loss= 1.20505 val_acc= 0.65672 time= 0.29300
Epoch: 0067 train_loss= 0.32547 train_acc= 0.94011 val_loss= 1.20370 val_acc= 0.65970 time= 0.28900
Epoch: 0068 train_loss= 0.30907 train_acc= 0.94408 val_loss= 1.20282 val_acc= 0.66866 time= 0.29000
Epoch: 0069 train_loss= 0.29462 train_acc= 0.94937 val_loss= 1.20311 val_acc= 0.66567 time= 0.29005
Epoch: 0070 train_loss= 0.27880 train_acc= 0.95235 val_loss= 1.20344 val_acc= 0.67164 time= 0.29100
Epoch: 0071 train_loss= 0.26634 train_acc= 0.95566 val_loss= 1.20347 val_acc= 0.67463 time= 0.28500
Epoch: 0072 train_loss= 0.25328 train_acc= 0.95963 val_loss= 1.20456 val_acc= 0.67463 time= 0.28900
Epoch: 0073 train_loss= 0.23973 train_acc= 0.96195 val_loss= 1.20747 val_acc= 0.67463 time= 0.29200
Early stopping...
Optimization Finished!
Test set results: cost= 1.16449 accuracy= 0.68959 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7254    0.7105    0.7179       342
           1     0.7156    0.7573    0.7358       103
           2     0.7456    0.6071    0.6693       140
           3     0.6207    0.4557    0.5255        79
           4     0.6536    0.7576    0.7018       132
           5     0.6814    0.7859    0.7300       313
           6     0.6667    0.7255    0.6948       102
           7     0.5714    0.2857    0.3810        70
           8     0.6061    0.4000    0.4819        50
           9     0.6333    0.7355    0.6806       155
          10     0.8117    0.6684    0.7331       187
          11     0.6107    0.6450    0.6274       231
          12     0.7545    0.7079    0.7304       178
          13     0.7785    0.8083    0.7931       600
          14     0.7774    0.8407    0.8078       590
          15     0.7681    0.6974    0.7310        76
          16     0.7647    0.3824    0.5098        34
          17     0.5000    0.1000    0.1667        10
          18     0.4471    0.4940    0.4694       419
          19     0.6214    0.4961    0.5517       129
          20     0.6538    0.6071    0.6296        28
          21     1.0000    0.7241    0.8400        29
          22     0.5556    0.3261    0.4110        46

    accuracy                         0.6896      4043
   macro avg     0.6810    0.5965    0.6226      4043
weighted avg     0.6910    0.6896    0.6858      4043

Macro average Test Precision, Recall and F1-Score...
(0.681009575797631, 0.5964542344735546, 0.622595394508049, None)
Micro average Test Precision, Recall and F1-Score...
(0.6895869403907989, 0.6895869403907989, 0.6895869403907989, None)
embeddings:
14157 3357 4043
[[ 0.38845485  0.30320093  0.2820933  ...  0.29292038  0.566319
   0.4329282 ]
 [ 0.15656824  0.03858409 -0.03874122 ...  0.00792528  0.34075493
   0.07033756]
 [ 0.44436133  0.17342913  0.08150903 ...  0.14655425  0.3244608
   0.14487357]
 ...
 [ 0.2185387   0.13080199  0.10583638 ...  0.09592842  0.232789
   0.09928568]
 [ 0.3678819  -0.0598759  -0.06575182 ...  0.17089684  0.505924
   0.37597567]
 [ 0.03038399  0.10202188  0.12438771 ...  0.19388151  0.45575467
   0.21342105]]

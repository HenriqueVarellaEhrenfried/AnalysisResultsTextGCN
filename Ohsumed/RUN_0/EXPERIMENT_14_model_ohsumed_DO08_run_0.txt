(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13545 train_acc= 0.05559 val_loss= 3.11355 val_acc= 0.21194 time= 0.59032
Epoch: 0002 train_loss= 3.11376 train_acc= 0.18696 val_loss= 3.06532 val_acc= 0.20597 time= 0.28800
Epoch: 0003 train_loss= 3.06681 train_acc= 0.18200 val_loss= 2.99302 val_acc= 0.20597 time= 0.28800
Epoch: 0004 train_loss= 2.99547 train_acc= 0.18034 val_loss= 2.90478 val_acc= 0.20597 time= 0.29734
Epoch: 0005 train_loss= 2.91012 train_acc= 0.17670 val_loss= 2.81654 val_acc= 0.20597 time= 0.28900
Epoch: 0006 train_loss= 2.82198 train_acc= 0.17737 val_loss= 2.74519 val_acc= 0.20597 time= 0.28900
Epoch: 0007 train_loss= 2.75463 train_acc= 0.17770 val_loss= 2.70394 val_acc= 0.20597 time= 0.29209
Epoch: 0008 train_loss= 2.71759 train_acc= 0.17604 val_loss= 2.69625 val_acc= 0.20597 time= 0.29403
Epoch: 0009 train_loss= 2.71146 train_acc= 0.17604 val_loss= 2.69889 val_acc= 0.20000 time= 0.28900
Epoch: 0010 train_loss= 2.71383 train_acc= 0.17240 val_loss= 2.68946 val_acc= 0.20000 time= 0.28800
Epoch: 0011 train_loss= 2.70090 train_acc= 0.17207 val_loss= 2.66510 val_acc= 0.20597 time= 0.29800
Epoch: 0012 train_loss= 2.66295 train_acc= 0.17340 val_loss= 2.63615 val_acc= 0.20597 time= 0.29200
Epoch: 0013 train_loss= 2.62929 train_acc= 0.17637 val_loss= 2.61069 val_acc= 0.21194 time= 0.29100
Epoch: 0014 train_loss= 2.59366 train_acc= 0.18762 val_loss= 2.58950 val_acc= 0.23582 time= 0.29000
Epoch: 0015 train_loss= 2.56792 train_acc= 0.20847 val_loss= 2.56953 val_acc= 0.25672 time= 0.29700
Epoch: 0016 train_loss= 2.54240 train_acc= 0.22833 val_loss= 2.54694 val_acc= 0.27463 time= 0.28997
Epoch: 0017 train_loss= 2.51506 train_acc= 0.25612 val_loss= 2.51979 val_acc= 0.29552 time= 0.28812
Epoch: 0018 train_loss= 2.48380 train_acc= 0.27432 val_loss= 2.48768 val_acc= 0.29851 time= 0.29200
Epoch: 0019 train_loss= 2.45208 train_acc= 0.29219 val_loss= 2.45179 val_acc= 0.31045 time= 0.29500
Epoch: 0020 train_loss= 2.41391 train_acc= 0.30146 val_loss= 2.41353 val_acc= 0.31045 time= 0.29016
Epoch: 0021 train_loss= 2.37059 train_acc= 0.31105 val_loss= 2.37402 val_acc= 0.31343 time= 0.29000
Epoch: 0022 train_loss= 2.32555 train_acc= 0.31304 val_loss= 2.33410 val_acc= 0.31343 time= 0.29500
Epoch: 0023 train_loss= 2.28810 train_acc= 0.32197 val_loss= 2.29402 val_acc= 0.31940 time= 0.28800
Epoch: 0024 train_loss= 2.24193 train_acc= 0.32991 val_loss= 2.25396 val_acc= 0.32836 time= 0.28900
Epoch: 0025 train_loss= 2.19216 train_acc= 0.34679 val_loss= 2.21377 val_acc= 0.34030 time= 0.29604
Epoch: 0026 train_loss= 2.14670 train_acc= 0.37028 val_loss= 2.17338 val_acc= 0.36716 time= 0.29209
Epoch: 0027 train_loss= 2.09986 train_acc= 0.40139 val_loss= 2.13310 val_acc= 0.39104 time= 0.28800
Epoch: 0028 train_loss= 2.03777 train_acc= 0.45169 val_loss= 2.09309 val_acc= 0.43582 time= 0.29000
Epoch: 0029 train_loss= 1.99149 train_acc= 0.48445 val_loss= 2.05320 val_acc= 0.46269 time= 0.29599
Epoch: 0030 train_loss= 1.93262 train_acc= 0.51787 val_loss= 2.01281 val_acc= 0.48657 time= 0.29000
Epoch: 0031 train_loss= 1.88769 train_acc= 0.54335 val_loss= 1.97118 val_acc= 0.49552 time= 0.28797
Epoch: 0032 train_loss= 1.83066 train_acc= 0.56552 val_loss= 1.92894 val_acc= 0.51045 time= 0.29403
Epoch: 0033 train_loss= 1.79063 train_acc= 0.58008 val_loss= 1.88596 val_acc= 0.52239 time= 0.29402
Epoch: 0034 train_loss= 1.73543 train_acc= 0.58736 val_loss= 1.84363 val_acc= 0.52537 time= 0.28998
Epoch: 0035 train_loss= 1.67874 train_acc= 0.58934 val_loss= 1.80264 val_acc= 0.54030 time= 0.28902
Epoch: 0036 train_loss= 1.62512 train_acc= 0.59795 val_loss= 1.76402 val_acc= 0.53731 time= 0.29500
Epoch: 0037 train_loss= 1.57741 train_acc= 0.61118 val_loss= 1.72736 val_acc= 0.53731 time= 0.29600
Epoch: 0038 train_loss= 1.51939 train_acc= 0.62740 val_loss= 1.69280 val_acc= 0.54030 time= 0.29100
Epoch: 0039 train_loss= 1.47715 train_acc= 0.63336 val_loss= 1.65983 val_acc= 0.55522 time= 0.29400
Epoch: 0040 train_loss= 1.42773 train_acc= 0.64196 val_loss= 1.62854 val_acc= 0.57313 time= 0.29606
Epoch: 0041 train_loss= 1.38039 train_acc= 0.65122 val_loss= 1.59754 val_acc= 0.57313 time= 0.29000
Epoch: 0042 train_loss= 1.33343 train_acc= 0.66843 val_loss= 1.56734 val_acc= 0.58209 time= 0.29204
Epoch: 0043 train_loss= 1.29758 train_acc= 0.67935 val_loss= 1.53829 val_acc= 0.58209 time= 0.29297
Epoch: 0044 train_loss= 1.23963 train_acc= 0.68432 val_loss= 1.51077 val_acc= 0.58209 time= 0.29300
Epoch: 0045 train_loss= 1.19793 train_acc= 0.69954 val_loss= 1.48536 val_acc= 0.59104 time= 0.29100
Epoch: 0046 train_loss= 1.14841 train_acc= 0.70880 val_loss= 1.46124 val_acc= 0.60299 time= 0.29400
Epoch: 0047 train_loss= 1.12752 train_acc= 0.70251 val_loss= 1.43667 val_acc= 0.60597 time= 0.29200
Epoch: 0048 train_loss= 1.08008 train_acc= 0.72369 val_loss= 1.41401 val_acc= 0.61194 time= 0.29200
Epoch: 0049 train_loss= 1.05412 train_acc= 0.73130 val_loss= 1.39300 val_acc= 0.61493 time= 0.29300
Epoch: 0050 train_loss= 1.01242 train_acc= 0.74454 val_loss= 1.37260 val_acc= 0.62687 time= 0.29100
Epoch: 0051 train_loss= 0.98088 train_acc= 0.76142 val_loss= 1.35435 val_acc= 0.62687 time= 0.29200
Epoch: 0052 train_loss= 0.93821 train_acc= 0.76903 val_loss= 1.33675 val_acc= 0.62687 time= 0.29200
Epoch: 0053 train_loss= 0.91537 train_acc= 0.77862 val_loss= 1.32030 val_acc= 0.63284 time= 0.29397
Epoch: 0054 train_loss= 0.88145 train_acc= 0.77697 val_loss= 1.30570 val_acc= 0.62687 time= 0.29003
Epoch: 0055 train_loss= 0.84428 train_acc= 0.79550 val_loss= 1.29256 val_acc= 0.62687 time= 0.29500
Epoch: 0056 train_loss= 0.81300 train_acc= 0.80179 val_loss= 1.28013 val_acc= 0.63881 time= 0.29000
Epoch: 0057 train_loss= 0.78014 train_acc= 0.80973 val_loss= 1.26937 val_acc= 0.63881 time= 0.29200
Epoch: 0058 train_loss= 0.75347 train_acc= 0.81039 val_loss= 1.26142 val_acc= 0.63284 time= 0.29300
Epoch: 0059 train_loss= 0.74059 train_acc= 0.81271 val_loss= 1.25474 val_acc= 0.62985 time= 0.29200
Epoch: 0060 train_loss= 0.70885 train_acc= 0.82727 val_loss= 1.24622 val_acc= 0.62687 time= 0.29100
Epoch: 0061 train_loss= 0.68563 train_acc= 0.83256 val_loss= 1.23511 val_acc= 0.62985 time= 0.29100
Epoch: 0062 train_loss= 0.65489 train_acc= 0.84547 val_loss= 1.22394 val_acc= 0.63284 time= 0.29308
Epoch: 0063 train_loss= 0.63425 train_acc= 0.84844 val_loss= 1.21374 val_acc= 0.62985 time= 0.29000
Epoch: 0064 train_loss= 0.61160 train_acc= 0.85374 val_loss= 1.20719 val_acc= 0.62985 time= 0.29200
Epoch: 0065 train_loss= 0.59808 train_acc= 0.85341 val_loss= 1.20121 val_acc= 0.64179 time= 0.28800
Epoch: 0066 train_loss= 0.56960 train_acc= 0.85837 val_loss= 1.19547 val_acc= 0.64776 time= 0.29200
Epoch: 0067 train_loss= 0.55099 train_acc= 0.86962 val_loss= 1.19172 val_acc= 0.65672 time= 0.29300
Epoch: 0068 train_loss= 0.52386 train_acc= 0.87856 val_loss= 1.18780 val_acc= 0.65970 time= 0.29300
Epoch: 0069 train_loss= 0.52503 train_acc= 0.87459 val_loss= 1.18297 val_acc= 0.65672 time= 0.29197
Epoch: 0070 train_loss= 0.49110 train_acc= 0.88154 val_loss= 1.18146 val_acc= 0.65970 time= 0.29103
Epoch: 0071 train_loss= 0.48145 train_acc= 0.88484 val_loss= 1.17880 val_acc= 0.65672 time= 0.29400
Epoch: 0072 train_loss= 0.46010 train_acc= 0.89610 val_loss= 1.17365 val_acc= 0.66866 time= 0.29371
Epoch: 0073 train_loss= 0.44745 train_acc= 0.89775 val_loss= 1.16913 val_acc= 0.67463 time= 0.29100
Epoch: 0074 train_loss= 0.43684 train_acc= 0.90139 val_loss= 1.16697 val_acc= 0.68060 time= 0.29300
Epoch: 0075 train_loss= 0.41976 train_acc= 0.90139 val_loss= 1.16103 val_acc= 0.68060 time= 0.29000
Epoch: 0076 train_loss= 0.40654 train_acc= 0.91198 val_loss= 1.15508 val_acc= 0.67164 time= 0.28900
Epoch: 0077 train_loss= 0.39629 train_acc= 0.90834 val_loss= 1.15372 val_acc= 0.66567 time= 0.29600
Epoch: 0078 train_loss= 0.37091 train_acc= 0.91794 val_loss= 1.15632 val_acc= 0.65970 time= 0.29413
Epoch: 0079 train_loss= 0.36817 train_acc= 0.91827 val_loss= 1.15979 val_acc= 0.65672 time= 0.29097
Epoch: 0080 train_loss= 0.35928 train_acc= 0.92422 val_loss= 1.15894 val_acc= 0.65970 time= 0.29603
Epoch: 0081 train_loss= 0.34501 train_acc= 0.92852 val_loss= 1.15641 val_acc= 0.67164 time= 0.29402
Epoch: 0082 train_loss= 0.33686 train_acc= 0.93415 val_loss= 1.15582 val_acc= 0.67463 time= 0.29097
Epoch: 0083 train_loss= 0.32130 train_acc= 0.93382 val_loss= 1.15530 val_acc= 0.67761 time= 0.29100
Epoch: 0084 train_loss= 0.31206 train_acc= 0.93316 val_loss= 1.15393 val_acc= 0.67463 time= 0.29100
Epoch: 0085 train_loss= 0.30760 train_acc= 0.93150 val_loss= 1.15383 val_acc= 0.66866 time= 0.29200
Epoch: 0086 train_loss= 0.30034 train_acc= 0.93316 val_loss= 1.15694 val_acc= 0.67164 time= 0.28906
Early stopping...
Optimization Finished!
Test set results: cost= 1.16178 accuracy= 0.68563 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7331    0.6667    0.6983       342
           1     0.7130    0.7476    0.7299       103
           2     0.7200    0.6429    0.6792       140
           3     0.7105    0.3418    0.4615        79
           4     0.6312    0.7652    0.6918       132
           5     0.7023    0.7764    0.7375       313
           6     0.6726    0.7451    0.7070       102
           7     0.5946    0.3143    0.4112        70
           8     0.6667    0.3200    0.4324        50
           9     0.6175    0.7290    0.6686       155
          10     0.8440    0.6364    0.7256       187
          11     0.6050    0.6234    0.6141       231
          12     0.7853    0.7191    0.7507       178
          13     0.7652    0.8200    0.7916       600
          14     0.7917    0.8373    0.8138       590
          15     0.7714    0.7105    0.7397        76
          16     0.6875    0.3235    0.4400        34
          17     1.0000    0.1000    0.1818        10
          18     0.4079    0.5179    0.4564       419
          19     0.6442    0.5194    0.5751       129
          20     0.7391    0.6071    0.6667        28
          21     0.9545    0.7241    0.8235        29
          22     0.6667    0.3043    0.4179        46

    accuracy                         0.6856      4043
   macro avg     0.7141    0.5866    0.6180      4043
weighted avg     0.6956    0.6856    0.6827      4043

Macro average Test Precision, Recall and F1-Score...
(0.7140869860487965, 0.5866038086171832, 0.6180212569117921, None)
Micro average Test Precision, Recall and F1-Score...
(0.6856294830571358, 0.6856294830571358, 0.6856294830571358, None)
embeddings:
14157 3357 4043
[[ 0.2608381   0.38732246  0.29792953 ...  0.23299912  0.22183077
   0.29494718]
 [ 0.01680673  0.09417155 -0.0081646  ...  0.11178238 -0.15868117
   0.01093603]
 [ 0.2525324   0.04926025  0.09550792 ...  0.09963116  0.24421944
   0.0808005 ]
 ...
 [ 0.14436124  0.17521489  0.11280107 ...  0.11930261  0.19472542
   0.15276918]
 [-0.0615793   0.24479088  0.19170895 ...  0.18670267  0.09837347
   0.09013075]
 [ 0.13582246  0.2693553   0.12436087 ...  0.009235    0.15596378
   0.18860708]]

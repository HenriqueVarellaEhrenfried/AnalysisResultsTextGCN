(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.06618 val_loss= 3.12753 val_acc= 0.31940 time= 0.59082
Epoch: 0002 train_loss= 3.12757 train_acc= 0.28160 val_loss= 3.11233 val_acc= 0.30448 time= 0.29200
Epoch: 0003 train_loss= 3.11253 train_acc= 0.27134 val_loss= 3.09021 val_acc= 0.30149 time= 0.29200
Epoch: 0004 train_loss= 3.09099 train_acc= 0.27234 val_loss= 3.06103 val_acc= 0.30448 time= 0.29500
Epoch: 0005 train_loss= 3.06124 train_acc= 0.27035 val_loss= 3.02500 val_acc= 0.30448 time= 0.29100
Epoch: 0006 train_loss= 3.02671 train_acc= 0.27498 val_loss= 2.98289 val_acc= 0.30746 time= 0.29000
Epoch: 0007 train_loss= 2.98370 train_acc= 0.28127 val_loss= 2.93618 val_acc= 0.31343 time= 0.28701
Epoch: 0008 train_loss= 2.93847 train_acc= 0.27498 val_loss= 2.88721 val_acc= 0.31343 time= 0.29399
Epoch: 0009 train_loss= 2.88727 train_acc= 0.26969 val_loss= 2.83892 val_acc= 0.32239 time= 0.29100
Epoch: 0010 train_loss= 2.84509 train_acc= 0.26903 val_loss= 2.79443 val_acc= 0.31940 time= 0.29100
Epoch: 0011 train_loss= 2.79968 train_acc= 0.26274 val_loss= 2.75650 val_acc= 0.31343 time= 0.28897
Epoch: 0012 train_loss= 2.75899 train_acc= 0.26737 val_loss= 2.72726 val_acc= 0.31940 time= 0.29903
Epoch: 0013 train_loss= 2.73324 train_acc= 0.26307 val_loss= 2.70763 val_acc= 0.31642 time= 0.29496
Epoch: 0014 train_loss= 2.71968 train_acc= 0.26770 val_loss= 2.69622 val_acc= 0.30448 time= 0.29403
Epoch: 0015 train_loss= 2.70604 train_acc= 0.26572 val_loss= 2.68975 val_acc= 0.26567 time= 0.29446
Epoch: 0016 train_loss= 2.70458 train_acc= 0.23693 val_loss= 2.68446 val_acc= 0.22985 time= 0.29603
Epoch: 0017 train_loss= 2.69726 train_acc= 0.20880 val_loss= 2.67797 val_acc= 0.20896 time= 0.28800
Epoch: 0018 train_loss= 2.69578 train_acc= 0.18498 val_loss= 2.66920 val_acc= 0.20597 time= 0.29097
Epoch: 0019 train_loss= 2.68144 train_acc= 0.17571 val_loss= 2.65816 val_acc= 0.20597 time= 0.29600
Epoch: 0020 train_loss= 2.66937 train_acc= 0.17505 val_loss= 2.64561 val_acc= 0.20597 time= 0.30702
Epoch: 0021 train_loss= 2.64754 train_acc= 0.17439 val_loss= 2.63246 val_acc= 0.20597 time= 0.29197
Epoch: 0022 train_loss= 2.63323 train_acc= 0.17538 val_loss= 2.61943 val_acc= 0.20896 time= 0.29303
Epoch: 0023 train_loss= 2.61853 train_acc= 0.17902 val_loss= 2.60688 val_acc= 0.21194 time= 0.29200
Epoch: 0024 train_loss= 2.60368 train_acc= 0.18266 val_loss= 2.59484 val_acc= 0.22388 time= 0.29097
Epoch: 0025 train_loss= 2.58084 train_acc= 0.19424 val_loss= 2.58297 val_acc= 0.23284 time= 0.28903
Epoch: 0026 train_loss= 2.57565 train_acc= 0.20053 val_loss= 2.57077 val_acc= 0.24776 time= 0.29101
Epoch: 0027 train_loss= 2.55727 train_acc= 0.21608 val_loss= 2.55779 val_acc= 0.25970 time= 0.29727
Epoch: 0028 train_loss= 2.53733 train_acc= 0.22634 val_loss= 2.54360 val_acc= 0.26866 time= 0.29095
Epoch: 0029 train_loss= 2.52610 train_acc= 0.24619 val_loss= 2.52813 val_acc= 0.28060 time= 0.29216
Epoch: 0030 train_loss= 2.50354 train_acc= 0.25347 val_loss= 2.51138 val_acc= 0.28955 time= 0.29299
Epoch: 0031 train_loss= 2.48609 train_acc= 0.27399 val_loss= 2.49349 val_acc= 0.29552 time= 0.29303
Epoch: 0032 train_loss= 2.47209 train_acc= 0.27763 val_loss= 2.47452 val_acc= 0.30448 time= 0.28900
Epoch: 0033 train_loss= 2.44850 train_acc= 0.27697 val_loss= 2.45475 val_acc= 0.30746 time= 0.29200
Epoch: 0034 train_loss= 2.41854 train_acc= 0.29153 val_loss= 2.43430 val_acc= 0.30746 time= 0.28997
Epoch: 0035 train_loss= 2.40808 train_acc= 0.28954 val_loss= 2.41338 val_acc= 0.30746 time= 0.29352
Epoch: 0036 train_loss= 2.37570 train_acc= 0.29881 val_loss= 2.39204 val_acc= 0.30746 time= 0.29101
Epoch: 0037 train_loss= 2.36449 train_acc= 0.30278 val_loss= 2.37044 val_acc= 0.30746 time= 0.29200
Epoch: 0038 train_loss= 2.33809 train_acc= 0.30774 val_loss= 2.34881 val_acc= 0.30746 time= 0.29500
Epoch: 0039 train_loss= 2.32094 train_acc= 0.30841 val_loss= 2.32723 val_acc= 0.30746 time= 0.29696
Epoch: 0040 train_loss= 2.28775 train_acc= 0.31337 val_loss= 2.30565 val_acc= 0.31343 time= 0.29736
Epoch: 0041 train_loss= 2.27018 train_acc= 0.32032 val_loss= 2.28415 val_acc= 0.31940 time= 0.29069
Epoch: 0042 train_loss= 2.23229 train_acc= 0.33786 val_loss= 2.26264 val_acc= 0.33433 time= 0.29299
Epoch: 0043 train_loss= 2.21755 train_acc= 0.35242 val_loss= 2.24096 val_acc= 0.34627 time= 0.28897
Epoch: 0044 train_loss= 2.18253 train_acc= 0.37062 val_loss= 2.21904 val_acc= 0.36418 time= 0.29000
Epoch: 0045 train_loss= 2.15281 train_acc= 0.39676 val_loss= 2.19683 val_acc= 0.37612 time= 0.28700
Epoch: 0046 train_loss= 2.12974 train_acc= 0.40635 val_loss= 2.17441 val_acc= 0.39104 time= 0.29500
Epoch: 0047 train_loss= 2.09643 train_acc= 0.42290 val_loss= 2.15168 val_acc= 0.40597 time= 0.29100
Epoch: 0048 train_loss= 2.07237 train_acc= 0.44044 val_loss= 2.12860 val_acc= 0.42687 time= 0.28900
Epoch: 0049 train_loss= 2.05120 train_acc= 0.45301 val_loss= 2.10533 val_acc= 0.44179 time= 0.29100
Epoch: 0050 train_loss= 2.02220 train_acc= 0.47783 val_loss= 2.08174 val_acc= 0.46269 time= 0.29600
Epoch: 0051 train_loss= 1.98365 train_acc= 0.48842 val_loss= 2.05819 val_acc= 0.46269 time= 0.28900
Epoch: 0052 train_loss= 1.96083 train_acc= 0.51357 val_loss= 2.03432 val_acc= 0.46567 time= 0.29004
Epoch: 0053 train_loss= 1.93336 train_acc= 0.51522 val_loss= 2.01034 val_acc= 0.46866 time= 0.29474
Epoch: 0054 train_loss= 1.90378 train_acc= 0.51820 val_loss= 1.98670 val_acc= 0.48358 time= 0.29500
Epoch: 0055 train_loss= 1.87550 train_acc= 0.52647 val_loss= 1.96319 val_acc= 0.48657 time= 0.28800
Epoch: 0056 train_loss= 1.84082 train_acc= 0.53342 val_loss= 1.94011 val_acc= 0.48358 time= 0.29100
Epoch: 0057 train_loss= 1.81127 train_acc= 0.53772 val_loss= 1.91744 val_acc= 0.48358 time= 0.29394
Epoch: 0058 train_loss= 1.80526 train_acc= 0.54169 val_loss= 1.89525 val_acc= 0.49851 time= 0.29300
Epoch: 0059 train_loss= 1.76415 train_acc= 0.54699 val_loss= 1.87344 val_acc= 0.51343 time= 0.29400
Epoch: 0060 train_loss= 1.73312 train_acc= 0.57015 val_loss= 1.85203 val_acc= 0.51343 time= 0.29817
Epoch: 0061 train_loss= 1.71011 train_acc= 0.56949 val_loss= 1.83117 val_acc= 0.52239 time= 0.30197
Epoch: 0062 train_loss= 1.69653 train_acc= 0.56883 val_loss= 1.81095 val_acc= 0.52836 time= 0.29003
Epoch: 0063 train_loss= 1.64279 train_acc= 0.58570 val_loss= 1.79168 val_acc= 0.52836 time= 0.29197
Epoch: 0064 train_loss= 1.63266 train_acc= 0.59398 val_loss= 1.77298 val_acc= 0.53134 time= 0.29104
Epoch: 0065 train_loss= 1.61682 train_acc= 0.59365 val_loss= 1.75377 val_acc= 0.52836 time= 0.29799
Epoch: 0066 train_loss= 1.57542 train_acc= 0.60953 val_loss= 1.73442 val_acc= 0.53134 time= 0.28900
Epoch: 0067 train_loss= 1.54309 train_acc= 0.60523 val_loss= 1.71541 val_acc= 0.53433 time= 0.29100
Epoch: 0068 train_loss= 1.50755 train_acc= 0.62012 val_loss= 1.69696 val_acc= 0.54030 time= 0.29197
Epoch: 0069 train_loss= 1.49491 train_acc= 0.61317 val_loss= 1.67889 val_acc= 0.54030 time= 0.29476
Epoch: 0070 train_loss= 1.46799 train_acc= 0.62641 val_loss= 1.66120 val_acc= 0.54627 time= 0.29100
Epoch: 0071 train_loss= 1.45114 train_acc= 0.63733 val_loss= 1.64368 val_acc= 0.55522 time= 0.29000
Epoch: 0072 train_loss= 1.42764 train_acc= 0.63997 val_loss= 1.62671 val_acc= 0.55522 time= 0.29500
Epoch: 0073 train_loss= 1.40861 train_acc= 0.64097 val_loss= 1.61020 val_acc= 0.56418 time= 0.29500
Epoch: 0074 train_loss= 1.39965 train_acc= 0.64990 val_loss= 1.59450 val_acc= 0.55821 time= 0.28900
Epoch: 0075 train_loss= 1.35913 train_acc= 0.65817 val_loss= 1.57904 val_acc= 0.55821 time= 0.29200
Epoch: 0076 train_loss= 1.32757 train_acc= 0.66711 val_loss= 1.56399 val_acc= 0.56716 time= 0.29401
Epoch: 0077 train_loss= 1.32213 train_acc= 0.66148 val_loss= 1.54914 val_acc= 0.57612 time= 0.29199
Epoch: 0078 train_loss= 1.29012 train_acc= 0.67472 val_loss= 1.53487 val_acc= 0.57612 time= 0.29200
Epoch: 0079 train_loss= 1.27837 train_acc= 0.66876 val_loss= 1.52061 val_acc= 0.57612 time= 0.29200
Epoch: 0080 train_loss= 1.25665 train_acc= 0.68365 val_loss= 1.50677 val_acc= 0.57910 time= 0.30284
Epoch: 0081 train_loss= 1.24794 train_acc= 0.68994 val_loss= 1.49362 val_acc= 0.57910 time= 0.29209
Epoch: 0082 train_loss= 1.20531 train_acc= 0.69259 val_loss= 1.48139 val_acc= 0.58209 time= 0.29297
Epoch: 0083 train_loss= 1.19560 train_acc= 0.70582 val_loss= 1.47066 val_acc= 0.59104 time= 0.29400
Epoch: 0084 train_loss= 1.17331 train_acc= 0.71079 val_loss= 1.46009 val_acc= 0.59104 time= 0.30417
Epoch: 0085 train_loss= 1.15741 train_acc= 0.70582 val_loss= 1.44938 val_acc= 0.59403 time= 0.30100
Epoch: 0086 train_loss= 1.12966 train_acc= 0.71046 val_loss= 1.43828 val_acc= 0.60896 time= 0.29600
Epoch: 0087 train_loss= 1.11845 train_acc= 0.72336 val_loss= 1.42772 val_acc= 0.60597 time= 0.31082
Epoch: 0088 train_loss= 1.08946 train_acc= 0.72799 val_loss= 1.41734 val_acc= 0.60597 time= 0.31100
Epoch: 0089 train_loss= 1.07701 train_acc= 0.73494 val_loss= 1.40679 val_acc= 0.61194 time= 0.29604
Epoch: 0090 train_loss= 1.05507 train_acc= 0.72303 val_loss= 1.39556 val_acc= 0.61493 time= 0.29601
Epoch: 0091 train_loss= 1.04683 train_acc= 0.73693 val_loss= 1.38454 val_acc= 0.62090 time= 0.29792
Epoch: 0092 train_loss= 1.03714 train_acc= 0.74289 val_loss= 1.37458 val_acc= 0.62388 time= 0.29498
Epoch: 0093 train_loss= 1.01149 train_acc= 0.75381 val_loss= 1.36555 val_acc= 0.62090 time= 0.29800
Epoch: 0094 train_loss= 1.00212 train_acc= 0.75182 val_loss= 1.35731 val_acc= 0.62687 time= 0.29700
Epoch: 0095 train_loss= 0.98309 train_acc= 0.74983 val_loss= 1.34954 val_acc= 0.62687 time= 0.31200
Epoch: 0096 train_loss= 0.97252 train_acc= 0.76009 val_loss= 1.34168 val_acc= 0.62687 time= 0.30100
Epoch: 0097 train_loss= 0.95815 train_acc= 0.76704 val_loss= 1.33379 val_acc= 0.63284 time= 0.30000
Epoch: 0098 train_loss= 0.94959 train_acc= 0.75844 val_loss= 1.32717 val_acc= 0.63284 time= 0.29300
Epoch: 0099 train_loss= 0.92045 train_acc= 0.77234 val_loss= 1.32113 val_acc= 0.62985 time= 0.29900
Epoch: 0100 train_loss= 0.89858 train_acc= 0.77432 val_loss= 1.31570 val_acc= 0.63582 time= 0.29200
Epoch: 0101 train_loss= 0.90332 train_acc= 0.78359 val_loss= 1.30996 val_acc= 0.63284 time= 0.29200
Epoch: 0102 train_loss= 0.89165 train_acc= 0.77929 val_loss= 1.30425 val_acc= 0.63881 time= 0.29600
Epoch: 0103 train_loss= 0.87151 train_acc= 0.78392 val_loss= 1.29769 val_acc= 0.63881 time= 0.29200
Epoch: 0104 train_loss= 0.85112 train_acc= 0.78954 val_loss= 1.29021 val_acc= 0.63881 time= 0.29000
Epoch: 0105 train_loss= 0.84178 train_acc= 0.79848 val_loss= 1.28289 val_acc= 0.64179 time= 0.29057
Epoch: 0106 train_loss= 0.83911 train_acc= 0.78888 val_loss= 1.27585 val_acc= 0.63881 time= 0.29400
Epoch: 0107 train_loss= 0.82362 train_acc= 0.79153 val_loss= 1.26924 val_acc= 0.64478 time= 0.30000
Epoch: 0108 train_loss= 0.79799 train_acc= 0.80510 val_loss= 1.26324 val_acc= 0.64478 time= 0.30600
Epoch: 0109 train_loss= 0.80476 train_acc= 0.80576 val_loss= 1.25764 val_acc= 0.65075 time= 0.29500
Epoch: 0110 train_loss= 0.79004 train_acc= 0.80278 val_loss= 1.25254 val_acc= 0.65373 time= 0.30497
Epoch: 0111 train_loss= 0.77220 train_acc= 0.80212 val_loss= 1.24803 val_acc= 0.64478 time= 0.29700
Epoch: 0112 train_loss= 0.76279 train_acc= 0.81767 val_loss= 1.24393 val_acc= 0.64478 time= 0.30300
Epoch: 0113 train_loss= 0.75195 train_acc= 0.81602 val_loss= 1.23935 val_acc= 0.63582 time= 0.30380
Epoch: 0114 train_loss= 0.74543 train_acc= 0.82065 val_loss= 1.23426 val_acc= 0.64478 time= 0.30503
Epoch: 0115 train_loss= 0.73405 train_acc= 0.81800 val_loss= 1.22995 val_acc= 0.63881 time= 0.30000
Epoch: 0116 train_loss= 0.72319 train_acc= 0.82164 val_loss= 1.22615 val_acc= 0.64179 time= 0.29500
Epoch: 0117 train_loss= 0.72528 train_acc= 0.82032 val_loss= 1.22298 val_acc= 0.64179 time= 0.29800
Epoch: 0118 train_loss= 0.72129 train_acc= 0.81966 val_loss= 1.22016 val_acc= 0.63582 time= 0.29297
Epoch: 0119 train_loss= 0.70243 train_acc= 0.82760 val_loss= 1.21673 val_acc= 0.63881 time= 0.29103
Epoch: 0120 train_loss= 0.67739 train_acc= 0.83488 val_loss= 1.21253 val_acc= 0.64478 time= 0.29000
Epoch: 0121 train_loss= 0.67950 train_acc= 0.84183 val_loss= 1.20779 val_acc= 0.64179 time= 0.29697
Epoch: 0122 train_loss= 0.66459 train_acc= 0.82892 val_loss= 1.20372 val_acc= 0.64478 time= 0.29200
Epoch: 0123 train_loss= 0.65764 train_acc= 0.84381 val_loss= 1.20040 val_acc= 0.64478 time= 0.29224
Epoch: 0124 train_loss= 0.65752 train_acc= 0.83984 val_loss= 1.19787 val_acc= 0.64776 time= 0.29402
Epoch: 0125 train_loss= 0.64146 train_acc= 0.84447 val_loss= 1.19525 val_acc= 0.64478 time= 0.30301
Epoch: 0126 train_loss= 0.63405 train_acc= 0.84977 val_loss= 1.19285 val_acc= 0.65075 time= 0.29299
Epoch: 0127 train_loss= 0.61572 train_acc= 0.85374 val_loss= 1.18991 val_acc= 0.65373 time= 0.28903
Epoch: 0128 train_loss= 0.62161 train_acc= 0.84911 val_loss= 1.18618 val_acc= 0.65672 time= 0.29011
Epoch: 0129 train_loss= 0.59266 train_acc= 0.85837 val_loss= 1.18267 val_acc= 0.65970 time= 0.29400
Epoch: 0130 train_loss= 0.61362 train_acc= 0.85903 val_loss= 1.18020 val_acc= 0.65672 time= 0.29100
Epoch: 0131 train_loss= 0.59976 train_acc= 0.85672 val_loss= 1.17853 val_acc= 0.65672 time= 0.29100
Epoch: 0132 train_loss= 0.58858 train_acc= 0.86201 val_loss= 1.17678 val_acc= 0.65075 time= 0.29497
Epoch: 0133 train_loss= 0.58833 train_acc= 0.86433 val_loss= 1.17456 val_acc= 0.64776 time= 0.29803
Epoch: 0134 train_loss= 0.57018 train_acc= 0.86466 val_loss= 1.17263 val_acc= 0.64179 time= 0.29655
Epoch: 0135 train_loss= 0.55374 train_acc= 0.87326 val_loss= 1.17028 val_acc= 0.64776 time= 0.29800
Epoch: 0136 train_loss= 0.55920 train_acc= 0.87028 val_loss= 1.16830 val_acc= 0.64478 time= 0.29871
Epoch: 0137 train_loss= 0.53934 train_acc= 0.87558 val_loss= 1.16585 val_acc= 0.65075 time= 0.29200
Epoch: 0138 train_loss= 0.54522 train_acc= 0.87558 val_loss= 1.16368 val_acc= 0.64776 time= 0.29300
Epoch: 0139 train_loss= 0.52944 train_acc= 0.87690 val_loss= 1.16127 val_acc= 0.65075 time= 0.29200
Epoch: 0140 train_loss= 0.52447 train_acc= 0.87525 val_loss= 1.15926 val_acc= 0.66269 time= 0.29488
Epoch: 0141 train_loss= 0.51422 train_acc= 0.87889 val_loss= 1.15744 val_acc= 0.65970 time= 0.29800
Epoch: 0142 train_loss= 0.50393 train_acc= 0.88319 val_loss= 1.15636 val_acc= 0.66269 time= 0.29300
Epoch: 0143 train_loss= 0.51872 train_acc= 0.88187 val_loss= 1.15563 val_acc= 0.66269 time= 0.29197
Epoch: 0144 train_loss= 0.49492 train_acc= 0.88518 val_loss= 1.15581 val_acc= 0.65672 time= 0.29803
Epoch: 0145 train_loss= 0.49117 train_acc= 0.88716 val_loss= 1.15646 val_acc= 0.65970 time= 0.29301
Epoch: 0146 train_loss= 0.48758 train_acc= 0.89246 val_loss= 1.15623 val_acc= 0.65970 time= 0.29099
Epoch: 0147 train_loss= 0.48891 train_acc= 0.89246 val_loss= 1.15444 val_acc= 0.66269 time= 0.29056
Epoch: 0148 train_loss= 0.47935 train_acc= 0.89047 val_loss= 1.15223 val_acc= 0.66269 time= 0.29400
Epoch: 0149 train_loss= 0.48121 train_acc= 0.88815 val_loss= 1.14943 val_acc= 0.65672 time= 0.28900
Epoch: 0150 train_loss= 0.46263 train_acc= 0.89775 val_loss= 1.14835 val_acc= 0.65672 time= 0.29200
Epoch: 0151 train_loss= 0.45817 train_acc= 0.89874 val_loss= 1.14808 val_acc= 0.66269 time= 0.29699
Epoch: 0152 train_loss= 0.45127 train_acc= 0.90338 val_loss= 1.14686 val_acc= 0.66269 time= 0.28800
Epoch: 0153 train_loss= 0.44630 train_acc= 0.89742 val_loss= 1.14564 val_acc= 0.66567 time= 0.28800
Epoch: 0154 train_loss= 0.44215 train_acc= 0.89808 val_loss= 1.14501 val_acc= 0.66269 time= 0.28800
Epoch: 0155 train_loss= 0.44018 train_acc= 0.90536 val_loss= 1.14572 val_acc= 0.65970 time= 0.29597
Epoch: 0156 train_loss= 0.44725 train_acc= 0.89212 val_loss= 1.14765 val_acc= 0.65970 time= 0.29003
Early stopping...
Optimization Finished!
Test set results: cost= 1.16520 accuracy= 0.67945 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6949    0.7193    0.7069       342
           1     0.7075    0.7282    0.7177       103
           2     0.7573    0.5571    0.6420       140
           3     0.6429    0.3418    0.4463        79
           4     0.6861    0.7121    0.6989       132
           5     0.6527    0.7987    0.7184       313
           6     0.6330    0.6765    0.6540       102
           7     0.6286    0.3143    0.4190        70
           8     0.6000    0.2400    0.3429        50
           9     0.6243    0.7290    0.6726       155
          10     0.8310    0.6310    0.7173       187
          11     0.6245    0.6623    0.6429       231
          12     0.7622    0.7022    0.7310       178
          13     0.7716    0.8050    0.7879       600
          14     0.7738    0.8407    0.8058       590
          15     0.8065    0.6579    0.7246        76
          16     0.8182    0.2647    0.4000        34
          17     0.0000    0.0000    0.0000        10
          18     0.4107    0.5107    0.4553       419
          19     0.6667    0.5116    0.5789       129
          20     0.8000    0.4286    0.5581        28
          21     1.0000    0.7241    0.8400        29
          22     0.5385    0.3043    0.3889        46

    accuracy                         0.6794      4043
   macro avg     0.6709    0.5591    0.5935      4043
weighted avg     0.6866    0.6794    0.6748      4043

Macro average Test Precision, Recall and F1-Score...
(0.6709095583556529, 0.5591436888171536, 0.593459577401555, None)
Micro average Test Precision, Recall and F1-Score...
(0.6794459559732872, 0.6794459559732872, 0.6794459559732872, None)
embeddings:
14157 3357 4043
[[ 0.2584354   0.34043485  0.22227962 ...  0.27783394  0.25524235
   0.22667629]
 [ 0.0627006   0.1174731   0.07094114 ...  0.14508325  0.13178556
   0.05866973]
 [ 0.2797844   0.05762305  0.3482894  ...  0.20841476  0.1906782
   0.19765092]
 ...
 [ 0.25013652  0.1436312   0.06366949 ...  0.12811309  0.15144217
   0.06403697]
 [ 0.00056574 -0.00349563  0.22687781 ...  0.02824645 -0.02664078
   0.30072102]
 [ 0.08170242  0.25026342  0.03812587 ...  0.1927401   0.11325444
   0.01004785]]

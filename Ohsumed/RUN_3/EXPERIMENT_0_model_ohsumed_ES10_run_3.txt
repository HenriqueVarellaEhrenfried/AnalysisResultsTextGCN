(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13558 train_acc= 0.00927 val_loss= 3.11683 val_acc= 0.20896 time= 0.58697
Epoch: 0002 train_loss= 3.11682 train_acc= 0.18729 val_loss= 3.07337 val_acc= 0.20597 time= 0.29009
Epoch: 0003 train_loss= 3.07397 train_acc= 0.17505 val_loss= 3.00562 val_acc= 0.20000 time= 0.29500
Epoch: 0004 train_loss= 3.00758 train_acc= 0.17306 val_loss= 2.92013 val_acc= 0.20000 time= 0.29607
Epoch: 0005 train_loss= 2.92356 train_acc= 0.17174 val_loss= 2.83159 val_acc= 0.20000 time= 0.28903
Epoch: 0006 train_loss= 2.83695 train_acc= 0.17141 val_loss= 2.75655 val_acc= 0.20000 time= 0.29400
Epoch: 0007 train_loss= 2.76277 train_acc= 0.17141 val_loss= 2.70815 val_acc= 0.20000 time= 0.28900
Epoch: 0008 train_loss= 2.71686 train_acc= 0.17174 val_loss= 2.69302 val_acc= 0.20000 time= 0.29228
Epoch: 0009 train_loss= 2.70441 train_acc= 0.17240 val_loss= 2.69433 val_acc= 0.20000 time= 0.28900
Epoch: 0010 train_loss= 2.70551 train_acc= 0.17240 val_loss= 2.68699 val_acc= 0.20597 time= 0.29100
Epoch: 0011 train_loss= 2.69372 train_acc= 0.17306 val_loss= 2.66320 val_acc= 0.20597 time= 0.28997
Epoch: 0012 train_loss= 2.66173 train_acc= 0.17637 val_loss= 2.63167 val_acc= 0.21493 time= 0.29502
Epoch: 0013 train_loss= 2.62221 train_acc= 0.18729 val_loss= 2.60212 val_acc= 0.23284 time= 0.28800
Epoch: 0014 train_loss= 2.58370 train_acc= 0.20351 val_loss= 2.57792 val_acc= 0.25075 time= 0.29831
Epoch: 0015 train_loss= 2.55031 train_acc= 0.22568 val_loss= 2.55629 val_acc= 0.26567 time= 0.29300
Epoch: 0016 train_loss= 2.52239 train_acc= 0.24520 val_loss= 2.53289 val_acc= 0.28657 time= 0.29100
Epoch: 0017 train_loss= 2.49332 train_acc= 0.26671 val_loss= 2.50457 val_acc= 0.29851 time= 0.29300
Epoch: 0018 train_loss= 2.46396 train_acc= 0.28690 val_loss= 2.47078 val_acc= 0.30746 time= 0.28700
Epoch: 0019 train_loss= 2.42364 train_acc= 0.30907 val_loss= 2.43248 val_acc= 0.31343 time= 0.29400
Epoch: 0020 train_loss= 2.38241 train_acc= 0.32296 val_loss= 2.39135 val_acc= 0.31940 time= 0.29000
Epoch: 0021 train_loss= 2.33751 train_acc= 0.33587 val_loss= 2.34870 val_acc= 0.32239 time= 0.29200
Epoch: 0022 train_loss= 2.29320 train_acc= 0.34414 val_loss= 2.30545 val_acc= 0.32239 time= 0.28900
Epoch: 0023 train_loss= 2.24093 train_acc= 0.35076 val_loss= 2.26200 val_acc= 0.33134 time= 0.29272
Epoch: 0024 train_loss= 2.19253 train_acc= 0.35970 val_loss= 2.21843 val_acc= 0.35224 time= 0.29100
Epoch: 0025 train_loss= 2.13714 train_acc= 0.38253 val_loss= 2.17455 val_acc= 0.36418 time= 0.29601
Epoch: 0026 train_loss= 2.08539 train_acc= 0.40503 val_loss= 2.13062 val_acc= 0.38209 time= 0.29000
Epoch: 0027 train_loss= 2.03139 train_acc= 0.43647 val_loss= 2.08703 val_acc= 0.41791 time= 0.29400
Epoch: 0028 train_loss= 1.97608 train_acc= 0.48147 val_loss= 2.04423 val_acc= 0.45075 time= 0.29300
Epoch: 0029 train_loss= 1.91867 train_acc= 0.51688 val_loss= 2.00225 val_acc= 0.47164 time= 0.29000
Epoch: 0030 train_loss= 1.85805 train_acc= 0.54931 val_loss= 1.96032 val_acc= 0.49851 time= 0.29400
Epoch: 0031 train_loss= 1.80465 train_acc= 0.58041 val_loss= 1.91749 val_acc= 0.51940 time= 0.29200
Epoch: 0032 train_loss= 1.75152 train_acc= 0.59464 val_loss= 1.87389 val_acc= 0.53134 time= 0.29300
Epoch: 0033 train_loss= 1.69581 train_acc= 0.60556 val_loss= 1.83039 val_acc= 0.53433 time= 0.28900
Epoch: 0034 train_loss= 1.63811 train_acc= 0.60788 val_loss= 1.78785 val_acc= 0.53731 time= 0.29200
Epoch: 0035 train_loss= 1.57833 train_acc= 0.62144 val_loss= 1.74742 val_acc= 0.54627 time= 0.29200
Epoch: 0036 train_loss= 1.52386 train_acc= 0.63567 val_loss= 1.70915 val_acc= 0.54627 time= 0.29300
Epoch: 0037 train_loss= 1.46618 train_acc= 0.64527 val_loss= 1.67228 val_acc= 0.56716 time= 0.29400
Epoch: 0038 train_loss= 1.41527 train_acc= 0.65586 val_loss= 1.63647 val_acc= 0.56418 time= 0.29731
Epoch: 0039 train_loss= 1.36015 train_acc= 0.66214 val_loss= 1.60232 val_acc= 0.57015 time= 0.29500
Epoch: 0040 train_loss= 1.30983 train_acc= 0.67472 val_loss= 1.56999 val_acc= 0.57015 time= 0.28897
Epoch: 0041 train_loss= 1.25499 train_acc= 0.68630 val_loss= 1.53985 val_acc= 0.58209 time= 0.29303
Epoch: 0042 train_loss= 1.21073 train_acc= 0.70351 val_loss= 1.51076 val_acc= 0.58209 time= 0.29403
Epoch: 0043 train_loss= 1.16430 train_acc= 0.71244 val_loss= 1.48264 val_acc= 0.58507 time= 0.29600
Epoch: 0044 train_loss= 1.12268 train_acc= 0.72369 val_loss= 1.45515 val_acc= 0.59104 time= 0.28800
Epoch: 0045 train_loss= 1.07897 train_acc= 0.74156 val_loss= 1.42929 val_acc= 0.59403 time= 0.29500
Epoch: 0046 train_loss= 1.03289 train_acc= 0.74785 val_loss= 1.40534 val_acc= 0.60000 time= 0.29000
Epoch: 0047 train_loss= 0.98354 train_acc= 0.76439 val_loss= 1.38334 val_acc= 0.59701 time= 0.29297
Epoch: 0048 train_loss= 0.94015 train_acc= 0.77697 val_loss= 1.36268 val_acc= 0.60597 time= 0.29003
Epoch: 0049 train_loss= 0.89874 train_acc= 0.78954 val_loss= 1.34251 val_acc= 0.61493 time= 0.29600
Epoch: 0050 train_loss= 0.86428 train_acc= 0.79815 val_loss= 1.32370 val_acc= 0.62090 time= 0.29497
Epoch: 0051 train_loss= 0.82914 train_acc= 0.80410 val_loss= 1.30658 val_acc= 0.61493 time= 0.28903
Epoch: 0052 train_loss= 0.79437 train_acc= 0.81204 val_loss= 1.29116 val_acc= 0.60896 time= 0.29397
Epoch: 0053 train_loss= 0.75487 train_acc= 0.82727 val_loss= 1.27611 val_acc= 0.61791 time= 0.29400
Epoch: 0054 train_loss= 0.72351 train_acc= 0.83455 val_loss= 1.26200 val_acc= 0.62388 time= 0.29212
Epoch: 0055 train_loss= 0.69061 train_acc= 0.84249 val_loss= 1.24931 val_acc= 0.62687 time= 0.29130
Epoch: 0056 train_loss= 0.65841 train_acc= 0.84878 val_loss= 1.23822 val_acc= 0.62388 time= 0.29500
Epoch: 0057 train_loss= 0.63213 train_acc= 0.85440 val_loss= 1.22852 val_acc= 0.63284 time= 0.29503
Epoch: 0058 train_loss= 0.60026 train_acc= 0.86234 val_loss= 1.21988 val_acc= 0.63582 time= 0.29500
Epoch: 0059 train_loss= 0.57230 train_acc= 0.87525 val_loss= 1.21220 val_acc= 0.63881 time= 0.29000
Epoch: 0060 train_loss= 0.54871 train_acc= 0.88220 val_loss= 1.20461 val_acc= 0.64478 time= 0.29400
Epoch: 0061 train_loss= 0.51812 train_acc= 0.88815 val_loss= 1.19660 val_acc= 0.65075 time= 0.29407
Epoch: 0062 train_loss= 0.49212 train_acc= 0.89940 val_loss= 1.18885 val_acc= 0.65672 time= 0.29005
Epoch: 0063 train_loss= 0.47394 train_acc= 0.89841 val_loss= 1.18164 val_acc= 0.65970 time= 0.29300
Epoch: 0064 train_loss= 0.44899 train_acc= 0.90536 val_loss= 1.17620 val_acc= 0.65970 time= 0.29652
Epoch: 0065 train_loss= 0.42448 train_acc= 0.90999 val_loss= 1.17216 val_acc= 0.65672 time= 0.29200
Epoch: 0066 train_loss= 0.40727 train_acc= 0.91231 val_loss= 1.17072 val_acc= 0.66567 time= 0.29273
Epoch: 0067 train_loss= 0.39127 train_acc= 0.91827 val_loss= 1.16866 val_acc= 0.67164 time= 0.29300
Epoch: 0068 train_loss= 0.36946 train_acc= 0.92786 val_loss= 1.16564 val_acc= 0.66866 time= 0.29100
Epoch: 0069 train_loss= 0.35703 train_acc= 0.92720 val_loss= 1.16456 val_acc= 0.66567 time= 0.29300
Epoch: 0070 train_loss= 0.33895 train_acc= 0.93349 val_loss= 1.16419 val_acc= 0.65970 time= 0.29100
Epoch: 0071 train_loss= 0.32018 train_acc= 0.93812 val_loss= 1.16451 val_acc= 0.65970 time= 0.29500
Epoch: 0072 train_loss= 0.30871 train_acc= 0.94573 val_loss= 1.16524 val_acc= 0.65373 time= 0.29503
Epoch: 0073 train_loss= 0.29305 train_acc= 0.94871 val_loss= 1.16188 val_acc= 0.66269 time= 0.29000
Epoch: 0074 train_loss= 0.27856 train_acc= 0.95169 val_loss= 1.15879 val_acc= 0.65970 time= 0.28929
Epoch: 0075 train_loss= 0.26337 train_acc= 0.95301 val_loss= 1.15888 val_acc= 0.66567 time= 0.29467
Epoch: 0076 train_loss= 0.25339 train_acc= 0.95797 val_loss= 1.16027 val_acc= 0.66866 time= 0.29300
Epoch: 0077 train_loss= 0.24407 train_acc= 0.95797 val_loss= 1.16407 val_acc= 0.66567 time= 0.29198
Early stopping...
Optimization Finished!
Test set results: cost= 1.16323 accuracy= 0.68538 time= 0.12803
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7245    0.6842    0.7038       342
           1     0.7238    0.7379    0.7308       103
           2     0.7456    0.6071    0.6693       140
           3     0.6739    0.3924    0.4960        79
           4     0.6490    0.7424    0.6926       132
           5     0.6994    0.7732    0.7344       313
           6     0.6729    0.7059    0.6890       102
           7     0.6098    0.3571    0.4505        70
           8     0.6129    0.3800    0.4691        50
           9     0.6322    0.7097    0.6687       155
          10     0.8392    0.6417    0.7273       187
          11     0.6111    0.6667    0.6377       231
          12     0.7651    0.7135    0.7384       178
          13     0.7717    0.8167    0.7935       600
          14     0.7645    0.8475    0.8039       590
          15     0.7534    0.7237    0.7383        76
          16     0.7333    0.3235    0.4490        34
          17     0.3333    0.1000    0.1538        10
          18     0.4116    0.4893    0.4471       419
          19     0.7273    0.4961    0.5899       129
          20     0.5926    0.5714    0.5818        28
          21     0.9545    0.7241    0.8235        29
          22     0.5172    0.3261    0.4000        46

    accuracy                         0.6854      4043
   macro avg     0.6747    0.5883    0.6169      4043
weighted avg     0.6908    0.6854    0.6824      4043

Macro average Test Precision, Recall and F1-Score...
(0.6747332876049361, 0.5882673446751088, 0.6168794397636084, None)
Micro average Test Precision, Recall and F1-Score...
(0.6853821419737819, 0.6853821419737819, 0.6853821419737819, None)
embeddings:
14157 3357 4043
[[ 0.49212912  0.24887922  0.19329095 ...  0.43585563  0.32180852
   0.39718843]
 [ 0.3188238  -0.01568362  0.1707299  ...  0.3183766   0.05433736
   0.00498664]
 [ 0.2752798   0.13487539  0.263946   ...  0.53219795  0.271895
   0.0877014 ]
 ...
 [ 0.1721615   0.09975123  0.18273474 ...  0.14713661  0.14532661
   0.1831711 ]
 [ 0.29479557 -0.08687629  0.38343924 ...  0.31631318  0.06765775
   0.02065518]
 [ 0.2648101   0.14353064  0.12786753 ...  0.08534505  0.13891628
   0.29585946]]

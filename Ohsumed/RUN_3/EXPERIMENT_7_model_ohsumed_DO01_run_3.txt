(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.06519 val_loss= 3.11374 val_acc= 0.20597 time= 0.58335
Epoch: 0002 train_loss= 3.11372 train_acc= 0.18432 val_loss= 3.06650 val_acc= 0.20597 time= 0.29003
Epoch: 0003 train_loss= 3.06687 train_acc= 0.17604 val_loss= 2.99439 val_acc= 0.20597 time= 0.29000
Epoch: 0004 train_loss= 2.99531 train_acc= 0.17373 val_loss= 2.90431 val_acc= 0.20299 time= 0.29001
Epoch: 0005 train_loss= 2.90638 train_acc= 0.17306 val_loss= 2.81223 val_acc= 0.20299 time= 0.29697
Epoch: 0006 train_loss= 2.81585 train_acc= 0.17273 val_loss= 2.73751 val_acc= 0.20597 time= 0.29299
Epoch: 0007 train_loss= 2.74322 train_acc= 0.17340 val_loss= 2.69606 val_acc= 0.20597 time= 0.29203
Epoch: 0008 train_loss= 2.70373 train_acc= 0.17505 val_loss= 2.68887 val_acc= 0.20597 time= 0.29551
Epoch: 0009 train_loss= 2.69723 train_acc= 0.17538 val_loss= 2.68973 val_acc= 0.20299 time= 0.29336
Epoch: 0010 train_loss= 2.69651 train_acc= 0.17273 val_loss= 2.67680 val_acc= 0.20597 time= 0.29099
Epoch: 0011 train_loss= 2.67784 train_acc= 0.17306 val_loss= 2.64949 val_acc= 0.20597 time= 0.29844
Epoch: 0012 train_loss= 2.64036 train_acc= 0.17505 val_loss= 2.61880 val_acc= 0.20896 time= 0.29800
Epoch: 0013 train_loss= 2.59873 train_acc= 0.18531 val_loss= 2.59180 val_acc= 0.22985 time= 0.29725
Epoch: 0014 train_loss= 2.56188 train_acc= 0.19954 val_loss= 2.56819 val_acc= 0.25075 time= 0.29400
Epoch: 0015 train_loss= 2.52943 train_acc= 0.22502 val_loss= 2.54414 val_acc= 0.27761 time= 0.28900
Epoch: 0016 train_loss= 2.49871 train_acc= 0.26208 val_loss= 2.51606 val_acc= 0.29254 time= 0.29400
Epoch: 0017 train_loss= 2.46488 train_acc= 0.30344 val_loss= 2.48228 val_acc= 0.31045 time= 0.29005
Epoch: 0018 train_loss= 2.42674 train_acc= 0.33984 val_loss= 2.44323 val_acc= 0.32239 time= 0.28997
Epoch: 0019 train_loss= 2.38277 train_acc= 0.36201 val_loss= 2.40055 val_acc= 0.32836 time= 0.29400
Epoch: 0020 train_loss= 2.33749 train_acc= 0.37062 val_loss= 2.35615 val_acc= 0.33134 time= 0.29500
Epoch: 0021 train_loss= 2.28765 train_acc= 0.37690 val_loss= 2.31131 val_acc= 0.33134 time= 0.28903
Epoch: 0022 train_loss= 2.23734 train_acc= 0.38087 val_loss= 2.26666 val_acc= 0.34030 time= 0.28708
Epoch: 0023 train_loss= 2.18612 train_acc= 0.38815 val_loss= 2.22225 val_acc= 0.34627 time= 0.29100
Epoch: 0024 train_loss= 2.13247 train_acc= 0.40602 val_loss= 2.17783 val_acc= 0.37313 time= 0.29500
Epoch: 0025 train_loss= 2.07794 train_acc= 0.43283 val_loss= 2.13330 val_acc= 0.40896 time= 0.28733
Epoch: 0026 train_loss= 2.01954 train_acc= 0.46823 val_loss= 2.08905 val_acc= 0.42985 time= 0.28800
Epoch: 0027 train_loss= 1.96131 train_acc= 0.50860 val_loss= 2.04543 val_acc= 0.45970 time= 0.29577
Epoch: 0028 train_loss= 1.90282 train_acc= 0.54070 val_loss= 2.00215 val_acc= 0.49552 time= 0.29300
Epoch: 0029 train_loss= 1.84461 train_acc= 0.57048 val_loss= 1.95824 val_acc= 0.51045 time= 0.29200
Epoch: 0030 train_loss= 1.78746 train_acc= 0.58901 val_loss= 1.91314 val_acc= 0.52239 time= 0.29100
Epoch: 0031 train_loss= 1.72871 train_acc= 0.59927 val_loss= 1.86726 val_acc= 0.52239 time= 0.29400
Epoch: 0032 train_loss= 1.66881 train_acc= 0.60688 val_loss= 1.82192 val_acc= 0.53433 time= 0.29200
Epoch: 0033 train_loss= 1.60909 train_acc= 0.61582 val_loss= 1.77869 val_acc= 0.53731 time= 0.29101
Epoch: 0034 train_loss= 1.55109 train_acc= 0.62442 val_loss= 1.73816 val_acc= 0.54627 time= 0.29199
Epoch: 0035 train_loss= 1.49479 train_acc= 0.63766 val_loss= 1.70022 val_acc= 0.55522 time= 0.29377
Epoch: 0036 train_loss= 1.44079 train_acc= 0.64560 val_loss= 1.66437 val_acc= 0.55821 time= 0.28897
Epoch: 0037 train_loss= 1.38629 train_acc= 0.65486 val_loss= 1.63018 val_acc= 0.56716 time= 0.29103
Epoch: 0038 train_loss= 1.33431 train_acc= 0.67141 val_loss= 1.59746 val_acc= 0.57313 time= 0.29215
Epoch: 0039 train_loss= 1.28245 train_acc= 0.68200 val_loss= 1.56615 val_acc= 0.56418 time= 0.30343
Epoch: 0040 train_loss= 1.23122 train_acc= 0.69391 val_loss= 1.53577 val_acc= 0.57612 time= 0.29400
Epoch: 0041 train_loss= 1.18188 train_acc= 0.70516 val_loss= 1.50617 val_acc= 0.57015 time= 0.29219
Epoch: 0042 train_loss= 1.13321 train_acc= 0.71972 val_loss= 1.47776 val_acc= 0.57910 time= 0.29913
Epoch: 0043 train_loss= 1.08614 train_acc= 0.73461 val_loss= 1.45102 val_acc= 0.58209 time= 0.29500
Epoch: 0044 train_loss= 1.03941 train_acc= 0.75414 val_loss= 1.42578 val_acc= 0.58507 time= 0.29500
Epoch: 0045 train_loss= 0.99489 train_acc= 0.76175 val_loss= 1.40223 val_acc= 0.59104 time= 0.29600
Epoch: 0046 train_loss= 0.95226 train_acc= 0.77498 val_loss= 1.38012 val_acc= 0.59701 time= 0.29403
Epoch: 0047 train_loss= 0.91318 train_acc= 0.78425 val_loss= 1.35959 val_acc= 0.61194 time= 0.29000
Epoch: 0048 train_loss= 0.86992 train_acc= 0.79881 val_loss= 1.34059 val_acc= 0.61493 time= 0.29397
Epoch: 0049 train_loss= 0.82989 train_acc= 0.80973 val_loss= 1.32307 val_acc= 0.60896 time= 0.29500
Epoch: 0050 train_loss= 0.79143 train_acc= 0.81966 val_loss= 1.30658 val_acc= 0.61194 time= 0.30500
Epoch: 0051 train_loss= 0.75530 train_acc= 0.82958 val_loss= 1.29129 val_acc= 0.62090 time= 0.29203
Epoch: 0052 train_loss= 0.71891 train_acc= 0.83984 val_loss= 1.27713 val_acc= 0.63284 time= 0.29300
Epoch: 0053 train_loss= 0.68399 train_acc= 0.85109 val_loss= 1.26443 val_acc= 0.62985 time= 0.30190
Epoch: 0054 train_loss= 0.65229 train_acc= 0.85837 val_loss= 1.25277 val_acc= 0.63582 time= 0.29103
Epoch: 0055 train_loss= 0.62187 train_acc= 0.86466 val_loss= 1.24242 val_acc= 0.62985 time= 0.28899
Epoch: 0056 train_loss= 0.59039 train_acc= 0.87558 val_loss= 1.23280 val_acc= 0.63582 time= 0.29097
Epoch: 0057 train_loss= 0.56279 train_acc= 0.87790 val_loss= 1.22379 val_acc= 0.62985 time= 0.29900
Epoch: 0058 train_loss= 0.53401 train_acc= 0.88484 val_loss= 1.21494 val_acc= 0.63582 time= 0.29603
Epoch: 0059 train_loss= 0.50856 train_acc= 0.89345 val_loss= 1.20647 val_acc= 0.63881 time= 0.29000
Epoch: 0060 train_loss= 0.48142 train_acc= 0.90073 val_loss= 1.19892 val_acc= 0.63881 time= 0.29101
Epoch: 0061 train_loss= 0.45712 train_acc= 0.90503 val_loss= 1.19233 val_acc= 0.64179 time= 0.29700
Epoch: 0062 train_loss= 0.43469 train_acc= 0.91165 val_loss= 1.18655 val_acc= 0.63582 time= 0.29100
Epoch: 0063 train_loss= 0.41337 train_acc= 0.91860 val_loss= 1.18166 val_acc= 0.64776 time= 0.28600
Epoch: 0064 train_loss= 0.39274 train_acc= 0.92158 val_loss= 1.17829 val_acc= 0.64776 time= 0.29097
Epoch: 0065 train_loss= 0.37289 train_acc= 0.92753 val_loss= 1.17630 val_acc= 0.65075 time= 0.29700
Epoch: 0066 train_loss= 0.35349 train_acc= 0.93051 val_loss= 1.17528 val_acc= 0.65373 time= 0.28903
Epoch: 0067 train_loss= 0.33484 train_acc= 0.93680 val_loss= 1.17351 val_acc= 0.65970 time= 0.28897
Epoch: 0068 train_loss= 0.31817 train_acc= 0.93911 val_loss= 1.17132 val_acc= 0.65970 time= 0.29100
Epoch: 0069 train_loss= 0.30185 train_acc= 0.94474 val_loss= 1.16918 val_acc= 0.65970 time= 0.29103
Epoch: 0070 train_loss= 0.28718 train_acc= 0.95003 val_loss= 1.16793 val_acc= 0.65672 time= 0.29100
Epoch: 0071 train_loss= 0.27199 train_acc= 0.95235 val_loss= 1.16826 val_acc= 0.66567 time= 0.28900
Epoch: 0072 train_loss= 0.25791 train_acc= 0.95897 val_loss= 1.16939 val_acc= 0.67164 time= 0.29300
Epoch: 0073 train_loss= 0.24436 train_acc= 0.96161 val_loss= 1.17041 val_acc= 0.66866 time= 0.28800
Epoch: 0074 train_loss= 0.23181 train_acc= 0.96492 val_loss= 1.17203 val_acc= 0.66269 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.16430 accuracy= 0.69082 time= 0.13100
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7038    0.7018    0.7028       342
           1     0.6842    0.7573    0.7189       103
           2     0.7500    0.6214    0.6797       140
           3     0.6034    0.4430    0.5109        79
           4     0.6622    0.7424    0.7000       132
           5     0.6860    0.7955    0.7367       313
           6     0.7059    0.7059    0.7059       102
           7     0.5854    0.3429    0.4324        70
           8     0.6129    0.3800    0.4691        50
           9     0.6230    0.7355    0.6746       155
          10     0.8662    0.6578    0.7477       187
          11     0.6107    0.6450    0.6274       231
          12     0.7758    0.7191    0.7464       178
          13     0.7661    0.8133    0.7890       600
          14     0.7767    0.8492    0.8113       590
          15     0.7639    0.7237    0.7432        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4430    0.4726    0.4573       419
          19     0.6538    0.5271    0.5837       129
          20     0.6667    0.6429    0.6545        28
          21     1.0000    0.7586    0.8627        29
          22     0.6364    0.3043    0.4118        46

    accuracy                         0.6908      4043
   macro avg     0.6862    0.5997    0.6262      4043
weighted avg     0.6923    0.6908    0.6867      4043

Macro average Test Precision, Recall and F1-Score...
(0.6861622782439428, 0.5996598449591037, 0.6262309056640438, None)
Micro average Test Precision, Recall and F1-Score...
(0.6908236458075686, 0.6908236458075686, 0.6908236458075686, None)
embeddings:
14157 3357 4043
[[ 0.35679418  0.69791925  0.49746233 ...  0.36848402  0.5666471
   0.27966395]
 [ 0.00641758  0.31888688  0.05557117 ...  0.07890441  0.3761173
   0.00081054]
 [ 0.09425836  0.45292097  0.43429306 ...  0.23040533  0.4616833
   0.22064571]
 ...
 [ 0.10749269  0.21872994  0.2769129  ...  0.13787124  0.19146037
   0.14561048]
 [ 0.12752023  0.15640761 -0.03364801 ... -0.01855035 -0.02653486
  -0.06471218]
 [ 0.16474149  0.34393433  0.17889738 ...  0.21668002  0.24658167
   0.10211345]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13557 train_acc= 0.01158 val_loss= 3.11596 val_acc= 0.20597 time= 0.58693
Epoch: 0002 train_loss= 3.11588 train_acc= 0.17472 val_loss= 3.07331 val_acc= 0.20000 time= 0.29003
Epoch: 0003 train_loss= 3.07334 train_acc= 0.17306 val_loss= 3.00763 val_acc= 0.20000 time= 0.29197
Epoch: 0004 train_loss= 3.00808 train_acc= 0.17207 val_loss= 2.92372 val_acc= 0.20000 time= 0.29600
Epoch: 0005 train_loss= 2.92526 train_acc= 0.17207 val_loss= 2.83427 val_acc= 0.20000 time= 0.29503
Epoch: 0006 train_loss= 2.83785 train_acc= 0.17207 val_loss= 2.75703 val_acc= 0.20299 time= 0.29209
Epoch: 0007 train_loss= 2.76318 train_acc= 0.17340 val_loss= 2.70788 val_acc= 0.20597 time= 0.29441
Epoch: 0008 train_loss= 2.71595 train_acc= 0.17505 val_loss= 2.69175 val_acc= 0.20597 time= 0.29400
Epoch: 0009 train_loss= 2.70160 train_acc= 0.17737 val_loss= 2.69140 val_acc= 0.20597 time= 0.30000
Epoch: 0010 train_loss= 2.70044 train_acc= 0.17373 val_loss= 2.68501 val_acc= 0.20597 time= 0.29303
Epoch: 0011 train_loss= 2.68985 train_acc= 0.17373 val_loss= 2.66445 val_acc= 0.20597 time= 0.29500
Epoch: 0012 train_loss= 2.66106 train_acc= 0.17406 val_loss= 2.63651 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.62257 train_acc= 0.18233 val_loss= 2.60883 val_acc= 0.22687 time= 0.29700
Epoch: 0014 train_loss= 2.58462 train_acc= 0.19259 val_loss= 2.58374 val_acc= 0.24776 time= 0.29097
Epoch: 0015 train_loss= 2.55059 train_acc= 0.21443 val_loss= 2.55946 val_acc= 0.25970 time= 0.29600
Epoch: 0016 train_loss= 2.51836 train_acc= 0.24090 val_loss= 2.53281 val_acc= 0.28657 time= 0.29600
Epoch: 0017 train_loss= 2.48610 train_acc= 0.27035 val_loss= 2.50163 val_acc= 0.29851 time= 0.29300
Epoch: 0018 train_loss= 2.45066 train_acc= 0.30079 val_loss= 2.46524 val_acc= 0.31642 time= 0.29200
Epoch: 0019 train_loss= 2.41094 train_acc= 0.32760 val_loss= 2.42451 val_acc= 0.32239 time= 0.29600
Epoch: 0020 train_loss= 2.36663 train_acc= 0.34844 val_loss= 2.38080 val_acc= 0.33433 time= 0.29203
Epoch: 0021 train_loss= 2.31767 train_acc= 0.36135 val_loss= 2.33550 val_acc= 0.34627 time= 0.28900
Epoch: 0022 train_loss= 2.26582 train_acc= 0.37326 val_loss= 2.28950 val_acc= 0.34925 time= 0.29100
Epoch: 0023 train_loss= 2.21306 train_acc= 0.37988 val_loss= 2.24321 val_acc= 0.35821 time= 0.29397
Epoch: 0024 train_loss= 2.15790 train_acc= 0.39378 val_loss= 2.19671 val_acc= 0.35821 time= 0.29600
Epoch: 0025 train_loss= 2.10032 train_acc= 0.40933 val_loss= 2.14994 val_acc= 0.37910 time= 0.29100
Epoch: 0026 train_loss= 2.04015 train_acc= 0.43911 val_loss= 2.10312 val_acc= 0.40597 time= 0.29270
Epoch: 0027 train_loss= 1.98074 train_acc= 0.47485 val_loss= 2.05654 val_acc= 0.44478 time= 0.29384
Epoch: 0028 train_loss= 1.92158 train_acc= 0.50860 val_loss= 2.01025 val_acc= 0.46866 time= 0.28800
Epoch: 0029 train_loss= 1.85936 train_acc= 0.54335 val_loss= 1.96354 val_acc= 0.48955 time= 0.28901
Epoch: 0030 train_loss= 1.79795 train_acc= 0.57379 val_loss= 1.91592 val_acc= 0.50149 time= 0.29500
Epoch: 0031 train_loss= 1.73262 train_acc= 0.59696 val_loss= 1.86776 val_acc= 0.52537 time= 0.29000
Epoch: 0032 train_loss= 1.67001 train_acc= 0.61118 val_loss= 1.81989 val_acc= 0.52836 time= 0.28799
Epoch: 0033 train_loss= 1.60640 train_acc= 0.61681 val_loss= 1.77331 val_acc= 0.52537 time= 0.28800
Epoch: 0034 train_loss= 1.54406 train_acc= 0.62872 val_loss= 1.72845 val_acc= 0.53433 time= 0.29207
Epoch: 0035 train_loss= 1.48422 train_acc= 0.64428 val_loss= 1.68575 val_acc= 0.53731 time= 0.29000
Epoch: 0036 train_loss= 1.42553 train_acc= 0.65652 val_loss= 1.64551 val_acc= 0.54925 time= 0.28836
Epoch: 0037 train_loss= 1.36632 train_acc= 0.66876 val_loss= 1.60780 val_acc= 0.56119 time= 0.29000
Epoch: 0038 train_loss= 1.30975 train_acc= 0.68365 val_loss= 1.57244 val_acc= 0.57313 time= 0.29400
Epoch: 0039 train_loss= 1.25542 train_acc= 0.69457 val_loss= 1.53908 val_acc= 0.57910 time= 0.28900
Epoch: 0040 train_loss= 1.20168 train_acc= 0.71013 val_loss= 1.50734 val_acc= 0.57612 time= 0.28797
Epoch: 0041 train_loss= 1.14915 train_acc= 0.72005 val_loss= 1.47721 val_acc= 0.57910 time= 0.29000
Epoch: 0042 train_loss= 1.09964 train_acc= 0.73197 val_loss= 1.44916 val_acc= 0.58806 time= 0.29603
Epoch: 0043 train_loss= 1.04713 train_acc= 0.74553 val_loss= 1.42322 val_acc= 0.59403 time= 0.28900
Epoch: 0044 train_loss= 0.99906 train_acc= 0.76142 val_loss= 1.39886 val_acc= 0.59701 time= 0.28800
Epoch: 0045 train_loss= 0.95352 train_acc= 0.77631 val_loss= 1.37575 val_acc= 0.60299 time= 0.28959
Epoch: 0046 train_loss= 0.90934 train_acc= 0.78458 val_loss= 1.35380 val_acc= 0.60299 time= 0.29436
Epoch: 0047 train_loss= 0.86616 train_acc= 0.79616 val_loss= 1.33339 val_acc= 0.60896 time= 0.28700
Epoch: 0048 train_loss= 0.82433 train_acc= 0.80774 val_loss= 1.31469 val_acc= 0.61194 time= 0.29000
Epoch: 0049 train_loss= 0.78591 train_acc= 0.81469 val_loss= 1.29751 val_acc= 0.61791 time= 0.29285
Epoch: 0050 train_loss= 0.74415 train_acc= 0.82826 val_loss= 1.28237 val_acc= 0.62687 time= 0.29403
Epoch: 0051 train_loss= 0.70809 train_acc= 0.83752 val_loss= 1.26900 val_acc= 0.63881 time= 0.28955
Epoch: 0052 train_loss= 0.67376 train_acc= 0.84679 val_loss= 1.25729 val_acc= 0.64179 time= 0.28900
Epoch: 0053 train_loss= 0.63710 train_acc= 0.86003 val_loss= 1.24713 val_acc= 0.64478 time= 0.29697
Epoch: 0054 train_loss= 0.60733 train_acc= 0.86267 val_loss= 1.23816 val_acc= 0.65075 time= 0.29300
Epoch: 0055 train_loss= 0.57586 train_acc= 0.87095 val_loss= 1.23017 val_acc= 0.65373 time= 0.29100
Epoch: 0056 train_loss= 0.54569 train_acc= 0.87988 val_loss= 1.22260 val_acc= 0.65075 time= 0.29003
Epoch: 0057 train_loss= 0.51719 train_acc= 0.88584 val_loss= 1.21517 val_acc= 0.64478 time= 0.29214
Epoch: 0058 train_loss= 0.49166 train_acc= 0.89378 val_loss= 1.20782 val_acc= 0.64179 time= 0.29206
Epoch: 0059 train_loss= 0.46198 train_acc= 0.90238 val_loss= 1.20109 val_acc= 0.65075 time= 0.29500
Epoch: 0060 train_loss= 0.43950 train_acc= 0.90735 val_loss= 1.19543 val_acc= 0.65373 time= 0.29200
Epoch: 0061 train_loss= 0.41644 train_acc= 0.91396 val_loss= 1.19024 val_acc= 0.65075 time= 0.29600
Epoch: 0062 train_loss= 0.39374 train_acc= 0.92091 val_loss= 1.18631 val_acc= 0.65075 time= 0.29100
Epoch: 0063 train_loss= 0.37443 train_acc= 0.92687 val_loss= 1.18428 val_acc= 0.65373 time= 0.29100
Epoch: 0064 train_loss= 0.35408 train_acc= 0.93117 val_loss= 1.18319 val_acc= 0.65672 time= 0.29001
Epoch: 0065 train_loss= 0.33569 train_acc= 0.93547 val_loss= 1.18304 val_acc= 0.65970 time= 0.29799
Epoch: 0066 train_loss= 0.31760 train_acc= 0.93977 val_loss= 1.18291 val_acc= 0.66567 time= 0.29101
Epoch: 0067 train_loss= 0.30125 train_acc= 0.94441 val_loss= 1.18202 val_acc= 0.66567 time= 0.29099
Epoch: 0068 train_loss= 0.28547 train_acc= 0.95003 val_loss= 1.18082 val_acc= 0.66866 time= 0.29300
Epoch: 0069 train_loss= 0.27010 train_acc= 0.95103 val_loss= 1.18029 val_acc= 0.66866 time= 0.29400
Epoch: 0070 train_loss= 0.25581 train_acc= 0.95500 val_loss= 1.18053 val_acc= 0.66866 time= 0.29400
Epoch: 0071 train_loss= 0.24220 train_acc= 0.96128 val_loss= 1.18115 val_acc= 0.67463 time= 0.29201
Epoch: 0072 train_loss= 0.22963 train_acc= 0.96327 val_loss= 1.18198 val_acc= 0.67463 time= 0.29265
Epoch: 0073 train_loss= 0.21782 train_acc= 0.96625 val_loss= 1.18398 val_acc= 0.67761 time= 0.29400
Early stopping...
Optimization Finished!
Test set results: cost= 1.15947 accuracy= 0.69008 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7224    0.7076    0.7149       342
           1     0.6870    0.7670    0.7248       103
           2     0.7083    0.6071    0.6538       140
           3     0.6032    0.4810    0.5352        79
           4     0.7042    0.7576    0.7299       132
           5     0.6930    0.7859    0.7365       313
           6     0.6990    0.7059    0.7024       102
           7     0.5610    0.3286    0.4144        70
           8     0.5714    0.4000    0.4706        50
           9     0.6236    0.7161    0.6667       155
          10     0.8403    0.6471    0.7311       187
          11     0.6183    0.6450    0.6314       231
          12     0.7679    0.7247    0.7457       178
          13     0.7636    0.8183    0.7900       600
          14     0.7800    0.8475    0.8123       590
          15     0.7606    0.7105    0.7347        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4308    0.4606    0.4452       419
          19     0.6355    0.5271    0.5763       129
          20     0.6667    0.6429    0.6545        28
          21     1.0000    0.7241    0.8400        29
          22     0.6538    0.3696    0.4722        46

    accuracy                         0.6901      4043
   macro avg     0.6825    0.6012    0.6270      4043
weighted avg     0.6907    0.6901    0.6863      4043

Macro average Test Precision, Recall and F1-Score...
(0.6824509584259874, 0.6011834709064504, 0.6269574111336865, None)
Micro average Test Precision, Recall and F1-Score...
(0.6900816225575068, 0.6900816225575068, 0.6900816225575068, None)
embeddings:
14157 3357 4043
[[ 0.28470224  0.3781924   0.33527645 ...  0.3547884   0.3186255
   0.45523906]
 [ 0.14747706  0.21962762  0.04851253 ...  0.24878578  0.07824828
   0.3313434 ]
 [ 0.2644462   0.23562357  0.2060839  ...  0.28219348  0.23782608
   0.2723623 ]
 ...
 [ 0.08364388  0.13371255  0.11147617 ...  0.20222409  0.11370082
   0.23225185]
 [ 0.40351507  0.27197674  0.00699742 ...  0.26583105  0.0843864
   0.5051041 ]
 [-0.02847683  0.15907714  0.18927866 ...  0.2331735   0.09623799
   0.29087144]]

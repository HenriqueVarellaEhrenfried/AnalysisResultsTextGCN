(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13549 train_acc= 0.03276 val_loss= 3.11693 val_acc= 0.26567 time= 0.59806
Epoch: 0002 train_loss= 3.11673 train_acc= 0.25149 val_loss= 3.07365 val_acc= 0.27761 time= 0.29755
Epoch: 0003 train_loss= 3.07412 train_acc= 0.25811 val_loss= 3.00562 val_acc= 0.28060 time= 0.29099
Epoch: 0004 train_loss= 3.00757 train_acc= 0.25248 val_loss= 2.91964 val_acc= 0.28060 time= 0.28900
Epoch: 0005 train_loss= 2.92205 train_acc= 0.25480 val_loss= 2.83084 val_acc= 0.28060 time= 0.29534
Epoch: 0006 train_loss= 2.83442 train_acc= 0.25381 val_loss= 2.75706 val_acc= 0.28358 time= 0.29297
Epoch: 0007 train_loss= 2.76511 train_acc= 0.25811 val_loss= 2.71143 val_acc= 0.28060 time= 0.29404
Epoch: 0008 train_loss= 2.72207 train_acc= 0.25943 val_loss= 2.69846 val_acc= 0.24179 time= 0.30599
Epoch: 0009 train_loss= 2.71209 train_acc= 0.21410 val_loss= 2.69811 val_acc= 0.20597 time= 0.29900
Epoch: 0010 train_loss= 2.71351 train_acc= 0.17505 val_loss= 2.68996 val_acc= 0.20000 time= 0.30100
Epoch: 0011 train_loss= 2.70267 train_acc= 0.17141 val_loss= 2.66889 val_acc= 0.20000 time= 0.30700
Epoch: 0012 train_loss= 2.67500 train_acc= 0.17141 val_loss= 2.64284 val_acc= 0.20000 time= 0.29600
Epoch: 0013 train_loss= 2.63621 train_acc= 0.17141 val_loss= 2.61910 val_acc= 0.20299 time= 0.29916
Epoch: 0014 train_loss= 2.60414 train_acc= 0.17240 val_loss= 2.59892 val_acc= 0.20896 time= 0.29400
Epoch: 0015 train_loss= 2.57903 train_acc= 0.17637 val_loss= 2.58012 val_acc= 0.22388 time= 0.29400
Epoch: 0016 train_loss= 2.54775 train_acc= 0.19590 val_loss= 2.55949 val_acc= 0.24478 time= 0.29197
Epoch: 0017 train_loss= 2.52609 train_acc= 0.22138 val_loss= 2.53472 val_acc= 0.27164 time= 0.29803
Epoch: 0018 train_loss= 2.49803 train_acc= 0.25414 val_loss= 2.50504 val_acc= 0.28358 time= 0.29197
Epoch: 0019 train_loss= 2.46776 train_acc= 0.28259 val_loss= 2.47122 val_acc= 0.29851 time= 0.29203
Epoch: 0020 train_loss= 2.43091 train_acc= 0.30576 val_loss= 2.43444 val_acc= 0.31642 time= 0.29397
Epoch: 0021 train_loss= 2.38971 train_acc= 0.31899 val_loss= 2.39608 val_acc= 0.32836 time= 0.29203
Epoch: 0022 train_loss= 2.35019 train_acc= 0.33620 val_loss= 2.35701 val_acc= 0.32836 time= 0.29100
Epoch: 0023 train_loss= 2.30419 train_acc= 0.34646 val_loss= 2.31749 val_acc= 0.33433 time= 0.29000
Epoch: 0024 train_loss= 2.26383 train_acc= 0.36003 val_loss= 2.27774 val_acc= 0.33433 time= 0.29400
Epoch: 0025 train_loss= 2.21543 train_acc= 0.36731 val_loss= 2.23771 val_acc= 0.35224 time= 0.29400
Epoch: 0026 train_loss= 2.16948 train_acc= 0.38551 val_loss= 2.19759 val_acc= 0.37015 time= 0.29300
Epoch: 0027 train_loss= 2.11260 train_acc= 0.41860 val_loss= 2.15769 val_acc= 0.40896 time= 0.29600
Epoch: 0028 train_loss= 2.06416 train_acc= 0.45467 val_loss= 2.11808 val_acc= 0.43284 time= 0.29400
Epoch: 0029 train_loss= 2.00935 train_acc= 0.48511 val_loss= 2.07837 val_acc= 0.44776 time= 0.29298
Epoch: 0030 train_loss= 1.96050 train_acc= 0.52019 val_loss= 2.03766 val_acc= 0.48358 time= 0.29101
Epoch: 0031 train_loss= 1.91798 train_acc= 0.53739 val_loss= 1.99570 val_acc= 0.48955 time= 0.29197
Epoch: 0032 train_loss= 1.86043 train_acc= 0.55758 val_loss= 1.95288 val_acc= 0.50448 time= 0.30203
Epoch: 0033 train_loss= 1.80759 train_acc= 0.57379 val_loss= 1.91026 val_acc= 0.52239 time= 0.29300
Epoch: 0034 train_loss= 1.75081 train_acc= 0.58074 val_loss= 1.86819 val_acc= 0.52836 time= 0.29500
Epoch: 0035 train_loss= 1.69726 train_acc= 0.59497 val_loss= 1.82697 val_acc= 0.52537 time= 0.29587
Epoch: 0036 train_loss= 1.65398 train_acc= 0.59464 val_loss= 1.78761 val_acc= 0.53134 time= 0.29700
Epoch: 0037 train_loss= 1.59047 train_acc= 0.60953 val_loss= 1.75026 val_acc= 0.53134 time= 0.29600
Epoch: 0038 train_loss= 1.55158 train_acc= 0.61549 val_loss= 1.71471 val_acc= 0.53731 time= 0.29111
Epoch: 0039 train_loss= 1.49942 train_acc= 0.62343 val_loss= 1.68023 val_acc= 0.55224 time= 0.29500
Epoch: 0040 train_loss= 1.44967 train_acc= 0.63766 val_loss= 1.64731 val_acc= 0.55821 time= 0.29296
Epoch: 0041 train_loss= 1.39962 train_acc= 0.64560 val_loss= 1.61569 val_acc= 0.56119 time= 0.29300
Epoch: 0042 train_loss= 1.35585 train_acc= 0.65520 val_loss= 1.58595 val_acc= 0.55224 time= 0.29702
Epoch: 0043 train_loss= 1.30400 train_acc= 0.66909 val_loss= 1.55764 val_acc= 0.56716 time= 0.29673
Epoch: 0044 train_loss= 1.25818 train_acc= 0.68068 val_loss= 1.52995 val_acc= 0.57612 time= 0.29403
Epoch: 0045 train_loss= 1.22234 train_acc= 0.68762 val_loss= 1.50487 val_acc= 0.57015 time= 0.29197
Epoch: 0046 train_loss= 1.18606 train_acc= 0.69557 val_loss= 1.48263 val_acc= 0.57910 time= 0.29300
Epoch: 0047 train_loss= 1.14679 train_acc= 0.70582 val_loss= 1.46088 val_acc= 0.57910 time= 0.30200
Epoch: 0048 train_loss= 1.10167 train_acc= 0.72270 val_loss= 1.43821 val_acc= 0.58806 time= 0.29703
Epoch: 0049 train_loss= 1.05847 train_acc= 0.74421 val_loss= 1.41634 val_acc= 0.58806 time= 0.29397
Epoch: 0050 train_loss= 1.03731 train_acc= 0.74388 val_loss= 1.39709 val_acc= 0.59701 time= 0.29600
Epoch: 0051 train_loss= 0.98788 train_acc= 0.75083 val_loss= 1.37828 val_acc= 0.60597 time= 0.29403
Epoch: 0052 train_loss= 0.95325 train_acc= 0.75745 val_loss= 1.35915 val_acc= 0.60597 time= 0.29000
Epoch: 0053 train_loss= 0.93256 train_acc= 0.77101 val_loss= 1.34164 val_acc= 0.60896 time= 0.29000
Epoch: 0054 train_loss= 0.89889 train_acc= 0.77995 val_loss= 1.32587 val_acc= 0.62090 time= 0.29400
Epoch: 0055 train_loss= 0.86017 train_acc= 0.78392 val_loss= 1.31166 val_acc= 0.62388 time= 0.29397
Epoch: 0056 train_loss= 0.82635 train_acc= 0.79682 val_loss= 1.29965 val_acc= 0.62687 time= 0.29610
Epoch: 0057 train_loss= 0.79835 train_acc= 0.80510 val_loss= 1.28952 val_acc= 0.63284 time= 0.28900
Epoch: 0058 train_loss= 0.77482 train_acc= 0.80774 val_loss= 1.28019 val_acc= 0.63881 time= 0.29450
Epoch: 0059 train_loss= 0.75094 train_acc= 0.81403 val_loss= 1.27083 val_acc= 0.63582 time= 0.28800
Epoch: 0060 train_loss= 0.71889 train_acc= 0.81966 val_loss= 1.26148 val_acc= 0.63582 time= 0.28997
Epoch: 0061 train_loss= 0.70493 train_acc= 0.82131 val_loss= 1.25063 val_acc= 0.63582 time= 0.29403
Epoch: 0062 train_loss= 0.67442 train_acc= 0.83058 val_loss= 1.23966 val_acc= 0.64478 time= 0.29519
Epoch: 0063 train_loss= 0.65473 train_acc= 0.84613 val_loss= 1.23067 val_acc= 0.63582 time= 0.28901
Epoch: 0064 train_loss= 0.62333 train_acc= 0.84745 val_loss= 1.22478 val_acc= 0.64179 time= 0.28896
Epoch: 0065 train_loss= 0.60533 train_acc= 0.85076 val_loss= 1.21741 val_acc= 0.64179 time= 0.28900
Epoch: 0066 train_loss= 0.58778 train_acc= 0.85705 val_loss= 1.20724 val_acc= 0.64776 time= 0.29800
Epoch: 0067 train_loss= 0.55864 train_acc= 0.86698 val_loss= 1.20183 val_acc= 0.64776 time= 0.29403
Epoch: 0068 train_loss= 0.53906 train_acc= 0.87326 val_loss= 1.19757 val_acc= 0.65672 time= 0.29000
Epoch: 0069 train_loss= 0.52933 train_acc= 0.87293 val_loss= 1.19251 val_acc= 0.65075 time= 0.29239
Epoch: 0070 train_loss= 0.50935 train_acc= 0.88220 val_loss= 1.18926 val_acc= 0.65373 time= 0.29400
Epoch: 0071 train_loss= 0.49384 train_acc= 0.88451 val_loss= 1.18758 val_acc= 0.65075 time= 0.29000
Epoch: 0072 train_loss= 0.46843 train_acc= 0.88418 val_loss= 1.18109 val_acc= 0.65970 time= 0.28903
Epoch: 0073 train_loss= 0.46325 train_acc= 0.88948 val_loss= 1.17528 val_acc= 0.65970 time= 0.29597
Epoch: 0074 train_loss= 0.43660 train_acc= 0.90172 val_loss= 1.17168 val_acc= 0.65672 time= 0.29003
Epoch: 0075 train_loss= 0.43467 train_acc= 0.90271 val_loss= 1.16794 val_acc= 0.65373 time= 0.29000
Epoch: 0076 train_loss= 0.41205 train_acc= 0.91032 val_loss= 1.16414 val_acc= 0.65373 time= 0.28900
Epoch: 0077 train_loss= 0.39784 train_acc= 0.91165 val_loss= 1.16216 val_acc= 0.65672 time= 0.29196
Epoch: 0078 train_loss= 0.37888 train_acc= 0.91827 val_loss= 1.16191 val_acc= 0.65672 time= 0.29003
Epoch: 0079 train_loss= 0.37224 train_acc= 0.91893 val_loss= 1.15988 val_acc= 0.66269 time= 0.28607
Epoch: 0080 train_loss= 0.35756 train_acc= 0.91926 val_loss= 1.15701 val_acc= 0.65672 time= 0.29099
Epoch: 0081 train_loss= 0.34982 train_acc= 0.92588 val_loss= 1.15797 val_acc= 0.66269 time= 0.29510
Epoch: 0082 train_loss= 0.33496 train_acc= 0.92720 val_loss= 1.15878 val_acc= 0.66567 time= 0.29097
Epoch: 0083 train_loss= 0.32971 train_acc= 0.93514 val_loss= 1.15831 val_acc= 0.66567 time= 0.29600
Epoch: 0084 train_loss= 0.31870 train_acc= 0.93812 val_loss= 1.15728 val_acc= 0.66269 time= 0.29759
Epoch: 0085 train_loss= 0.30861 train_acc= 0.93812 val_loss= 1.15523 val_acc= 0.66567 time= 0.30100
Epoch: 0086 train_loss= 0.29683 train_acc= 0.94110 val_loss= 1.15475 val_acc= 0.65373 time= 0.29700
Epoch: 0087 train_loss= 0.28601 train_acc= 0.93812 val_loss= 1.15783 val_acc= 0.65075 time= 0.29603
Epoch: 0088 train_loss= 0.27870 train_acc= 0.94375 val_loss= 1.16031 val_acc= 0.65373 time= 0.30297
Early stopping...
Optimization Finished!
Test set results: cost= 1.16278 accuracy= 0.67920 time= 0.13500
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7255    0.6491    0.6852       342
           1     0.6750    0.7864    0.7265       103
           2     0.7434    0.6000    0.6640       140
           3     0.5965    0.4304    0.5000        79
           4     0.6667    0.7273    0.6957       132
           5     0.7018    0.7668    0.7328       313
           6     0.6796    0.6863    0.6829       102
           7     0.6786    0.2714    0.3878        70
           8     0.5526    0.4200    0.4773        50
           9     0.6183    0.7419    0.6745       155
          10     0.8322    0.6364    0.7212       187
          11     0.6091    0.6407    0.6245       231
          12     0.8027    0.6629    0.7262       178
          13     0.7765    0.8050    0.7905       600
          14     0.7857    0.8390    0.8115       590
          15     0.7500    0.6711    0.7083        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.3864    0.5274    0.4460       419
          19     0.6634    0.5194    0.5826       129
          20     0.6842    0.4643    0.5532        28
          21     1.0000    0.7586    0.8627        29
          22     0.6667    0.3043    0.4179        46

    accuracy                         0.6792      4043
   macro avg     0.6889    0.5809    0.6138      4043
weighted avg     0.6924    0.6792    0.6785      4043

Macro average Test Precision, Recall and F1-Score...
(0.6888978581723758, 0.5809404189622484, 0.6138210192309684, None)
Micro average Test Precision, Recall and F1-Score...
(0.6791986148899333, 0.6791986148899333, 0.6791986148899333, None)
embeddings:
14157 3357 4043
[[ 0.33927816  0.31719398  0.3643806  ...  0.34566587  0.41608813
   0.4576979 ]
 [ 0.229164    0.09624261  0.14150012 ... -0.00261468 -0.03114666
   0.09722326]
 [ 0.09001338  0.22825979  0.38166252 ...  0.07044028  0.05219423
   0.23306456]
 ...
 [ 0.08781611  0.1851751   0.18931346 ...  0.16485941  0.20163456
   0.21926263]
 [ 0.32645592  0.09972802  0.39512348 ...  0.44490704 -0.00727945
   0.08242016]
 [ 0.17518814  0.31100088 -0.0327953  ...  0.03630387  0.30650398
   0.42920092]]

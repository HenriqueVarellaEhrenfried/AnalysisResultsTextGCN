(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13554 train_acc= 0.02978 val_loss= 3.11562 val_acc= 0.24179 time= 0.58195
Epoch: 0002 train_loss= 3.11543 train_acc= 0.21046 val_loss= 3.07113 val_acc= 0.22985 time= 0.28900
Epoch: 0003 train_loss= 3.07109 train_acc= 0.19226 val_loss= 3.00170 val_acc= 0.22388 time= 0.29688
Epoch: 0004 train_loss= 3.00207 train_acc= 0.19060 val_loss= 2.91361 val_acc= 0.22090 time= 0.28800
Epoch: 0005 train_loss= 2.91500 train_acc= 0.18928 val_loss= 2.82181 val_acc= 0.22090 time= 0.28900
Epoch: 0006 train_loss= 2.82527 train_acc= 0.18928 val_loss= 2.74500 val_acc= 0.22985 time= 0.29284
Epoch: 0007 train_loss= 2.75102 train_acc= 0.19292 val_loss= 2.69820 val_acc= 0.22985 time= 0.29105
Epoch: 0008 train_loss= 2.70592 train_acc= 0.19921 val_loss= 2.68637 val_acc= 0.22687 time= 0.29103
Epoch: 0009 train_loss= 2.69587 train_acc= 0.19126 val_loss= 2.68770 val_acc= 0.20896 time= 0.28700
Epoch: 0010 train_loss= 2.69615 train_acc= 0.17968 val_loss= 2.67707 val_acc= 0.20896 time= 0.29397
Epoch: 0011 train_loss= 2.67980 train_acc= 0.17704 val_loss= 2.65047 val_acc= 0.20896 time= 0.29503
Epoch: 0012 train_loss= 2.64381 train_acc= 0.18233 val_loss= 2.61757 val_acc= 0.22388 time= 0.29100
Epoch: 0013 train_loss= 2.60119 train_acc= 0.19060 val_loss= 2.58700 val_acc= 0.23881 time= 0.29200
Epoch: 0014 train_loss= 2.56103 train_acc= 0.20682 val_loss= 2.56069 val_acc= 0.25970 time= 0.29500
Epoch: 0015 train_loss= 2.52698 train_acc= 0.23064 val_loss= 2.53557 val_acc= 0.28060 time= 0.29200
Epoch: 0016 train_loss= 2.49474 train_acc= 0.25910 val_loss= 2.50772 val_acc= 0.29552 time= 0.28800
Epoch: 0017 train_loss= 2.46281 train_acc= 0.29054 val_loss= 2.47473 val_acc= 0.31642 time= 0.28900
Epoch: 0018 train_loss= 2.42561 train_acc= 0.32197 val_loss= 2.43628 val_acc= 0.32537 time= 0.29434
Epoch: 0019 train_loss= 2.38221 train_acc= 0.34216 val_loss= 2.39361 val_acc= 0.34030 time= 0.29297
Epoch: 0020 train_loss= 2.33767 train_acc= 0.35970 val_loss= 2.34845 val_acc= 0.34328 time= 0.29409
Epoch: 0021 train_loss= 2.28775 train_acc= 0.36995 val_loss= 2.30226 val_acc= 0.34627 time= 0.29200
Epoch: 0022 train_loss= 2.23623 train_acc= 0.37227 val_loss= 2.25596 val_acc= 0.35224 time= 0.29600
Epoch: 0023 train_loss= 2.18466 train_acc= 0.37955 val_loss= 2.20996 val_acc= 0.35522 time= 0.29000
Epoch: 0024 train_loss= 2.13119 train_acc= 0.39444 val_loss= 2.16426 val_acc= 0.36716 time= 0.29000
Epoch: 0025 train_loss= 2.07612 train_acc= 0.41132 val_loss= 2.11886 val_acc= 0.38507 time= 0.29000
Epoch: 0026 train_loss= 2.01749 train_acc= 0.44176 val_loss= 2.07407 val_acc= 0.42090 time= 0.29600
Epoch: 0027 train_loss= 1.96181 train_acc= 0.47816 val_loss= 2.03029 val_acc= 0.43582 time= 0.28900
Epoch: 0028 train_loss= 1.90255 train_acc= 0.51125 val_loss= 1.98739 val_acc= 0.48060 time= 0.29100
Epoch: 0029 train_loss= 1.84575 train_acc= 0.54434 val_loss= 1.94445 val_acc= 0.50746 time= 0.29523
Epoch: 0030 train_loss= 1.78814 train_acc= 0.57148 val_loss= 1.90047 val_acc= 0.52239 time= 0.29399
Epoch: 0031 train_loss= 1.73081 train_acc= 0.59398 val_loss= 1.85531 val_acc= 0.52239 time= 0.29000
Epoch: 0032 train_loss= 1.67072 train_acc= 0.60556 val_loss= 1.81021 val_acc= 0.52836 time= 0.29600
Epoch: 0033 train_loss= 1.61162 train_acc= 0.61615 val_loss= 1.76667 val_acc= 0.53134 time= 0.29390
Epoch: 0034 train_loss= 1.55367 train_acc= 0.62608 val_loss= 1.72564 val_acc= 0.53433 time= 0.29200
Epoch: 0035 train_loss= 1.49909 train_acc= 0.63203 val_loss= 1.68722 val_acc= 0.54030 time= 0.28800
Epoch: 0036 train_loss= 1.44694 train_acc= 0.64924 val_loss= 1.65095 val_acc= 0.54627 time= 0.28800
Epoch: 0037 train_loss= 1.39042 train_acc= 0.66049 val_loss= 1.61656 val_acc= 0.55224 time= 0.29200
Epoch: 0038 train_loss= 1.33758 train_acc= 0.67273 val_loss= 1.58413 val_acc= 0.57313 time= 0.28837
Epoch: 0039 train_loss= 1.28448 train_acc= 0.68564 val_loss= 1.55349 val_acc= 0.57612 time= 0.28900
Epoch: 0040 train_loss= 1.23573 train_acc= 0.69259 val_loss= 1.52403 val_acc= 0.57910 time= 0.28600
Epoch: 0041 train_loss= 1.18487 train_acc= 0.70417 val_loss= 1.49491 val_acc= 0.58507 time= 0.29300
Epoch: 0042 train_loss= 1.13524 train_acc= 0.72005 val_loss= 1.46604 val_acc= 0.59701 time= 0.29097
Epoch: 0043 train_loss= 1.08939 train_acc= 0.73064 val_loss= 1.43802 val_acc= 0.59701 time= 0.28800
Epoch: 0044 train_loss= 1.04457 train_acc= 0.74388 val_loss= 1.41166 val_acc= 0.59403 time= 0.29100
Epoch: 0045 train_loss= 0.99866 train_acc= 0.76109 val_loss= 1.38730 val_acc= 0.60000 time= 0.29946
Epoch: 0046 train_loss= 0.95417 train_acc= 0.76969 val_loss= 1.36450 val_acc= 0.60896 time= 0.28800
Epoch: 0047 train_loss= 0.91253 train_acc= 0.78557 val_loss= 1.34386 val_acc= 0.61493 time= 0.28900
Epoch: 0048 train_loss= 0.86919 train_acc= 0.79649 val_loss= 1.32545 val_acc= 0.61493 time= 0.29497
Epoch: 0049 train_loss= 0.83202 train_acc= 0.80741 val_loss= 1.30843 val_acc= 0.62687 time= 0.29603
Epoch: 0050 train_loss= 0.79465 train_acc= 0.81932 val_loss= 1.29196 val_acc= 0.62985 time= 0.29100
Epoch: 0051 train_loss= 0.75540 train_acc= 0.82760 val_loss= 1.27631 val_acc= 0.63284 time= 0.28800
Epoch: 0052 train_loss= 0.72024 train_acc= 0.83752 val_loss= 1.26129 val_acc= 0.62687 time= 0.29100
Epoch: 0053 train_loss= 0.68574 train_acc= 0.84480 val_loss= 1.24770 val_acc= 0.63881 time= 0.29300
Epoch: 0054 train_loss= 0.65164 train_acc= 0.85672 val_loss= 1.23595 val_acc= 0.64478 time= 0.28710
Epoch: 0055 train_loss= 0.61940 train_acc= 0.86400 val_loss= 1.22579 val_acc= 0.64478 time= 0.28801
Epoch: 0056 train_loss= 0.58837 train_acc= 0.87194 val_loss= 1.21658 val_acc= 0.64179 time= 0.29653
Epoch: 0057 train_loss= 0.55830 train_acc= 0.87955 val_loss= 1.20759 val_acc= 0.63881 time= 0.29099
Epoch: 0058 train_loss= 0.53363 train_acc= 0.88617 val_loss= 1.19914 val_acc= 0.65075 time= 0.28803
Epoch: 0059 train_loss= 0.50630 train_acc= 0.89345 val_loss= 1.19131 val_acc= 0.64478 time= 0.29100
Epoch: 0060 train_loss= 0.48044 train_acc= 0.90238 val_loss= 1.18370 val_acc= 0.64478 time= 0.29700
Epoch: 0061 train_loss= 0.45453 train_acc= 0.90536 val_loss= 1.17715 val_acc= 0.64179 time= 0.29400
Epoch: 0062 train_loss= 0.43229 train_acc= 0.91032 val_loss= 1.17233 val_acc= 0.65075 time= 0.29303
Epoch: 0063 train_loss= 0.40988 train_acc= 0.91860 val_loss= 1.16873 val_acc= 0.65075 time= 0.29000
Epoch: 0064 train_loss= 0.38814 train_acc= 0.92588 val_loss= 1.16562 val_acc= 0.65075 time= 0.30297
Epoch: 0065 train_loss= 0.36903 train_acc= 0.93249 val_loss= 1.16300 val_acc= 0.65373 time= 0.29900
Epoch: 0066 train_loss= 0.34997 train_acc= 0.93415 val_loss= 1.16113 val_acc= 0.66269 time= 0.30603
Epoch: 0067 train_loss= 0.33170 train_acc= 0.93911 val_loss= 1.15979 val_acc= 0.65970 time= 0.29097
Epoch: 0068 train_loss= 0.31525 train_acc= 0.94375 val_loss= 1.15882 val_acc= 0.65373 time= 0.29403
Epoch: 0069 train_loss= 0.29625 train_acc= 0.94838 val_loss= 1.15810 val_acc= 0.65970 time= 0.29000
Epoch: 0070 train_loss= 0.28189 train_acc= 0.94970 val_loss= 1.15728 val_acc= 0.66269 time= 0.28900
Epoch: 0071 train_loss= 0.26869 train_acc= 0.95334 val_loss= 1.15700 val_acc= 0.66567 time= 0.28999
Epoch: 0072 train_loss= 0.25442 train_acc= 0.95698 val_loss= 1.15823 val_acc= 0.65672 time= 0.29901
Epoch: 0073 train_loss= 0.24155 train_acc= 0.96128 val_loss= 1.16056 val_acc= 0.65970 time= 0.29000
Epoch: 0074 train_loss= 0.22987 train_acc= 0.96492 val_loss= 1.16326 val_acc= 0.66567 time= 0.28600
Early stopping...
Optimization Finished!
Test set results: cost= 1.16344 accuracy= 0.68612 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7278    0.6959    0.7115       342
           1     0.6930    0.7670    0.7281       103
           2     0.7523    0.5857    0.6586       140
           3     0.6596    0.3924    0.4921        79
           4     0.6554    0.7348    0.6929       132
           5     0.6739    0.7923    0.7283       313
           6     0.7000    0.7549    0.7264       102
           7     0.6571    0.3286    0.4381        70
           8     0.5294    0.3600    0.4286        50
           9     0.6117    0.7419    0.6706       155
          10     0.8310    0.6310    0.7173       187
          11     0.6239    0.6320    0.6280       231
          12     0.7486    0.7360    0.7422       178
          13     0.7781    0.8067    0.7921       600
          14     0.7786    0.8407    0.8085       590
          15     0.7286    0.6711    0.6986        76
          16     0.7059    0.3529    0.4706        34
          17     0.6667    0.2000    0.3077        10
          18     0.4206    0.4869    0.4513       419
          19     0.6346    0.5116    0.5665       129
          20     0.6429    0.6429    0.6429        28
          21     1.0000    0.7586    0.8627        29
          22     0.6667    0.3478    0.4571        46

    accuracy                         0.6861      4043
   macro avg     0.6907    0.5988    0.6270      4043
weighted avg     0.6912    0.6861    0.6829      4043

Macro average Test Precision, Recall and F1-Score...
(0.690713329718719, 0.5987719654814676, 0.626990022175015, None)
Micro average Test Precision, Recall and F1-Score...
(0.6861241652238437, 0.6861241652238437, 0.6861241652238437, None)
embeddings:
14157 3357 4043
[[ 0.39083877  0.5038183   0.55324435 ...  0.22871767  0.44715738
   0.48337793]
 [ 0.16030732  0.15047067 -0.0289754  ... -0.05532506  0.15935679
   0.26968372]
 [ 0.34969637  0.37673086  0.3680808  ...  0.3401845   0.21931547
   0.34531707]
 ...
 [ 0.14788398  0.14646839  0.2914251  ...  0.19469282  0.1561782
   0.13098447]
 [ 0.21619423  0.2263363   0.28075454 ...  0.3652052  -0.03433695
   0.4171252 ]
 [ 0.15281548  0.2882032   0.19512825 ...  0.14308691  0.3062232
  -0.03599769]]

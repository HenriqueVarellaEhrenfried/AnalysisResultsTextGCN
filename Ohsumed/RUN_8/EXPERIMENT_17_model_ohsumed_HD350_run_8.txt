(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.04997 val_loss= 3.11020 val_acc= 0.27164 time= 4.39399
Epoch: 0002 train_loss= 3.11030 train_acc= 0.24983 val_loss= 3.04569 val_acc= 0.25075 time= 4.27300
Epoch: 0003 train_loss= 3.04604 train_acc= 0.22733 val_loss= 2.94575 val_acc= 0.24478 time= 4.29500
Epoch: 0004 train_loss= 2.94638 train_acc= 0.21277 val_loss= 2.83340 val_acc= 0.24179 time= 4.25662
Epoch: 0005 train_loss= 2.83457 train_acc= 0.20814 val_loss= 2.74292 val_acc= 0.24478 time= 4.25762
Epoch: 0006 train_loss= 2.74694 train_acc= 0.21410 val_loss= 2.70080 val_acc= 0.24478 time= 4.26864
Epoch: 0007 train_loss= 2.70628 train_acc= 0.21277 val_loss= 2.69884 val_acc= 0.21194 time= 4.26300
Epoch: 0008 train_loss= 2.70671 train_acc= 0.18796 val_loss= 2.69307 val_acc= 0.20597 time= 4.27300
Epoch: 0009 train_loss= 2.70142 train_acc= 0.17505 val_loss= 2.66312 val_acc= 0.20597 time= 4.26500
Epoch: 0010 train_loss= 2.65900 train_acc= 0.17604 val_loss= 2.62243 val_acc= 0.20896 time= 4.31000
Epoch: 0011 train_loss= 2.60726 train_acc= 0.18498 val_loss= 2.58577 val_acc= 0.22985 time= 4.32516
Epoch: 0012 train_loss= 2.55836 train_acc= 0.20119 val_loss= 2.55500 val_acc= 0.25373 time= 4.25700
Epoch: 0013 train_loss= 2.51800 train_acc= 0.23627 val_loss= 2.52455 val_acc= 0.28657 time= 4.27500
Epoch: 0014 train_loss= 2.48197 train_acc= 0.27267 val_loss= 2.48878 val_acc= 0.30746 time= 4.26995
Epoch: 0015 train_loss= 2.43915 train_acc= 0.31337 val_loss= 2.44513 val_acc= 0.32537 time= 4.28900
Epoch: 0016 train_loss= 2.39008 train_acc= 0.35606 val_loss= 2.39420 val_acc= 0.33433 time= 4.26700
Epoch: 0017 train_loss= 2.33086 train_acc= 0.38650 val_loss= 2.33846 val_acc= 0.34627 time= 4.29600
Epoch: 0018 train_loss= 2.26999 train_acc= 0.39179 val_loss= 2.28081 val_acc= 0.34925 time= 4.27400
Epoch: 0019 train_loss= 2.20634 train_acc= 0.40007 val_loss= 2.22346 val_acc= 0.35522 time= 4.26900
Epoch: 0020 train_loss= 2.14210 train_acc= 0.40933 val_loss= 2.16712 val_acc= 0.36119 time= 4.25500
Epoch: 0021 train_loss= 2.07484 train_acc= 0.42224 val_loss= 2.11182 val_acc= 0.39403 time= 4.29400
Epoch: 0022 train_loss= 2.00637 train_acc= 0.45103 val_loss= 2.05757 val_acc= 0.42090 time= 4.29300
Epoch: 0023 train_loss= 1.94270 train_acc= 0.48346 val_loss= 2.00496 val_acc= 0.46567 time= 4.25165
Epoch: 0024 train_loss= 1.87399 train_acc= 0.51985 val_loss= 1.95463 val_acc= 0.48955 time= 4.30100
Epoch: 0025 train_loss= 1.80345 train_acc= 0.55791 val_loss= 1.90594 val_acc= 0.51343 time= 4.45700
Epoch: 0026 train_loss= 1.73686 train_acc= 0.58703 val_loss= 1.85753 val_acc= 0.53731 time= 4.29955
Epoch: 0027 train_loss= 1.66888 train_acc= 0.60622 val_loss= 1.80889 val_acc= 0.52537 time= 4.30331
Epoch: 0028 train_loss= 1.60070 train_acc= 0.61317 val_loss= 1.76003 val_acc= 0.52537 time= 4.28200
Epoch: 0029 train_loss= 1.53294 train_acc= 0.62541 val_loss= 1.71247 val_acc= 0.53433 time= 4.26927
Epoch: 0030 train_loss= 1.46922 train_acc= 0.63931 val_loss= 1.66759 val_acc= 0.54328 time= 4.28049
Epoch: 0031 train_loss= 1.40788 train_acc= 0.65156 val_loss= 1.62611 val_acc= 0.54627 time= 4.27000
Epoch: 0032 train_loss= 1.35070 train_acc= 0.66578 val_loss= 1.58729 val_acc= 0.57015 time= 4.27900
Epoch: 0033 train_loss= 1.29341 train_acc= 0.68200 val_loss= 1.55119 val_acc= 0.58507 time= 4.28000
Epoch: 0034 train_loss= 1.23490 train_acc= 0.68597 val_loss= 1.51854 val_acc= 0.58806 time= 4.25900
Epoch: 0035 train_loss= 1.18076 train_acc= 0.69590 val_loss= 1.48943 val_acc= 0.58806 time= 4.25300
Epoch: 0036 train_loss= 1.12501 train_acc= 0.71013 val_loss= 1.46190 val_acc= 0.58507 time= 4.26099
Epoch: 0037 train_loss= 1.07626 train_acc= 0.72303 val_loss= 1.43453 val_acc= 0.59403 time= 4.40152
Epoch: 0038 train_loss= 1.02136 train_acc= 0.73561 val_loss= 1.40763 val_acc= 0.59701 time= 4.41501
Epoch: 0039 train_loss= 0.97310 train_acc= 0.75447 val_loss= 1.38225 val_acc= 0.60000 time= 4.36800
Epoch: 0040 train_loss= 0.92565 train_acc= 0.77002 val_loss= 1.35872 val_acc= 0.60299 time= 4.32000
Epoch: 0041 train_loss= 0.87825 train_acc= 0.78359 val_loss= 1.33733 val_acc= 0.62090 time= 4.34100
Epoch: 0042 train_loss= 0.83696 train_acc= 0.79451 val_loss= 1.31826 val_acc= 0.61791 time= 4.31908
Epoch: 0043 train_loss= 0.78796 train_acc= 0.80907 val_loss= 1.30188 val_acc= 0.62687 time= 4.35161
Epoch: 0044 train_loss= 0.75352 train_acc= 0.81635 val_loss= 1.28645 val_acc= 0.62985 time= 4.32000
Epoch: 0045 train_loss= 0.71274 train_acc= 0.82958 val_loss= 1.27069 val_acc= 0.63284 time= 4.34200
Epoch: 0046 train_loss= 0.67256 train_acc= 0.83819 val_loss= 1.25483 val_acc= 0.63582 time= 4.30300
Epoch: 0047 train_loss= 0.64193 train_acc= 0.84878 val_loss= 1.24023 val_acc= 0.63582 time= 4.31500
Epoch: 0048 train_loss= 0.59974 train_acc= 0.85870 val_loss= 1.22891 val_acc= 0.62388 time= 4.30927
Epoch: 0049 train_loss= 0.57203 train_acc= 0.86466 val_loss= 1.22067 val_acc= 0.63284 time= 4.29600
Epoch: 0050 train_loss= 0.54254 train_acc= 0.87392 val_loss= 1.21364 val_acc= 0.63284 time= 4.32200
Epoch: 0051 train_loss= 0.51277 train_acc= 0.88551 val_loss= 1.20863 val_acc= 0.64179 time= 4.32600
Epoch: 0052 train_loss= 0.47781 train_acc= 0.89444 val_loss= 1.20283 val_acc= 0.63881 time= 4.33200
Epoch: 0053 train_loss= 0.45679 train_acc= 0.89940 val_loss= 1.19503 val_acc= 0.65075 time= 4.30500
Epoch: 0054 train_loss= 0.42955 train_acc= 0.90801 val_loss= 1.18599 val_acc= 0.65075 time= 4.34500
Epoch: 0055 train_loss= 0.40477 train_acc= 0.91760 val_loss= 1.17781 val_acc= 0.65075 time= 4.30800
Epoch: 0056 train_loss= 0.38190 train_acc= 0.91529 val_loss= 1.17232 val_acc= 0.65672 time= 4.31500
Epoch: 0057 train_loss= 0.35600 train_acc= 0.92621 val_loss= 1.16975 val_acc= 0.66567 time= 4.33700
Epoch: 0058 train_loss= 0.34142 train_acc= 0.92588 val_loss= 1.17007 val_acc= 0.66269 time= 4.34100
Epoch: 0059 train_loss= 0.32048 train_acc= 0.93249 val_loss= 1.17176 val_acc= 0.66269 time= 4.31000
Epoch: 0060 train_loss= 0.30214 train_acc= 0.94176 val_loss= 1.17227 val_acc= 0.66269 time= 4.30300
Epoch: 0061 train_loss= 0.28237 train_acc= 0.94739 val_loss= 1.17048 val_acc= 0.65672 time= 4.32800
Epoch: 0062 train_loss= 0.27013 train_acc= 0.94904 val_loss= 1.16839 val_acc= 0.66269 time= 4.32000
Epoch: 0063 train_loss= 0.25734 train_acc= 0.95169 val_loss= 1.16877 val_acc= 0.66567 time= 4.29300
Epoch: 0064 train_loss= 0.23944 train_acc= 0.95698 val_loss= 1.17025 val_acc= 0.67463 time= 4.32281
Epoch: 0065 train_loss= 0.22593 train_acc= 0.95996 val_loss= 1.17244 val_acc= 0.67463 time= 4.30800
Early stopping...
Optimization Finished!
Test set results: cost= 1.16746 accuracy= 0.68563 time= 1.49100
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7250    0.6784    0.7009       342
           1     0.6972    0.7379    0.7170       103
           2     0.7295    0.6357    0.6794       140
           3     0.6471    0.4177    0.5077        79
           4     0.6735    0.7500    0.7097       132
           5     0.7105    0.7764    0.7420       313
           6     0.6923    0.7059    0.6990       102
           7     0.6129    0.2714    0.3762        70
           8     0.6000    0.4200    0.4941        50
           9     0.6138    0.7484    0.6744       155
          10     0.8369    0.6310    0.7195       187
          11     0.6460    0.6320    0.6389       231
          12     0.7716    0.7022    0.7353       178
          13     0.7753    0.8050    0.7899       600
          14     0.7731    0.8373    0.8039       590
          15     0.7606    0.7105    0.7347        76
          16     0.6875    0.3235    0.4400        34
          17     0.5000    0.2000    0.2857        10
          18     0.4099    0.5155    0.4567       419
          19     0.6250    0.5426    0.5809       129
          20     0.6071    0.6071    0.6071        28
          21     1.0000    0.7241    0.8400        29
          22     0.6522    0.3261    0.4348        46

    accuracy                         0.6856      4043
   macro avg     0.6846    0.5956    0.6247      4043
weighted avg     0.6930    0.6856    0.6837      4043

Macro average Test Precision, Recall and F1-Score...
(0.6846474433137312, 0.5956033288334954, 0.624689693871943, None)
Micro average Test Precision, Recall and F1-Score...
(0.6856294830571358, 0.6856294830571358, 0.6856294830571358, None)
embeddings:
14157 3357 4043
[[ 0.32784113  0.3693418   0.28700936 ...  0.22472714  0.39322454
   0.09321647]
 [ 0.04234228  0.18951912 -0.00680929 ... -0.00968964  0.21132281
   0.07760605]
 [ 0.15188178  0.3375756   0.12984253 ...  0.08978853  0.15881267
   0.0490297 ]
 ...
 [ 0.144623    0.23454373  0.13189901 ...  0.10112374  0.15981501
   0.05017613]
 [ 0.017326   -0.04442035  0.19918936 ... -0.0230259   0.32175744
   0.03464897]
 [ 0.14764228  0.15769835  0.18387628 ...  0.1546996   0.07990787
   0.03246757]]

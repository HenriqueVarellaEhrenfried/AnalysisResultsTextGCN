(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13558 train_acc= 0.01754 val_loss= 3.11667 val_acc= 0.30448 time= 0.57695
Epoch: 0002 train_loss= 3.11625 train_acc= 0.28326 val_loss= 3.07430 val_acc= 0.31343 time= 0.29698
Epoch: 0003 train_loss= 3.07317 train_acc= 0.29120 val_loss= 3.00818 val_acc= 0.31940 time= 0.28600
Epoch: 0004 train_loss= 3.00642 train_acc= 0.29153 val_loss= 2.92416 val_acc= 0.31045 time= 0.28597
Epoch: 0005 train_loss= 2.92171 train_acc= 0.28657 val_loss= 2.83630 val_acc= 0.29254 time= 0.29000
Epoch: 0006 train_loss= 2.83440 train_acc= 0.26936 val_loss= 2.76170 val_acc= 0.26866 time= 0.29500
Epoch: 0007 train_loss= 2.76214 train_acc= 0.24752 val_loss= 2.71347 val_acc= 0.27164 time= 0.28900
Epoch: 0008 train_loss= 2.71779 train_acc= 0.25215 val_loss= 2.69540 val_acc= 0.31045 time= 0.29001
Epoch: 0009 train_loss= 2.70158 train_acc= 0.29120 val_loss= 2.68838 val_acc= 0.23881 time= 0.29200
Epoch: 0010 train_loss= 2.69536 train_acc= 0.20516 val_loss= 2.67648 val_acc= 0.20597 time= 0.29400
Epoch: 0011 train_loss= 2.68122 train_acc= 0.17472 val_loss= 2.65418 val_acc= 0.20299 time= 0.29400
Epoch: 0012 train_loss= 2.65184 train_acc= 0.17207 val_loss= 2.62585 val_acc= 0.20299 time= 0.29000
Epoch: 0013 train_loss= 2.61444 train_acc= 0.17240 val_loss= 2.59781 val_acc= 0.20896 time= 0.28800
Epoch: 0014 train_loss= 2.57684 train_acc= 0.17439 val_loss= 2.57239 val_acc= 0.21493 time= 0.29205
Epoch: 0015 train_loss= 2.54166 train_acc= 0.18498 val_loss= 2.54846 val_acc= 0.23582 time= 0.29370
Epoch: 0016 train_loss= 2.50989 train_acc= 0.20847 val_loss= 2.52321 val_acc= 0.26269 time= 0.29400
Epoch: 0017 train_loss= 2.47763 train_acc= 0.24553 val_loss= 2.49401 val_acc= 0.28358 time= 0.28901
Epoch: 0018 train_loss= 2.44259 train_acc= 0.29550 val_loss= 2.45968 val_acc= 0.31940 time= 0.28822
Epoch: 0019 train_loss= 2.40292 train_acc= 0.33653 val_loss= 2.42042 val_acc= 0.34328 time= 0.29400
Epoch: 0020 train_loss= 2.35829 train_acc= 0.36797 val_loss= 2.37725 val_acc= 0.35224 time= 0.29100
Epoch: 0021 train_loss= 2.31094 train_acc= 0.39047 val_loss= 2.33141 val_acc= 0.35821 time= 0.28600
Epoch: 0022 train_loss= 2.26002 train_acc= 0.39841 val_loss= 2.28406 val_acc= 0.36119 time= 0.28850
Epoch: 0023 train_loss= 2.20633 train_acc= 0.40437 val_loss= 2.23619 val_acc= 0.37313 time= 0.30034
Epoch: 0024 train_loss= 2.15063 train_acc= 0.41727 val_loss= 2.18848 val_acc= 0.37612 time= 0.28903
Epoch: 0025 train_loss= 2.09505 train_acc= 0.42720 val_loss= 2.14155 val_acc= 0.39701 time= 0.28900
Epoch: 0026 train_loss= 2.03673 train_acc= 0.44970 val_loss= 2.09571 val_acc= 0.41791 time= 0.29300
Epoch: 0027 train_loss= 1.97941 train_acc= 0.48180 val_loss= 2.05086 val_acc= 0.46269 time= 0.29628
Epoch: 0028 train_loss= 1.92186 train_acc= 0.50960 val_loss= 2.00688 val_acc= 0.46866 time= 0.29203
Epoch: 0029 train_loss= 1.86101 train_acc= 0.53805 val_loss= 1.96340 val_acc= 0.49254 time= 0.29097
Epoch: 0030 train_loss= 1.80247 train_acc= 0.56751 val_loss= 1.91980 val_acc= 0.51343 time= 0.28903
Epoch: 0031 train_loss= 1.74197 train_acc= 0.58901 val_loss= 1.87577 val_acc= 0.51940 time= 0.29700
Epoch: 0032 train_loss= 1.68121 train_acc= 0.60754 val_loss= 1.83132 val_acc= 0.53134 time= 0.28999
Epoch: 0033 train_loss= 1.62202 train_acc= 0.61582 val_loss= 1.78704 val_acc= 0.53134 time= 0.28998
Epoch: 0034 train_loss= 1.56350 train_acc= 0.62574 val_loss= 1.74384 val_acc= 0.54030 time= 0.28800
Epoch: 0035 train_loss= 1.50575 train_acc= 0.63865 val_loss= 1.70253 val_acc= 0.54328 time= 0.29508
Epoch: 0036 train_loss= 1.44958 train_acc= 0.64725 val_loss= 1.66351 val_acc= 0.55224 time= 0.29303
Epoch: 0037 train_loss= 1.39360 train_acc= 0.65884 val_loss= 1.62656 val_acc= 0.56119 time= 0.28797
Epoch: 0038 train_loss= 1.33962 train_acc= 0.67240 val_loss= 1.59174 val_acc= 0.56418 time= 0.28803
Epoch: 0039 train_loss= 1.28827 train_acc= 0.68729 val_loss= 1.55958 val_acc= 0.57313 time= 0.29196
Epoch: 0040 train_loss= 1.23572 train_acc= 0.69590 val_loss= 1.52996 val_acc= 0.57612 time= 0.29500
Epoch: 0041 train_loss= 1.18246 train_acc= 0.70748 val_loss= 1.50224 val_acc= 0.57313 time= 0.28700
Epoch: 0042 train_loss= 1.13500 train_acc= 0.71575 val_loss= 1.47507 val_acc= 0.57313 time= 0.28803
Epoch: 0043 train_loss= 1.08740 train_acc= 0.72899 val_loss= 1.44845 val_acc= 0.57612 time= 0.29097
Epoch: 0044 train_loss= 1.04090 train_acc= 0.74785 val_loss= 1.42367 val_acc= 0.58209 time= 0.29403
Epoch: 0045 train_loss= 0.99708 train_acc= 0.76274 val_loss= 1.40012 val_acc= 0.58806 time= 0.28700
Epoch: 0046 train_loss= 0.95157 train_acc= 0.77399 val_loss= 1.37756 val_acc= 0.59403 time= 0.29401
Epoch: 0047 train_loss= 0.90960 train_acc= 0.78590 val_loss= 1.35620 val_acc= 0.60000 time= 0.29000
Epoch: 0048 train_loss= 0.86843 train_acc= 0.79914 val_loss= 1.33641 val_acc= 0.60896 time= 0.29700
Epoch: 0049 train_loss= 0.82921 train_acc= 0.80675 val_loss= 1.31776 val_acc= 0.61791 time= 0.29197
Epoch: 0050 train_loss= 0.79193 train_acc= 0.81668 val_loss= 1.30044 val_acc= 0.62687 time= 0.29103
Epoch: 0051 train_loss= 0.75245 train_acc= 0.82793 val_loss= 1.28483 val_acc= 0.62985 time= 0.29000
Epoch: 0052 train_loss= 0.71823 train_acc= 0.83719 val_loss= 1.27103 val_acc= 0.62687 time= 0.29900
Epoch: 0053 train_loss= 0.68276 train_acc= 0.84348 val_loss= 1.25894 val_acc= 0.62985 time= 0.29297
Epoch: 0054 train_loss= 0.65108 train_acc= 0.85175 val_loss= 1.24850 val_acc= 0.62985 time= 0.28903
Epoch: 0055 train_loss= 0.62020 train_acc= 0.86102 val_loss= 1.23900 val_acc= 0.63881 time= 0.29100
Epoch: 0056 train_loss= 0.58917 train_acc= 0.86830 val_loss= 1.22990 val_acc= 0.64478 time= 0.29597
Epoch: 0057 train_loss= 0.55894 train_acc= 0.87690 val_loss= 1.22083 val_acc= 0.64776 time= 0.29203
Epoch: 0058 train_loss= 0.53275 train_acc= 0.88451 val_loss= 1.21188 val_acc= 0.64478 time= 0.28833
Epoch: 0059 train_loss= 0.50397 train_acc= 0.88915 val_loss= 1.20325 val_acc= 0.64478 time= 0.28706
Epoch: 0060 train_loss= 0.48004 train_acc= 0.89841 val_loss= 1.19555 val_acc= 0.64478 time= 0.30447
Epoch: 0061 train_loss= 0.45685 train_acc= 0.90503 val_loss= 1.18877 val_acc= 0.64776 time= 0.29400
Epoch: 0062 train_loss= 0.43233 train_acc= 0.91430 val_loss= 1.18300 val_acc= 0.64776 time= 0.28907
Epoch: 0063 train_loss= 0.40984 train_acc= 0.91827 val_loss= 1.17831 val_acc= 0.65373 time= 0.29300
Epoch: 0064 train_loss= 0.39015 train_acc= 0.92323 val_loss= 1.17498 val_acc= 0.65075 time= 0.29497
Epoch: 0065 train_loss= 0.37072 train_acc= 0.92786 val_loss= 1.17330 val_acc= 0.65075 time= 0.29203
Epoch: 0066 train_loss= 0.35171 train_acc= 0.93448 val_loss= 1.17218 val_acc= 0.64776 time= 0.29004
Epoch: 0067 train_loss= 0.33381 train_acc= 0.93944 val_loss= 1.17118 val_acc= 0.65075 time= 0.29100
Epoch: 0068 train_loss= 0.31717 train_acc= 0.94209 val_loss= 1.16947 val_acc= 0.65075 time= 0.28900
Epoch: 0069 train_loss= 0.30002 train_acc= 0.94904 val_loss= 1.16809 val_acc= 0.65672 time= 0.29400
Epoch: 0070 train_loss= 0.28487 train_acc= 0.94904 val_loss= 1.16731 val_acc= 0.66269 time= 0.29000
Epoch: 0071 train_loss= 0.27036 train_acc= 0.95367 val_loss= 1.16700 val_acc= 0.66269 time= 0.28900
Epoch: 0072 train_loss= 0.25690 train_acc= 0.95831 val_loss= 1.16694 val_acc= 0.66269 time= 0.28500
Epoch: 0073 train_loss= 0.24437 train_acc= 0.95963 val_loss= 1.16783 val_acc= 0.66567 time= 0.29400
Epoch: 0074 train_loss= 0.23158 train_acc= 0.96360 val_loss= 1.17007 val_acc= 0.66866 time= 0.29156
Early stopping...
Optimization Finished!
Test set results: cost= 1.15488 accuracy= 0.69132 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7339    0.7018    0.7175       342
           1     0.6814    0.7476    0.7130       103
           2     0.7544    0.6143    0.6772       140
           3     0.6316    0.4557    0.5294        79
           4     0.6846    0.7727    0.7260       132
           5     0.6891    0.7859    0.7343       313
           6     0.7048    0.7255    0.7150       102
           7     0.6571    0.3286    0.4381        70
           8     0.6061    0.4000    0.4819        50
           9     0.6188    0.7226    0.6667       155
          10     0.8345    0.6471    0.7289       187
          11     0.6073    0.6494    0.6276       231
          12     0.7544    0.7247    0.7393       178
          13     0.7690    0.8100    0.7890       600
          14     0.7752    0.8475    0.8097       590
          15     0.7639    0.7237    0.7432        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.4401    0.4821    0.4601       419
          19     0.6700    0.5194    0.5852       129
          20     0.6207    0.6429    0.6316        28
          21     0.9565    0.7586    0.8462        29
          22     0.5161    0.3478    0.4156        46

    accuracy                         0.6913      4043
   macro avg     0.6835    0.6026    0.6270      4043
weighted avg     0.6937    0.6913    0.6877      4043

Macro average Test Precision, Recall and F1-Score...
(0.6834508198770828, 0.6026355106121836, 0.6270426672931834, None)
Micro average Test Precision, Recall and F1-Score...
(0.6913183279742765, 0.6913183279742765, 0.6913183279742765, None)
embeddings:
14157 3357 4043
[[ 0.36698467  0.45383015  0.566431   ...  0.54615426  0.12511502
   0.47906783]
 [-0.08151022  0.15001845  0.3669638  ...  0.1052264   0.06074684
   0.09204184]
 [ 0.4926592   0.6172558   0.5455172  ...  0.08880521  0.07567582
   0.21075283]
 ...
 [ 0.24842176  0.2656605   0.27660093 ...  0.2132705   0.0969792
   0.1580976 ]
 [ 0.11321092  0.090188   -0.01253526 ...  0.42509446  0.03561244
  -0.06366725]
 [ 0.13992988  0.10437269  0.31248388 ...  0.39470425  0.07245422
   0.2930111 ]]

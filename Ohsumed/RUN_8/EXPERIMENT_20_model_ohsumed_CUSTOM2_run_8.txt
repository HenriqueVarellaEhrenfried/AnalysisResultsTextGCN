(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.04798 val_loss= 2.91451 val_acc= 0.20597 time= 0.59301
Epoch: 0002 train_loss= 2.91939 train_acc= 0.18531 val_loss= 2.74060 val_acc= 0.20896 time= 0.28700
Epoch: 0003 train_loss= 2.76751 train_acc= 0.18299 val_loss= 2.65563 val_acc= 0.20000 time= 0.28810
Epoch: 0004 train_loss= 2.67045 train_acc= 0.17174 val_loss= 2.54876 val_acc= 0.25970 time= 0.29397
Epoch: 0005 train_loss= 2.51555 train_acc= 0.23825 val_loss= 2.48278 val_acc= 0.33433 time= 0.29403
Epoch: 0006 train_loss= 2.42563 train_acc= 0.34778 val_loss= 2.34621 val_acc= 0.39701 time= 0.29000
Epoch: 0007 train_loss= 2.27564 train_acc= 0.41297 val_loss= 2.17067 val_acc= 0.40597 time= 0.29000
Epoch: 0008 train_loss= 2.09156 train_acc= 0.44110 val_loss= 2.03248 val_acc= 0.41493 time= 0.28707
Epoch: 0009 train_loss= 1.93794 train_acc= 0.46459 val_loss= 1.92569 val_acc= 0.48060 time= 0.29355
Epoch: 0010 train_loss= 1.78746 train_acc= 0.53243 val_loss= 1.82504 val_acc= 0.50746 time= 0.28897
Epoch: 0011 train_loss= 1.63137 train_acc= 0.58604 val_loss= 1.72631 val_acc= 0.51642 time= 0.29103
Epoch: 0012 train_loss= 1.47761 train_acc= 0.61416 val_loss= 1.63335 val_acc= 0.55224 time= 0.28900
Epoch: 0013 train_loss= 1.34723 train_acc= 0.66645 val_loss= 1.55955 val_acc= 0.57015 time= 0.29796
Epoch: 0014 train_loss= 1.23092 train_acc= 0.70549 val_loss= 1.47824 val_acc= 0.58806 time= 0.29203
Epoch: 0015 train_loss= 1.10129 train_acc= 0.71641 val_loss= 1.42254 val_acc= 0.59104 time= 0.28801
Epoch: 0016 train_loss= 0.99228 train_acc= 0.73263 val_loss= 1.39358 val_acc= 0.59403 time= 0.28698
Epoch: 0017 train_loss= 0.89135 train_acc= 0.76175 val_loss= 1.36867 val_acc= 0.58806 time= 0.29097
Epoch: 0018 train_loss= 0.79449 train_acc= 0.79914 val_loss= 1.34227 val_acc= 0.61493 time= 0.29313
Epoch: 0019 train_loss= 0.70352 train_acc= 0.81966 val_loss= 1.31079 val_acc= 0.62985 time= 0.28903
Epoch: 0020 train_loss= 0.61568 train_acc= 0.84613 val_loss= 1.27871 val_acc= 0.62985 time= 0.28604
Epoch: 0021 train_loss= 0.54665 train_acc= 0.85539 val_loss= 1.25786 val_acc= 0.62388 time= 0.29700
Epoch: 0022 train_loss= 0.47988 train_acc= 0.86797 val_loss= 1.26374 val_acc= 0.64478 time= 0.29200
Epoch: 0023 train_loss= 0.41074 train_acc= 0.89742 val_loss= 1.28400 val_acc= 0.65373 time= 0.28600
Epoch: 0024 train_loss= 0.36292 train_acc= 0.91099 val_loss= 1.28699 val_acc= 0.64179 time= 0.28527
Early stopping...
Optimization Finished!
Test set results: cost= 1.29327 accuracy= 0.67104 time= 0.12903
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6957    0.7018    0.6987       342
           1     0.6759    0.7087    0.6919       103
           2     0.6350    0.6214    0.6282       140
           3     0.4925    0.4177    0.4521        79
           4     0.5793    0.7197    0.6419       132
           5     0.7178    0.7476    0.7324       313
           6     0.6121    0.6961    0.6514       102
           7     0.5909    0.1857    0.2826        70
           8     0.6538    0.3400    0.4474        50
           9     0.6461    0.7419    0.6907       155
          10     0.7345    0.6952    0.7143       187
          11     0.6064    0.6537    0.6292       231
          12     0.7472    0.7472    0.7472       178
          13     0.7962    0.7683    0.7820       600
          14     0.7632    0.8356    0.7977       590
          15     0.7761    0.6842    0.7273        76
          16     0.5789    0.3235    0.4151        34
          17     0.0000    0.0000    0.0000        10
          18     0.4163    0.4630    0.4384       419
          19     0.5818    0.4961    0.5356       129
          20     0.6538    0.6071    0.6296        28
          21     1.0000    0.5172    0.6818        29
          22     0.7000    0.3043    0.4242        46

    accuracy                         0.6710      4043
   macro avg     0.6371    0.5642    0.5843      4043
weighted avg     0.6726    0.6710    0.6664      4043

Macro average Test Precision, Recall and F1-Score...
(0.6371122430582699, 0.5641851704728414, 0.5843310369996895, None)
Micro average Test Precision, Recall and F1-Score...
(0.6710363591392531, 0.6710363591392531, 0.6710363591392531, None)
embeddings:
14157 3357 4043
[[ 0.32095873  0.48479444 -0.5814229  ... -0.30926177  0.19660367
   0.30975813]
 [ 0.05597081  0.00858664 -0.13175109 ...  0.02275423  0.1137163
   0.0299252 ]
 [ 0.08199728  0.10223308 -0.35300502 ... -0.1683464   0.10967641
   0.26178333]
 ...
 [ 0.19939184  0.30002874 -0.17109287 ... -0.06672069  0.11928847
   0.12046287]
 [-0.06284602  0.07956307 -0.1791693  ...  0.0968866  -0.12786223
   0.39730567]
 [ 0.09545999  0.37296972 -0.18160737 ... -0.09275245  0.27438432
   0.20586412]]

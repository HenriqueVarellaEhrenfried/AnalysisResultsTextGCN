(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13552 train_acc= 0.01224 val_loss= 3.11584 val_acc= 0.20000 time= 0.59137
Epoch: 0002 train_loss= 3.11598 train_acc= 0.17141 val_loss= 3.07101 val_acc= 0.20000 time= 0.29100
Epoch: 0003 train_loss= 3.07175 train_acc= 0.17141 val_loss= 3.00120 val_acc= 0.20000 time= 0.28800
Epoch: 0004 train_loss= 3.00284 train_acc= 0.17141 val_loss= 2.91319 val_acc= 0.20000 time= 0.29400
Epoch: 0005 train_loss= 2.91578 train_acc= 0.17141 val_loss= 2.82184 val_acc= 0.20000 time= 0.29004
Epoch: 0006 train_loss= 2.82635 train_acc= 0.17141 val_loss= 2.74510 val_acc= 0.20000 time= 0.28800
Epoch: 0007 train_loss= 2.75207 train_acc= 0.17141 val_loss= 2.69780 val_acc= 0.20000 time= 0.28997
Epoch: 0008 train_loss= 2.70747 train_acc= 0.17141 val_loss= 2.68549 val_acc= 0.20000 time= 0.29321
Epoch: 0009 train_loss= 2.69549 train_acc= 0.17141 val_loss= 2.68681 val_acc= 0.20000 time= 0.29303
Epoch: 0010 train_loss= 2.69648 train_acc= 0.17141 val_loss= 2.67520 val_acc= 0.20597 time= 0.28804
Epoch: 0011 train_loss= 2.67785 train_acc= 0.17571 val_loss= 2.64717 val_acc= 0.22090 time= 0.29000
Epoch: 0012 train_loss= 2.64140 train_acc= 0.19226 val_loss= 2.61351 val_acc= 0.24776 time= 0.28998
Epoch: 0013 train_loss= 2.59734 train_acc= 0.21774 val_loss= 2.58347 val_acc= 0.27164 time= 0.29402
Epoch: 0014 train_loss= 2.55747 train_acc= 0.24520 val_loss= 2.55841 val_acc= 0.28060 time= 0.29000
Epoch: 0015 train_loss= 2.52421 train_acc= 0.26142 val_loss= 2.53427 val_acc= 0.29552 time= 0.29000
Epoch: 0016 train_loss= 2.49324 train_acc= 0.27697 val_loss= 2.50649 val_acc= 0.30149 time= 0.29400
Epoch: 0017 train_loss= 2.46165 train_acc= 0.30410 val_loss= 2.47268 val_acc= 0.31343 time= 0.30010
Epoch: 0018 train_loss= 2.42445 train_acc= 0.33422 val_loss= 2.43298 val_acc= 0.32537 time= 0.29106
Epoch: 0019 train_loss= 2.38073 train_acc= 0.35672 val_loss= 2.38920 val_acc= 0.33134 time= 0.28800
Epoch: 0020 train_loss= 2.33447 train_acc= 0.36234 val_loss= 2.34366 val_acc= 0.34030 time= 0.28605
Epoch: 0021 train_loss= 2.28486 train_acc= 0.36532 val_loss= 2.29816 val_acc= 0.34925 time= 0.29997
Epoch: 0022 train_loss= 2.23367 train_acc= 0.36863 val_loss= 2.25346 val_acc= 0.35224 time= 0.29000
Epoch: 0023 train_loss= 2.18213 train_acc= 0.37558 val_loss= 2.20943 val_acc= 0.35821 time= 0.28641
Epoch: 0024 train_loss= 2.12901 train_acc= 0.39146 val_loss= 2.16558 val_acc= 0.37313 time= 0.29000
Epoch: 0025 train_loss= 2.07455 train_acc= 0.41827 val_loss= 2.12165 val_acc= 0.39701 time= 0.29823
Epoch: 0026 train_loss= 2.01896 train_acc= 0.45169 val_loss= 2.07803 val_acc= 0.42985 time= 0.29100
Epoch: 0027 train_loss= 1.95735 train_acc= 0.49471 val_loss= 2.03528 val_acc= 0.46567 time= 0.28700
Epoch: 0028 train_loss= 1.90420 train_acc= 0.51952 val_loss= 1.99345 val_acc= 0.48955 time= 0.28900
Epoch: 0029 train_loss= 1.84864 train_acc= 0.54765 val_loss= 1.95166 val_acc= 0.49851 time= 0.29297
Epoch: 0030 train_loss= 1.78861 train_acc= 0.58173 val_loss= 1.90857 val_acc= 0.50746 time= 0.29303
Epoch: 0031 train_loss= 1.73132 train_acc= 0.59332 val_loss= 1.86403 val_acc= 0.51343 time= 0.29200
Epoch: 0032 train_loss= 1.67796 train_acc= 0.60390 val_loss= 1.81913 val_acc= 0.52537 time= 0.28900
Epoch: 0033 train_loss= 1.61638 train_acc= 0.61218 val_loss= 1.77576 val_acc= 0.53134 time= 0.29197
Epoch: 0034 train_loss= 1.55792 train_acc= 0.62674 val_loss= 1.73521 val_acc= 0.53134 time= 0.29347
Epoch: 0035 train_loss= 1.50467 train_acc= 0.63832 val_loss= 1.69736 val_acc= 0.53433 time= 0.29500
Epoch: 0036 train_loss= 1.45314 train_acc= 0.65023 val_loss= 1.66132 val_acc= 0.54030 time= 0.28800
Epoch: 0037 train_loss= 1.39973 train_acc= 0.65718 val_loss= 1.62672 val_acc= 0.53731 time= 0.28966
Epoch: 0038 train_loss= 1.35019 train_acc= 0.67108 val_loss= 1.59365 val_acc= 0.54925 time= 0.29700
Epoch: 0039 train_loss= 1.29293 train_acc= 0.68431 val_loss= 1.56287 val_acc= 0.57313 time= 0.28800
Epoch: 0040 train_loss= 1.24284 train_acc= 0.69126 val_loss= 1.53368 val_acc= 0.57910 time= 0.28721
Epoch: 0041 train_loss= 1.19469 train_acc= 0.69921 val_loss= 1.50499 val_acc= 0.57910 time= 0.28928
Epoch: 0042 train_loss= 1.14960 train_acc= 0.71211 val_loss= 1.47699 val_acc= 0.58209 time= 0.29900
Epoch: 0043 train_loss= 1.10240 train_acc= 0.72766 val_loss= 1.45048 val_acc= 0.57910 time= 0.28900
Epoch: 0044 train_loss= 1.05396 train_acc= 0.74388 val_loss= 1.42611 val_acc= 0.59403 time= 0.28900
Epoch: 0045 train_loss= 1.01285 train_acc= 0.75678 val_loss= 1.40388 val_acc= 0.60000 time= 0.29303
Epoch: 0046 train_loss= 0.96883 train_acc= 0.76770 val_loss= 1.38295 val_acc= 0.60000 time= 0.29600
Epoch: 0047 train_loss= 0.92729 train_acc= 0.77796 val_loss= 1.36313 val_acc= 0.60597 time= 0.28700
Epoch: 0048 train_loss= 0.88459 train_acc= 0.79021 val_loss= 1.34423 val_acc= 0.61194 time= 0.28800
Epoch: 0049 train_loss= 0.84796 train_acc= 0.80410 val_loss= 1.32595 val_acc= 0.60896 time= 0.29015
Epoch: 0050 train_loss= 0.81085 train_acc= 0.81403 val_loss= 1.30858 val_acc= 0.60597 time= 0.29572
Epoch: 0051 train_loss= 0.77500 train_acc= 0.82230 val_loss= 1.29234 val_acc= 0.61791 time= 0.29003
Epoch: 0052 train_loss= 0.73586 train_acc= 0.82991 val_loss= 1.27753 val_acc= 0.62090 time= 0.29301
Epoch: 0053 train_loss= 0.70391 train_acc= 0.83852 val_loss= 1.26487 val_acc= 0.62388 time= 0.29003
Epoch: 0054 train_loss= 0.66805 train_acc= 0.85142 val_loss= 1.25401 val_acc= 0.62687 time= 0.29500
Epoch: 0055 train_loss= 0.63752 train_acc= 0.85639 val_loss= 1.24358 val_acc= 0.62985 time= 0.29303
Epoch: 0056 train_loss= 0.60509 train_acc= 0.86664 val_loss= 1.23422 val_acc= 0.62388 time= 0.28897
Epoch: 0057 train_loss= 0.57717 train_acc= 0.87359 val_loss= 1.22484 val_acc= 0.62985 time= 0.28903
Epoch: 0058 train_loss= 0.54935 train_acc= 0.88319 val_loss= 1.21576 val_acc= 0.62985 time= 0.29304
Epoch: 0059 train_loss= 0.52299 train_acc= 0.88716 val_loss= 1.20711 val_acc= 0.64179 time= 0.29304
Epoch: 0060 train_loss= 0.49777 train_acc= 0.89345 val_loss= 1.19966 val_acc= 0.65672 time= 0.28900
Epoch: 0061 train_loss= 0.47402 train_acc= 0.90073 val_loss= 1.19322 val_acc= 0.65970 time= 0.28700
Epoch: 0062 train_loss= 0.44902 train_acc= 0.90768 val_loss= 1.18769 val_acc= 0.65970 time= 0.29109
Epoch: 0063 train_loss= 0.42783 train_acc= 0.91165 val_loss= 1.18326 val_acc= 0.66269 time= 0.29400
Epoch: 0064 train_loss= 0.40700 train_acc= 0.91827 val_loss= 1.17964 val_acc= 0.66567 time= 0.28700
Epoch: 0065 train_loss= 0.38555 train_acc= 0.92323 val_loss= 1.17622 val_acc= 0.66567 time= 0.28800
Epoch: 0066 train_loss= 0.36749 train_acc= 0.92919 val_loss= 1.17325 val_acc= 0.67164 time= 0.29500
Epoch: 0067 train_loss= 0.34839 train_acc= 0.93117 val_loss= 1.17119 val_acc= 0.67164 time= 0.29700
Epoch: 0068 train_loss= 0.33169 train_acc= 0.94044 val_loss= 1.16960 val_acc= 0.67761 time= 0.29200
Epoch: 0069 train_loss= 0.31752 train_acc= 0.94242 val_loss= 1.16882 val_acc= 0.67463 time= 0.29100
Epoch: 0070 train_loss= 0.30058 train_acc= 0.94805 val_loss= 1.16797 val_acc= 0.67761 time= 0.29497
Epoch: 0071 train_loss= 0.28427 train_acc= 0.95169 val_loss= 1.16652 val_acc= 0.67463 time= 0.29704
Epoch: 0072 train_loss= 0.26923 train_acc= 0.95566 val_loss= 1.16729 val_acc= 0.67164 time= 0.28996
Epoch: 0073 train_loss= 0.25817 train_acc= 0.95764 val_loss= 1.16940 val_acc= 0.66866 time= 0.29308
Epoch: 0074 train_loss= 0.24412 train_acc= 0.96095 val_loss= 1.17214 val_acc= 0.66567 time= 0.28608
Early stopping...
Optimization Finished!
Test set results: cost= 1.15921 accuracy= 0.68810 time= 0.13354
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7237    0.7047    0.7141       342
           1     0.6937    0.7476    0.7196       103
           2     0.7411    0.5929    0.6587       140
           3     0.6327    0.3924    0.4844        79
           4     0.6806    0.7424    0.7101       132
           5     0.6835    0.7796    0.7284       313
           6     0.7019    0.7157    0.7087       102
           7     0.6111    0.3143    0.4151        70
           8     0.5676    0.4200    0.4828        50
           9     0.6270    0.7484    0.6824       155
          10     0.8356    0.6524    0.7327       187
          11     0.6163    0.6537    0.6345       231
          12     0.7771    0.7247    0.7500       178
          13     0.7711    0.8083    0.7893       600
          14     0.7861    0.8407    0.8124       590
          15     0.7681    0.6974    0.7310        76
          16     0.7333    0.3235    0.4490        34
          17     0.5000    0.1000    0.1667        10
          18     0.4211    0.4964    0.4556       419
          19     0.6442    0.5194    0.5751       129
          20     0.6429    0.6429    0.6429        28
          21     1.0000    0.7241    0.8400        29
          22     0.5600    0.3043    0.3944        46

    accuracy                         0.6881      4043
   macro avg     0.6834    0.5933    0.6208      4043
weighted avg     0.6926    0.6881    0.6851      4043

Macro average Test Precision, Recall and F1-Score...
(0.6834156487844202, 0.5932915880323784, 0.6207738831580842, None)
Micro average Test Precision, Recall and F1-Score...
(0.6881028938906752, 0.6881028938906752, 0.6881028938906752, None)
embeddings:
14157 3357 4043
[[ 3.23876649e-01  4.87715542e-01  5.21961927e-01 ...  5.30337930e-01
   4.00979459e-01  4.68798548e-01]
 [ 4.54455614e-04  2.14270800e-01  2.52700776e-01 ...  2.14311630e-01
   1.34574026e-01  2.01561779e-01]
 [ 4.65338640e-02  4.77816999e-01  3.80714476e-01 ...  6.08205378e-01
   4.28860635e-01  3.99929881e-01]
 ...
 [ 1.23032458e-01  1.80015460e-01  2.31678024e-01 ...  2.20531613e-01
   2.30264142e-01  1.18423074e-01]
 [-1.59268137e-02 -1.85133815e-02 -6.02617189e-02 ...  5.16304791e-01
   1.20132729e-01  4.36489463e-01]
 [ 2.23405287e-01  3.41967732e-01  3.55119705e-01 ...  2.57215828e-01
   3.72570068e-01  1.75000921e-01]]

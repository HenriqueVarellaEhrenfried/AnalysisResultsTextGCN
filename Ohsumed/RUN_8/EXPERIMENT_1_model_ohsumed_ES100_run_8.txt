(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13543 train_acc= 0.13633 val_loss= 3.11190 val_acc= 0.20597 time= 0.58566
Epoch: 0002 train_loss= 3.11209 train_acc= 0.17240 val_loss= 3.06101 val_acc= 0.20000 time= 0.28697
Epoch: 0003 train_loss= 3.06153 train_acc= 0.17141 val_loss= 2.98417 val_acc= 0.20000 time= 0.28887
Epoch: 0004 train_loss= 2.98621 train_acc= 0.17141 val_loss= 2.88999 val_acc= 0.20000 time= 0.29900
Epoch: 0005 train_loss= 2.89379 train_acc= 0.17141 val_loss= 2.79762 val_acc= 0.20000 time= 0.28903
Epoch: 0006 train_loss= 2.80407 train_acc= 0.17141 val_loss= 2.72778 val_acc= 0.20000 time= 0.29297
Epoch: 0007 train_loss= 2.73863 train_acc= 0.17174 val_loss= 2.69431 val_acc= 0.20000 time= 0.29100
Epoch: 0008 train_loss= 2.70750 train_acc= 0.17141 val_loss= 2.69305 val_acc= 0.20000 time= 0.29722
Epoch: 0009 train_loss= 2.70729 train_acc= 0.17174 val_loss= 2.69313 val_acc= 0.20000 time= 0.28934
Epoch: 0010 train_loss= 2.70607 train_acc= 0.17174 val_loss= 2.67535 val_acc= 0.20597 time= 0.28948
Epoch: 0011 train_loss= 2.67927 train_acc= 0.17406 val_loss= 2.64593 val_acc= 0.20896 time= 0.29003
Epoch: 0012 train_loss= 2.63992 train_acc= 0.18101 val_loss= 2.61700 val_acc= 0.22687 time= 0.29600
Epoch: 0013 train_loss= 2.60078 train_acc= 0.19457 val_loss= 2.59315 val_acc= 0.24776 time= 0.28997
Epoch: 0014 train_loss= 2.56782 train_acc= 0.21674 val_loss= 2.57212 val_acc= 0.26269 time= 0.29103
Epoch: 0015 train_loss= 2.54122 train_acc= 0.23858 val_loss= 2.54954 val_acc= 0.28358 time= 0.28901
Epoch: 0016 train_loss= 2.51232 train_acc= 0.26307 val_loss= 2.52251 val_acc= 0.29254 time= 0.29698
Epoch: 0017 train_loss= 2.48049 train_acc= 0.28690 val_loss= 2.49007 val_acc= 0.30746 time= 0.29800
Epoch: 0018 train_loss= 2.44937 train_acc= 0.30146 val_loss= 2.45300 val_acc= 0.31343 time= 0.28800
Epoch: 0019 train_loss= 2.41101 train_acc= 0.31502 val_loss= 2.41266 val_acc= 0.31343 time= 0.28700
Epoch: 0020 train_loss= 2.36374 train_acc= 0.32627 val_loss= 2.37067 val_acc= 0.31642 time= 0.29417
Epoch: 0021 train_loss= 2.31773 train_acc= 0.32694 val_loss= 2.32827 val_acc= 0.31940 time= 0.29787
Epoch: 0022 train_loss= 2.27067 train_acc= 0.33653 val_loss= 2.28608 val_acc= 0.32239 time= 0.28900
Epoch: 0023 train_loss= 2.21958 train_acc= 0.34315 val_loss= 2.24419 val_acc= 0.33134 time= 0.28700
Epoch: 0024 train_loss= 2.17531 train_acc= 0.35903 val_loss= 2.20249 val_acc= 0.34328 time= 0.29097
Epoch: 0025 train_loss= 2.12039 train_acc= 0.38650 val_loss= 2.16096 val_acc= 0.36119 time= 0.29251
Epoch: 0026 train_loss= 2.07069 train_acc= 0.41165 val_loss= 2.11959 val_acc= 0.39104 time= 0.28782
Epoch: 0027 train_loss= 2.01684 train_acc= 0.45069 val_loss= 2.07844 val_acc= 0.43582 time= 0.29000
Epoch: 0028 train_loss= 1.96039 train_acc= 0.49107 val_loss= 2.03734 val_acc= 0.46269 time= 0.29201
Epoch: 0029 train_loss= 1.90662 train_acc= 0.51853 val_loss= 1.99579 val_acc= 0.48358 time= 0.29400
Epoch: 0030 train_loss= 1.85165 train_acc= 0.54931 val_loss= 1.95339 val_acc= 0.49552 time= 0.28700
Epoch: 0031 train_loss= 1.80055 train_acc= 0.57346 val_loss= 1.90993 val_acc= 0.51642 time= 0.29000
Epoch: 0032 train_loss= 1.73829 train_acc= 0.58868 val_loss= 1.86621 val_acc= 0.52836 time= 0.28800
Epoch: 0033 train_loss= 1.67962 train_acc= 0.60126 val_loss= 1.82342 val_acc= 0.53134 time= 0.29900
Epoch: 0034 train_loss= 1.62512 train_acc= 0.60821 val_loss= 1.78220 val_acc= 0.54328 time= 0.29001
Epoch: 0035 train_loss= 1.57077 train_acc= 0.61979 val_loss= 1.74323 val_acc= 0.54627 time= 0.29094
Epoch: 0036 train_loss= 1.51404 train_acc= 0.63005 val_loss= 1.70618 val_acc= 0.54925 time= 0.28900
Epoch: 0037 train_loss= 1.46380 train_acc= 0.64328 val_loss= 1.67054 val_acc= 0.54627 time= 0.29497
Epoch: 0038 train_loss= 1.41685 train_acc= 0.65288 val_loss= 1.63631 val_acc= 0.54030 time= 0.29409
Epoch: 0039 train_loss= 1.35370 train_acc= 0.67042 val_loss= 1.60345 val_acc= 0.55522 time= 0.29203
Epoch: 0040 train_loss= 1.30762 train_acc= 0.67869 val_loss= 1.57216 val_acc= 0.56418 time= 0.28597
Epoch: 0041 train_loss= 1.26318 train_acc= 0.68862 val_loss= 1.54154 val_acc= 0.57910 time= 0.30322
Epoch: 0042 train_loss= 1.21429 train_acc= 0.70218 val_loss= 1.51216 val_acc= 0.57015 time= 0.29900
Epoch: 0043 train_loss= 1.16359 train_acc= 0.71343 val_loss= 1.48408 val_acc= 0.56418 time= 0.28600
Epoch: 0044 train_loss= 1.11522 train_acc= 0.72766 val_loss= 1.45806 val_acc= 0.58209 time= 0.28810
Epoch: 0045 train_loss= 1.07202 train_acc= 0.74222 val_loss= 1.43350 val_acc= 0.58806 time= 0.29386
Epoch: 0046 train_loss= 1.03082 train_acc= 0.75017 val_loss= 1.41028 val_acc= 0.57910 time= 0.29500
Epoch: 0047 train_loss= 0.98724 train_acc= 0.76175 val_loss= 1.38893 val_acc= 0.59403 time= 0.29304
Epoch: 0048 train_loss= 0.93981 train_acc= 0.77167 val_loss= 1.36954 val_acc= 0.60597 time= 0.29100
Epoch: 0049 train_loss= 0.90215 train_acc= 0.77796 val_loss= 1.35037 val_acc= 0.61493 time= 0.29000
Epoch: 0050 train_loss= 0.86842 train_acc= 0.79219 val_loss= 1.33219 val_acc= 0.61791 time= 0.29725
Epoch: 0051 train_loss= 0.83566 train_acc= 0.80013 val_loss= 1.31527 val_acc= 0.60896 time= 0.28803
Epoch: 0052 train_loss= 0.79519 train_acc= 0.81337 val_loss= 1.30039 val_acc= 0.61493 time= 0.28900
Epoch: 0053 train_loss= 0.76557 train_acc= 0.82164 val_loss= 1.28705 val_acc= 0.62388 time= 0.28800
Epoch: 0054 train_loss= 0.72829 train_acc= 0.83322 val_loss= 1.27483 val_acc= 0.62985 time= 0.29660
Epoch: 0055 train_loss= 0.69803 train_acc= 0.84150 val_loss= 1.26330 val_acc= 0.63582 time= 0.28998
Epoch: 0056 train_loss= 0.66810 train_acc= 0.84116 val_loss= 1.25215 val_acc= 0.62985 time= 0.28874
Epoch: 0057 train_loss= 0.64045 train_acc= 0.85109 val_loss= 1.24217 val_acc= 0.63284 time= 0.28799
Epoch: 0058 train_loss= 0.61432 train_acc= 0.86036 val_loss= 1.23283 val_acc= 0.64179 time= 0.29519
Epoch: 0059 train_loss= 0.58833 train_acc= 0.86830 val_loss= 1.22405 val_acc= 0.64776 time= 0.29103
Epoch: 0060 train_loss= 0.56235 train_acc= 0.87525 val_loss= 1.21571 val_acc= 0.65373 time= 0.28800
Epoch: 0061 train_loss= 0.53290 train_acc= 0.88551 val_loss= 1.20760 val_acc= 0.65373 time= 0.28700
Epoch: 0062 train_loss= 0.51382 train_acc= 0.88683 val_loss= 1.20041 val_acc= 0.64776 time= 0.29900
Epoch: 0063 train_loss= 0.48947 train_acc= 0.89742 val_loss= 1.19556 val_acc= 0.65075 time= 0.29500
Epoch: 0064 train_loss= 0.46920 train_acc= 0.90139 val_loss= 1.19122 val_acc= 0.65373 time= 0.28899
Epoch: 0065 train_loss= 0.44642 train_acc= 0.90437 val_loss= 1.18701 val_acc= 0.65373 time= 0.28826
Epoch: 0066 train_loss= 0.42589 train_acc= 0.91264 val_loss= 1.18317 val_acc= 0.64478 time= 0.29295
Epoch: 0067 train_loss= 0.40929 train_acc= 0.91628 val_loss= 1.17905 val_acc= 0.64179 time= 0.29300
Epoch: 0068 train_loss= 0.38788 train_acc= 0.92158 val_loss= 1.17514 val_acc= 0.65672 time= 0.29400
Epoch: 0069 train_loss= 0.37045 train_acc= 0.92389 val_loss= 1.17214 val_acc= 0.65373 time= 0.29200
Epoch: 0070 train_loss= 0.35525 train_acc= 0.92522 val_loss= 1.17014 val_acc= 0.65373 time= 0.29200
Epoch: 0071 train_loss= 0.34038 train_acc= 0.93448 val_loss= 1.16834 val_acc= 0.65672 time= 0.29200
Epoch: 0072 train_loss= 0.32757 train_acc= 0.93878 val_loss= 1.16537 val_acc= 0.65970 time= 0.29000
Epoch: 0073 train_loss= 0.30784 train_acc= 0.94441 val_loss= 1.16349 val_acc= 0.66567 time= 0.29200
Epoch: 0074 train_loss= 0.30109 train_acc= 0.94408 val_loss= 1.16472 val_acc= 0.65672 time= 0.28600
Epoch: 0075 train_loss= 0.28732 train_acc= 0.94772 val_loss= 1.16629 val_acc= 0.65373 time= 0.29818
Epoch: 0076 train_loss= 0.27658 train_acc= 0.94871 val_loss= 1.16666 val_acc= 0.65672 time= 0.29468
Epoch: 0077 train_loss= 0.26309 train_acc= 0.95069 val_loss= 1.16587 val_acc= 0.65970 time= 0.29100
Epoch: 0078 train_loss= 0.25603 train_acc= 0.95367 val_loss= 1.16622 val_acc= 0.65075 time= 0.29100
Epoch: 0079 train_loss= 0.23914 train_acc= 0.96128 val_loss= 1.16601 val_acc= 0.65672 time= 0.29530
Epoch: 0080 train_loss= 0.23195 train_acc= 0.96360 val_loss= 1.16691 val_acc= 0.65672 time= 0.28997
Epoch: 0081 train_loss= 0.22526 train_acc= 0.96161 val_loss= 1.16957 val_acc= 0.65970 time= 0.28603
Epoch: 0082 train_loss= 0.21582 train_acc= 0.96691 val_loss= 1.17279 val_acc= 0.67164 time= 0.28897
Epoch: 0083 train_loss= 0.20620 train_acc= 0.96790 val_loss= 1.17546 val_acc= 0.66866 time= 0.29303
Epoch: 0084 train_loss= 0.19594 train_acc= 0.96790 val_loss= 1.17793 val_acc= 0.67761 time= 0.29309
Epoch: 0085 train_loss= 0.18822 train_acc= 0.97187 val_loss= 1.18074 val_acc= 0.66567 time= 0.28700
Epoch: 0086 train_loss= 0.18034 train_acc= 0.97253 val_loss= 1.18387 val_acc= 0.66567 time= 0.29100
Epoch: 0087 train_loss= 0.17437 train_acc= 0.97717 val_loss= 1.18639 val_acc= 0.66269 time= 0.29400
Epoch: 0088 train_loss= 0.16663 train_acc= 0.97452 val_loss= 1.18842 val_acc= 0.65672 time= 0.29300
Epoch: 0089 train_loss= 0.15862 train_acc= 0.97783 val_loss= 1.18924 val_acc= 0.65970 time= 0.29400
Epoch: 0090 train_loss= 0.15313 train_acc= 0.97981 val_loss= 1.19106 val_acc= 0.66567 time= 0.29000
Epoch: 0091 train_loss= 0.14791 train_acc= 0.98048 val_loss= 1.19477 val_acc= 0.66866 time= 0.29400
Epoch: 0092 train_loss= 0.14103 train_acc= 0.97981 val_loss= 1.19888 val_acc= 0.66866 time= 0.29301
Epoch: 0093 train_loss= 0.13735 train_acc= 0.98015 val_loss= 1.20363 val_acc= 0.67463 time= 0.29400
Epoch: 0094 train_loss= 0.13176 train_acc= 0.98114 val_loss= 1.20863 val_acc= 0.66866 time= 0.28899
Epoch: 0095 train_loss= 0.12582 train_acc= 0.98544 val_loss= 1.21398 val_acc= 0.66866 time= 0.28908
Epoch: 0096 train_loss= 0.12060 train_acc= 0.98544 val_loss= 1.21704 val_acc= 0.66269 time= 0.29701
Epoch: 0097 train_loss= 0.11403 train_acc= 0.98743 val_loss= 1.22034 val_acc= 0.66269 time= 0.28999
Epoch: 0098 train_loss= 0.11262 train_acc= 0.98610 val_loss= 1.22436 val_acc= 0.66269 time= 0.28709
Epoch: 0099 train_loss= 0.10934 train_acc= 0.98908 val_loss= 1.22734 val_acc= 0.65672 time= 0.29102
Epoch: 0100 train_loss= 0.10532 train_acc= 0.98776 val_loss= 1.23089 val_acc= 0.65672 time= 0.30203
Epoch: 0101 train_loss= 0.09963 train_acc= 0.99140 val_loss= 1.23432 val_acc= 0.65970 time= 0.29097
Epoch: 0102 train_loss= 0.09504 train_acc= 0.99040 val_loss= 1.23683 val_acc= 0.66567 time= 0.28903
Epoch: 0103 train_loss= 0.09386 train_acc= 0.99107 val_loss= 1.24051 val_acc= 0.66269 time= 0.28805
Epoch: 0104 train_loss= 0.09071 train_acc= 0.99206 val_loss= 1.24629 val_acc= 0.65970 time= 0.29500
Epoch: 0105 train_loss= 0.08733 train_acc= 0.99272 val_loss= 1.25278 val_acc= 0.65970 time= 0.29100
Epoch: 0106 train_loss= 0.08530 train_acc= 0.98974 val_loss= 1.26025 val_acc= 0.65373 time= 0.28800
Epoch: 0107 train_loss= 0.08192 train_acc= 0.99504 val_loss= 1.26456 val_acc= 0.65373 time= 0.29100
Epoch: 0108 train_loss= 0.08037 train_acc= 0.99239 val_loss= 1.26574 val_acc= 0.65672 time= 0.29630
Epoch: 0109 train_loss= 0.07472 train_acc= 0.99338 val_loss= 1.26773 val_acc= 0.65970 time= 0.29700
Epoch: 0110 train_loss= 0.07287 train_acc= 0.99504 val_loss= 1.27058 val_acc= 0.65970 time= 0.29049
Epoch: 0111 train_loss= 0.06995 train_acc= 0.99504 val_loss= 1.27445 val_acc= 0.65672 time= 0.28900
Epoch: 0112 train_loss= 0.07085 train_acc= 0.99471 val_loss= 1.27941 val_acc= 0.65672 time= 0.29097
Epoch: 0113 train_loss= 0.06609 train_acc= 0.99471 val_loss= 1.28579 val_acc= 0.65672 time= 0.29803
Epoch: 0114 train_loss= 0.06652 train_acc= 0.99570 val_loss= 1.29107 val_acc= 0.65373 time= 0.28597
Epoch: 0115 train_loss= 0.06335 train_acc= 0.99702 val_loss= 1.29546 val_acc= 0.65970 time= 0.28703
Epoch: 0116 train_loss= 0.06175 train_acc= 0.99669 val_loss= 1.29946 val_acc= 0.65970 time= 0.29497
Epoch: 0117 train_loss= 0.05926 train_acc= 0.99702 val_loss= 1.30222 val_acc= 0.65373 time= 0.30104
Epoch: 0118 train_loss= 0.06055 train_acc= 0.99636 val_loss= 1.30508 val_acc= 0.64776 time= 0.28999
Epoch: 0119 train_loss= 0.05697 train_acc= 0.99603 val_loss= 1.30990 val_acc= 0.65373 time= 0.29306
Epoch: 0120 train_loss= 0.05395 train_acc= 0.99669 val_loss= 1.31697 val_acc= 0.65672 time= 0.29397
Epoch: 0121 train_loss= 0.05369 train_acc= 0.99702 val_loss= 1.32393 val_acc= 0.65672 time= 0.29504
Epoch: 0122 train_loss= 0.05145 train_acc= 0.99801 val_loss= 1.32577 val_acc= 0.65373 time= 0.28806
Epoch: 0123 train_loss= 0.05138 train_acc= 0.99702 val_loss= 1.32574 val_acc= 0.65075 time= 0.28497
Epoch: 0124 train_loss= 0.04691 train_acc= 0.99901 val_loss= 1.32910 val_acc= 0.65970 time= 0.29003
Epoch: 0125 train_loss= 0.04763 train_acc= 0.99868 val_loss= 1.33255 val_acc= 0.65672 time= 0.29197
Epoch: 0126 train_loss= 0.04795 train_acc= 0.99735 val_loss= 1.33625 val_acc= 0.65075 time= 0.28934
Epoch: 0127 train_loss= 0.04558 train_acc= 0.99768 val_loss= 1.34446 val_acc= 0.65672 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.30157 accuracy= 0.68439 time= 0.13200
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7244    0.6608    0.6911       342
           1     0.6952    0.7087    0.7019       103
           2     0.7339    0.6500    0.6894       140
           3     0.6944    0.6329    0.6623        79
           4     0.6405    0.7424    0.6877       132
           5     0.6795    0.7923    0.7316       313
           6     0.7009    0.7353    0.7177       102
           7     0.6042    0.4143    0.4915        70
           8     0.6000    0.3600    0.4500        50
           9     0.6307    0.7161    0.6707       155
          10     0.8322    0.6364    0.7212       187
          11     0.6089    0.6537    0.6305       231
          12     0.7764    0.7022    0.7375       178
          13     0.7722    0.7967    0.7842       600
          14     0.7958    0.8254    0.8103       590
          15     0.7397    0.7105    0.7248        76
          16     0.6818    0.4412    0.5357        34
          17     0.1667    0.1000    0.1250        10
          18     0.4103    0.4749    0.4403       419
          19     0.6778    0.4729    0.5571       129
          20     0.6129    0.6786    0.6441        28
          21     0.9130    0.7241    0.8077        29
          22     0.4737    0.3913    0.4286        46

    accuracy                         0.6844      4043
   macro avg     0.6593    0.6096    0.6279      4043
weighted avg     0.6898    0.6844    0.6836      4043

Macro average Test Precision, Recall and F1-Score...
(0.6593478486906192, 0.6096017072367963, 0.6278629831643412, None)
Micro average Test Precision, Recall and F1-Score...
(0.6843927776403661, 0.6843927776403661, 0.6843927776403661, None)
embeddings:
14157 3357 4043
[[ 0.56826437  0.4600982   0.4285875  ...  0.38626316  0.5357161
   0.48516154]
 [ 0.39187688  0.11708985  0.09354927 ...  0.15005173  0.06032889
   0.35298547]
 [ 0.54898506  0.4557046   0.19454205 ...  0.09748056  0.18081683
   0.44239908]
 ...
 [ 0.3150009   0.26851767  0.23772153 ...  0.22197102  0.28620622
   0.21169136]
 [-0.05011584 -0.05920824  0.10087813 ... -0.05559249  0.4908439
   0.37907597]
 [ 0.32761186  0.04997073  0.3611522  ...  0.34233963  0.12000839
   0.00304435]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.03805 val_loss= 3.11674 val_acc= 0.20299 time= 0.57944
Epoch: 0002 train_loss= 3.11707 train_acc= 0.17472 val_loss= 3.07392 val_acc= 0.20000 time= 0.29236
Epoch: 0003 train_loss= 3.07470 train_acc= 0.17472 val_loss= 3.00852 val_acc= 0.20000 time= 0.29300
Epoch: 0004 train_loss= 3.01028 train_acc= 0.17340 val_loss= 2.92626 val_acc= 0.20000 time= 0.28700
Epoch: 0005 train_loss= 2.92768 train_acc= 0.17538 val_loss= 2.84070 val_acc= 0.20000 time= 0.28900
Epoch: 0006 train_loss= 2.84579 train_acc= 0.17704 val_loss= 2.76814 val_acc= 0.20000 time= 0.29200
Epoch: 0007 train_loss= 2.77776 train_acc= 0.17637 val_loss= 2.71995 val_acc= 0.20000 time= 0.29500
Epoch: 0008 train_loss= 2.72815 train_acc= 0.17704 val_loss= 2.70302 val_acc= 0.20000 time= 0.29100
Epoch: 0009 train_loss= 2.71735 train_acc= 0.17538 val_loss= 2.70405 val_acc= 0.20000 time= 0.28910
Epoch: 0010 train_loss= 2.71565 train_acc= 0.17207 val_loss= 2.70278 val_acc= 0.20000 time= 0.28619
Epoch: 0011 train_loss= 2.72137 train_acc= 0.17174 val_loss= 2.68675 val_acc= 0.20000 time= 0.29736
Epoch: 0012 train_loss= 2.70158 train_acc= 0.17273 val_loss= 2.66183 val_acc= 0.20000 time= 0.28800
Epoch: 0013 train_loss= 2.65985 train_acc= 0.17340 val_loss= 2.63741 val_acc= 0.20597 time= 0.28800
Epoch: 0014 train_loss= 2.63662 train_acc= 0.18001 val_loss= 2.61741 val_acc= 0.20896 time= 0.28599
Epoch: 0015 train_loss= 2.60739 train_acc= 0.18531 val_loss= 2.60043 val_acc= 0.22687 time= 0.29301
Epoch: 0016 train_loss= 2.58625 train_acc= 0.19854 val_loss= 2.58389 val_acc= 0.24776 time= 0.29100
Epoch: 0017 train_loss= 2.56676 train_acc= 0.21509 val_loss= 2.56446 val_acc= 0.25970 time= 0.28600
Epoch: 0018 train_loss= 2.53446 train_acc= 0.24553 val_loss= 2.54063 val_acc= 0.28060 time= 0.28900
Epoch: 0019 train_loss= 2.51196 train_acc= 0.26539 val_loss= 2.51215 val_acc= 0.29552 time= 0.29700
Epoch: 0020 train_loss= 2.48464 train_acc= 0.28226 val_loss= 2.47977 val_acc= 0.30746 time= 0.29502
Epoch: 0021 train_loss= 2.44731 train_acc= 0.28690 val_loss= 2.44490 val_acc= 0.31045 time= 0.28800
Epoch: 0022 train_loss= 2.41741 train_acc= 0.29682 val_loss= 2.40877 val_acc= 0.31045 time= 0.29200
Epoch: 0023 train_loss= 2.38103 train_acc= 0.30179 val_loss= 2.37218 val_acc= 0.31045 time= 0.29200
Epoch: 0024 train_loss= 2.34791 train_acc= 0.29484 val_loss= 2.33600 val_acc= 0.31045 time= 0.29300
Epoch: 0025 train_loss= 2.30262 train_acc= 0.30344 val_loss= 2.30018 val_acc= 0.31343 time= 0.28846
Epoch: 0026 train_loss= 2.25724 train_acc= 0.31668 val_loss= 2.26436 val_acc= 0.31940 time= 0.28800
Epoch: 0027 train_loss= 2.21365 train_acc= 0.32561 val_loss= 2.22839 val_acc= 0.32537 time= 0.29200
Epoch: 0028 train_loss= 2.17607 train_acc= 0.34216 val_loss= 2.19234 val_acc= 0.35224 time= 0.29200
Epoch: 0029 train_loss= 2.12498 train_acc= 0.38716 val_loss= 2.15647 val_acc= 0.37910 time= 0.28800
Epoch: 0030 train_loss= 2.08943 train_acc= 0.41463 val_loss= 2.12069 val_acc= 0.39701 time= 0.28900
Epoch: 0031 train_loss= 2.04811 train_acc= 0.44805 val_loss= 2.08471 val_acc= 0.44179 time= 0.28755
Epoch: 0032 train_loss= 1.99586 train_acc= 0.47684 val_loss= 2.04850 val_acc= 0.47463 time= 0.29900
Epoch: 0033 train_loss= 1.95568 train_acc= 0.49702 val_loss= 2.01148 val_acc= 0.48657 time= 0.29200
Epoch: 0034 train_loss= 1.91691 train_acc= 0.52283 val_loss= 1.97328 val_acc= 0.50448 time= 0.28800
Epoch: 0035 train_loss= 1.86624 train_acc= 0.53971 val_loss= 1.93467 val_acc= 0.50746 time= 0.29100
Epoch: 0036 train_loss= 1.81017 train_acc= 0.55493 val_loss= 1.89603 val_acc= 0.51343 time= 0.29397
Epoch: 0037 train_loss= 1.75357 train_acc= 0.55791 val_loss= 1.85807 val_acc= 0.51940 time= 0.29250
Epoch: 0038 train_loss= 1.72281 train_acc= 0.56056 val_loss= 1.82213 val_acc= 0.51642 time= 0.28703
Epoch: 0039 train_loss= 1.66476 train_acc= 0.56519 val_loss= 1.78755 val_acc= 0.52836 time= 0.29101
Epoch: 0040 train_loss= 1.63385 train_acc= 0.57280 val_loss= 1.75419 val_acc= 0.54328 time= 0.29700
Epoch: 0041 train_loss= 1.59277 train_acc= 0.58868 val_loss= 1.72193 val_acc= 0.55821 time= 0.29096
Epoch: 0042 train_loss= 1.53449 train_acc= 0.59563 val_loss= 1.69107 val_acc= 0.55224 time= 0.29003
Epoch: 0043 train_loss= 1.51128 train_acc= 0.60788 val_loss= 1.66248 val_acc= 0.55821 time= 0.28797
Epoch: 0044 train_loss= 1.46812 train_acc= 0.62376 val_loss= 1.63444 val_acc= 0.56418 time= 0.29800
Epoch: 0045 train_loss= 1.40924 train_acc= 0.63402 val_loss= 1.60697 val_acc= 0.56418 time= 0.29503
Epoch: 0046 train_loss= 1.38906 train_acc= 0.63898 val_loss= 1.57887 val_acc= 0.56716 time= 0.28897
Epoch: 0047 train_loss= 1.33797 train_acc= 0.64692 val_loss= 1.55243 val_acc= 0.56119 time= 0.28903
Epoch: 0048 train_loss= 1.29485 train_acc= 0.66578 val_loss= 1.52798 val_acc= 0.57015 time= 0.29300
Epoch: 0049 train_loss= 1.25873 train_acc= 0.66744 val_loss= 1.50546 val_acc= 0.58209 time= 0.29600
Epoch: 0050 train_loss= 1.22451 train_acc= 0.68266 val_loss= 1.48409 val_acc= 0.58209 time= 0.28797
Epoch: 0051 train_loss= 1.19870 train_acc= 0.68034 val_loss= 1.46351 val_acc= 0.58806 time= 0.28806
Epoch: 0052 train_loss= 1.16121 train_acc= 0.69722 val_loss= 1.44411 val_acc= 0.60299 time= 0.29000
Epoch: 0053 train_loss= 1.13495 train_acc= 0.70682 val_loss= 1.42650 val_acc= 0.60299 time= 0.29800
Epoch: 0054 train_loss= 1.11911 train_acc= 0.71575 val_loss= 1.40891 val_acc= 0.60896 time= 0.29000
Epoch: 0055 train_loss= 1.08445 train_acc= 0.71244 val_loss= 1.39117 val_acc= 0.61194 time= 0.28900
Epoch: 0056 train_loss= 1.03858 train_acc= 0.73329 val_loss= 1.37333 val_acc= 0.60597 time= 0.29600
Epoch: 0057 train_loss= 1.00209 train_acc= 0.74123 val_loss= 1.35655 val_acc= 0.61194 time= 0.30100
Epoch: 0058 train_loss= 0.99778 train_acc= 0.73825 val_loss= 1.34080 val_acc= 0.61493 time= 0.28700
Epoch: 0059 train_loss= 0.95571 train_acc= 0.74421 val_loss= 1.32779 val_acc= 0.60597 time= 0.28600
Epoch: 0060 train_loss= 0.93923 train_acc= 0.74818 val_loss= 1.31631 val_acc= 0.61791 time= 0.29100
Epoch: 0061 train_loss= 0.90974 train_acc= 0.76042 val_loss= 1.30551 val_acc= 0.62687 time= 0.29700
Epoch: 0062 train_loss= 0.88114 train_acc= 0.77366 val_loss= 1.29841 val_acc= 0.62388 time= 0.29300
Epoch: 0063 train_loss= 0.87097 train_acc= 0.77366 val_loss= 1.29306 val_acc= 0.62388 time= 0.28800
Epoch: 0064 train_loss= 0.84868 train_acc= 0.77796 val_loss= 1.28733 val_acc= 0.62985 time= 0.28900
Epoch: 0065 train_loss= 0.80766 train_acc= 0.78855 val_loss= 1.27830 val_acc= 0.62388 time= 0.29637
Epoch: 0066 train_loss= 0.79782 train_acc= 0.79153 val_loss= 1.26641 val_acc= 0.62687 time= 0.29900
Epoch: 0067 train_loss= 0.76555 train_acc= 0.80311 val_loss= 1.25552 val_acc= 0.62388 time= 0.29003
Epoch: 0068 train_loss= 0.74064 train_acc= 0.81370 val_loss= 1.24550 val_acc= 0.62090 time= 0.29201
Epoch: 0069 train_loss= 0.72896 train_acc= 0.80940 val_loss= 1.23400 val_acc= 0.62090 time= 0.29338
Epoch: 0070 train_loss= 0.72299 train_acc= 0.81238 val_loss= 1.22387 val_acc= 0.62985 time= 0.29996
Epoch: 0071 train_loss= 0.69553 train_acc= 0.82065 val_loss= 1.21919 val_acc= 0.64478 time= 0.29300
Epoch: 0072 train_loss= 0.67505 train_acc= 0.82396 val_loss= 1.21436 val_acc= 0.64478 time= 0.28827
Epoch: 0073 train_loss= 0.66476 train_acc= 0.82727 val_loss= 1.20948 val_acc= 0.64776 time= 0.28718
Epoch: 0074 train_loss= 0.65345 train_acc= 0.82991 val_loss= 1.20444 val_acc= 0.64776 time= 0.29966
Epoch: 0075 train_loss= 0.62866 train_acc= 0.83620 val_loss= 1.20042 val_acc= 0.65672 time= 0.28900
Epoch: 0076 train_loss= 0.61677 train_acc= 0.83587 val_loss= 1.19673 val_acc= 0.64478 time= 0.28700
Epoch: 0077 train_loss= 0.61436 train_acc= 0.83918 val_loss= 1.19240 val_acc= 0.63881 time= 0.28700
Epoch: 0078 train_loss= 0.59374 train_acc= 0.85043 val_loss= 1.18622 val_acc= 0.65075 time= 0.29472
Epoch: 0079 train_loss= 0.55668 train_acc= 0.86334 val_loss= 1.18133 val_acc= 0.65373 time= 0.28700
Epoch: 0080 train_loss= 0.56229 train_acc= 0.85672 val_loss= 1.17587 val_acc= 0.64776 time= 0.28800
Epoch: 0081 train_loss= 0.54117 train_acc= 0.86300 val_loss= 1.17082 val_acc= 0.65970 time= 0.29000
Epoch: 0082 train_loss= 0.53287 train_acc= 0.86929 val_loss= 1.16656 val_acc= 0.65970 time= 0.29900
Epoch: 0083 train_loss= 0.52501 train_acc= 0.87128 val_loss= 1.16358 val_acc= 0.66866 time= 0.29103
Epoch: 0084 train_loss= 0.49720 train_acc= 0.87922 val_loss= 1.16151 val_acc= 0.67164 time= 0.28800
Epoch: 0085 train_loss= 0.49106 train_acc= 0.87657 val_loss= 1.15837 val_acc= 0.66866 time= 0.28908
Epoch: 0086 train_loss= 0.48299 train_acc= 0.88518 val_loss= 1.15539 val_acc= 0.67164 time= 0.29441
Epoch: 0087 train_loss= 0.47634 train_acc= 0.88981 val_loss= 1.15486 val_acc= 0.67463 time= 0.29400
Epoch: 0088 train_loss= 0.44650 train_acc= 0.89345 val_loss= 1.15449 val_acc= 0.67164 time= 0.29100
Epoch: 0089 train_loss= 0.44682 train_acc= 0.89312 val_loss= 1.15542 val_acc= 0.66269 time= 0.29000
Epoch: 0090 train_loss= 0.44072 train_acc= 0.89775 val_loss= 1.15647 val_acc= 0.65373 time= 0.29180
Epoch: 0091 train_loss= 0.43703 train_acc= 0.90073 val_loss= 1.15273 val_acc= 0.65672 time= 0.29528
Epoch: 0092 train_loss= 0.43514 train_acc= 0.89742 val_loss= 1.14763 val_acc= 0.65970 time= 0.29000
Epoch: 0093 train_loss= 0.43058 train_acc= 0.89676 val_loss= 1.14414 val_acc= 0.65970 time= 0.28800
Epoch: 0094 train_loss= 0.40005 train_acc= 0.90702 val_loss= 1.14224 val_acc= 0.66567 time= 0.28800
Epoch: 0095 train_loss= 0.39142 train_acc= 0.90900 val_loss= 1.14240 val_acc= 0.67463 time= 0.29727
Epoch: 0096 train_loss= 0.37809 train_acc= 0.91496 val_loss= 1.14211 val_acc= 0.68060 time= 0.28800
Epoch: 0097 train_loss= 0.36942 train_acc= 0.91264 val_loss= 1.14247 val_acc= 0.67463 time= 0.28541
Epoch: 0098 train_loss= 0.36784 train_acc= 0.90602 val_loss= 1.14429 val_acc= 0.66269 time= 0.28568
Epoch: 0099 train_loss= 0.36193 train_acc= 0.92025 val_loss= 1.14753 val_acc= 0.66567 time= 0.29296
Early stopping...
Optimization Finished!
Test set results: cost= 1.15270 accuracy= 0.68637 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6979    0.6959    0.6969       342
           1     0.7037    0.7379    0.7204       103
           2     0.7458    0.6286    0.6822       140
           3     0.5741    0.3924    0.4662        79
           4     0.6533    0.7424    0.6950       132
           5     0.7122    0.7668    0.7385       313
           6     0.6923    0.7059    0.6990       102
           7     0.5897    0.3286    0.4220        70
           8     0.6552    0.3800    0.4810        50
           9     0.6588    0.7226    0.6892       155
          10     0.8403    0.6471    0.7311       187
          11     0.6102    0.6710    0.6392       231
          12     0.7616    0.7360    0.7486       178
          13     0.7718    0.8117    0.7912       600
          14     0.7757    0.8441    0.8084       590
          15     0.7681    0.6974    0.7310        76
          16     0.7500    0.3529    0.4800        34
          17     1.0000    0.1000    0.1818        10
          18     0.4124    0.4940    0.4495       419
          19     0.6598    0.4961    0.5664       129
          20     0.7647    0.4643    0.5778        28
          21     1.0000    0.7241    0.8400        29
          22     0.5556    0.3261    0.4110        46

    accuracy                         0.6864      4043
   macro avg     0.7110    0.5855    0.6194      4043
weighted avg     0.6923    0.6864    0.6835      4043

Macro average Test Precision, Recall and F1-Score...
(0.7110082554105245, 0.5854652387635163, 0.6194101511879309, None)
Micro average Test Precision, Recall and F1-Score...
(0.6863715063071977, 0.6863715063071977, 0.6863715063071977, None)
embeddings:
14157 3357 4043
[[ 0.32988584  0.23733754  0.20305136 ...  0.284635    0.10155278
   0.3588463 ]
 [ 0.10747804  0.12325049  0.08783565 ...  0.05966417 -0.1163557
   0.3635978 ]
 [ 0.30796784  0.01461284  0.20444591 ...  0.48716187 -0.04978915
   0.3828518 ]
 ...
 [ 0.14718108  0.17741069  0.06555406 ...  0.07788222  0.17555511
   0.14643875]
 [ 0.02280837  0.17710415  0.34615722 ...  0.3479876   0.00462032
   0.15281074]
 [ 0.24092808  0.02458918  0.00268305 ...  0.04443518  0.0280602
   0.15470563]]

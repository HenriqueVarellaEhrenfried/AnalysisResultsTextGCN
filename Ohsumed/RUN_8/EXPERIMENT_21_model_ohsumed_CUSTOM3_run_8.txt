(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13549 train_acc= 0.04600 val_loss= 3.11650 val_acc= 0.20000 time= 0.58400
Epoch: 0002 train_loss= 3.11664 train_acc= 0.17141 val_loss= 3.07420 val_acc= 0.20000 time= 0.28903
Epoch: 0003 train_loss= 3.07414 train_acc= 0.17141 val_loss= 3.00927 val_acc= 0.20000 time= 0.29497
Epoch: 0004 train_loss= 3.00983 train_acc= 0.17141 val_loss= 2.92735 val_acc= 0.20000 time= 0.29303
Epoch: 0005 train_loss= 2.93060 train_acc= 0.17141 val_loss= 2.84113 val_acc= 0.20000 time= 0.28800
Epoch: 0006 train_loss= 2.84501 train_acc= 0.17141 val_loss= 2.76551 val_acc= 0.20000 time= 0.28802
Epoch: 0007 train_loss= 2.77123 train_acc= 0.17174 val_loss= 2.71361 val_acc= 0.20000 time= 0.29700
Epoch: 0008 train_loss= 2.72404 train_acc= 0.17141 val_loss= 2.69414 val_acc= 0.20000 time= 0.29503
Epoch: 0009 train_loss= 2.70917 train_acc= 0.17141 val_loss= 2.69692 val_acc= 0.20000 time= 0.28700
Epoch: 0010 train_loss= 2.71484 train_acc= 0.17141 val_loss= 2.69704 val_acc= 0.20000 time= 0.28706
Epoch: 0011 train_loss= 2.71720 train_acc= 0.17207 val_loss= 2.68088 val_acc= 0.20000 time= 0.28852
Epoch: 0012 train_loss= 2.70017 train_acc= 0.17240 val_loss= 2.65480 val_acc= 0.20597 time= 0.29400
Epoch: 0013 train_loss= 2.66431 train_acc= 0.17439 val_loss= 2.62966 val_acc= 0.20896 time= 0.29000
Epoch: 0014 train_loss= 2.62855 train_acc= 0.18332 val_loss= 2.60936 val_acc= 0.22687 time= 0.28500
Epoch: 0015 train_loss= 2.59797 train_acc= 0.19755 val_loss= 2.59215 val_acc= 0.25075 time= 0.29002
Epoch: 0016 train_loss= 2.57485 train_acc= 0.22005 val_loss= 2.57418 val_acc= 0.25970 time= 0.29700
Epoch: 0017 train_loss= 2.55557 train_acc= 0.23693 val_loss= 2.55254 val_acc= 0.27164 time= 0.28765
Epoch: 0018 train_loss= 2.52517 train_acc= 0.26109 val_loss= 2.52601 val_acc= 0.28657 time= 0.29099
Epoch: 0019 train_loss= 2.49716 train_acc= 0.27101 val_loss= 2.49503 val_acc= 0.29254 time= 0.28897
Epoch: 0020 train_loss= 2.46318 train_acc= 0.28855 val_loss= 2.46075 val_acc= 0.29552 time= 0.29403
Epoch: 0021 train_loss= 2.43312 train_acc= 0.29285 val_loss= 2.42428 val_acc= 0.30149 time= 0.28800
Epoch: 0022 train_loss= 2.39658 train_acc= 0.29484 val_loss= 2.38662 val_acc= 0.30746 time= 0.29200
Epoch: 0023 train_loss= 2.35288 train_acc= 0.30874 val_loss= 2.34870 val_acc= 0.31045 time= 0.28800
Epoch: 0024 train_loss= 2.30887 train_acc= 0.31072 val_loss= 2.31073 val_acc= 0.31045 time= 0.29466
Epoch: 0025 train_loss= 2.27097 train_acc= 0.31668 val_loss= 2.27242 val_acc= 0.32537 time= 0.29400
Epoch: 0026 train_loss= 2.23722 train_acc= 0.33190 val_loss= 2.23389 val_acc= 0.34925 time= 0.28899
Epoch: 0027 train_loss= 2.18948 train_acc= 0.35804 val_loss= 2.19567 val_acc= 0.37015 time= 0.29006
Epoch: 0028 train_loss= 2.14819 train_acc= 0.37988 val_loss= 2.15788 val_acc= 0.38806 time= 0.29300
Epoch: 0029 train_loss= 2.10197 train_acc= 0.41926 val_loss= 2.12069 val_acc= 0.41791 time= 0.29231
Epoch: 0030 train_loss= 2.05631 train_acc= 0.44540 val_loss= 2.08416 val_acc= 0.46567 time= 0.28610
Epoch: 0031 train_loss= 1.99406 train_acc= 0.48345 val_loss= 2.04733 val_acc= 0.47761 time= 0.28509
Epoch: 0032 train_loss= 1.95715 train_acc= 0.50927 val_loss= 2.01063 val_acc= 0.48657 time= 0.28997
Epoch: 0033 train_loss= 1.91279 train_acc= 0.52217 val_loss= 1.97357 val_acc= 0.49552 time= 0.29403
Epoch: 0034 train_loss= 1.86433 train_acc= 0.53640 val_loss= 1.93617 val_acc= 0.50746 time= 0.29207
Epoch: 0035 train_loss= 1.81802 train_acc= 0.53971 val_loss= 1.89841 val_acc= 0.51343 time= 0.29000
Epoch: 0036 train_loss= 1.77617 train_acc= 0.54964 val_loss= 1.86131 val_acc= 0.51940 time= 0.28900
Epoch: 0037 train_loss= 1.71722 train_acc= 0.56717 val_loss= 1.82543 val_acc= 0.53134 time= 0.29900
Epoch: 0038 train_loss= 1.67721 train_acc= 0.56784 val_loss= 1.79040 val_acc= 0.52537 time= 0.28999
Epoch: 0039 train_loss= 1.63318 train_acc= 0.58273 val_loss= 1.75626 val_acc= 0.53731 time= 0.28599
Epoch: 0040 train_loss= 1.60128 train_acc= 0.59166 val_loss= 1.72339 val_acc= 0.54627 time= 0.28699
Epoch: 0041 train_loss= 1.55074 train_acc= 0.60556 val_loss= 1.69148 val_acc= 0.55522 time= 0.29297
Epoch: 0042 train_loss= 1.49702 train_acc= 0.61615 val_loss= 1.66223 val_acc= 0.55522 time= 0.29204
Epoch: 0043 train_loss= 1.45976 train_acc= 0.61482 val_loss= 1.63413 val_acc= 0.56119 time= 0.28800
Epoch: 0044 train_loss= 1.41013 train_acc= 0.63203 val_loss= 1.60787 val_acc= 0.56716 time= 0.28599
Epoch: 0045 train_loss= 1.38491 train_acc= 0.63766 val_loss= 1.58184 val_acc= 0.56716 time= 0.29397
Epoch: 0046 train_loss= 1.35576 train_acc= 0.64361 val_loss= 1.55613 val_acc= 0.56716 time= 0.29227
Epoch: 0047 train_loss= 1.31933 train_acc= 0.65685 val_loss= 1.53147 val_acc= 0.57313 time= 0.29900
Epoch: 0048 train_loss= 1.26652 train_acc= 0.67670 val_loss= 1.50896 val_acc= 0.57910 time= 0.29000
Epoch: 0049 train_loss= 1.24131 train_acc= 0.67406 val_loss= 1.48881 val_acc= 0.58209 time= 0.29209
Epoch: 0050 train_loss= 1.20315 train_acc= 0.68630 val_loss= 1.46974 val_acc= 0.58806 time= 0.29300
Epoch: 0051 train_loss= 1.18185 train_acc= 0.69623 val_loss= 1.45171 val_acc= 0.60000 time= 0.28803
Epoch: 0052 train_loss= 1.14845 train_acc= 0.70185 val_loss= 1.43296 val_acc= 0.60299 time= 0.29016
Epoch: 0053 train_loss= 1.12197 train_acc= 0.70814 val_loss= 1.41405 val_acc= 0.60299 time= 0.29200
Epoch: 0054 train_loss= 1.10285 train_acc= 0.70847 val_loss= 1.39515 val_acc= 0.61493 time= 0.29351
Epoch: 0055 train_loss= 1.05330 train_acc= 0.72601 val_loss= 1.37562 val_acc= 0.61791 time= 0.29000
Epoch: 0056 train_loss= 1.00813 train_acc= 0.73726 val_loss= 1.35832 val_acc= 0.62687 time= 0.28748
Epoch: 0057 train_loss= 1.02173 train_acc= 0.73858 val_loss= 1.34335 val_acc= 0.62388 time= 0.29497
Epoch: 0058 train_loss= 0.97304 train_acc= 0.74487 val_loss= 1.33036 val_acc= 0.61791 time= 0.29803
Epoch: 0059 train_loss= 0.95124 train_acc= 0.75513 val_loss= 1.32025 val_acc= 0.61493 time= 0.29300
Epoch: 0060 train_loss= 0.89968 train_acc= 0.77366 val_loss= 1.31183 val_acc= 0.62090 time= 0.29201
Epoch: 0061 train_loss= 0.90057 train_acc= 0.76109 val_loss= 1.30362 val_acc= 0.62388 time= 0.29499
Epoch: 0062 train_loss= 0.89156 train_acc= 0.76638 val_loss= 1.29431 val_acc= 0.62985 time= 0.29597
Epoch: 0063 train_loss= 0.84075 train_acc= 0.77995 val_loss= 1.28581 val_acc= 0.62985 time= 0.28903
Epoch: 0064 train_loss= 0.81597 train_acc= 0.78921 val_loss= 1.27599 val_acc= 0.63284 time= 0.28400
Epoch: 0065 train_loss= 0.78885 train_acc= 0.79914 val_loss= 1.26767 val_acc= 0.63582 time= 0.37497
Epoch: 0066 train_loss= 0.77521 train_acc= 0.79815 val_loss= 1.25892 val_acc= 0.64179 time= 0.29918
Epoch: 0067 train_loss= 0.76750 train_acc= 0.79252 val_loss= 1.25009 val_acc= 0.63284 time= 0.29304
Epoch: 0068 train_loss= 0.75246 train_acc= 0.79848 val_loss= 1.24197 val_acc= 0.62687 time= 0.28900
Epoch: 0069 train_loss= 0.70548 train_acc= 0.81767 val_loss= 1.23452 val_acc= 0.62388 time= 0.29895
Epoch: 0070 train_loss= 0.71701 train_acc= 0.81138 val_loss= 1.22809 val_acc= 0.63582 time= 0.29585
Epoch: 0071 train_loss= 0.67772 train_acc= 0.83091 val_loss= 1.22372 val_acc= 0.64478 time= 0.30277
Epoch: 0072 train_loss= 0.67137 train_acc= 0.82760 val_loss= 1.22087 val_acc= 0.65373 time= 0.29100
Epoch: 0073 train_loss= 0.66481 train_acc= 0.82561 val_loss= 1.21685 val_acc= 0.64478 time= 0.29596
Epoch: 0074 train_loss= 0.64172 train_acc= 0.84514 val_loss= 1.21074 val_acc= 0.64776 time= 0.29400
Epoch: 0075 train_loss= 0.62565 train_acc= 0.84216 val_loss= 1.20324 val_acc= 0.64478 time= 0.29804
Epoch: 0076 train_loss= 0.59776 train_acc= 0.85076 val_loss= 1.19411 val_acc= 0.63881 time= 0.29099
Epoch: 0077 train_loss= 0.58600 train_acc= 0.85043 val_loss= 1.18535 val_acc= 0.65075 time= 0.29601
Epoch: 0078 train_loss= 0.57751 train_acc= 0.85341 val_loss= 1.17944 val_acc= 0.65970 time= 0.28804
Epoch: 0079 train_loss= 0.56506 train_acc= 0.85440 val_loss= 1.17679 val_acc= 0.66866 time= 0.29847
Epoch: 0080 train_loss= 0.54757 train_acc= 0.85804 val_loss= 1.17351 val_acc= 0.65970 time= 0.29797
Epoch: 0081 train_loss= 0.53567 train_acc= 0.86929 val_loss= 1.17128 val_acc= 0.66269 time= 0.29203
Epoch: 0082 train_loss= 0.51123 train_acc= 0.87591 val_loss= 1.17223 val_acc= 0.65970 time= 0.29197
Epoch: 0083 train_loss= 0.52729 train_acc= 0.86896 val_loss= 1.17276 val_acc= 0.65075 time= 0.30303
Epoch: 0084 train_loss= 0.50813 train_acc= 0.87856 val_loss= 1.17118 val_acc= 0.65075 time= 0.28900
Epoch: 0085 train_loss= 0.48519 train_acc= 0.88518 val_loss= 1.16743 val_acc= 0.65970 time= 0.28701
Epoch: 0086 train_loss= 0.47932 train_acc= 0.88319 val_loss= 1.16229 val_acc= 0.65672 time= 0.29550
Epoch: 0087 train_loss= 0.45661 train_acc= 0.88518 val_loss= 1.15847 val_acc= 0.65672 time= 0.29704
Epoch: 0088 train_loss= 0.46080 train_acc= 0.89113 val_loss= 1.15462 val_acc= 0.66269 time= 0.28803
Epoch: 0089 train_loss= 0.44259 train_acc= 0.89444 val_loss= 1.15235 val_acc= 0.66866 time= 0.30035
Epoch: 0090 train_loss= 0.43492 train_acc= 0.89742 val_loss= 1.15280 val_acc= 0.66269 time= 0.32800
Epoch: 0091 train_loss= 0.42291 train_acc= 0.89179 val_loss= 1.15316 val_acc= 0.66866 time= 0.47300
Epoch: 0092 train_loss= 0.41439 train_acc= 0.90304 val_loss= 1.15136 val_acc= 0.66269 time= 0.40207
Epoch: 0093 train_loss= 0.41585 train_acc= 0.90404 val_loss= 1.15142 val_acc= 0.66866 time= 0.57123
Epoch: 0094 train_loss= 0.39905 train_acc= 0.90602 val_loss= 1.15193 val_acc= 0.67463 time= 0.37300
Epoch: 0095 train_loss= 0.38702 train_acc= 0.91463 val_loss= 1.15452 val_acc= 0.67164 time= 0.32500
Epoch: 0096 train_loss= 0.37429 train_acc= 0.90867 val_loss= 1.15925 val_acc= 0.67463 time= 0.32051
Early stopping...
Optimization Finished!
Test set results: cost= 1.16038 accuracy= 0.68464 time= 0.13100
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7080    0.7018    0.7048       342
           1     0.6870    0.7670    0.7248       103
           2     0.7154    0.6286    0.6692       140
           3     0.6429    0.3418    0.4463        79
           4     0.6579    0.7576    0.7042       132
           5     0.6787    0.7827    0.7270       313
           6     0.6981    0.7255    0.7115       102
           7     0.5833    0.2000    0.2979        70
           8     0.6400    0.3200    0.4267        50
           9     0.6243    0.7290    0.6726       155
          10     0.8092    0.6578    0.7257       187
          11     0.6329    0.6494    0.6410       231
          12     0.7826    0.7079    0.7434       178
          13     0.7747    0.7967    0.7855       600
          14     0.7560    0.8508    0.8006       590
          15     0.7639    0.7237    0.7432        76
          16     0.6875    0.3235    0.4400        34
          17     0.5000    0.1000    0.1667        10
          18     0.4251    0.5012    0.4600       419
          19     0.6667    0.4961    0.5689       129
          20     0.7200    0.6429    0.6792        28
          21     1.0000    0.7241    0.8400        29
          22     0.7222    0.2826    0.4062        46

    accuracy                         0.6846      4043
   macro avg     0.6903    0.5831    0.6124      4043
weighted avg     0.6887    0.6846    0.6790      4043

Macro average Test Precision, Recall and F1-Score...
(0.6902781097207635, 0.5830675115823363, 0.6124160065266469, None)
Micro average Test Precision, Recall and F1-Score...
(0.68464011872372, 0.68464011872372, 0.68464011872372, None)
embeddings:
14157 3357 4043
[[ 0.340852    0.26325288  0.29453623 ...  0.2644239   0.33912173
   0.3713562 ]
 [ 0.14975923  0.06258313 -0.02981365 ... -0.03084571  0.38440222
   0.05633508]
 [ 0.22528054  0.15206468  0.17092974 ...  0.06914066  0.24004947
   0.27067292]
 ...
 [ 0.12036544  0.03492141  0.15103851 ...  0.1214635   0.16308223
   0.23078908]
 [ 0.18957004  0.2530133  -0.07076631 ...  0.032215    0.30194402
  -0.0777873 ]
 [ 0.23645407  0.11479773  0.12957306 ...  0.18390054  0.20618998
   0.10886668]]

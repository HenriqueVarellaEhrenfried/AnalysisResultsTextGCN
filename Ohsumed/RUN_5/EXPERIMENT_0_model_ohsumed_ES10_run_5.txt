(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.04831 val_loss= 3.11333 val_acc= 0.20597 time= 0.58536
Epoch: 0002 train_loss= 3.11374 train_acc= 0.18167 val_loss= 3.06532 val_acc= 0.20597 time= 0.29000
Epoch: 0003 train_loss= 3.06648 train_acc= 0.17803 val_loss= 2.99190 val_acc= 0.20597 time= 0.29000
Epoch: 0004 train_loss= 2.99432 train_acc= 0.17670 val_loss= 2.90055 val_acc= 0.20597 time= 0.29700
Epoch: 0005 train_loss= 2.90335 train_acc= 0.17472 val_loss= 2.80865 val_acc= 0.20597 time= 0.29601
Epoch: 0006 train_loss= 2.81420 train_acc= 0.17373 val_loss= 2.73700 val_acc= 0.20597 time= 0.29200
Epoch: 0007 train_loss= 2.74554 train_acc= 0.17406 val_loss= 2.69924 val_acc= 0.20597 time= 0.29000
Epoch: 0008 train_loss= 2.70974 train_acc= 0.17538 val_loss= 2.69144 val_acc= 0.20597 time= 0.28700
Epoch: 0009 train_loss= 2.70223 train_acc= 0.17373 val_loss= 2.69089 val_acc= 0.20597 time= 0.29600
Epoch: 0010 train_loss= 2.69994 train_acc= 0.17406 val_loss= 2.67793 val_acc= 0.20597 time= 0.28800
Epoch: 0011 train_loss= 2.68464 train_acc= 0.17340 val_loss= 2.65116 val_acc= 0.20896 time= 0.28800
Epoch: 0012 train_loss= 2.64781 train_acc= 0.18266 val_loss= 2.62069 val_acc= 0.22985 time= 0.28800
Epoch: 0013 train_loss= 2.60915 train_acc= 0.19689 val_loss= 2.59325 val_acc= 0.24776 time= 0.29100
Epoch: 0014 train_loss= 2.57362 train_acc= 0.21277 val_loss= 2.56920 val_acc= 0.26269 time= 0.28800
Epoch: 0015 train_loss= 2.54306 train_acc= 0.24123 val_loss= 2.54545 val_acc= 0.28060 time= 0.28700
Epoch: 0016 train_loss= 2.51388 train_acc= 0.26075 val_loss= 2.51893 val_acc= 0.29552 time= 0.29000
Epoch: 0017 train_loss= 2.48185 train_acc= 0.27796 val_loss= 2.48792 val_acc= 0.30448 time= 0.29300
Epoch: 0018 train_loss= 2.44942 train_acc= 0.29517 val_loss= 2.45225 val_acc= 0.31045 time= 0.28600
Epoch: 0019 train_loss= 2.41060 train_acc= 0.30708 val_loss= 2.41296 val_acc= 0.31940 time= 0.28800
Epoch: 0020 train_loss= 2.36773 train_acc= 0.31602 val_loss= 2.37144 val_acc= 0.31940 time= 0.28800
Epoch: 0021 train_loss= 2.32122 train_acc= 0.32694 val_loss= 2.32890 val_acc= 0.32537 time= 0.29300
Epoch: 0022 train_loss= 2.27285 train_acc= 0.33388 val_loss= 2.28612 val_acc= 0.32836 time= 0.28500
Epoch: 0023 train_loss= 2.22587 train_acc= 0.33752 val_loss= 2.24358 val_acc= 0.33134 time= 0.29100
Epoch: 0024 train_loss= 2.17764 train_acc= 0.34944 val_loss= 2.20135 val_acc= 0.33731 time= 0.29700
Epoch: 0025 train_loss= 2.12788 train_acc= 0.37028 val_loss= 2.15942 val_acc= 0.35224 time= 0.29300
Epoch: 0026 train_loss= 2.07303 train_acc= 0.40040 val_loss= 2.11769 val_acc= 0.37910 time= 0.28900
Epoch: 0027 train_loss= 2.01925 train_acc= 0.44044 val_loss= 2.07629 val_acc= 0.41493 time= 0.29300
Epoch: 0028 train_loss= 1.96355 train_acc= 0.47816 val_loss= 2.03540 val_acc= 0.45373 time= 0.29100
Epoch: 0029 train_loss= 1.91077 train_acc= 0.51919 val_loss= 1.99457 val_acc= 0.47761 time= 0.29300
Epoch: 0030 train_loss= 1.85535 train_acc= 0.54765 val_loss= 1.95327 val_acc= 0.49552 time= 0.28700
Epoch: 0031 train_loss= 1.79466 train_acc= 0.56784 val_loss= 1.91121 val_acc= 0.51940 time= 0.29000
Epoch: 0032 train_loss= 1.74831 train_acc= 0.58240 val_loss= 1.86854 val_acc= 0.52537 time= 0.29484
Epoch: 0033 train_loss= 1.68619 train_acc= 0.59993 val_loss= 1.82580 val_acc= 0.52836 time= 0.28800
Epoch: 0034 train_loss= 1.63273 train_acc= 0.60887 val_loss= 1.78427 val_acc= 0.53433 time= 0.28800
Epoch: 0035 train_loss= 1.57475 train_acc= 0.61846 val_loss= 1.74470 val_acc= 0.54030 time= 0.28800
Epoch: 0036 train_loss= 1.52141 train_acc= 0.63369 val_loss= 1.70756 val_acc= 0.54627 time= 0.29460
Epoch: 0037 train_loss= 1.47163 train_acc= 0.64097 val_loss= 1.67186 val_acc= 0.54627 time= 0.29100
Epoch: 0038 train_loss= 1.41767 train_acc= 0.65420 val_loss= 1.63736 val_acc= 0.54925 time= 0.28900
Epoch: 0039 train_loss= 1.36592 train_acc= 0.66545 val_loss= 1.60483 val_acc= 0.56716 time= 0.29000
Epoch: 0040 train_loss= 1.31707 train_acc= 0.67836 val_loss= 1.57440 val_acc= 0.57313 time= 0.29500
Epoch: 0041 train_loss= 1.26203 train_acc= 0.68597 val_loss= 1.54573 val_acc= 0.56716 time= 0.29000
Epoch: 0042 train_loss= 1.21826 train_acc= 0.69854 val_loss= 1.51805 val_acc= 0.57313 time= 0.28925
Epoch: 0043 train_loss= 1.17155 train_acc= 0.71013 val_loss= 1.49148 val_acc= 0.58209 time= 0.28700
Epoch: 0044 train_loss= 1.12207 train_acc= 0.72833 val_loss= 1.46596 val_acc= 0.57910 time= 0.29199
Epoch: 0045 train_loss= 1.08159 train_acc= 0.73925 val_loss= 1.44210 val_acc= 0.59701 time= 0.29201
Epoch: 0046 train_loss= 1.03665 train_acc= 0.75050 val_loss= 1.41819 val_acc= 0.60299 time= 0.29029
Epoch: 0047 train_loss= 0.98912 train_acc= 0.76175 val_loss= 1.39501 val_acc= 0.60000 time= 0.29000
Epoch: 0048 train_loss= 0.95058 train_acc= 0.77399 val_loss= 1.37325 val_acc= 0.61493 time= 0.29500
Epoch: 0049 train_loss= 0.90755 train_acc= 0.78657 val_loss= 1.35372 val_acc= 0.62090 time= 0.28900
Epoch: 0050 train_loss= 0.87264 train_acc= 0.78954 val_loss= 1.33639 val_acc= 0.61791 time= 0.29100
Epoch: 0051 train_loss= 0.83736 train_acc= 0.80708 val_loss= 1.32073 val_acc= 0.62388 time= 0.29397
Epoch: 0052 train_loss= 0.80262 train_acc= 0.81635 val_loss= 1.30627 val_acc= 0.62388 time= 0.29503
Epoch: 0053 train_loss= 0.76292 train_acc= 0.81999 val_loss= 1.29226 val_acc= 0.63284 time= 0.28800
Epoch: 0054 train_loss= 0.73551 train_acc= 0.82760 val_loss= 1.27903 val_acc= 0.63582 time= 0.28900
Epoch: 0055 train_loss= 0.69905 train_acc= 0.83587 val_loss= 1.26699 val_acc= 0.62985 time= 0.28902
Epoch: 0056 train_loss= 0.66403 train_acc= 0.84712 val_loss= 1.25628 val_acc= 0.63881 time= 0.29401
Epoch: 0057 train_loss= 0.63818 train_acc= 0.85473 val_loss= 1.24548 val_acc= 0.65672 time= 0.29042
Epoch: 0058 train_loss= 0.60741 train_acc= 0.86201 val_loss= 1.23589 val_acc= 0.65672 time= 0.28900
Epoch: 0059 train_loss= 0.58335 train_acc= 0.86929 val_loss= 1.22641 val_acc= 0.65373 time= 0.29276
Epoch: 0060 train_loss= 0.55412 train_acc= 0.87690 val_loss= 1.21810 val_acc= 0.64776 time= 0.29420
Epoch: 0061 train_loss= 0.53075 train_acc= 0.88154 val_loss= 1.21100 val_acc= 0.64776 time= 0.29200
Epoch: 0062 train_loss= 0.50637 train_acc= 0.88981 val_loss= 1.20452 val_acc= 0.65672 time= 0.29000
Epoch: 0063 train_loss= 0.48493 train_acc= 0.89411 val_loss= 1.19940 val_acc= 0.65373 time= 0.29500
Epoch: 0064 train_loss= 0.46124 train_acc= 0.89974 val_loss= 1.19533 val_acc= 0.64776 time= 0.29099
Epoch: 0065 train_loss= 0.44027 train_acc= 0.90371 val_loss= 1.19003 val_acc= 0.64776 time= 0.29254
Epoch: 0066 train_loss= 0.42099 train_acc= 0.91330 val_loss= 1.18376 val_acc= 0.64776 time= 0.29000
Epoch: 0067 train_loss= 0.40157 train_acc= 0.91661 val_loss= 1.17932 val_acc= 0.66567 time= 0.29500
Epoch: 0068 train_loss= 0.38484 train_acc= 0.92356 val_loss= 1.17605 val_acc= 0.66269 time= 0.28797
Epoch: 0069 train_loss= 0.36410 train_acc= 0.92753 val_loss= 1.17445 val_acc= 0.64776 time= 0.28803
Epoch: 0070 train_loss= 0.34785 train_acc= 0.93117 val_loss= 1.17456 val_acc= 0.64776 time= 0.28800
Epoch: 0071 train_loss= 0.33866 train_acc= 0.93316 val_loss= 1.17310 val_acc= 0.65672 time= 0.29100
Epoch: 0072 train_loss= 0.32096 train_acc= 0.93812 val_loss= 1.17191 val_acc= 0.65672 time= 0.29197
Epoch: 0073 train_loss= 0.30875 train_acc= 0.93878 val_loss= 1.17160 val_acc= 0.66567 time= 0.28903
Epoch: 0074 train_loss= 0.29298 train_acc= 0.94606 val_loss= 1.17051 val_acc= 0.67463 time= 0.29000
Epoch: 0075 train_loss= 0.27842 train_acc= 0.95136 val_loss= 1.16864 val_acc= 0.67164 time= 0.29270
Epoch: 0076 train_loss= 0.26445 train_acc= 0.95566 val_loss= 1.16755 val_acc= 0.66567 time= 0.28800
Epoch: 0077 train_loss= 0.25570 train_acc= 0.95036 val_loss= 1.16622 val_acc= 0.66866 time= 0.28643
Epoch: 0078 train_loss= 0.24282 train_acc= 0.95864 val_loss= 1.16694 val_acc= 0.66269 time= 0.28592
Epoch: 0079 train_loss= 0.23680 train_acc= 0.96095 val_loss= 1.16974 val_acc= 0.67164 time= 0.29572
Epoch: 0080 train_loss= 0.22445 train_acc= 0.96492 val_loss= 1.17322 val_acc= 0.67164 time= 0.29001
Early stopping...
Optimization Finished!
Test set results: cost= 1.15715 accuracy= 0.69181 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7201    0.7222    0.7212       342
           1     0.6937    0.7476    0.7196       103
           2     0.7297    0.5786    0.6454       140
           3     0.6182    0.4304    0.5075        79
           4     0.6513    0.7500    0.6972       132
           5     0.6693    0.8019    0.7297       313
           6     0.6696    0.7353    0.7009       102
           7     0.6389    0.3286    0.4340        70
           8     0.6061    0.4000    0.4819        50
           9     0.6094    0.7548    0.6744       155
          10     0.8389    0.6684    0.7440       187
          11     0.6383    0.6494    0.6438       231
          12     0.7500    0.7247    0.7371       178
          13     0.7651    0.8250    0.7939       600
          14     0.7834    0.8458    0.8134       590
          15     0.7467    0.7368    0.7417        76
          16     0.7333    0.3235    0.4490        34
          17     0.5000    0.1000    0.1667        10
          18     0.4523    0.4415    0.4469       419
          19     0.6321    0.5194    0.5702       129
          20     0.6000    0.6429    0.6207        28
          21     0.9545    0.7241    0.8235        29
          22     0.6667    0.3478    0.4571        46

    accuracy                         0.6918      4043
   macro avg     0.6812    0.5999    0.6226      4043
weighted avg     0.6911    0.6918    0.6861      4043

Macro average Test Precision, Recall and F1-Score...
(0.6812000766146379, 0.5999456100566917, 0.6225968164366866, None)
Micro average Test Precision, Recall and F1-Score...
(0.6918130101409844, 0.6918130101409844, 0.6918130101409844, None)
embeddings:
14157 3357 4043
[[ 0.3557811   0.45876008  0.33647946 ...  0.674498    0.36750022
   0.4001414 ]
 [ 0.1182846   0.2631483   0.04028831 ...  0.48222736  0.09780426
   0.02727936]
 [ 0.16172667  0.1777054   0.12396697 ...  0.57118237  0.31986654
   0.40573478]
 ...
 [ 0.11670236  0.1607908   0.13760588 ...  0.18730435  0.13847241
   0.10908388]
 [ 0.10636283  0.37737665 -0.03430459 ...  0.54857767  0.04011467
   0.01972962]
 [ 0.05659909  0.08807429  0.1345728  ...  0.30926642  0.12328867
   0.26149085]]

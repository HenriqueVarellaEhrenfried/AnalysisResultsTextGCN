(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.02647 val_loss= 3.11357 val_acc= 0.20597 time= 0.58763
Epoch: 0002 train_loss= 3.11323 train_acc= 0.17538 val_loss= 3.06524 val_acc= 0.20299 time= 0.29500
Epoch: 0003 train_loss= 3.06458 train_acc= 0.17340 val_loss= 2.99189 val_acc= 0.20000 time= 0.28700
Epoch: 0004 train_loss= 2.99078 train_acc= 0.17240 val_loss= 2.90119 val_acc= 0.20000 time= 0.28806
Epoch: 0005 train_loss= 2.90127 train_acc= 0.17207 val_loss= 2.81069 val_acc= 0.20000 time= 0.29398
Epoch: 0006 train_loss= 2.81298 train_acc= 0.17273 val_loss= 2.73858 val_acc= 0.20299 time= 0.29203
Epoch: 0007 train_loss= 2.74439 train_acc= 0.17273 val_loss= 2.69962 val_acc= 0.20597 time= 0.28798
Epoch: 0008 train_loss= 2.70849 train_acc= 0.17406 val_loss= 2.69227 val_acc= 0.20597 time= 0.29400
Epoch: 0009 train_loss= 2.70315 train_acc= 0.17439 val_loss= 2.69134 val_acc= 0.20299 time= 0.29500
Epoch: 0010 train_loss= 2.70216 train_acc= 0.17273 val_loss= 2.67620 val_acc= 0.20597 time= 0.29300
Epoch: 0011 train_loss= 2.68140 train_acc= 0.17406 val_loss= 2.64740 val_acc= 0.20896 time= 0.29001
Epoch: 0012 train_loss= 2.64333 train_acc= 0.17704 val_loss= 2.61621 val_acc= 0.22388 time= 0.28900
Epoch: 0013 train_loss= 2.60088 train_acc= 0.18862 val_loss= 2.58924 val_acc= 0.23881 time= 0.29399
Epoch: 0014 train_loss= 2.56390 train_acc= 0.20847 val_loss= 2.56614 val_acc= 0.25970 time= 0.29403
Epoch: 0015 train_loss= 2.53237 train_acc= 0.23395 val_loss= 2.54293 val_acc= 0.28060 time= 0.29100
Epoch: 0016 train_loss= 2.50222 train_acc= 0.26042 val_loss= 2.51583 val_acc= 0.30149 time= 0.28700
Epoch: 0017 train_loss= 2.46989 train_acc= 0.28326 val_loss= 2.48291 val_acc= 0.31045 time= 0.29697
Epoch: 0018 train_loss= 2.43364 train_acc= 0.30410 val_loss= 2.44414 val_acc= 0.31343 time= 0.29303
Epoch: 0019 train_loss= 2.39072 train_acc= 0.32363 val_loss= 2.40092 val_acc= 0.31940 time= 0.28900
Epoch: 0020 train_loss= 2.34413 train_acc= 0.33620 val_loss= 2.35533 val_acc= 0.32239 time= 0.28800
Epoch: 0021 train_loss= 2.29606 train_acc= 0.34282 val_loss= 2.30943 val_acc= 0.32537 time= 0.29297
Epoch: 0022 train_loss= 2.24525 train_acc= 0.34580 val_loss= 2.26437 val_acc= 0.34030 time= 0.29104
Epoch: 0023 train_loss= 2.19452 train_acc= 0.35275 val_loss= 2.22025 val_acc= 0.34627 time= 0.28999
Epoch: 0024 train_loss= 2.14243 train_acc= 0.36896 val_loss= 2.17662 val_acc= 0.35821 time= 0.28604
Epoch: 0025 train_loss= 2.08926 train_acc= 0.38948 val_loss= 2.13323 val_acc= 0.38209 time= 0.29878
Epoch: 0026 train_loss= 2.03266 train_acc= 0.42488 val_loss= 2.09027 val_acc= 0.40597 time= 0.29096
Epoch: 0027 train_loss= 1.97543 train_acc= 0.47121 val_loss= 2.04806 val_acc= 0.44179 time= 0.28803
Epoch: 0028 train_loss= 1.91766 train_acc= 0.51158 val_loss= 2.00634 val_acc= 0.47164 time= 0.28601
Epoch: 0029 train_loss= 1.86106 train_acc= 0.54831 val_loss= 1.96429 val_acc= 0.49552 time= 0.29400
Epoch: 0030 train_loss= 1.80425 train_acc= 0.57478 val_loss= 1.92115 val_acc= 0.51642 time= 0.29000
Epoch: 0031 train_loss= 1.74700 train_acc= 0.59067 val_loss= 1.87687 val_acc= 0.52836 time= 0.28800
Epoch: 0032 train_loss= 1.68474 train_acc= 0.60291 val_loss= 1.83221 val_acc= 0.52537 time= 0.29197
Epoch: 0033 train_loss= 1.62572 train_acc= 0.61019 val_loss= 1.78845 val_acc= 0.53134 time= 0.29700
Epoch: 0034 train_loss= 1.56809 train_acc= 0.61681 val_loss= 1.74677 val_acc= 0.53134 time= 0.29100
Epoch: 0035 train_loss= 1.51039 train_acc= 0.63402 val_loss= 1.70745 val_acc= 0.53731 time= 0.28900
Epoch: 0036 train_loss= 1.45560 train_acc= 0.64692 val_loss= 1.67013 val_acc= 0.54925 time= 0.29300
Epoch: 0037 train_loss= 1.40195 train_acc= 0.66281 val_loss= 1.63451 val_acc= 0.56418 time= 0.29603
Epoch: 0038 train_loss= 1.34627 train_acc= 0.67704 val_loss= 1.60037 val_acc= 0.57015 time= 0.29205
Epoch: 0039 train_loss= 1.29161 train_acc= 0.68531 val_loss= 1.56797 val_acc= 0.58209 time= 0.29400
Epoch: 0040 train_loss= 1.23926 train_acc= 0.69755 val_loss= 1.53720 val_acc= 0.58507 time= 0.28800
Epoch: 0041 train_loss= 1.18798 train_acc= 0.71013 val_loss= 1.50750 val_acc= 0.58806 time= 0.29597
Epoch: 0042 train_loss= 1.13896 train_acc= 0.72038 val_loss= 1.47887 val_acc= 0.58806 time= 0.29140
Epoch: 0043 train_loss= 1.09157 train_acc= 0.73792 val_loss= 1.45153 val_acc= 0.59104 time= 0.29100
Epoch: 0044 train_loss= 1.04472 train_acc= 0.75678 val_loss= 1.42602 val_acc= 0.59403 time= 0.29000
Epoch: 0045 train_loss= 0.99924 train_acc= 0.76439 val_loss= 1.40210 val_acc= 0.60299 time= 0.30240
Epoch: 0046 train_loss= 0.95570 train_acc= 0.77730 val_loss= 1.37997 val_acc= 0.60299 time= 0.29111
Epoch: 0047 train_loss= 0.91154 train_acc= 0.79285 val_loss= 1.35975 val_acc= 0.60896 time= 0.28999
Epoch: 0048 train_loss= 0.86907 train_acc= 0.80344 val_loss= 1.34103 val_acc= 0.62388 time= 0.28800
Epoch: 0049 train_loss= 0.83072 train_acc= 0.81138 val_loss= 1.32346 val_acc= 0.62090 time= 0.29347
Epoch: 0050 train_loss= 0.79233 train_acc= 0.81833 val_loss= 1.30685 val_acc= 0.62388 time= 0.29201
Epoch: 0051 train_loss= 0.75476 train_acc= 0.82627 val_loss= 1.29134 val_acc= 0.62388 time= 0.28771
Epoch: 0052 train_loss= 0.71923 train_acc= 0.83289 val_loss= 1.27685 val_acc= 0.62687 time= 0.28800
Epoch: 0053 train_loss= 0.68225 train_acc= 0.84547 val_loss= 1.26325 val_acc= 0.62687 time= 0.29500
Epoch: 0054 train_loss= 0.65053 train_acc= 0.85407 val_loss= 1.25111 val_acc= 0.62687 time= 0.28800
Epoch: 0055 train_loss= 0.61891 train_acc= 0.86234 val_loss= 1.24060 val_acc= 0.62985 time= 0.28900
Epoch: 0056 train_loss= 0.58764 train_acc= 0.87260 val_loss= 1.23135 val_acc= 0.62985 time= 0.29400
Epoch: 0057 train_loss= 0.55789 train_acc= 0.87955 val_loss= 1.22330 val_acc= 0.63881 time= 0.29600
Epoch: 0058 train_loss= 0.53089 train_acc= 0.88319 val_loss= 1.21610 val_acc= 0.63881 time= 0.28900
Epoch: 0059 train_loss= 0.50195 train_acc= 0.89279 val_loss= 1.20932 val_acc= 0.63284 time= 0.28700
Epoch: 0060 train_loss= 0.47952 train_acc= 0.89874 val_loss= 1.20240 val_acc= 0.63284 time= 0.29102
Epoch: 0061 train_loss= 0.45582 train_acc= 0.90338 val_loss= 1.19462 val_acc= 0.62985 time= 0.29321
Epoch: 0062 train_loss= 0.43053 train_acc= 0.90834 val_loss= 1.18739 val_acc= 0.63881 time= 0.28900
Epoch: 0063 train_loss= 0.40898 train_acc= 0.91760 val_loss= 1.18145 val_acc= 0.64478 time= 0.29112
Epoch: 0064 train_loss= 0.38835 train_acc= 0.92224 val_loss= 1.17720 val_acc= 0.64776 time= 0.29000
Epoch: 0065 train_loss= 0.36766 train_acc= 0.92919 val_loss= 1.17539 val_acc= 0.63881 time= 0.29400
Epoch: 0066 train_loss= 0.34870 train_acc= 0.93514 val_loss= 1.17488 val_acc= 0.64478 time= 0.28900
Epoch: 0067 train_loss= 0.33193 train_acc= 0.93944 val_loss= 1.17458 val_acc= 0.65075 time= 0.29303
Epoch: 0068 train_loss= 0.31602 train_acc= 0.94308 val_loss= 1.17457 val_acc= 0.65970 time= 0.29107
Epoch: 0069 train_loss= 0.29872 train_acc= 0.94672 val_loss= 1.17345 val_acc= 0.66567 time= 0.29570
Epoch: 0070 train_loss= 0.28386 train_acc= 0.95069 val_loss= 1.17185 val_acc= 0.65970 time= 0.29103
Epoch: 0071 train_loss= 0.26866 train_acc= 0.95533 val_loss= 1.16992 val_acc= 0.65970 time= 0.29006
Epoch: 0072 train_loss= 0.25654 train_acc= 0.95698 val_loss= 1.16752 val_acc= 0.66269 time= 0.28900
Epoch: 0073 train_loss= 0.24212 train_acc= 0.95996 val_loss= 1.16802 val_acc= 0.67164 time= 0.29439
Epoch: 0074 train_loss= 0.22926 train_acc= 0.96559 val_loss= 1.17070 val_acc= 0.67463 time= 0.29111
Epoch: 0075 train_loss= 0.21750 train_acc= 0.96691 val_loss= 1.17535 val_acc= 0.66567 time= 0.29100
Early stopping...
Optimization Finished!
Test set results: cost= 1.15713 accuracy= 0.68786 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7211    0.7105    0.7158       342
           1     0.6842    0.7573    0.7189       103
           2     0.7213    0.6286    0.6718       140
           3     0.6471    0.4177    0.5077        79
           4     0.6600    0.7500    0.7021       132
           5     0.7011    0.8019    0.7481       313
           6     0.6667    0.7059    0.6857       102
           7     0.5610    0.3286    0.4144        70
           8     0.5556    0.4000    0.4651        50
           9     0.6011    0.7290    0.6589       155
          10     0.8462    0.6471    0.7333       187
          11     0.6203    0.6364    0.6282       231
          12     0.7937    0.7135    0.7515       178
          13     0.7658    0.8067    0.7857       600
          14     0.7852    0.8424    0.8128       590
          15     0.7647    0.6842    0.7222        76
          16     0.6842    0.3824    0.4906        34
          17     0.5000    0.2000    0.2857        10
          18     0.4264    0.4702    0.4472       419
          19     0.6381    0.5194    0.5726       129
          20     0.6429    0.6429    0.6429        28
          21     1.0000    0.7241    0.8400        29
          22     0.5769    0.3261    0.4167        46

    accuracy                         0.6879      4043
   macro avg     0.6767    0.6011    0.6269      4043
weighted avg     0.6902    0.6879    0.6847      4043

Macro average Test Precision, Recall and F1-Score...
(0.6766676835728476, 0.6010713630660799, 0.6268645625989152, None)
Micro average Test Precision, Recall and F1-Score...
(0.6878555528073212, 0.6878555528073212, 0.6878555528073212, None)
embeddings:
14157 3357 4043
[[ 0.5786562   0.40326262  0.5397976  ...  0.352855    0.59433246
  -0.1064989 ]
 [-0.01054991  0.01425424  0.29457974 ...  0.17303264  0.20057438
  -0.02997575]
 [ 0.14150363  0.1143045   0.43071032 ...  0.43543428  0.46932802
  -0.06467598]
 ...
 [ 0.24227221  0.15188242  0.26674616 ...  0.13860232  0.29169288
  -0.03170275]
 [-0.01143602 -0.04138606  0.3484497  ...  0.32453373  0.00566539
  -0.0336259 ]
 [ 0.38092056  0.24082273  0.12506148 ...  0.11457302  0.28680345
  -0.02458292]]

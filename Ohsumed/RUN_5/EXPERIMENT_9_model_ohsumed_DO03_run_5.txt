(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13558 train_acc= 0.00927 val_loss= 3.11765 val_acc= 0.24179 time= 0.64160
Epoch: 0002 train_loss= 3.11761 train_acc= 0.21343 val_loss= 3.07551 val_acc= 0.22985 time= 0.31002
Epoch: 0003 train_loss= 3.07577 train_acc= 0.20615 val_loss= 3.00864 val_acc= 0.22985 time= 0.29503
Epoch: 0004 train_loss= 3.00982 train_acc= 0.19325 val_loss= 2.92226 val_acc= 0.20896 time= 0.29097
Epoch: 0005 train_loss= 2.92463 train_acc= 0.18663 val_loss= 2.82985 val_acc= 0.20896 time= 0.29503
Epoch: 0006 train_loss= 2.83478 train_acc= 0.18332 val_loss= 2.75096 val_acc= 0.20896 time= 0.28900
Epoch: 0007 train_loss= 2.75907 train_acc= 0.18365 val_loss= 2.70236 val_acc= 0.20896 time= 0.29001
Epoch: 0008 train_loss= 2.71263 train_acc= 0.18332 val_loss= 2.68711 val_acc= 0.20896 time= 0.28796
Epoch: 0009 train_loss= 2.70007 train_acc= 0.18167 val_loss= 2.68650 val_acc= 0.20896 time= 0.29503
Epoch: 0010 train_loss= 2.69955 train_acc= 0.17670 val_loss= 2.67831 val_acc= 0.20896 time= 0.29100
Epoch: 0011 train_loss= 2.68521 train_acc= 0.17770 val_loss= 2.65548 val_acc= 0.20896 time= 0.29197
Epoch: 0012 train_loss= 2.65598 train_acc= 0.18167 val_loss= 2.62564 val_acc= 0.22388 time= 0.29513
Epoch: 0013 train_loss= 2.61516 train_acc= 0.18994 val_loss= 2.59742 val_acc= 0.23881 time= 0.29177
Epoch: 0014 train_loss= 2.57606 train_acc= 0.20748 val_loss= 2.57296 val_acc= 0.25970 time= 0.29100
Epoch: 0015 train_loss= 2.54337 train_acc= 0.22799 val_loss= 2.54916 val_acc= 0.27164 time= 0.28700
Epoch: 0016 train_loss= 2.51038 train_acc= 0.25347 val_loss= 2.52234 val_acc= 0.28955 time= 0.29466
Epoch: 0017 train_loss= 2.47952 train_acc= 0.27730 val_loss= 2.49066 val_acc= 0.30149 time= 0.29405
Epoch: 0018 train_loss= 2.44332 train_acc= 0.30311 val_loss= 2.45424 val_acc= 0.31045 time= 0.28940
Epoch: 0019 train_loss= 2.40296 train_acc= 0.32330 val_loss= 2.41423 val_acc= 0.31940 time= 0.29200
Epoch: 0020 train_loss= 2.36103 train_acc= 0.33587 val_loss= 2.37207 val_acc= 0.32537 time= 0.29294
Epoch: 0021 train_loss= 2.31437 train_acc= 0.34249 val_loss= 2.32876 val_acc= 0.32836 time= 0.28999
Epoch: 0022 train_loss= 2.26497 train_acc= 0.35407 val_loss= 2.28466 val_acc= 0.33134 time= 0.28900
Epoch: 0023 train_loss= 2.21677 train_acc= 0.36267 val_loss= 2.23997 val_acc= 0.34328 time= 0.28700
Epoch: 0024 train_loss= 2.16352 train_acc= 0.38187 val_loss= 2.19510 val_acc= 0.36119 time= 0.29583
Epoch: 0025 train_loss= 2.10941 train_acc= 0.40371 val_loss= 2.15055 val_acc= 0.37612 time= 0.29703
Epoch: 0026 train_loss= 2.05199 train_acc= 0.43349 val_loss= 2.10689 val_acc= 0.41493 time= 0.29402
Epoch: 0027 train_loss= 1.99527 train_acc= 0.47750 val_loss= 2.06437 val_acc= 0.45075 time= 0.29299
Epoch: 0028 train_loss= 1.94095 train_acc= 0.51026 val_loss= 2.02240 val_acc= 0.47463 time= 0.29698
Epoch: 0029 train_loss= 1.87928 train_acc= 0.54567 val_loss= 1.98002 val_acc= 0.49254 time= 0.29000
Epoch: 0030 train_loss= 1.82908 train_acc= 0.57081 val_loss= 1.93605 val_acc= 0.51642 time= 0.28597
Epoch: 0031 train_loss= 1.76755 train_acc= 0.59596 val_loss= 1.89043 val_acc= 0.52239 time= 0.29003
Epoch: 0032 train_loss= 1.70524 train_acc= 0.60258 val_loss= 1.84475 val_acc= 0.52537 time= 0.29697
Epoch: 0033 train_loss= 1.64607 train_acc= 0.61615 val_loss= 1.80034 val_acc= 0.53433 time= 0.29003
Epoch: 0034 train_loss= 1.58798 train_acc= 0.62277 val_loss= 1.75779 val_acc= 0.53134 time= 0.29200
Epoch: 0035 train_loss= 1.52925 train_acc= 0.63501 val_loss= 1.71731 val_acc= 0.53134 time= 0.28900
Epoch: 0036 train_loss= 1.47319 train_acc= 0.64626 val_loss= 1.67910 val_acc= 0.53731 time= 0.29400
Epoch: 0037 train_loss= 1.41619 train_acc= 0.65850 val_loss= 1.64240 val_acc= 0.55224 time= 0.28900
Epoch: 0038 train_loss= 1.36519 train_acc= 0.66976 val_loss= 1.60733 val_acc= 0.56119 time= 0.28900
Epoch: 0039 train_loss= 1.30945 train_acc= 0.68663 val_loss= 1.57428 val_acc= 0.57612 time= 0.28999
Epoch: 0040 train_loss= 1.25704 train_acc= 0.69623 val_loss= 1.54293 val_acc= 0.57313 time= 0.29200
Epoch: 0041 train_loss= 1.20626 train_acc= 0.70715 val_loss= 1.51239 val_acc= 0.57612 time= 0.28900
Epoch: 0042 train_loss= 1.15845 train_acc= 0.70913 val_loss= 1.48319 val_acc= 0.58209 time= 0.28900
Epoch: 0043 train_loss= 1.11112 train_acc= 0.72336 val_loss= 1.45567 val_acc= 0.59104 time= 0.28900
Epoch: 0044 train_loss= 1.05809 train_acc= 0.74189 val_loss= 1.43038 val_acc= 0.60000 time= 0.29451
Epoch: 0045 train_loss= 1.01760 train_acc= 0.75447 val_loss= 1.40690 val_acc= 0.60000 time= 0.28900
Epoch: 0046 train_loss= 0.97326 train_acc= 0.76439 val_loss= 1.38474 val_acc= 0.60597 time= 0.28700
Epoch: 0047 train_loss= 0.93108 train_acc= 0.77664 val_loss= 1.36419 val_acc= 0.60299 time= 0.29000
Epoch: 0048 train_loss= 0.88764 train_acc= 0.78888 val_loss= 1.34479 val_acc= 0.61194 time= 0.29500
Epoch: 0049 train_loss= 0.84703 train_acc= 0.80113 val_loss= 1.32649 val_acc= 0.61194 time= 0.28700
Epoch: 0050 train_loss= 0.81111 train_acc= 0.81204 val_loss= 1.30901 val_acc= 0.61791 time= 0.29200
Epoch: 0051 train_loss= 0.77550 train_acc= 0.81668 val_loss= 1.29268 val_acc= 0.62388 time= 0.28800
Epoch: 0052 train_loss= 0.73170 train_acc= 0.82859 val_loss= 1.27778 val_acc= 0.63881 time= 0.29700
Epoch: 0053 train_loss= 0.70193 train_acc= 0.83852 val_loss= 1.26384 val_acc= 0.65075 time= 0.29200
Epoch: 0054 train_loss= 0.66694 train_acc= 0.84778 val_loss= 1.25112 val_acc= 0.64179 time= 0.28802
Epoch: 0055 train_loss= 0.63641 train_acc= 0.85606 val_loss= 1.23995 val_acc= 0.64478 time= 0.29200
Epoch: 0056 train_loss= 0.60623 train_acc= 0.86466 val_loss= 1.23013 val_acc= 0.64478 time= 0.29423
Epoch: 0057 train_loss= 0.57279 train_acc= 0.87161 val_loss= 1.22164 val_acc= 0.65373 time= 0.28800
Epoch: 0058 train_loss= 0.54906 train_acc= 0.87955 val_loss= 1.21348 val_acc= 0.65373 time= 0.28900
Epoch: 0059 train_loss= 0.52112 train_acc= 0.88617 val_loss= 1.20599 val_acc= 0.65672 time= 0.29800
Epoch: 0060 train_loss= 0.49373 train_acc= 0.89444 val_loss= 1.19872 val_acc= 0.65075 time= 0.31001
Epoch: 0061 train_loss= 0.46868 train_acc= 0.90205 val_loss= 1.19199 val_acc= 0.65672 time= 0.30400
Epoch: 0062 train_loss= 0.44740 train_acc= 0.91231 val_loss= 1.18589 val_acc= 0.65672 time= 0.30311
Epoch: 0063 train_loss= 0.42611 train_acc= 0.91132 val_loss= 1.18053 val_acc= 0.65672 time= 0.29805
Epoch: 0064 train_loss= 0.40433 train_acc= 0.91529 val_loss= 1.17729 val_acc= 0.65672 time= 0.31600
Epoch: 0065 train_loss= 0.38198 train_acc= 0.92488 val_loss= 1.17541 val_acc= 0.65970 time= 0.31297
Epoch: 0066 train_loss= 0.36506 train_acc= 0.92952 val_loss= 1.17336 val_acc= 0.65672 time= 0.29699
Epoch: 0067 train_loss= 0.34477 train_acc= 0.93647 val_loss= 1.17258 val_acc= 0.65373 time= 0.29345
Epoch: 0068 train_loss= 0.32989 train_acc= 0.93580 val_loss= 1.17263 val_acc= 0.65672 time= 0.30184
Epoch: 0069 train_loss= 0.31127 train_acc= 0.94209 val_loss= 1.17246 val_acc= 0.66567 time= 0.30800
Epoch: 0070 train_loss= 0.29528 train_acc= 0.94805 val_loss= 1.17256 val_acc= 0.66567 time= 0.29100
Epoch: 0071 train_loss= 0.28137 train_acc= 0.94871 val_loss= 1.17093 val_acc= 0.66567 time= 0.29504
Epoch: 0072 train_loss= 0.26668 train_acc= 0.95367 val_loss= 1.16932 val_acc= 0.66567 time= 0.29201
Epoch: 0073 train_loss= 0.25472 train_acc= 0.95566 val_loss= 1.16859 val_acc= 0.66567 time= 0.29099
Epoch: 0074 train_loss= 0.24263 train_acc= 0.95831 val_loss= 1.17114 val_acc= 0.66567 time= 0.28900
Epoch: 0075 train_loss= 0.23116 train_acc= 0.96228 val_loss= 1.17480 val_acc= 0.66567 time= 0.29500
Early stopping...
Optimization Finished!
Test set results: cost= 1.15997 accuracy= 0.68637 time= 0.13000
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7262    0.6901    0.7076       342
           1     0.6750    0.7864    0.7265       103
           2     0.7373    0.6214    0.6744       140
           3     0.6000    0.4177    0.4925        79
           4     0.6579    0.7576    0.7042       132
           5     0.6786    0.7891    0.7297       313
           6     0.6952    0.7157    0.7053       102
           7     0.5946    0.3143    0.4112        70
           8     0.5758    0.3800    0.4578        50
           9     0.6150    0.7419    0.6725       155
          10     0.8392    0.6417    0.7273       187
          11     0.6058    0.6320    0.6186       231
          12     0.7514    0.7303    0.7407       178
          13     0.7781    0.8067    0.7921       600
          14     0.7717    0.8424    0.8055       590
          15     0.7536    0.6842    0.7172        76
          16     0.7222    0.3824    0.5000        34
          17     0.5000    0.1000    0.1667        10
          18     0.4286    0.4726    0.4495       419
          19     0.6634    0.5194    0.5826       129
          20     0.6429    0.6429    0.6429        28
          21     1.0000    0.7586    0.8627        29
          22     0.6364    0.3043    0.4118        46

    accuracy                         0.6864      4043
   macro avg     0.6804    0.5970    0.6217      4043
weighted avg     0.6890    0.6864    0.6825      4043

Macro average Test Precision, Recall and F1-Score...
(0.680381080999995, 0.5970296567984578, 0.62171889410237, None)
Micro average Test Precision, Recall and F1-Score...
(0.6863715063071977, 0.6863715063071977, 0.6863715063071977, None)
embeddings:
14157 3357 4043
[[ 4.0613881e-01  3.4728882e-01  4.1786516e-01 ...  4.4047129e-01
   5.4882544e-01  5.8351767e-01]
 [ 1.4045534e-01 -5.7668962e-02  1.0280870e-01 ...  1.7432356e-01
   1.6366038e-02  2.7891332e-01]
 [ 1.6297665e-01  2.5758979e-01  2.3944397e-01 ...  9.5287114e-02
   2.5421739e-01  4.7000426e-01]
 ...
 [ 1.4893495e-01  1.6741224e-01  1.7495514e-01 ...  1.1599330e-01
   2.2871345e-01  1.9053154e-01]
 [-6.4563006e-03 -4.8486829e-02 -3.8879532e-02 ...  4.4231123e-01
   3.1532235e-03  1.8505752e-04]
 [ 2.5257474e-01  1.9122151e-01  1.2936513e-01 ...  1.7143357e-01
   4.0281662e-01  3.2221597e-01]]

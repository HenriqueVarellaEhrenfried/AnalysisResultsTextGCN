(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13552 train_acc= 0.03276 val_loss= 3.10072 val_acc= 0.20000 time= 5.80816
Epoch: 0002 train_loss= 3.10099 train_acc= 0.17174 val_loss= 3.01256 val_acc= 0.20000 time= 5.64406
Epoch: 0003 train_loss= 3.01381 train_acc= 0.17141 val_loss= 2.88385 val_acc= 0.20000 time= 5.65900
Epoch: 0004 train_loss= 2.88726 train_acc= 0.17141 val_loss= 2.76201 val_acc= 0.20000 time= 5.65257
Epoch: 0005 train_loss= 2.76857 train_acc= 0.17141 val_loss= 2.69918 val_acc= 0.20000 time= 5.63604
Epoch: 0006 train_loss= 2.71131 train_acc= 0.17141 val_loss= 2.70356 val_acc= 0.20000 time= 5.66000
Epoch: 0007 train_loss= 2.71780 train_acc= 0.17141 val_loss= 2.69994 val_acc= 0.20000 time= 5.63804
Epoch: 0008 train_loss= 2.71000 train_acc= 0.17141 val_loss= 2.66048 val_acc= 0.20597 time= 5.64405
Epoch: 0009 train_loss= 2.65844 train_acc= 0.17406 val_loss= 2.61600 val_acc= 0.21493 time= 5.64300
Epoch: 0010 train_loss= 2.59792 train_acc= 0.18729 val_loss= 2.58233 val_acc= 0.24776 time= 5.65204
Epoch: 0011 train_loss= 2.55028 train_acc= 0.21443 val_loss= 2.55249 val_acc= 0.27164 time= 5.63708
Epoch: 0012 train_loss= 2.51097 train_acc= 0.24983 val_loss= 2.51680 val_acc= 0.29254 time= 5.66103
Epoch: 0013 train_loss= 2.46818 train_acc= 0.29881 val_loss= 2.47104 val_acc= 0.31045 time= 5.66200
Epoch: 0014 train_loss= 2.41462 train_acc= 0.34646 val_loss= 2.41610 val_acc= 0.32537 time= 5.64269
Epoch: 0015 train_loss= 2.35656 train_acc= 0.37723 val_loss= 2.35540 val_acc= 0.33433 time= 5.66240
Epoch: 0016 train_loss= 2.28902 train_acc= 0.37756 val_loss= 2.29238 val_acc= 0.34030 time= 5.67304
Epoch: 0017 train_loss= 2.22125 train_acc= 0.38981 val_loss= 2.22946 val_acc= 0.35522 time= 5.65700
Epoch: 0018 train_loss= 2.14655 train_acc= 0.39775 val_loss= 2.16791 val_acc= 0.37015 time= 5.63400
Epoch: 0019 train_loss= 2.07533 train_acc= 0.42058 val_loss= 2.10740 val_acc= 0.39701 time= 5.63700
Epoch: 0020 train_loss= 1.99637 train_acc= 0.46128 val_loss= 2.04786 val_acc= 0.43582 time= 5.63059
Epoch: 0021 train_loss= 1.92160 train_acc= 0.50860 val_loss= 1.98991 val_acc= 0.47164 time= 5.63497
Epoch: 0022 train_loss= 1.84388 train_acc= 0.54931 val_loss= 1.93378 val_acc= 0.49552 time= 5.64605
Epoch: 0023 train_loss= 1.77027 train_acc= 0.57975 val_loss= 1.87794 val_acc= 0.52836 time= 5.65289
Epoch: 0024 train_loss= 1.68842 train_acc= 0.60291 val_loss= 1.82070 val_acc= 0.52836 time= 5.63796
Epoch: 0025 train_loss= 1.61328 train_acc= 0.61913 val_loss= 1.76352 val_acc= 0.53731 time= 5.62607
Epoch: 0026 train_loss= 1.53919 train_acc= 0.63203 val_loss= 1.71010 val_acc= 0.54328 time= 5.65100
Epoch: 0027 train_loss= 1.46584 train_acc= 0.64825 val_loss= 1.66193 val_acc= 0.54925 time= 5.63501
Epoch: 0028 train_loss= 1.40013 train_acc= 0.65718 val_loss= 1.61791 val_acc= 0.54925 time= 5.63001
Epoch: 0029 train_loss= 1.33538 train_acc= 0.67439 val_loss= 1.57695 val_acc= 0.56119 time= 5.64100
Epoch: 0030 train_loss= 1.26566 train_acc= 0.68531 val_loss= 1.53872 val_acc= 0.57910 time= 5.64700
Epoch: 0031 train_loss= 1.20511 train_acc= 0.69656 val_loss= 1.50265 val_acc= 0.57910 time= 5.65503
Epoch: 0032 train_loss= 1.14116 train_acc= 0.71211 val_loss= 1.46725 val_acc= 0.58209 time= 5.64902
Epoch: 0033 train_loss= 1.08585 train_acc= 0.71972 val_loss= 1.43316 val_acc= 0.58209 time= 5.65797
Epoch: 0034 train_loss= 1.02506 train_acc= 0.73958 val_loss= 1.40242 val_acc= 0.59104 time= 5.63405
Epoch: 0035 train_loss= 0.96818 train_acc= 0.75546 val_loss= 1.37497 val_acc= 0.59403 time= 5.65300
Epoch: 0036 train_loss= 0.91893 train_acc= 0.77267 val_loss= 1.34972 val_acc= 0.60000 time= 5.64400
Epoch: 0037 train_loss= 0.86392 train_acc= 0.78987 val_loss= 1.32679 val_acc= 0.61791 time= 5.62641
Epoch: 0038 train_loss= 0.81635 train_acc= 0.80443 val_loss= 1.30625 val_acc= 0.62687 time= 5.63500
Epoch: 0039 train_loss= 0.77168 train_acc= 0.81105 val_loss= 1.28772 val_acc= 0.62985 time= 5.64400
Epoch: 0040 train_loss= 0.72597 train_acc= 0.82859 val_loss= 1.26993 val_acc= 0.62388 time= 5.64498
Epoch: 0041 train_loss= 0.68160 train_acc= 0.84050 val_loss= 1.25279 val_acc= 0.62090 time= 5.64600
Epoch: 0042 train_loss= 0.64043 train_acc= 0.84878 val_loss= 1.23861 val_acc= 0.62388 time= 5.63500
Epoch: 0043 train_loss= 0.60285 train_acc= 0.86069 val_loss= 1.22690 val_acc= 0.63582 time= 5.66217
Epoch: 0044 train_loss= 0.56381 train_acc= 0.86863 val_loss= 1.21662 val_acc= 0.63582 time= 5.65701
Epoch: 0045 train_loss= 0.52579 train_acc= 0.87690 val_loss= 1.20708 val_acc= 0.63582 time= 5.66499
Epoch: 0046 train_loss= 0.49244 train_acc= 0.89279 val_loss= 1.19976 val_acc= 0.64179 time= 5.62201
Epoch: 0047 train_loss= 0.46200 train_acc= 0.89643 val_loss= 1.19345 val_acc= 0.64478 time= 5.65201
Epoch: 0048 train_loss= 0.43231 train_acc= 0.90205 val_loss= 1.18732 val_acc= 0.64776 time= 5.62901
Epoch: 0049 train_loss= 0.40203 train_acc= 0.91330 val_loss= 1.18118 val_acc= 0.65075 time= 5.65700
Epoch: 0050 train_loss= 0.37342 train_acc= 0.92025 val_loss= 1.17540 val_acc= 0.65970 time= 5.62200
Epoch: 0051 train_loss= 0.35393 train_acc= 0.92521 val_loss= 1.17180 val_acc= 0.66866 time= 5.79047
Epoch: 0052 train_loss= 0.32835 train_acc= 0.93084 val_loss= 1.17107 val_acc= 0.67164 time= 5.71898
Epoch: 0053 train_loss= 0.30631 train_acc= 0.93779 val_loss= 1.16969 val_acc= 0.67463 time= 5.63386
Epoch: 0054 train_loss= 0.28974 train_acc= 0.94077 val_loss= 1.17099 val_acc= 0.67463 time= 5.65700
Epoch: 0055 train_loss= 0.26784 train_acc= 0.94805 val_loss= 1.17356 val_acc= 0.67761 time= 5.69400
Epoch: 0056 train_loss= 0.24779 train_acc= 0.95301 val_loss= 1.17470 val_acc= 0.67164 time= 5.65900
Epoch: 0057 train_loss= 0.23354 train_acc= 0.95897 val_loss= 1.17732 val_acc= 0.67164 time= 5.63264
Early stopping...
Optimization Finished!
Test set results: cost= 1.17756 accuracy= 0.68662 time= 1.96900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7130    0.6901    0.7013       342
           1     0.6903    0.7573    0.7222       103
           2     0.7034    0.5929    0.6434       140
           3     0.6296    0.4304    0.5113        79
           4     0.6405    0.7424    0.6877       132
           5     0.6960    0.7827    0.7368       313
           6     0.6697    0.7157    0.6919       102
           7     0.6111    0.3143    0.4151        70
           8     0.5429    0.3800    0.4471        50
           9     0.6230    0.7355    0.6746       155
          10     0.8425    0.6578    0.7387       187
          11     0.6235    0.6667    0.6444       231
          12     0.7661    0.7360    0.7507       178
          13     0.7651    0.8033    0.7837       600
          14     0.7750    0.8407    0.8065       590
          15     0.7671    0.7368    0.7517        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.4295    0.4726    0.4500       419
          19     0.6700    0.5194    0.5852       129
          20     0.6071    0.6071    0.6071        28
          21     1.0000    0.7241    0.8400        29
          22     0.6667    0.3478    0.4571        46

    accuracy                         0.6866      4043
   macro avg     0.6818    0.5959    0.6214      4043
weighted avg     0.6892    0.6866    0.6829      4043

Macro average Test Precision, Recall and F1-Score...
(0.6818264346998693, 0.5959311027275518, 0.6214476514554859, None)
Micro average Test Precision, Recall and F1-Score...
(0.6866188473905516, 0.6866188473905516, 0.6866188473905516, None)
embeddings:
14157 3357 4043
[[ 0.2925337   0.2307687   0.2916712  ...  0.37196505  0.2338332
   0.23364814]
 [ 0.09734693  0.11785268 -0.01437499 ...  0.05459805  0.02322038
   0.00333124]
 [ 0.19611774  0.1462202   0.07053901 ...  0.28130117  0.1671006
   0.08051116]
 ...
 [ 0.09659529  0.0823957   0.1039852  ...  0.099648    0.07151624
   0.13112997]
 [ 0.01345037  0.01777494 -0.00424786 ...  0.03336433  0.19351557
   0.09867872]
 [ 0.18035045  0.16698731  0.23761812 ...  0.23119286  0.05812803
   0.01540426]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13552 train_acc= 0.03077 val_loss= 3.11630 val_acc= 0.29254 time= 0.58965
Epoch: 0002 train_loss= 3.11613 train_acc= 0.28094 val_loss= 3.07290 val_acc= 0.29254 time= 0.29803
Epoch: 0003 train_loss= 3.07298 train_acc= 0.27565 val_loss= 3.00563 val_acc= 0.29254 time= 0.29941
Epoch: 0004 train_loss= 3.00587 train_acc= 0.27366 val_loss= 2.92062 val_acc= 0.28657 time= 0.29003
Epoch: 0005 train_loss= 2.92209 train_acc= 0.26870 val_loss= 2.83173 val_acc= 0.28955 time= 0.29097
Epoch: 0006 train_loss= 2.83620 train_acc= 0.27598 val_loss= 2.75683 val_acc= 0.29552 time= 0.29230
Epoch: 0007 train_loss= 2.76236 train_acc= 0.29351 val_loss= 2.71205 val_acc= 0.29254 time= 0.29800
Epoch: 0008 train_loss= 2.71982 train_acc= 0.27598 val_loss= 2.69840 val_acc= 0.26567 time= 0.29658
Epoch: 0009 train_loss= 2.70545 train_acc= 0.24156 val_loss= 2.69465 val_acc= 0.22388 time= 0.30372
Epoch: 0010 train_loss= 2.70292 train_acc= 0.19060 val_loss= 2.68367 val_acc= 0.20896 time= 0.29000
Epoch: 0011 train_loss= 2.68791 train_acc= 0.17505 val_loss= 2.66077 val_acc= 0.20896 time= 0.29100
Epoch: 0012 train_loss= 2.65700 train_acc= 0.17505 val_loss= 2.63204 val_acc= 0.20896 time= 0.28900
Epoch: 0013 train_loss= 2.61820 train_acc= 0.17836 val_loss= 2.60438 val_acc= 0.21791 time= 0.28712
Epoch: 0014 train_loss= 2.58220 train_acc= 0.18994 val_loss= 2.57956 val_acc= 0.23582 time= 0.28997
Epoch: 0015 train_loss= 2.55046 train_acc= 0.20715 val_loss= 2.55584 val_acc= 0.25970 time= 0.29707
Epoch: 0016 train_loss= 2.52104 train_acc= 0.23428 val_loss= 2.52999 val_acc= 0.27761 time= 0.29397
Epoch: 0017 train_loss= 2.49243 train_acc= 0.26439 val_loss= 2.49965 val_acc= 0.29851 time= 0.29003
Epoch: 0018 train_loss= 2.45528 train_acc= 0.28888 val_loss= 2.46414 val_acc= 0.30746 time= 0.29342
Epoch: 0019 train_loss= 2.41661 train_acc= 0.31072 val_loss= 2.42407 val_acc= 0.31940 time= 0.30100
Epoch: 0020 train_loss= 2.37444 train_acc= 0.33289 val_loss= 2.38085 val_acc= 0.33433 time= 0.29503
Epoch: 0021 train_loss= 2.32634 train_acc= 0.34348 val_loss= 2.33594 val_acc= 0.34030 time= 0.28900
Epoch: 0022 train_loss= 2.27715 train_acc= 0.35076 val_loss= 2.29054 val_acc= 0.34030 time= 0.29125
Epoch: 0023 train_loss= 2.22579 train_acc= 0.35374 val_loss= 2.24543 val_acc= 0.34030 time= 0.29103
Epoch: 0024 train_loss= 2.17013 train_acc= 0.36830 val_loss= 2.20073 val_acc= 0.34925 time= 0.29397
Epoch: 0025 train_loss= 2.11688 train_acc= 0.38054 val_loss= 2.15613 val_acc= 0.36716 time= 0.28804
Epoch: 0026 train_loss= 2.06349 train_acc= 0.40635 val_loss= 2.11130 val_acc= 0.39403 time= 0.29462
Epoch: 0027 train_loss= 2.00455 train_acc= 0.44176 val_loss= 2.06663 val_acc= 0.42090 time= 0.29197
Epoch: 0028 train_loss= 1.94412 train_acc= 0.47948 val_loss= 2.02242 val_acc= 0.45970 time= 0.29200
Epoch: 0029 train_loss= 1.89143 train_acc= 0.51820 val_loss= 1.97869 val_acc= 0.49254 time= 0.29103
Epoch: 0030 train_loss= 1.82971 train_acc= 0.55526 val_loss= 1.93473 val_acc= 0.51642 time= 0.29497
Epoch: 0031 train_loss= 1.77022 train_acc= 0.58934 val_loss= 1.88978 val_acc= 0.52836 time= 0.29400
Epoch: 0032 train_loss= 1.70862 train_acc= 0.60324 val_loss= 1.84393 val_acc= 0.52836 time= 0.29700
Epoch: 0033 train_loss= 1.65054 train_acc= 0.61350 val_loss= 1.79831 val_acc= 0.52239 time= 0.29197
Epoch: 0034 train_loss= 1.59152 train_acc= 0.62177 val_loss= 1.75396 val_acc= 0.53731 time= 0.29900
Epoch: 0035 train_loss= 1.53041 train_acc= 0.63501 val_loss= 1.71220 val_acc= 0.54030 time= 0.29103
Epoch: 0036 train_loss= 1.47921 train_acc= 0.64428 val_loss= 1.67310 val_acc= 0.56418 time= 0.29000
Epoch: 0037 train_loss= 1.41729 train_acc= 0.65884 val_loss= 1.63595 val_acc= 0.56418 time= 0.29800
Epoch: 0038 train_loss= 1.36490 train_acc= 0.67009 val_loss= 1.60093 val_acc= 0.57612 time= 0.29758
Epoch: 0039 train_loss= 1.31282 train_acc= 0.68068 val_loss= 1.56857 val_acc= 0.57910 time= 0.28996
Epoch: 0040 train_loss= 1.26257 train_acc= 0.69126 val_loss= 1.53887 val_acc= 0.58507 time= 0.29203
Epoch: 0041 train_loss= 1.21252 train_acc= 0.70417 val_loss= 1.51076 val_acc= 0.58806 time= 0.28900
Epoch: 0042 train_loss= 1.16663 train_acc= 0.71046 val_loss= 1.48274 val_acc= 0.59403 time= 0.29400
Epoch: 0043 train_loss= 1.11913 train_acc= 0.73031 val_loss= 1.45538 val_acc= 0.58806 time= 0.28897
Epoch: 0044 train_loss= 1.07613 train_acc= 0.74322 val_loss= 1.42934 val_acc= 0.58806 time= 0.29500
Epoch: 0045 train_loss= 1.03073 train_acc= 0.75116 val_loss= 1.40516 val_acc= 0.59403 time= 0.28990
Epoch: 0046 train_loss= 0.98766 train_acc= 0.76671 val_loss= 1.38144 val_acc= 0.60000 time= 0.29180
Epoch: 0047 train_loss= 0.94454 train_acc= 0.77631 val_loss= 1.35939 val_acc= 0.60597 time= 0.29099
Epoch: 0048 train_loss= 0.90226 train_acc= 0.78127 val_loss= 1.33923 val_acc= 0.60896 time= 0.29200
Epoch: 0049 train_loss= 0.87054 train_acc= 0.79418 val_loss= 1.32094 val_acc= 0.61493 time= 0.29305
Epoch: 0050 train_loss= 0.83018 train_acc= 0.80013 val_loss= 1.30459 val_acc= 0.61791 time= 0.29300
Epoch: 0051 train_loss= 0.79903 train_acc= 0.81238 val_loss= 1.29030 val_acc= 0.60597 time= 0.29100
Epoch: 0052 train_loss= 0.75980 train_acc= 0.82131 val_loss= 1.27633 val_acc= 0.61194 time= 0.29100
Epoch: 0053 train_loss= 0.72900 train_acc= 0.82958 val_loss= 1.26203 val_acc= 0.62985 time= 0.29500
Epoch: 0054 train_loss= 0.69643 train_acc= 0.83686 val_loss= 1.24944 val_acc= 0.63284 time= 0.29800
Epoch: 0055 train_loss= 0.66641 train_acc= 0.83951 val_loss= 1.23822 val_acc= 0.63284 time= 0.29100
Epoch: 0056 train_loss= 0.63639 train_acc= 0.85208 val_loss= 1.22749 val_acc= 0.64179 time= 0.29000
Epoch: 0057 train_loss= 0.60389 train_acc= 0.86367 val_loss= 1.21806 val_acc= 0.64478 time= 0.28900
Epoch: 0058 train_loss= 0.58369 train_acc= 0.86598 val_loss= 1.20889 val_acc= 0.63284 time= 0.29340
Epoch: 0059 train_loss= 0.55265 train_acc= 0.87823 val_loss= 1.20112 val_acc= 0.64179 time= 0.29417
Epoch: 0060 train_loss= 0.52752 train_acc= 0.88518 val_loss= 1.19376 val_acc= 0.63284 time= 0.29103
Epoch: 0061 train_loss= 0.50699 train_acc= 0.88683 val_loss= 1.18717 val_acc= 0.64478 time= 0.30003
Epoch: 0062 train_loss= 0.49141 train_acc= 0.89014 val_loss= 1.18086 val_acc= 0.65373 time= 0.29900
Epoch: 0063 train_loss= 0.46406 train_acc= 0.90073 val_loss= 1.17464 val_acc= 0.66269 time= 0.29053
Epoch: 0064 train_loss= 0.44290 train_acc= 0.90801 val_loss= 1.16992 val_acc= 0.65672 time= 0.28900
Epoch: 0065 train_loss= 0.43044 train_acc= 0.90834 val_loss= 1.16553 val_acc= 0.65075 time= 0.29200
Epoch: 0066 train_loss= 0.40596 train_acc= 0.91595 val_loss= 1.16312 val_acc= 0.65672 time= 0.29504
Epoch: 0067 train_loss= 0.38855 train_acc= 0.92091 val_loss= 1.16105 val_acc= 0.65672 time= 0.29403
Epoch: 0068 train_loss= 0.37033 train_acc= 0.92555 val_loss= 1.15958 val_acc= 0.65672 time= 0.29297
Epoch: 0069 train_loss= 0.35765 train_acc= 0.93051 val_loss= 1.15602 val_acc= 0.65373 time= 0.29500
Epoch: 0070 train_loss= 0.34017 train_acc= 0.93448 val_loss= 1.15245 val_acc= 0.65672 time= 0.29400
Epoch: 0071 train_loss= 0.32284 train_acc= 0.94143 val_loss= 1.14980 val_acc= 0.65373 time= 0.29200
Epoch: 0072 train_loss= 0.30975 train_acc= 0.94176 val_loss= 1.14853 val_acc= 0.65373 time= 0.28803
Epoch: 0073 train_loss= 0.29911 train_acc= 0.94408 val_loss= 1.14790 val_acc= 0.65373 time= 0.29329
Epoch: 0074 train_loss= 0.28837 train_acc= 0.94838 val_loss= 1.14943 val_acc= 0.65970 time= 0.29064
Epoch: 0075 train_loss= 0.27454 train_acc= 0.95103 val_loss= 1.15217 val_acc= 0.66567 time= 0.29069
Epoch: 0076 train_loss= 0.26336 train_acc= 0.95202 val_loss= 1.15562 val_acc= 0.66269 time= 0.28700
Early stopping...
Optimization Finished!
Test set results: cost= 1.15638 accuracy= 0.68786 time= 0.12803
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7194    0.7047    0.7120       342
           1     0.6930    0.7670    0.7281       103
           2     0.7500    0.6214    0.6797       140
           3     0.5965    0.4304    0.5000        79
           4     0.6168    0.7803    0.6890       132
           5     0.6833    0.7859    0.7311       313
           6     0.6832    0.6765    0.6798       102
           7     0.6053    0.3286    0.4259        70
           8     0.5758    0.3800    0.4578        50
           9     0.6298    0.7355    0.6786       155
          10     0.8531    0.6524    0.7394       187
          11     0.6498    0.6104    0.6295       231
          12     0.7442    0.7191    0.7314       178
          13     0.7686    0.8083    0.7880       600
          14     0.7738    0.8525    0.8113       590
          15     0.7671    0.7368    0.7517        76
          16     0.7333    0.3235    0.4490        34
          17     0.5000    0.1000    0.1667        10
          18     0.4301    0.4702    0.4493       419
          19     0.6505    0.5194    0.5776       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.7241    0.8400        29
          22     0.5926    0.3478    0.4384        46

    accuracy                         0.6879      4043
   macro avg     0.6799    0.5964    0.6211      4043
weighted avg     0.6899    0.6879    0.6838      4043

Macro average Test Precision, Recall and F1-Score...
(0.6798663025753392, 0.5964243837729574, 0.621111787489841, None)
Micro average Test Precision, Recall and F1-Score...
(0.6878555528073212, 0.6878555528073212, 0.6878555528073212, None)
embeddings:
14157 3357 4043
[[ 0.33731976  0.22672273  0.45468363 ...  0.4645125   0.44703597
   0.52127635]
 [ 0.09444068  0.07569879  0.43637556 ...  0.02273321  0.25276193
   0.3238957 ]
 [ 0.05928802  0.05792832  0.7002578  ...  0.5070395   0.41157517
   0.5117889 ]
 ...
 [ 0.04986632  0.14404899  0.2864905  ...  0.26660177  0.3167278
   0.23104024]
 [ 0.24905588  0.1022033   0.44491252 ... -0.02485621 -0.03752197
   0.44943005]
 [ 0.20933779  0.13645527  0.03042909 ...  0.20839517  0.23779812
   0.05538898]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.04302 val_loss= 3.11459 val_acc= 0.20299 time= 0.58307
Epoch: 0002 train_loss= 3.11469 train_acc= 0.17240 val_loss= 3.06829 val_acc= 0.20000 time= 0.29704
Epoch: 0003 train_loss= 3.06855 train_acc= 0.17174 val_loss= 2.99747 val_acc= 0.20000 time= 0.28900
Epoch: 0004 train_loss= 2.99912 train_acc= 0.17141 val_loss= 2.90880 val_acc= 0.20000 time= 0.28500
Epoch: 0005 train_loss= 2.91090 train_acc= 0.17141 val_loss= 2.81720 val_acc= 0.20000 time= 0.28700
Epoch: 0006 train_loss= 2.82103 train_acc= 0.17174 val_loss= 2.74165 val_acc= 0.20000 time= 0.29400
Epoch: 0007 train_loss= 2.74797 train_acc= 0.17174 val_loss= 2.69817 val_acc= 0.20000 time= 0.28740
Epoch: 0008 train_loss= 2.70751 train_acc= 0.17207 val_loss= 2.68973 val_acc= 0.20000 time= 0.28697
Epoch: 0009 train_loss= 2.70197 train_acc= 0.17240 val_loss= 2.69164 val_acc= 0.20299 time= 0.28803
Epoch: 0010 train_loss= 2.70192 train_acc= 0.17240 val_loss= 2.67939 val_acc= 0.20597 time= 0.29492
Epoch: 0011 train_loss= 2.68421 train_acc= 0.17340 val_loss= 2.65188 val_acc= 0.20896 time= 0.28900
Epoch: 0012 train_loss= 2.64788 train_acc= 0.17770 val_loss= 2.62008 val_acc= 0.21493 time= 0.28797
Epoch: 0013 train_loss= 2.60563 train_acc= 0.18729 val_loss= 2.59212 val_acc= 0.23582 time= 0.28922
Epoch: 0014 train_loss= 2.56965 train_acc= 0.20582 val_loss= 2.56828 val_acc= 0.25672 time= 0.29410
Epoch: 0015 train_loss= 2.53726 train_acc= 0.22667 val_loss= 2.54428 val_acc= 0.27463 time= 0.28796
Epoch: 0016 train_loss= 2.50593 train_acc= 0.25281 val_loss= 2.51640 val_acc= 0.28657 time= 0.29003
Epoch: 0017 train_loss= 2.47236 train_acc= 0.28259 val_loss= 2.48305 val_acc= 0.29552 time= 0.28997
Epoch: 0018 train_loss= 2.43750 train_acc= 0.30510 val_loss= 2.44469 val_acc= 0.31045 time= 0.29503
Epoch: 0019 train_loss= 2.39498 train_acc= 0.32098 val_loss= 2.40287 val_acc= 0.31343 time= 0.28600
Epoch: 0020 train_loss= 2.34890 train_acc= 0.32727 val_loss= 2.35935 val_acc= 0.31940 time= 0.28900
Epoch: 0021 train_loss= 2.30557 train_acc= 0.33984 val_loss= 2.31525 val_acc= 0.32537 time= 0.29346
Epoch: 0022 train_loss= 2.25806 train_acc= 0.35308 val_loss= 2.27106 val_acc= 0.33731 time= 0.29303
Epoch: 0023 train_loss= 2.20777 train_acc= 0.36466 val_loss= 2.22662 val_acc= 0.34328 time= 0.29004
Epoch: 0024 train_loss= 2.15510 train_acc= 0.37988 val_loss= 2.18179 val_acc= 0.36119 time= 0.29214
Epoch: 0025 train_loss= 2.10151 train_acc= 0.40602 val_loss= 2.13694 val_acc= 0.38806 time= 0.29200
Epoch: 0026 train_loss= 2.04600 train_acc= 0.43382 val_loss= 2.09270 val_acc= 0.42687 time= 0.29000
Epoch: 0027 train_loss= 1.98945 train_acc= 0.48544 val_loss= 2.04967 val_acc= 0.46866 time= 0.29000
Epoch: 0028 train_loss= 1.93368 train_acc= 0.51820 val_loss= 2.00761 val_acc= 0.48358 time= 0.28800
Epoch: 0029 train_loss= 1.87937 train_acc= 0.54467 val_loss= 1.96527 val_acc= 0.49552 time= 0.29400
Epoch: 0030 train_loss= 1.82307 train_acc= 0.57644 val_loss= 1.92183 val_acc= 0.51940 time= 0.29155
Epoch: 0031 train_loss= 1.76365 train_acc= 0.59265 val_loss= 1.87736 val_acc= 0.52537 time= 0.29100
Epoch: 0032 train_loss= 1.70685 train_acc= 0.60291 val_loss= 1.83308 val_acc= 0.51940 time= 0.29000
Epoch: 0033 train_loss= 1.64987 train_acc= 0.60821 val_loss= 1.79042 val_acc= 0.52537 time= 0.29501
Epoch: 0034 train_loss= 1.59072 train_acc= 0.61979 val_loss= 1.75008 val_acc= 0.53731 time= 0.28800
Epoch: 0035 train_loss= 1.53504 train_acc= 0.62773 val_loss= 1.71175 val_acc= 0.53731 time= 0.28700
Epoch: 0036 train_loss= 1.48492 train_acc= 0.64097 val_loss= 1.67521 val_acc= 0.54627 time= 0.28700
Epoch: 0037 train_loss= 1.43767 train_acc= 0.64792 val_loss= 1.63924 val_acc= 0.54925 time= 0.29297
Epoch: 0038 train_loss= 1.38393 train_acc= 0.66810 val_loss= 1.60456 val_acc= 0.57612 time= 0.28703
Epoch: 0039 train_loss= 1.32902 train_acc= 0.68200 val_loss= 1.57208 val_acc= 0.57313 time= 0.28700
Epoch: 0040 train_loss= 1.28403 train_acc= 0.68498 val_loss= 1.54195 val_acc= 0.58507 time= 0.28797
Epoch: 0041 train_loss= 1.23494 train_acc= 0.69590 val_loss= 1.51321 val_acc= 0.58507 time= 0.29403
Epoch: 0042 train_loss= 1.18366 train_acc= 0.71013 val_loss= 1.48603 val_acc= 0.58806 time= 0.29100
Epoch: 0043 train_loss= 1.14198 train_acc= 0.72038 val_loss= 1.46018 val_acc= 0.58806 time= 0.28900
Epoch: 0044 train_loss= 1.09292 train_acc= 0.73130 val_loss= 1.43597 val_acc= 0.58806 time= 0.28997
Epoch: 0045 train_loss= 1.04823 train_acc= 0.74719 val_loss= 1.41287 val_acc= 0.59403 time= 0.29204
Epoch: 0046 train_loss= 1.00430 train_acc= 0.75711 val_loss= 1.39018 val_acc= 0.60000 time= 0.28900
Epoch: 0047 train_loss= 0.96995 train_acc= 0.76870 val_loss= 1.36858 val_acc= 0.60597 time= 0.28900
Epoch: 0048 train_loss= 0.92612 train_acc= 0.77167 val_loss= 1.34895 val_acc= 0.60597 time= 0.28800
Epoch: 0049 train_loss= 0.89168 train_acc= 0.78756 val_loss= 1.33103 val_acc= 0.61194 time= 0.29500
Epoch: 0050 train_loss= 0.85546 train_acc= 0.79782 val_loss= 1.31455 val_acc= 0.61791 time= 0.28997
Epoch: 0051 train_loss= 0.81670 train_acc= 0.80344 val_loss= 1.29946 val_acc= 0.62388 time= 0.29104
Epoch: 0052 train_loss= 0.78376 train_acc= 0.81502 val_loss= 1.28494 val_acc= 0.62687 time= 0.29014
Epoch: 0053 train_loss= 0.74527 train_acc= 0.82528 val_loss= 1.27107 val_acc= 0.62388 time= 0.29285
Epoch: 0054 train_loss= 0.71203 train_acc= 0.83521 val_loss= 1.25819 val_acc= 0.63284 time= 0.29100
Epoch: 0055 train_loss= 0.68477 train_acc= 0.84447 val_loss= 1.24629 val_acc= 0.63284 time= 0.28901
Epoch: 0056 train_loss= 0.65006 train_acc= 0.84944 val_loss= 1.23511 val_acc= 0.62985 time= 0.28999
Epoch: 0057 train_loss= 0.62487 train_acc= 0.85539 val_loss= 1.22524 val_acc= 0.63284 time= 0.29297
Epoch: 0058 train_loss= 0.59957 train_acc= 0.86466 val_loss= 1.21604 val_acc= 0.63284 time= 0.29111
Epoch: 0059 train_loss= 0.57386 train_acc= 0.87028 val_loss= 1.20775 val_acc= 0.63582 time= 0.29000
Epoch: 0060 train_loss= 0.55029 train_acc= 0.87690 val_loss= 1.20057 val_acc= 0.64776 time= 0.29300
Epoch: 0061 train_loss= 0.52757 train_acc= 0.88352 val_loss= 1.19358 val_acc= 0.65970 time= 0.29200
Epoch: 0062 train_loss= 0.50116 train_acc= 0.89146 val_loss= 1.18673 val_acc= 0.64776 time= 0.28902
Epoch: 0063 train_loss= 0.47591 train_acc= 0.89709 val_loss= 1.18147 val_acc= 0.65075 time= 0.28901
Epoch: 0064 train_loss= 0.45436 train_acc= 0.90569 val_loss= 1.17719 val_acc= 0.65373 time= 0.29800
Epoch: 0065 train_loss= 0.43100 train_acc= 0.91264 val_loss= 1.17143 val_acc= 0.64776 time= 0.28803
Epoch: 0066 train_loss= 0.41973 train_acc= 0.90933 val_loss= 1.16610 val_acc= 0.64776 time= 0.28904
Epoch: 0067 train_loss= 0.39918 train_acc= 0.91463 val_loss= 1.16227 val_acc= 0.65970 time= 0.28696
Epoch: 0068 train_loss= 0.38031 train_acc= 0.92720 val_loss= 1.15780 val_acc= 0.65970 time= 0.29303
Epoch: 0069 train_loss= 0.36323 train_acc= 0.92621 val_loss= 1.15421 val_acc= 0.66269 time= 0.28807
Epoch: 0070 train_loss= 0.34578 train_acc= 0.93448 val_loss= 1.15321 val_acc= 0.66567 time= 0.28600
Epoch: 0071 train_loss= 0.33355 train_acc= 0.93746 val_loss= 1.15319 val_acc= 0.67164 time= 0.28800
Epoch: 0072 train_loss= 0.31384 train_acc= 0.93944 val_loss= 1.15435 val_acc= 0.66269 time= 0.29400
Epoch: 0073 train_loss= 0.30340 train_acc= 0.94441 val_loss= 1.15532 val_acc= 0.66269 time= 0.28996
Epoch: 0074 train_loss= 0.28799 train_acc= 0.94739 val_loss= 1.15361 val_acc= 0.66567 time= 0.29200
Epoch: 0075 train_loss= 0.27859 train_acc= 0.94772 val_loss= 1.15165 val_acc= 0.66866 time= 0.28993
Epoch: 0076 train_loss= 0.26472 train_acc= 0.95367 val_loss= 1.15194 val_acc= 0.66567 time= 0.29325
Epoch: 0077 train_loss= 0.25211 train_acc= 0.95566 val_loss= 1.15175 val_acc= 0.67164 time= 0.28900
Epoch: 0078 train_loss= 0.23999 train_acc= 0.95831 val_loss= 1.15435 val_acc= 0.66866 time= 0.29000
Epoch: 0079 train_loss= 0.23307 train_acc= 0.96161 val_loss= 1.15841 val_acc= 0.66866 time= 0.29060
Epoch: 0080 train_loss= 0.22394 train_acc= 0.96525 val_loss= 1.15848 val_acc= 0.67761 time= 0.29300
Epoch: 0081 train_loss= 0.21138 train_acc= 0.96790 val_loss= 1.15937 val_acc= 0.67761 time= 0.28500
Epoch: 0082 train_loss= 0.20833 train_acc= 0.96724 val_loss= 1.15912 val_acc= 0.67761 time= 0.29216
Epoch: 0083 train_loss= 0.19520 train_acc= 0.96856 val_loss= 1.16040 val_acc= 0.66866 time= 0.29297
Epoch: 0084 train_loss= 0.19066 train_acc= 0.96956 val_loss= 1.16398 val_acc= 0.67164 time= 0.29304
Epoch: 0085 train_loss= 0.18181 train_acc= 0.97121 val_loss= 1.17103 val_acc= 0.67164 time= 0.29100
Epoch: 0086 train_loss= 0.17547 train_acc= 0.97220 val_loss= 1.17108 val_acc= 0.66866 time= 0.28900
Epoch: 0087 train_loss= 0.16845 train_acc= 0.97485 val_loss= 1.17287 val_acc= 0.67463 time= 0.29097
Epoch: 0088 train_loss= 0.15806 train_acc= 0.97684 val_loss= 1.17557 val_acc= 0.68358 time= 0.29703
Epoch: 0089 train_loss= 0.15708 train_acc= 0.97651 val_loss= 1.17829 val_acc= 0.68060 time= 0.29001
Epoch: 0090 train_loss= 0.14803 train_acc= 0.97981 val_loss= 1.18270 val_acc= 0.67463 time= 0.28699
Epoch: 0091 train_loss= 0.14018 train_acc= 0.98345 val_loss= 1.18804 val_acc= 0.67164 time= 0.29227
Epoch: 0092 train_loss= 0.13771 train_acc= 0.98279 val_loss= 1.19179 val_acc= 0.67164 time= 0.28996
Epoch: 0093 train_loss= 0.12923 train_acc= 0.98114 val_loss= 1.19259 val_acc= 0.67164 time= 0.29402
Epoch: 0094 train_loss= 0.12753 train_acc= 0.98676 val_loss= 1.19431 val_acc= 0.67761 time= 0.29202
Epoch: 0095 train_loss= 0.12203 train_acc= 0.98610 val_loss= 1.19808 val_acc= 0.68060 time= 0.29600
Epoch: 0096 train_loss= 0.11899 train_acc= 0.98643 val_loss= 1.20244 val_acc= 0.67463 time= 0.29004
Epoch: 0097 train_loss= 0.11167 train_acc= 0.98709 val_loss= 1.20682 val_acc= 0.66567 time= 0.28800
Epoch: 0098 train_loss= 0.11128 train_acc= 0.98776 val_loss= 1.21120 val_acc= 0.65970 time= 0.29400
Epoch: 0099 train_loss= 0.10503 train_acc= 0.99107 val_loss= 1.21520 val_acc= 0.66567 time= 0.29382
Epoch: 0100 train_loss= 0.10282 train_acc= 0.98776 val_loss= 1.21946 val_acc= 0.66269 time= 0.29101
Epoch: 0101 train_loss= 0.10007 train_acc= 0.98974 val_loss= 1.22199 val_acc= 0.66567 time= 0.28583
Epoch: 0102 train_loss= 0.09478 train_acc= 0.99173 val_loss= 1.22463 val_acc= 0.67164 time= 0.28897
Epoch: 0103 train_loss= 0.09288 train_acc= 0.99239 val_loss= 1.22763 val_acc= 0.67164 time= 0.29303
Epoch: 0104 train_loss= 0.08941 train_acc= 0.99206 val_loss= 1.23282 val_acc= 0.67164 time= 0.28797
Epoch: 0105 train_loss= 0.08562 train_acc= 0.99338 val_loss= 1.23713 val_acc= 0.66866 time= 0.28903
Epoch: 0106 train_loss= 0.08352 train_acc= 0.99206 val_loss= 1.24231 val_acc= 0.67164 time= 0.29000
Epoch: 0107 train_loss= 0.08113 train_acc= 0.99206 val_loss= 1.24780 val_acc= 0.67463 time= 0.29400
Epoch: 0108 train_loss= 0.08006 train_acc= 0.99272 val_loss= 1.25292 val_acc= 0.67164 time= 0.28800
Epoch: 0109 train_loss= 0.07614 train_acc= 0.99404 val_loss= 1.25621 val_acc= 0.66866 time= 0.29200
Epoch: 0110 train_loss= 0.07373 train_acc= 0.99471 val_loss= 1.25686 val_acc= 0.66567 time= 0.28900
Epoch: 0111 train_loss= 0.07229 train_acc= 0.99404 val_loss= 1.25879 val_acc= 0.66567 time= 0.29300
Epoch: 0112 train_loss= 0.06671 train_acc= 0.99603 val_loss= 1.26145 val_acc= 0.66567 time= 0.29197
Epoch: 0113 train_loss= 0.06617 train_acc= 0.99471 val_loss= 1.26444 val_acc= 0.66567 time= 0.31401
Epoch: 0114 train_loss= 0.06476 train_acc= 0.99537 val_loss= 1.26889 val_acc= 0.66567 time= 0.29704
Epoch: 0115 train_loss= 0.06428 train_acc= 0.99702 val_loss= 1.27330 val_acc= 0.66866 time= 0.29506
Epoch: 0116 train_loss= 0.06117 train_acc= 0.99669 val_loss= 1.27788 val_acc= 0.66567 time= 0.34697
Epoch: 0117 train_loss= 0.05787 train_acc= 0.99636 val_loss= 1.28233 val_acc= 0.66567 time= 0.32400
Epoch: 0118 train_loss= 0.05822 train_acc= 0.99504 val_loss= 1.28718 val_acc= 0.66269 time= 0.31271
Epoch: 0119 train_loss= 0.05555 train_acc= 0.99768 val_loss= 1.29240 val_acc= 0.66269 time= 0.30900
Epoch: 0120 train_loss= 0.05424 train_acc= 0.99669 val_loss= 1.29850 val_acc= 0.65970 time= 0.29800
Epoch: 0121 train_loss= 0.05242 train_acc= 0.99702 val_loss= 1.30376 val_acc= 0.65970 time= 0.30000
Epoch: 0122 train_loss= 0.05390 train_acc= 0.99801 val_loss= 1.30805 val_acc= 0.66269 time= 0.31600
Epoch: 0123 train_loss= 0.05138 train_acc= 0.99835 val_loss= 1.31064 val_acc= 0.66269 time= 0.30200
Epoch: 0124 train_loss= 0.04973 train_acc= 0.99669 val_loss= 1.31364 val_acc= 0.66269 time= 0.29800
Epoch: 0125 train_loss= 0.04696 train_acc= 0.99801 val_loss= 1.31677 val_acc= 0.66567 time= 0.30200
Epoch: 0126 train_loss= 0.04594 train_acc= 0.99868 val_loss= 1.31733 val_acc= 0.66866 time= 0.31300
Epoch: 0127 train_loss= 0.04431 train_acc= 0.99901 val_loss= 1.31748 val_acc= 0.66866 time= 0.29700
Early stopping...
Optimization Finished!
Test set results: cost= 1.29971 accuracy= 0.68786 time= 0.13500
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7321    0.6871    0.7089       342
           1     0.6864    0.7864    0.7330       103
           2     0.7054    0.6500    0.6766       140
           3     0.7288    0.5443    0.6232        79
           4     0.6985    0.7197    0.7090       132
           5     0.6980    0.7827    0.7380       313
           6     0.6972    0.7451    0.7204       102
           7     0.6512    0.4000    0.4956        70
           8     0.6970    0.4600    0.5542        50
           9     0.6129    0.7355    0.6686       155
          10     0.8657    0.6203    0.7227       187
          11     0.5887    0.6753    0.6290       231
          12     0.7688    0.6910    0.7278       178
          13     0.7708    0.8017    0.7859       600
          14     0.7786    0.8407    0.8085       590
          15     0.7432    0.7237    0.7333        76
          16     0.6667    0.4706    0.5517        34
          17     0.2000    0.1000    0.1333        10
          18     0.4179    0.4558    0.4361       419
          19     0.6593    0.4651    0.5455       129
          20     0.6129    0.6786    0.6441        28
          21     0.8750    0.7241    0.7925        29
          22     0.4688    0.3261    0.3846        46

    accuracy                         0.6879      4043
   macro avg     0.6663    0.6123    0.6314      4043
weighted avg     0.6917    0.6879    0.6857      4043

Macro average Test Precision, Recall and F1-Score...
(0.6662616258761019, 0.6123437498759884, 0.6314113630567831, None)
Micro average Test Precision, Recall and F1-Score...
(0.6878555528073212, 0.6878555528073212, 0.6878555528073212, None)
embeddings:
14157 3357 4043
[[ 0.30023858  0.43865678  0.45038435 ...  0.5899324   0.42200977
   0.29484442]
 [ 0.00414141 -0.02813453  0.18262927 ...  0.24048823  0.26784074
   0.12613444]
 [ 0.05997846  0.2274984   0.14153187 ...  0.24458085  0.4357821
   0.104371  ]
 ...
 [ 0.14082411  0.16717577  0.19621348 ...  0.25116098  0.21356384
   0.11491754]
 [ 0.13378733 -0.03359011  0.28130156 ...  0.16011724  0.44560182
   0.22703163]
 [ 0.16123247  0.16933052  0.1314291  ...  0.36717057  0.11131039
   0.16940516]]

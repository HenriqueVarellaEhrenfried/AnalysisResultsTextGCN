(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.04831 val_loss= 2.91875 val_acc= 0.20597 time= 0.58621
Epoch: 0002 train_loss= 2.92151 train_acc= 0.17968 val_loss= 2.73817 val_acc= 0.20597 time= 0.29700
Epoch: 0003 train_loss= 2.75699 train_acc= 0.17869 val_loss= 2.66781 val_acc= 0.20597 time= 0.28600
Epoch: 0004 train_loss= 2.68954 train_acc= 0.17604 val_loss= 2.54047 val_acc= 0.29254 time= 0.28700
Epoch: 0005 train_loss= 2.51763 train_acc= 0.26671 val_loss= 2.46213 val_acc= 0.32239 time= 0.29397
Epoch: 0006 train_loss= 2.41552 train_acc= 0.31502 val_loss= 2.33197 val_acc= 0.36716 time= 0.28703
Epoch: 0007 train_loss= 2.27429 train_acc= 0.38617 val_loss= 2.17386 val_acc= 0.40000 time= 0.28500
Epoch: 0008 train_loss= 2.10515 train_acc= 0.42422 val_loss= 2.04234 val_acc= 0.41194 time= 0.29097
Epoch: 0009 train_loss= 1.96187 train_acc= 0.45897 val_loss= 1.92771 val_acc= 0.47164 time= 0.29403
Epoch: 0010 train_loss= 1.81206 train_acc= 0.52151 val_loss= 1.82737 val_acc= 0.51940 time= 0.28897
Epoch: 0011 train_loss= 1.66381 train_acc= 0.57644 val_loss= 1.74634 val_acc= 0.52537 time= 0.28704
Epoch: 0012 train_loss= 1.51788 train_acc= 0.61350 val_loss= 1.67438 val_acc= 0.54328 time= 0.28899
Epoch: 0013 train_loss= 1.39214 train_acc= 0.64428 val_loss= 1.59711 val_acc= 0.55522 time= 0.29100
Epoch: 0014 train_loss= 1.27774 train_acc= 0.67803 val_loss= 1.51880 val_acc= 0.56716 time= 0.28800
Epoch: 0015 train_loss= 1.15847 train_acc= 0.70318 val_loss= 1.45403 val_acc= 0.58507 time= 0.28897
Epoch: 0016 train_loss= 1.05138 train_acc= 0.71774 val_loss= 1.40999 val_acc= 0.59701 time= 0.29003
Epoch: 0017 train_loss= 0.94816 train_acc= 0.73958 val_loss= 1.37858 val_acc= 0.60000 time= 0.29297
Epoch: 0018 train_loss= 0.85124 train_acc= 0.77763 val_loss= 1.35429 val_acc= 0.60896 time= 0.29103
Epoch: 0019 train_loss= 0.75736 train_acc= 0.80973 val_loss= 1.33162 val_acc= 0.62985 time= 0.29001
Epoch: 0020 train_loss= 0.67501 train_acc= 0.83190 val_loss= 1.29954 val_acc= 0.61194 time= 0.29198
Epoch: 0021 train_loss= 0.59054 train_acc= 0.84778 val_loss= 1.27622 val_acc= 0.64179 time= 0.28997
Epoch: 0022 train_loss= 0.51641 train_acc= 0.86102 val_loss= 1.26110 val_acc= 0.65075 time= 0.28900
Epoch: 0023 train_loss= 0.45081 train_acc= 0.87988 val_loss= 1.26476 val_acc= 0.63881 time= 0.29203
Epoch: 0024 train_loss= 0.39518 train_acc= 0.89775 val_loss= 1.26500 val_acc= 0.66567 time= 0.29466
Epoch: 0025 train_loss= 0.34527 train_acc= 0.91628 val_loss= 1.26584 val_acc= 0.65970 time= 0.29200
Epoch: 0026 train_loss= 0.29574 train_acc= 0.92952 val_loss= 1.28045 val_acc= 0.66866 time= 0.29200
Epoch: 0027 train_loss= 0.25974 train_acc= 0.93779 val_loss= 1.27481 val_acc= 0.68060 time= 0.28927
Epoch: 0028 train_loss= 0.22372 train_acc= 0.94639 val_loss= 1.28518 val_acc= 0.69254 time= 0.29500
Epoch: 0029 train_loss= 0.19362 train_acc= 0.95897 val_loss= 1.30646 val_acc= 0.69254 time= 0.28797
Early stopping...
Optimization Finished!
Test set results: cost= 1.31989 accuracy= 0.67945 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7217    0.6520    0.6851       342
           1     0.6557    0.7767    0.7111       103
           2     0.7091    0.5571    0.6240       140
           3     0.5714    0.4051    0.4741        79
           4     0.6940    0.7045    0.6992       132
           5     0.7087    0.7540    0.7307       313
           6     0.6789    0.7255    0.7014       102
           7     0.6154    0.3429    0.4404        70
           8     0.5588    0.3800    0.4524        50
           9     0.6706    0.7355    0.7015       155
          10     0.7853    0.6845    0.7314       187
          11     0.6040    0.6537    0.6279       231
          12     0.7514    0.7472    0.7493       178
          13     0.7689    0.7817    0.7752       600
          14     0.8060    0.8169    0.8114       590
          15     0.7397    0.7105    0.7248        76
          16     0.6364    0.4118    0.5000        34
          17     0.3333    0.1000    0.1538        10
          18     0.4011    0.5227    0.4539       419
          19     0.6250    0.5426    0.5809       129
          20     0.6071    0.6071    0.6071        28
          21     0.9545    0.7241    0.8235        29
          22     0.6522    0.3261    0.4348        46

    accuracy                         0.6794      4043
   macro avg     0.6630    0.5940    0.6171      4043
weighted avg     0.6871    0.6794    0.6791      4043

Macro average Test Precision, Recall and F1-Score...
(0.663013878577276, 0.5940116589022876, 0.6171330874182636, None)
Micro average Test Precision, Recall and F1-Score...
(0.6794459559732872, 0.6794459559732872, 0.6794459559732872, None)
embeddings:
14157 3357 4043
[[ 0.07358862  0.38685995  0.19406463 ...  0.6023834   0.3172048
   0.36962637]
 [ 0.12190674  0.36118168  0.08178893 ...  0.65768665  0.13308331
   0.0178421 ]
 [-0.01288625  0.17549725 -0.02771781 ...  0.6053366   0.40184486
   0.48568535]
 ...
 [ 0.04811579  0.14875036  0.1466187  ...  0.10182144  0.18216595
   0.12959822]
 [-0.04214528  0.63295215 -0.09413244 ...  0.9874633   0.04981124
  -0.023488  ]
 [ 0.00949709  0.06081524  0.1739512  ...  0.329883    0.07598338
   0.2738911 ]]

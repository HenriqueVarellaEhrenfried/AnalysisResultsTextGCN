(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13548 train_acc= 0.02250 val_loss= 3.11600 val_acc= 0.20597 time= 0.58199
Epoch: 0002 train_loss= 3.11613 train_acc= 0.18134 val_loss= 3.07213 val_acc= 0.20000 time= 0.29005
Epoch: 0003 train_loss= 3.07301 train_acc= 0.17637 val_loss= 3.00489 val_acc= 0.20000 time= 0.29400
Epoch: 0004 train_loss= 3.00772 train_acc= 0.17373 val_loss= 2.91984 val_acc= 0.20000 time= 0.29306
Epoch: 0005 train_loss= 2.92635 train_acc= 0.17373 val_loss= 2.83047 val_acc= 0.20000 time= 0.28811
Epoch: 0006 train_loss= 2.83823 train_acc= 0.17439 val_loss= 2.75527 val_acc= 0.20000 time= 0.29000
Epoch: 0007 train_loss= 2.76513 train_acc= 0.17505 val_loss= 2.70987 val_acc= 0.20000 time= 0.29500
Epoch: 0008 train_loss= 2.72563 train_acc= 0.17571 val_loss= 2.69834 val_acc= 0.20000 time= 0.28900
Epoch: 0009 train_loss= 2.71836 train_acc= 0.17472 val_loss= 2.70302 val_acc= 0.20000 time= 0.29200
Epoch: 0010 train_loss= 2.72264 train_acc= 0.17240 val_loss= 2.69913 val_acc= 0.20000 time= 0.29000
Epoch: 0011 train_loss= 2.72125 train_acc= 0.17141 val_loss= 2.68037 val_acc= 0.20000 time= 0.29630
Epoch: 0012 train_loss= 2.68754 train_acc= 0.17174 val_loss= 2.65539 val_acc= 0.20000 time= 0.29000
Epoch: 0013 train_loss= 2.65902 train_acc= 0.17240 val_loss= 2.63241 val_acc= 0.20597 time= 0.29100
Epoch: 0014 train_loss= 2.62567 train_acc= 0.17439 val_loss= 2.61364 val_acc= 0.21493 time= 0.29100
Epoch: 0015 train_loss= 2.60056 train_acc= 0.18465 val_loss= 2.59636 val_acc= 0.22985 time= 0.29726
Epoch: 0016 train_loss= 2.57472 train_acc= 0.19821 val_loss= 2.57740 val_acc= 0.25672 time= 0.29000
Epoch: 0017 train_loss= 2.55184 train_acc= 0.22369 val_loss= 2.55450 val_acc= 0.26269 time= 0.29100
Epoch: 0018 train_loss= 2.52937 train_acc= 0.24487 val_loss= 2.52705 val_acc= 0.28060 time= 0.28900
Epoch: 0019 train_loss= 2.49760 train_acc= 0.27366 val_loss= 2.49559 val_acc= 0.29254 time= 0.29686
Epoch: 0020 train_loss= 2.46697 train_acc= 0.28921 val_loss= 2.46154 val_acc= 0.30448 time= 0.29310
Epoch: 0021 train_loss= 2.43000 train_acc= 0.30212 val_loss= 2.42585 val_acc= 0.31045 time= 0.29000
Epoch: 0022 train_loss= 2.40291 train_acc= 0.29848 val_loss= 2.38963 val_acc= 0.31343 time= 0.29000
Epoch: 0023 train_loss= 2.35585 train_acc= 0.31568 val_loss= 2.35334 val_acc= 0.31940 time= 0.29900
Epoch: 0024 train_loss= 2.31741 train_acc= 0.31866 val_loss= 2.31730 val_acc= 0.32239 time= 0.29001
Epoch: 0025 train_loss= 2.28996 train_acc= 0.32296 val_loss= 2.28129 val_acc= 0.32537 time= 0.29032
Epoch: 0026 train_loss= 2.24454 train_acc= 0.34083 val_loss= 2.24533 val_acc= 0.34030 time= 0.29097
Epoch: 0027 train_loss= 2.19960 train_acc= 0.36102 val_loss= 2.20923 val_acc= 0.37313 time= 0.29403
Epoch: 0028 train_loss= 2.15485 train_acc= 0.39775 val_loss= 2.17286 val_acc= 0.38806 time= 0.29097
Epoch: 0029 train_loss= 2.11189 train_acc= 0.42158 val_loss= 2.13671 val_acc= 0.41791 time= 0.29000
Epoch: 0030 train_loss= 2.06369 train_acc= 0.45036 val_loss= 2.10016 val_acc= 0.45075 time= 0.29402
Epoch: 0031 train_loss= 2.01972 train_acc= 0.47551 val_loss= 2.06297 val_acc= 0.45373 time= 0.29401
Epoch: 0032 train_loss= 1.97187 train_acc= 0.48610 val_loss= 2.02546 val_acc= 0.46567 time= 0.28804
Epoch: 0033 train_loss= 1.92914 train_acc= 0.51588 val_loss= 1.98777 val_acc= 0.48358 time= 0.29300
Epoch: 0034 train_loss= 1.88127 train_acc= 0.52184 val_loss= 1.94980 val_acc= 0.48955 time= 0.29386
Epoch: 0035 train_loss= 1.83418 train_acc= 0.53938 val_loss= 1.91262 val_acc= 0.49552 time= 0.29700
Epoch: 0036 train_loss= 1.78516 train_acc= 0.53839 val_loss= 1.87650 val_acc= 0.49851 time= 0.28800
Epoch: 0037 train_loss= 1.75150 train_acc= 0.55129 val_loss= 1.84111 val_acc= 0.49851 time= 0.29300
Epoch: 0038 train_loss= 1.69960 train_acc= 0.55791 val_loss= 1.80656 val_acc= 0.51642 time= 0.29397
Epoch: 0039 train_loss= 1.65758 train_acc= 0.55725 val_loss= 1.77346 val_acc= 0.52239 time= 0.29904
Epoch: 0040 train_loss= 1.61136 train_acc= 0.57545 val_loss= 1.74085 val_acc= 0.52836 time= 0.29096
Epoch: 0041 train_loss= 1.58635 train_acc= 0.58140 val_loss= 1.70918 val_acc= 0.54328 time= 0.29003
Epoch: 0042 train_loss= 1.52936 train_acc= 0.60754 val_loss= 1.67902 val_acc= 0.55522 time= 0.29400
Epoch: 0043 train_loss= 1.48661 train_acc= 0.61218 val_loss= 1.65080 val_acc= 0.56119 time= 0.29448
Epoch: 0044 train_loss= 1.46045 train_acc= 0.61979 val_loss= 1.62465 val_acc= 0.56119 time= 0.29003
Epoch: 0045 train_loss= 1.40839 train_acc= 0.62872 val_loss= 1.59996 val_acc= 0.55522 time= 0.28997
Epoch: 0046 train_loss= 1.36538 train_acc= 0.63468 val_loss= 1.57541 val_acc= 0.55522 time= 0.29503
Epoch: 0047 train_loss= 1.34878 train_acc= 0.64097 val_loss= 1.54836 val_acc= 0.55522 time= 0.29400
Epoch: 0048 train_loss= 1.30919 train_acc= 0.65122 val_loss= 1.52261 val_acc= 0.56418 time= 0.29007
Epoch: 0049 train_loss= 1.26179 train_acc= 0.66281 val_loss= 1.49886 val_acc= 0.57015 time= 0.28897
Epoch: 0050 train_loss= 1.23309 train_acc= 0.67141 val_loss= 1.47642 val_acc= 0.58507 time= 0.29503
Epoch: 0051 train_loss= 1.18259 train_acc= 0.69193 val_loss= 1.45585 val_acc= 0.58507 time= 0.29598
Epoch: 0052 train_loss= 1.16725 train_acc= 0.69954 val_loss= 1.43829 val_acc= 0.58806 time= 0.28804
Epoch: 0053 train_loss= 1.13305 train_acc= 0.70748 val_loss= 1.42080 val_acc= 0.58806 time= 0.28999
Epoch: 0054 train_loss= 1.11129 train_acc= 0.70053 val_loss= 1.40469 val_acc= 0.59104 time= 0.29707
Epoch: 0055 train_loss= 1.07871 train_acc= 0.72932 val_loss= 1.38929 val_acc= 0.59701 time= 0.29399
Epoch: 0056 train_loss= 1.03618 train_acc= 0.72932 val_loss= 1.37394 val_acc= 0.60597 time= 0.28913
Epoch: 0057 train_loss= 1.02285 train_acc= 0.72634 val_loss= 1.35782 val_acc= 0.60896 time= 0.29300
Epoch: 0058 train_loss= 0.98997 train_acc= 0.73825 val_loss= 1.34305 val_acc= 0.60896 time= 0.29700
Epoch: 0059 train_loss= 0.96045 train_acc= 0.75116 val_loss= 1.33041 val_acc= 0.60597 time= 0.29103
Epoch: 0060 train_loss= 0.93227 train_acc= 0.75447 val_loss= 1.31709 val_acc= 0.62090 time= 0.29205
Epoch: 0061 train_loss= 0.90369 train_acc= 0.76770 val_loss= 1.30535 val_acc= 0.62388 time= 0.29297
Epoch: 0062 train_loss= 0.87270 train_acc= 0.76969 val_loss= 1.29433 val_acc= 0.62388 time= 0.29400
Epoch: 0063 train_loss= 0.85768 train_acc= 0.77862 val_loss= 1.28573 val_acc= 0.62388 time= 0.29203
Epoch: 0064 train_loss= 0.82828 train_acc= 0.78259 val_loss= 1.27716 val_acc= 0.62687 time= 0.29298
Epoch: 0065 train_loss= 0.81213 train_acc= 0.79153 val_loss= 1.26719 val_acc= 0.62388 time= 0.29003
Epoch: 0066 train_loss= 0.77632 train_acc= 0.80046 val_loss= 1.25794 val_acc= 0.62090 time= 0.29705
Epoch: 0067 train_loss= 0.78150 train_acc= 0.80013 val_loss= 1.24835 val_acc= 0.62090 time= 0.29203
Epoch: 0068 train_loss= 0.75407 train_acc= 0.80278 val_loss= 1.24053 val_acc= 0.62090 time= 0.28802
Epoch: 0069 train_loss= 0.70550 train_acc= 0.82065 val_loss= 1.23441 val_acc= 0.64179 time= 0.29098
Epoch: 0070 train_loss= 0.72119 train_acc= 0.81039 val_loss= 1.23017 val_acc= 0.63881 time= 0.29426
Epoch: 0071 train_loss= 0.69518 train_acc= 0.82760 val_loss= 1.22639 val_acc= 0.63582 time= 0.29271
Epoch: 0072 train_loss= 0.67380 train_acc= 0.83355 val_loss= 1.22152 val_acc= 0.62985 time= 0.29200
Epoch: 0073 train_loss= 0.66549 train_acc= 0.82793 val_loss= 1.21571 val_acc= 0.64179 time= 0.28803
Epoch: 0074 train_loss= 0.64637 train_acc= 0.84348 val_loss= 1.21015 val_acc= 0.63881 time= 0.29612
Epoch: 0075 train_loss= 0.62511 train_acc= 0.84050 val_loss= 1.20179 val_acc= 0.64179 time= 0.29003
Epoch: 0076 train_loss= 0.61512 train_acc= 0.84249 val_loss= 1.19395 val_acc= 0.64179 time= 0.29000
Epoch: 0077 train_loss= 0.59489 train_acc= 0.84646 val_loss= 1.18740 val_acc= 0.65373 time= 0.28900
Epoch: 0078 train_loss= 0.58126 train_acc= 0.85639 val_loss= 1.18243 val_acc= 0.65970 time= 0.29501
Epoch: 0079 train_loss= 0.57770 train_acc= 0.85440 val_loss= 1.18010 val_acc= 0.65672 time= 0.29200
Epoch: 0080 train_loss= 0.54801 train_acc= 0.86334 val_loss= 1.17822 val_acc= 0.65075 time= 0.28800
Epoch: 0081 train_loss= 0.53668 train_acc= 0.87492 val_loss= 1.17748 val_acc= 0.66269 time= 0.29100
Epoch: 0082 train_loss= 0.51936 train_acc= 0.87823 val_loss= 1.17771 val_acc= 0.64478 time= 0.29297
Epoch: 0083 train_loss= 0.49423 train_acc= 0.88352 val_loss= 1.17779 val_acc= 0.64776 time= 0.29003
Epoch: 0084 train_loss= 0.50533 train_acc= 0.87624 val_loss= 1.17336 val_acc= 0.65672 time= 0.28897
Epoch: 0085 train_loss= 0.49497 train_acc= 0.88021 val_loss= 1.16892 val_acc= 0.65672 time= 0.29308
Epoch: 0086 train_loss= 0.46757 train_acc= 0.88716 val_loss= 1.16519 val_acc= 0.66567 time= 0.29352
Epoch: 0087 train_loss= 0.46712 train_acc= 0.88782 val_loss= 1.16217 val_acc= 0.66866 time= 0.29011
Epoch: 0088 train_loss= 0.45859 train_acc= 0.89444 val_loss= 1.15977 val_acc= 0.66866 time= 0.29097
Epoch: 0089 train_loss= 0.43716 train_acc= 0.90139 val_loss= 1.16199 val_acc= 0.67463 time= 0.28903
Epoch: 0090 train_loss= 0.42077 train_acc= 0.90106 val_loss= 1.16693 val_acc= 0.67164 time= 0.29822
Epoch: 0091 train_loss= 0.42342 train_acc= 0.90437 val_loss= 1.16567 val_acc= 0.67164 time= 0.29203
Epoch: 0092 train_loss= 0.40296 train_acc= 0.91066 val_loss= 1.15991 val_acc= 0.67463 time= 0.29103
Epoch: 0093 train_loss= 0.39701 train_acc= 0.90735 val_loss= 1.15516 val_acc= 0.67463 time= 0.28822
Epoch: 0094 train_loss= 0.39498 train_acc= 0.90470 val_loss= 1.15490 val_acc= 0.67463 time= 0.29400
Epoch: 0095 train_loss= 0.38929 train_acc= 0.91628 val_loss= 1.15374 val_acc= 0.67761 time= 0.29400
Epoch: 0096 train_loss= 0.37678 train_acc= 0.90999 val_loss= 1.15303 val_acc= 0.67761 time= 0.29000
Epoch: 0097 train_loss= 0.37397 train_acc= 0.91099 val_loss= 1.15316 val_acc= 0.68060 time= 0.29200
Epoch: 0098 train_loss= 0.35523 train_acc= 0.91959 val_loss= 1.15517 val_acc= 0.68955 time= 0.29700
Epoch: 0099 train_loss= 0.35740 train_acc= 0.91628 val_loss= 1.15599 val_acc= 0.68657 time= 0.29049
Epoch: 0100 train_loss= 0.33831 train_acc= 0.92985 val_loss= 1.15614 val_acc= 0.67761 time= 0.28802
Epoch: 0101 train_loss= 0.33107 train_acc= 0.92488 val_loss= 1.15741 val_acc= 0.66866 time= 0.29000
Early stopping...
Optimization Finished!
Test set results: cost= 1.15906 accuracy= 0.68538 time= 0.13300
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7126    0.6959    0.7041       342
           1     0.7156    0.7573    0.7358       103
           2     0.7288    0.6143    0.6667       140
           3     0.6383    0.3797    0.4762        79
           4     0.6519    0.7803    0.7103       132
           5     0.6797    0.7796    0.7262       313
           6     0.6635    0.6765    0.6699       102
           7     0.5789    0.3143    0.4074        70
           8     0.6538    0.3400    0.4474        50
           9     0.6512    0.7226    0.6850       155
          10     0.8503    0.6684    0.7485       187
          11     0.6129    0.6580    0.6347       231
          12     0.7834    0.6910    0.7343       178
          13     0.7675    0.8033    0.7850       600
          14     0.7742    0.8542    0.8122       590
          15     0.7571    0.6974    0.7260        76
          16     0.6471    0.3235    0.4314        34
          17     0.5000    0.1000    0.1667        10
          18     0.4184    0.4893    0.4510       419
          19     0.6702    0.4884    0.5650       129
          20     0.6667    0.6429    0.6545        28
          21     1.0000    0.7241    0.8400        29
          22     0.5385    0.3043    0.3889        46

    accuracy                         0.6854      4043
   macro avg     0.6809    0.5872    0.6160      4043
weighted avg     0.6892    0.6854    0.6815      4043

Macro average Test Precision, Recall and F1-Score...
(0.6808942286504213, 0.5871880831228914, 0.6159737994460946, None)
Micro average Test Precision, Recall and F1-Score...
(0.6853821419737819, 0.6853821419737819, 0.6853821419737819, None)
embeddings:
14157 3357 4043
[[ 0.27687532  0.3495589   0.3270392  ...  0.26880503  0.31940138
   0.32504147]
 [ 0.16007103  0.04270158 -0.01452424 ...  0.05028135  0.20530003
   0.23562393]
 [ 0.31787616  0.09235055  0.18215384 ...  0.14363337  0.09375177
   0.41418964]
 ...
 [ 0.11827967  0.19768839  0.16272092 ...  0.13555856  0.08986401
   0.13565701]
 [ 0.04907816  0.29422536  0.03681634 ...  0.34099537  0.29477328
   0.19843745]
 [ 0.00671499  0.06727049  0.4439603  ...  0.22340728  0.15536006
   0.0965814 ]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13544 train_acc= 0.06453 val_loss= 3.10985 val_acc= 0.20597 time= 0.58486
Epoch: 0002 train_loss= 3.11046 train_acc= 0.17472 val_loss= 3.05727 val_acc= 0.20597 time= 0.29800
Epoch: 0003 train_loss= 3.05915 train_acc= 0.17340 val_loss= 2.98040 val_acc= 0.20299 time= 0.28999
Epoch: 0004 train_loss= 2.98294 train_acc= 0.17439 val_loss= 2.88837 val_acc= 0.20597 time= 0.28600
Epoch: 0005 train_loss= 2.89214 train_acc= 0.17340 val_loss= 2.79848 val_acc= 0.20597 time= 0.28500
Epoch: 0006 train_loss= 2.80582 train_acc= 0.17406 val_loss= 2.73060 val_acc= 0.20597 time= 0.29386
Epoch: 0007 train_loss= 2.74025 train_acc= 0.17704 val_loss= 2.69879 val_acc= 0.20597 time= 0.28700
Epoch: 0008 train_loss= 2.71205 train_acc= 0.17935 val_loss= 2.69677 val_acc= 0.20597 time= 0.28700
Epoch: 0009 train_loss= 2.71017 train_acc= 0.17803 val_loss= 2.69711 val_acc= 0.20597 time= 0.29000
Epoch: 0010 train_loss= 2.71102 train_acc= 0.17737 val_loss= 2.68139 val_acc= 0.20896 time= 0.29620
Epoch: 0011 train_loss= 2.68964 train_acc= 0.17670 val_loss= 2.65339 val_acc= 0.21493 time= 0.29000
Epoch: 0012 train_loss= 2.65225 train_acc= 0.18795 val_loss= 2.62448 val_acc= 0.22985 time= 0.29100
Epoch: 0013 train_loss= 2.61219 train_acc= 0.19490 val_loss= 2.60031 val_acc= 0.24776 time= 0.28900
Epoch: 0014 train_loss= 2.58051 train_acc= 0.22005 val_loss= 2.57955 val_acc= 0.26269 time= 0.29300
Epoch: 0015 train_loss= 2.55656 train_acc= 0.24090 val_loss= 2.55822 val_acc= 0.28060 time= 0.28700
Epoch: 0016 train_loss= 2.53153 train_acc= 0.25877 val_loss= 2.53327 val_acc= 0.28955 time= 0.28800
Epoch: 0017 train_loss= 2.50198 train_acc= 0.27730 val_loss= 2.50353 val_acc= 0.30746 time= 0.28800
Epoch: 0018 train_loss= 2.46989 train_acc= 0.28921 val_loss= 2.46948 val_acc= 0.31045 time= 0.29300
Epoch: 0019 train_loss= 2.43456 train_acc= 0.30013 val_loss= 2.43238 val_acc= 0.31343 time= 0.28800
Epoch: 0020 train_loss= 2.39839 train_acc= 0.30973 val_loss= 2.39352 val_acc= 0.31642 time= 0.29100
Epoch: 0021 train_loss= 2.35465 train_acc= 0.31436 val_loss= 2.35410 val_acc= 0.32239 time= 0.29216
Epoch: 0022 train_loss= 2.30982 train_acc= 0.32561 val_loss= 2.31466 val_acc= 0.32239 time= 0.29200
Epoch: 0023 train_loss= 2.26737 train_acc= 0.32958 val_loss= 2.27526 val_acc= 0.32537 time= 0.29278
Epoch: 0024 train_loss= 2.22321 train_acc= 0.34017 val_loss= 2.23587 val_acc= 0.33433 time= 0.28900
Epoch: 0025 train_loss= 2.17633 train_acc= 0.35440 val_loss= 2.19654 val_acc= 0.35224 time= 0.29249
Epoch: 0026 train_loss= 2.12759 train_acc= 0.38385 val_loss= 2.15737 val_acc= 0.37910 time= 0.29503
Epoch: 0027 train_loss= 2.07796 train_acc= 0.41396 val_loss= 2.11858 val_acc= 0.40896 time= 0.30000
Epoch: 0028 train_loss= 2.03067 train_acc= 0.44639 val_loss= 2.08031 val_acc= 0.43284 time= 0.28700
Epoch: 0029 train_loss= 1.98287 train_acc= 0.48743 val_loss= 2.04225 val_acc= 0.45970 time= 0.29155
Epoch: 0030 train_loss= 1.93048 train_acc= 0.51952 val_loss= 2.00367 val_acc= 0.49254 time= 0.29403
Epoch: 0031 train_loss= 1.88204 train_acc= 0.54897 val_loss= 1.96388 val_acc= 0.49851 time= 0.28897
Epoch: 0032 train_loss= 1.83368 train_acc= 0.56453 val_loss= 1.92307 val_acc= 0.50448 time= 0.28703
Epoch: 0033 train_loss= 1.77943 train_acc= 0.57015 val_loss= 1.88218 val_acc= 0.50746 time= 0.29318
Epoch: 0034 train_loss= 1.72184 train_acc= 0.58769 val_loss= 1.84195 val_acc= 0.51642 time= 0.29000
Epoch: 0035 train_loss= 1.66738 train_acc= 0.58901 val_loss= 1.80262 val_acc= 0.51940 time= 0.29000
Epoch: 0036 train_loss= 1.61236 train_acc= 0.60457 val_loss= 1.76523 val_acc= 0.52239 time= 0.28797
Epoch: 0037 train_loss= 1.56711 train_acc= 0.61085 val_loss= 1.72980 val_acc= 0.53731 time= 0.29100
Epoch: 0038 train_loss= 1.52286 train_acc= 0.61416 val_loss= 1.69561 val_acc= 0.53433 time= 0.29300
Epoch: 0039 train_loss= 1.46701 train_acc= 0.63104 val_loss= 1.66205 val_acc= 0.54925 time= 0.28803
Epoch: 0040 train_loss= 1.42197 train_acc= 0.63964 val_loss= 1.62932 val_acc= 0.55821 time= 0.28916
Epoch: 0041 train_loss= 1.36920 train_acc= 0.65387 val_loss= 1.59701 val_acc= 0.56418 time= 0.29701
Epoch: 0042 train_loss= 1.32117 train_acc= 0.66810 val_loss= 1.56656 val_acc= 0.57313 time= 0.29303
Epoch: 0043 train_loss= 1.28372 train_acc= 0.67306 val_loss= 1.53727 val_acc= 0.58507 time= 0.28800
Epoch: 0044 train_loss= 1.23459 train_acc= 0.68134 val_loss= 1.50915 val_acc= 0.58806 time= 0.28700
Epoch: 0045 train_loss= 1.19337 train_acc= 0.69887 val_loss= 1.48149 val_acc= 0.58806 time= 0.29700
Epoch: 0046 train_loss= 1.14630 train_acc= 0.71377 val_loss= 1.45638 val_acc= 0.59701 time= 0.29100
Epoch: 0047 train_loss= 1.10124 train_acc= 0.72270 val_loss= 1.43302 val_acc= 0.60299 time= 0.29000
Epoch: 0048 train_loss= 1.06367 train_acc= 0.74255 val_loss= 1.41049 val_acc= 0.60896 time= 0.28800
Epoch: 0049 train_loss= 1.01332 train_acc= 0.75480 val_loss= 1.38987 val_acc= 0.60896 time= 0.29504
Epoch: 0050 train_loss= 0.98910 train_acc= 0.76009 val_loss= 1.37012 val_acc= 0.60896 time= 0.29300
Epoch: 0051 train_loss= 0.95989 train_acc= 0.76506 val_loss= 1.35098 val_acc= 0.61194 time= 0.28900
Epoch: 0052 train_loss= 0.91488 train_acc= 0.77366 val_loss= 1.33315 val_acc= 0.62090 time= 0.28900
Epoch: 0053 train_loss= 0.87643 train_acc= 0.78458 val_loss= 1.31575 val_acc= 0.61791 time= 0.29219
Epoch: 0054 train_loss= 0.84463 train_acc= 0.79517 val_loss= 1.30076 val_acc= 0.63284 time= 0.29000
Epoch: 0055 train_loss= 0.80854 train_acc= 0.80741 val_loss= 1.28611 val_acc= 0.62687 time= 0.28700
Epoch: 0056 train_loss= 0.78581 train_acc= 0.80510 val_loss= 1.27287 val_acc= 0.62687 time= 0.29100
Epoch: 0057 train_loss= 0.75370 train_acc= 0.81535 val_loss= 1.26021 val_acc= 0.63582 time= 0.29200
Epoch: 0058 train_loss= 0.72420 train_acc= 0.83024 val_loss= 1.24852 val_acc= 0.62687 time= 0.28900
Epoch: 0059 train_loss= 0.69646 train_acc= 0.83951 val_loss= 1.23662 val_acc= 0.64179 time= 0.28800
Epoch: 0060 train_loss= 0.66813 train_acc= 0.83885 val_loss= 1.22667 val_acc= 0.63284 time= 0.28700
Epoch: 0061 train_loss= 0.64340 train_acc= 0.84811 val_loss= 1.21829 val_acc= 0.63881 time= 0.29526
Epoch: 0062 train_loss= 0.61807 train_acc= 0.85639 val_loss= 1.21133 val_acc= 0.64179 time= 0.28899
Epoch: 0063 train_loss= 0.59126 train_acc= 0.86367 val_loss= 1.20535 val_acc= 0.63881 time= 0.28900
Epoch: 0064 train_loss= 0.57346 train_acc= 0.86995 val_loss= 1.19795 val_acc= 0.64478 time= 0.28900
Epoch: 0065 train_loss= 0.54748 train_acc= 0.87723 val_loss= 1.18911 val_acc= 0.65075 time= 0.29200
Epoch: 0066 train_loss= 0.53271 train_acc= 0.88451 val_loss= 1.17978 val_acc= 0.64776 time= 0.28899
Epoch: 0067 train_loss= 0.51126 train_acc= 0.88584 val_loss= 1.17403 val_acc= 0.65373 time= 0.28800
Epoch: 0068 train_loss= 0.48874 train_acc= 0.89113 val_loss= 1.17119 val_acc= 0.65373 time= 0.28800
Epoch: 0069 train_loss= 0.47062 train_acc= 0.88915 val_loss= 1.16858 val_acc= 0.65075 time= 0.29399
Epoch: 0070 train_loss= 0.45150 train_acc= 0.89676 val_loss= 1.16562 val_acc= 0.65970 time= 0.29100
Epoch: 0071 train_loss= 0.43062 train_acc= 0.90437 val_loss= 1.16252 val_acc= 0.65970 time= 0.28800
Epoch: 0072 train_loss= 0.42430 train_acc= 0.90867 val_loss= 1.15750 val_acc= 0.65970 time= 0.28600
Epoch: 0073 train_loss= 0.39871 train_acc= 0.91463 val_loss= 1.15217 val_acc= 0.65970 time= 0.29300
Epoch: 0074 train_loss= 0.39082 train_acc= 0.91562 val_loss= 1.14834 val_acc= 0.65672 time= 0.28700
Epoch: 0075 train_loss= 0.37166 train_acc= 0.92091 val_loss= 1.14550 val_acc= 0.65075 time= 0.28903
Epoch: 0076 train_loss= 0.35324 train_acc= 0.92720 val_loss= 1.14393 val_acc= 0.65373 time= 0.28600
Epoch: 0077 train_loss= 0.34480 train_acc= 0.92852 val_loss= 1.14194 val_acc= 0.66269 time= 0.29400
Epoch: 0078 train_loss= 0.33507 train_acc= 0.93349 val_loss= 1.14187 val_acc= 0.66269 time= 0.29000
Epoch: 0079 train_loss= 0.31443 train_acc= 0.94044 val_loss= 1.14364 val_acc= 0.65373 time= 0.28800
Epoch: 0080 train_loss= 0.30680 train_acc= 0.94242 val_loss= 1.14797 val_acc= 0.64776 time= 0.28800
Epoch: 0081 train_loss= 0.29612 train_acc= 0.94408 val_loss= 1.15030 val_acc= 0.65075 time= 0.29200
Early stopping...
Optimization Finished!
Test set results: cost= 1.16168 accuracy= 0.68044 time= 0.13000
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7314    0.6608    0.6943       342
           1     0.7054    0.7670    0.7349       103
           2     0.7843    0.5714    0.6612       140
           3     0.5862    0.4304    0.4964        79
           4     0.6168    0.7803    0.6890       132
           5     0.6863    0.7827    0.7313       313
           6     0.6789    0.7255    0.7014       102
           7     0.5833    0.3000    0.3962        70
           8     0.6071    0.3400    0.4359        50
           9     0.6369    0.7355    0.6826       155
          10     0.8356    0.6524    0.7327       187
          11     0.6320    0.6320    0.6320       231
          12     0.7871    0.6854    0.7327       178
          13     0.7862    0.7967    0.7914       600
          14     0.7816    0.8373    0.8085       590
          15     0.7619    0.6316    0.6906        76
          16     0.7692    0.2941    0.4255        34
          17     1.0000    0.1000    0.1818        10
          18     0.3892    0.5155    0.4435       419
          19     0.6442    0.5194    0.5751       129
          20     0.6296    0.6071    0.6182        28
          21     0.9565    0.7586    0.8462        29
          22     0.5357    0.3261    0.4054        46

    accuracy                         0.6804      4043
   macro avg     0.7011    0.5848    0.6133      4043
weighted avg     0.6925    0.6804    0.6793      4043

Macro average Test Precision, Recall and F1-Score...
(0.701111125118044, 0.5847770088344868, 0.6133466808737752, None)
Micro average Test Precision, Recall and F1-Score...
(0.680435320306703, 0.680435320306703, 0.680435320306703, None)
embeddings:
14157 3357 4043
[[ 0.16435635  0.3873026   0.41157067 ...  0.37392402  0.32015747
   0.43201536]
 [ 0.16281658  0.09407902  0.19959208 ...  0.14990678 -0.06994689
   0.10679303]
 [ 0.40814993  0.19022001  0.58461624 ...  0.3188357   0.41471308
   0.20897661]
 ...
 [ 0.1736576   0.08853661  0.13182779 ...  0.13046378  0.13419612
   0.15053892]
 [ 0.20942207  0.08634378 -0.00201351 ...  0.0110881  -0.05917097
   0.2502596 ]
 [ 0.09907009  0.14687979  0.10956363 ...  0.20831776  0.13627109
   0.13323177]]

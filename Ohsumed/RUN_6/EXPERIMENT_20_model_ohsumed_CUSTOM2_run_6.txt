(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13542 train_acc= 0.07942 val_loss= 2.90302 val_acc= 0.20597 time= 0.58405
Epoch: 0002 train_loss= 2.90820 train_acc= 0.17704 val_loss= 2.73979 val_acc= 0.20597 time= 0.29099
Epoch: 0003 train_loss= 2.76667 train_acc= 0.17538 val_loss= 2.64148 val_acc= 0.20299 time= 0.29197
Epoch: 0004 train_loss= 2.65635 train_acc= 0.17240 val_loss= 2.53417 val_acc= 0.27761 time= 0.29397
Epoch: 0005 train_loss= 2.51120 train_acc= 0.25811 val_loss= 2.44896 val_acc= 0.31343 time= 0.28700
Epoch: 0006 train_loss= 2.40452 train_acc= 0.31635 val_loss= 2.30439 val_acc= 0.37910 time= 0.28397
Epoch: 0007 train_loss= 2.25524 train_acc= 0.37723 val_loss= 2.14516 val_acc= 0.38507 time= 0.29103
Epoch: 0008 train_loss= 2.07692 train_acc= 0.41827 val_loss= 2.01971 val_acc= 0.42687 time= 0.29297
Epoch: 0009 train_loss= 1.92785 train_acc= 0.48511 val_loss= 1.90544 val_acc= 0.51045 time= 0.28803
Epoch: 0010 train_loss= 1.77336 train_acc= 0.55129 val_loss= 1.80467 val_acc= 0.52537 time= 0.28701
Epoch: 0011 train_loss= 1.61995 train_acc= 0.59365 val_loss= 1.72435 val_acc= 0.54030 time= 0.28699
Epoch: 0012 train_loss= 1.47858 train_acc= 0.62541 val_loss= 1.64847 val_acc= 0.55224 time= 0.29509
Epoch: 0013 train_loss= 1.34192 train_acc= 0.66314 val_loss= 1.56557 val_acc= 0.56418 time= 0.29015
Epoch: 0014 train_loss= 1.22430 train_acc= 0.69490 val_loss= 1.49146 val_acc= 0.57612 time= 0.28600
Epoch: 0015 train_loss= 1.11286 train_acc= 0.70814 val_loss= 1.43889 val_acc= 0.58507 time= 0.29200
Epoch: 0016 train_loss= 1.00992 train_acc= 0.72634 val_loss= 1.39951 val_acc= 0.60896 time= 0.29333
Epoch: 0017 train_loss= 0.90132 train_acc= 0.75844 val_loss= 1.37659 val_acc= 0.60597 time= 0.29022
Epoch: 0018 train_loss= 0.80953 train_acc= 0.78855 val_loss= 1.36778 val_acc= 0.61194 time= 0.28500
Epoch: 0019 train_loss= 0.72769 train_acc= 0.81337 val_loss= 1.33439 val_acc= 0.62090 time= 0.28600
Epoch: 0020 train_loss= 0.64302 train_acc= 0.83322 val_loss= 1.30076 val_acc= 0.62388 time= 0.29400
Epoch: 0021 train_loss= 0.56362 train_acc= 0.84547 val_loss= 1.28259 val_acc= 0.61791 time= 0.28705
Epoch: 0022 train_loss= 0.49228 train_acc= 0.86764 val_loss= 1.28033 val_acc= 0.63284 time= 0.28597
Epoch: 0023 train_loss= 0.41845 train_acc= 0.89576 val_loss= 1.28415 val_acc= 0.65373 time= 0.28503
Epoch: 0024 train_loss= 0.37588 train_acc= 0.91032 val_loss= 1.30963 val_acc= 0.63284 time= 0.29700
Early stopping...
Optimization Finished!
Test set results: cost= 1.31675 accuracy= 0.65595 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7195    0.6374    0.6760       342
           1     0.6577    0.7087    0.6822       103
           2     0.7071    0.5000    0.5858       140
           3     0.4681    0.2785    0.3492        79
           4     0.6139    0.7348    0.6690       132
           5     0.6930    0.7859    0.7365       313
           6     0.6066    0.7255    0.6607       102
           7     0.4828    0.2000    0.2828        70
           8     0.5517    0.3200    0.4051        50
           9     0.6333    0.7355    0.6806       155
          10     0.7457    0.6898    0.7167       187
          11     0.6359    0.5974    0.6161       231
          12     0.7736    0.6910    0.7300       178
          13     0.7915    0.7717    0.7814       600
          14     0.8024    0.7915    0.7969       590
          15     0.7692    0.6579    0.7092        76
          16     0.5625    0.2647    0.3600        34
          17     0.0000    0.0000    0.0000        10
          18     0.3495    0.5346    0.4226       419
          19     0.5455    0.5116    0.5280       129
          20     0.7333    0.3929    0.5116        28
          21     1.0000    0.4483    0.6190        29
          22     0.6522    0.3261    0.4348        46

    accuracy                         0.6559      4043
   macro avg     0.6302    0.5350    0.5632      4043
weighted avg     0.6716    0.6559    0.6559      4043

Macro average Test Precision, Recall and F1-Score...
(0.6302060895383774, 0.5349526526106093, 0.5632295034522232, None)
Micro average Test Precision, Recall and F1-Score...
(0.6559485530546624, 0.6559485530546624, 0.6559485530546624, None)
embeddings:
14157 3357 4043
[[ 0.32089856 -0.69224584  0.21719341 ...  0.18711278  0.42144212
   0.48757654]
 [ 0.08310652 -0.1742484   0.20348887 ... -0.06748907  0.15073836
   0.26988953]
 [ 0.16111583 -0.43195313  0.40084383 ... -0.04631725  0.34737343
   0.44016698]
 ...
 [ 0.17589273 -0.21576832  0.12353146 ...  0.08490387  0.19461505
   0.35276023]
 [-0.11450162 -0.22912034 -0.04884031 ... -0.17285584  0.44085315
  -0.0630687 ]
 [ 0.16184694 -0.21235618  0.20920332 ...  0.20712377  0.13355313
   0.20677876]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.04567 val_loss= 3.10016 val_acc= 0.24478 time= 5.82805
Epoch: 0002 train_loss= 3.10005 train_acc= 0.20946 val_loss= 3.00951 val_acc= 0.22985 time= 5.69802
Epoch: 0003 train_loss= 3.00969 train_acc= 0.19292 val_loss= 2.87949 val_acc= 0.22388 time= 5.67163
Epoch: 0004 train_loss= 2.88117 train_acc= 0.19093 val_loss= 2.75998 val_acc= 0.22985 time= 5.68900
Epoch: 0005 train_loss= 2.76584 train_acc= 0.19656 val_loss= 2.70146 val_acc= 0.23582 time= 5.70508
Epoch: 0006 train_loss= 2.71235 train_acc= 0.19888 val_loss= 2.70252 val_acc= 0.20896 time= 5.69658
Epoch: 0007 train_loss= 2.71649 train_acc= 0.17803 val_loss= 2.69313 val_acc= 0.20299 time= 5.68999
Epoch: 0008 train_loss= 2.70421 train_acc= 0.17240 val_loss= 2.65256 val_acc= 0.20597 time= 5.66597
Epoch: 0009 train_loss= 2.64994 train_acc= 0.17406 val_loss= 2.60914 val_acc= 0.21194 time= 5.68502
Epoch: 0010 train_loss= 2.59063 train_acc= 0.18498 val_loss= 2.57644 val_acc= 0.24478 time= 5.68500
Epoch: 0011 train_loss= 2.54516 train_acc= 0.21046 val_loss= 2.54690 val_acc= 0.27463 time= 5.69599
Epoch: 0012 train_loss= 2.50528 train_acc= 0.25182 val_loss= 2.51090 val_acc= 0.29851 time= 5.70801
Epoch: 0013 train_loss= 2.46149 train_acc= 0.30377 val_loss= 2.46449 val_acc= 0.32239 time= 5.71014
Epoch: 0014 train_loss= 2.41208 train_acc= 0.34944 val_loss= 2.40850 val_acc= 0.34030 time= 5.68400
Epoch: 0015 train_loss= 2.35147 train_acc= 0.37690 val_loss= 2.34679 val_acc= 0.35821 time= 5.73604
Epoch: 0016 train_loss= 2.28476 train_acc= 0.38981 val_loss= 2.28345 val_acc= 0.36119 time= 5.69814
Epoch: 0017 train_loss= 2.21506 train_acc= 0.38650 val_loss= 2.22101 val_acc= 0.36119 time= 5.67120
Epoch: 0018 train_loss= 2.14565 train_acc= 0.39510 val_loss= 2.16020 val_acc= 0.36418 time= 5.71399
Epoch: 0019 train_loss= 2.07411 train_acc= 0.41430 val_loss= 2.10088 val_acc= 0.38806 time= 5.69056
Epoch: 0020 train_loss= 1.99713 train_acc= 0.44308 val_loss= 2.04300 val_acc= 0.42687 time= 5.67705
Epoch: 0021 train_loss= 1.92467 train_acc= 0.49107 val_loss= 1.98744 val_acc= 0.47164 time= 5.69359
Epoch: 0022 train_loss= 1.84714 train_acc= 0.53408 val_loss= 1.93458 val_acc= 0.51343 time= 5.64903
Epoch: 0023 train_loss= 1.77515 train_acc= 0.57876 val_loss= 1.88213 val_acc= 0.52537 time= 5.68500
Epoch: 0024 train_loss= 1.70316 train_acc= 0.59993 val_loss= 1.82765 val_acc= 0.53134 time= 5.67801
Epoch: 0025 train_loss= 1.62780 train_acc= 0.61284 val_loss= 1.77212 val_acc= 0.53134 time= 5.69199
Epoch: 0026 train_loss= 1.55367 train_acc= 0.62442 val_loss= 1.71816 val_acc= 0.52836 time= 5.70101
Epoch: 0027 train_loss= 1.48354 train_acc= 0.62938 val_loss= 1.66768 val_acc= 0.53731 time= 5.68812
Epoch: 0028 train_loss= 1.41151 train_acc= 0.64361 val_loss= 1.62208 val_acc= 0.54925 time= 5.66807
Epoch: 0029 train_loss= 1.34683 train_acc= 0.67340 val_loss= 1.58039 val_acc= 0.55522 time= 5.70301
Epoch: 0030 train_loss= 1.28344 train_acc= 0.68498 val_loss= 1.54133 val_acc= 0.57612 time= 5.67300
Epoch: 0031 train_loss= 1.21744 train_acc= 0.69623 val_loss= 1.50574 val_acc= 0.57910 time= 5.66402
Epoch: 0032 train_loss= 1.15791 train_acc= 0.70847 val_loss= 1.47323 val_acc= 0.57910 time= 5.68206
Epoch: 0033 train_loss= 1.09466 train_acc= 0.72138 val_loss= 1.44140 val_acc= 0.59104 time= 5.79400
Epoch: 0034 train_loss= 1.03599 train_acc= 0.73792 val_loss= 1.41174 val_acc= 0.60000 time= 5.77088
Epoch: 0035 train_loss= 0.98278 train_acc= 0.75281 val_loss= 1.38446 val_acc= 0.60000 time= 5.69000
Epoch: 0036 train_loss= 0.93080 train_acc= 0.76969 val_loss= 1.35804 val_acc= 0.60000 time= 5.67200
Epoch: 0037 train_loss= 0.88007 train_acc= 0.77995 val_loss= 1.33258 val_acc= 0.62090 time= 5.77600
Epoch: 0038 train_loss= 0.82841 train_acc= 0.79881 val_loss= 1.31008 val_acc= 0.62687 time= 5.66712
Epoch: 0039 train_loss= 0.77980 train_acc= 0.80940 val_loss= 1.29002 val_acc= 0.61194 time= 5.69900
Epoch: 0040 train_loss= 0.73360 train_acc= 0.82462 val_loss= 1.27291 val_acc= 0.60896 time= 5.66034
Epoch: 0041 train_loss= 0.68796 train_acc= 0.83719 val_loss= 1.25732 val_acc= 0.60896 time= 5.72593
Epoch: 0042 train_loss= 0.64909 train_acc= 0.84348 val_loss= 1.24429 val_acc= 0.61493 time= 5.67520
Epoch: 0043 train_loss= 0.61028 train_acc= 0.85043 val_loss= 1.23375 val_acc= 0.62090 time= 5.70800
Epoch: 0044 train_loss= 0.56935 train_acc= 0.86764 val_loss= 1.22426 val_acc= 0.62388 time= 5.70594
Epoch: 0045 train_loss= 0.53563 train_acc= 0.87922 val_loss= 1.21476 val_acc= 0.63582 time= 5.67901
Epoch: 0046 train_loss= 0.49933 train_acc= 0.88319 val_loss= 1.20656 val_acc= 0.65075 time= 5.66701
Epoch: 0047 train_loss= 0.46980 train_acc= 0.89047 val_loss= 1.19741 val_acc= 0.64776 time= 5.67642
Epoch: 0048 train_loss= 0.44126 train_acc= 0.90271 val_loss= 1.18992 val_acc= 0.63582 time= 5.68098
Epoch: 0049 train_loss= 0.40969 train_acc= 0.91330 val_loss= 1.18551 val_acc= 0.65075 time= 5.68584
Epoch: 0050 train_loss= 0.38506 train_acc= 0.91926 val_loss= 1.18104 val_acc= 0.65075 time= 5.66800
Epoch: 0051 train_loss= 0.36195 train_acc= 0.92422 val_loss= 1.17839 val_acc= 0.67164 time= 5.65801
Epoch: 0052 train_loss= 0.33687 train_acc= 0.93249 val_loss= 1.17622 val_acc= 0.66866 time= 5.66299
Epoch: 0053 train_loss= 0.31163 train_acc= 0.92985 val_loss= 1.17520 val_acc= 0.66567 time= 5.68896
Epoch: 0054 train_loss= 0.29399 train_acc= 0.94110 val_loss= 1.17622 val_acc= 0.66269 time= 5.68812
Epoch: 0055 train_loss= 0.27383 train_acc= 0.94739 val_loss= 1.17666 val_acc= 0.66269 time= 5.68299
Epoch: 0056 train_loss= 0.25508 train_acc= 0.95367 val_loss= 1.17807 val_acc= 0.66866 time= 5.67800
Epoch: 0057 train_loss= 0.23937 train_acc= 0.95731 val_loss= 1.17771 val_acc= 0.66567 time= 5.66500
Epoch: 0058 train_loss= 0.22294 train_acc= 0.96128 val_loss= 1.17974 val_acc= 0.67463 time= 5.68400
Early stopping...
Optimization Finished!
Test set results: cost= 1.17079 accuracy= 0.68662 time= 1.95900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7281    0.6813    0.7039       342
           1     0.6724    0.7573    0.7123       103
           2     0.7395    0.6286    0.6795       140
           3     0.5238    0.4177    0.4648        79
           4     0.6806    0.7424    0.7101       132
           5     0.6989    0.7859    0.7398       313
           6     0.7059    0.7059    0.7059       102
           7     0.6286    0.3143    0.4190        70
           8     0.5676    0.4200    0.4828        50
           9     0.6188    0.7226    0.6667       155
          10     0.8435    0.6631    0.7425       187
          11     0.6287    0.6450    0.6368       231
          12     0.7636    0.7079    0.7347       178
          13     0.7626    0.8083    0.7848       600
          14     0.7888    0.8356    0.8115       590
          15     0.7671    0.7368    0.7517        76
          16     0.6667    0.3529    0.4615        34
          17     0.5000    0.1000    0.1667        10
          18     0.4216    0.4940    0.4549       419
          19     0.6381    0.5194    0.5726       129
          20     0.6071    0.6071    0.6071        28
          21     0.9565    0.7586    0.8462        29
          22     0.5833    0.3043    0.4000        46

    accuracy                         0.6866      4043
   macro avg     0.6736    0.5961    0.6198      4043
weighted avg     0.6902    0.6866    0.6840      4043

Macro average Test Precision, Recall and F1-Score...
(0.673555881801729, 0.5960521470203826, 0.6198251763482284, None)
Micro average Test Precision, Recall and F1-Score...
(0.6866188473905516, 0.6866188473905516, 0.6866188473905516, None)
embeddings:
14157 3357 4043
[[ 0.17097707  0.39736336  0.28619298 ...  0.29810795  0.266153
   0.332736  ]
 [ 0.10507639  0.16007343  0.07149893 ...  0.09533875  0.13416645
   0.23461212]
 [ 0.07636779  0.3815115   0.15547697 ...  0.24156344  0.16000585
   0.38473025]
 ...
 [ 0.02725125  0.18013598  0.12622924 ...  0.11588041  0.08654173
   0.06373466]
 [ 0.17580546 -0.08610246  0.03451084 ...  0.2546505   0.15973787
   0.2487244 ]
 [ 0.11287331  0.14904527  0.18820848 ...  0.17772369  0.16401796
   0.07008303]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.02846 val_loss= 3.11496 val_acc= 0.29254 time= 0.58035
Epoch: 0002 train_loss= 3.11510 train_acc= 0.27962 val_loss= 3.07021 val_acc= 0.29851 time= 0.29897
Epoch: 0003 train_loss= 3.07079 train_acc= 0.27862 val_loss= 3.00191 val_acc= 0.29851 time= 0.29600
Epoch: 0004 train_loss= 3.00340 train_acc= 0.28193 val_loss= 2.91672 val_acc= 0.29254 time= 0.28903
Epoch: 0005 train_loss= 2.92001 train_acc= 0.27565 val_loss= 2.82907 val_acc= 0.29254 time= 0.28697
Epoch: 0006 train_loss= 2.83278 train_acc= 0.27002 val_loss= 2.75498 val_acc= 0.28955 time= 0.29294
Epoch: 0007 train_loss= 2.76016 train_acc= 0.26307 val_loss= 2.70838 val_acc= 0.28060 time= 0.29203
Epoch: 0008 train_loss= 2.71512 train_acc= 0.25149 val_loss= 2.69456 val_acc= 0.24478 time= 0.28700
Epoch: 0009 train_loss= 2.70213 train_acc= 0.22105 val_loss= 2.69397 val_acc= 0.20597 time= 0.29200
Epoch: 0010 train_loss= 2.70340 train_acc= 0.18001 val_loss= 2.68533 val_acc= 0.20597 time= 0.29100
Epoch: 0011 train_loss= 2.69046 train_acc= 0.17340 val_loss= 2.66218 val_acc= 0.20597 time= 0.29403
Epoch: 0012 train_loss= 2.65953 train_acc= 0.17273 val_loss= 2.63132 val_acc= 0.20597 time= 0.29101
Epoch: 0013 train_loss= 2.61790 train_acc= 0.17571 val_loss= 2.60192 val_acc= 0.20896 time= 0.28899
Epoch: 0014 train_loss= 2.57754 train_acc= 0.18299 val_loss= 2.57675 val_acc= 0.22985 time= 0.29101
Epoch: 0015 train_loss= 2.54387 train_acc= 0.19821 val_loss= 2.55306 val_acc= 0.25075 time= 0.29199
Epoch: 0016 train_loss= 2.51355 train_acc= 0.23362 val_loss= 2.52706 val_acc= 0.28657 time= 0.29000
Epoch: 0017 train_loss= 2.48091 train_acc= 0.26704 val_loss= 2.49661 val_acc= 0.29552 time= 0.29000
Epoch: 0018 train_loss= 2.44408 train_acc= 0.29749 val_loss= 2.46126 val_acc= 0.31343 time= 0.29097
Epoch: 0019 train_loss= 2.40559 train_acc= 0.32660 val_loss= 2.42143 val_acc= 0.31940 time= 0.29503
Epoch: 0020 train_loss= 2.35962 train_acc= 0.34315 val_loss= 2.37825 val_acc= 0.32537 time= 0.29199
Epoch: 0021 train_loss= 2.31075 train_acc= 0.35672 val_loss= 2.33292 val_acc= 0.33134 time= 0.28697
Epoch: 0022 train_loss= 2.25955 train_acc= 0.36995 val_loss= 2.28651 val_acc= 0.33134 time= 0.29013
Epoch: 0023 train_loss= 2.20463 train_acc= 0.38418 val_loss= 2.23965 val_acc= 0.33134 time= 0.29800
Epoch: 0024 train_loss= 2.15193 train_acc= 0.39179 val_loss= 2.19271 val_acc= 0.34627 time= 0.28800
Epoch: 0025 train_loss= 2.09434 train_acc= 0.41727 val_loss= 2.14599 val_acc= 0.37910 time= 0.29100
Epoch: 0026 train_loss= 2.03482 train_acc= 0.44871 val_loss= 2.09972 val_acc= 0.41791 time= 0.28945
Epoch: 0027 train_loss= 1.97718 train_acc= 0.48610 val_loss= 2.05375 val_acc= 0.45672 time= 0.29397
Epoch: 0028 train_loss= 1.91783 train_acc= 0.51787 val_loss= 2.00805 val_acc= 0.48955 time= 0.28655
Epoch: 0029 train_loss= 1.86061 train_acc= 0.55791 val_loss= 1.96214 val_acc= 0.51045 time= 0.28600
Epoch: 0030 train_loss= 1.79535 train_acc= 0.57876 val_loss= 1.91597 val_acc= 0.52239 time= 0.29001
Epoch: 0031 train_loss= 1.73627 train_acc= 0.60126 val_loss= 1.86963 val_acc= 0.53134 time= 0.29103
Epoch: 0032 train_loss= 1.67635 train_acc= 0.61218 val_loss= 1.82364 val_acc= 0.52537 time= 0.28700
Epoch: 0033 train_loss= 1.61361 train_acc= 0.62210 val_loss= 1.77924 val_acc= 0.52836 time= 0.28900
Epoch: 0034 train_loss= 1.55719 train_acc= 0.63071 val_loss= 1.73721 val_acc= 0.53433 time= 0.28897
Epoch: 0035 train_loss= 1.49846 train_acc= 0.64792 val_loss= 1.69766 val_acc= 0.54328 time= 0.29416
Epoch: 0036 train_loss= 1.43940 train_acc= 0.65520 val_loss= 1.66074 val_acc= 0.55821 time= 0.28800
Epoch: 0037 train_loss= 1.38510 train_acc= 0.66612 val_loss= 1.62529 val_acc= 0.56418 time= 0.28745
Epoch: 0038 train_loss= 1.33282 train_acc= 0.67902 val_loss= 1.59087 val_acc= 0.58209 time= 0.28800
Epoch: 0039 train_loss= 1.28474 train_acc= 0.69292 val_loss= 1.55719 val_acc= 0.58209 time= 0.29200
Epoch: 0040 train_loss= 1.23049 train_acc= 0.70417 val_loss= 1.52472 val_acc= 0.58209 time= 0.29123
Epoch: 0041 train_loss= 1.17922 train_acc= 0.71211 val_loss= 1.49462 val_acc= 0.58209 time= 0.28800
Epoch: 0042 train_loss= 1.13127 train_acc= 0.71674 val_loss= 1.46652 val_acc= 0.58806 time= 0.28597
Epoch: 0043 train_loss= 1.08574 train_acc= 0.73163 val_loss= 1.44017 val_acc= 0.58806 time= 0.29403
Epoch: 0044 train_loss= 1.04326 train_acc= 0.74983 val_loss= 1.41554 val_acc= 0.58806 time= 0.29000
Epoch: 0045 train_loss= 0.99542 train_acc= 0.75976 val_loss= 1.39251 val_acc= 0.60597 time= 0.28500
Epoch: 0046 train_loss= 0.95767 train_acc= 0.77565 val_loss= 1.37052 val_acc= 0.60597 time= 0.29097
Epoch: 0047 train_loss= 0.91350 train_acc= 0.78160 val_loss= 1.34998 val_acc= 0.61493 time= 0.29632
Epoch: 0048 train_loss= 0.87704 train_acc= 0.79252 val_loss= 1.33133 val_acc= 0.61493 time= 0.29000
Epoch: 0049 train_loss= 0.83912 train_acc= 0.80245 val_loss= 1.31429 val_acc= 0.61791 time= 0.29000
Epoch: 0050 train_loss= 0.80307 train_acc= 0.81337 val_loss= 1.29794 val_acc= 0.62388 time= 0.28600
Epoch: 0051 train_loss= 0.76257 train_acc= 0.82462 val_loss= 1.28225 val_acc= 0.62388 time= 0.29808
Epoch: 0052 train_loss= 0.73044 train_acc= 0.83289 val_loss= 1.26795 val_acc= 0.63284 time= 0.28703
Epoch: 0053 train_loss= 0.69676 train_acc= 0.83918 val_loss= 1.25484 val_acc= 0.63582 time= 0.28806
Epoch: 0054 train_loss= 0.66496 train_acc= 0.84878 val_loss= 1.24263 val_acc= 0.64179 time= 0.28763
Epoch: 0055 train_loss= 0.63732 train_acc= 0.85705 val_loss= 1.23205 val_acc= 0.62985 time= 0.29600
Epoch: 0056 train_loss= 0.60191 train_acc= 0.86698 val_loss= 1.22244 val_acc= 0.62985 time= 0.28800
Epoch: 0057 train_loss= 0.57634 train_acc= 0.86797 val_loss= 1.21336 val_acc= 0.63881 time= 0.28697
Epoch: 0058 train_loss= 0.54706 train_acc= 0.87955 val_loss= 1.20451 val_acc= 0.63284 time= 0.29107
Epoch: 0059 train_loss= 0.52704 train_acc= 0.88352 val_loss= 1.19632 val_acc= 0.63881 time= 0.29801
Epoch: 0060 train_loss= 0.50242 train_acc= 0.89179 val_loss= 1.18955 val_acc= 0.64179 time= 0.28600
Epoch: 0061 train_loss= 0.47815 train_acc= 0.90007 val_loss= 1.18380 val_acc= 0.64179 time= 0.28900
Epoch: 0062 train_loss= 0.45385 train_acc= 0.90702 val_loss= 1.17868 val_acc= 0.64776 time= 0.28698
Epoch: 0063 train_loss= 0.43237 train_acc= 0.91066 val_loss= 1.17387 val_acc= 0.64776 time= 0.29303
Epoch: 0064 train_loss= 0.41854 train_acc= 0.91198 val_loss= 1.16927 val_acc= 0.65075 time= 0.28697
Epoch: 0065 train_loss= 0.39461 train_acc= 0.91794 val_loss= 1.16540 val_acc= 0.65075 time= 0.29203
Epoch: 0066 train_loss= 0.37903 train_acc= 0.92058 val_loss= 1.16244 val_acc= 0.65075 time= 0.28797
Epoch: 0067 train_loss= 0.35841 train_acc= 0.92555 val_loss= 1.15973 val_acc= 0.65373 time= 0.29303
Epoch: 0068 train_loss= 0.34306 train_acc= 0.93249 val_loss= 1.15575 val_acc= 0.65672 time= 0.29097
Epoch: 0069 train_loss= 0.32777 train_acc= 0.93613 val_loss= 1.15400 val_acc= 0.66567 time= 0.28800
Epoch: 0070 train_loss= 0.31411 train_acc= 0.94242 val_loss= 1.15337 val_acc= 0.66269 time= 0.28903
Epoch: 0071 train_loss= 0.29574 train_acc= 0.94838 val_loss= 1.15276 val_acc= 0.65970 time= 0.29336
Epoch: 0072 train_loss= 0.28255 train_acc= 0.94937 val_loss= 1.15360 val_acc= 0.66269 time= 0.28818
Epoch: 0073 train_loss= 0.27007 train_acc= 0.95334 val_loss= 1.15535 val_acc= 0.66567 time= 0.28897
Epoch: 0074 train_loss= 0.25976 train_acc= 0.95797 val_loss= 1.15696 val_acc= 0.66567 time= 0.28909
Epoch: 0075 train_loss= 0.24405 train_acc= 0.95764 val_loss= 1.15805 val_acc= 0.66866 time= 0.29300
Early stopping...
Optimization Finished!
Test set results: cost= 1.15774 accuracy= 0.68711 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7052    0.7135    0.7093       342
           1     0.6870    0.7670    0.7248       103
           2     0.7288    0.6143    0.6667       140
           3     0.6102    0.4557    0.5217        79
           4     0.6273    0.7652    0.6894       132
           5     0.6796    0.7859    0.7289       313
           6     0.6667    0.7255    0.6948       102
           7     0.5882    0.2857    0.3846        70
           8     0.5588    0.3800    0.4524        50
           9     0.6264    0.7355    0.6766       155
          10     0.8182    0.6738    0.7390       187
          11     0.6207    0.6234    0.6220       231
          12     0.7665    0.7191    0.7420       178
          13     0.7888    0.7967    0.7927       600
          14     0.7812    0.8475    0.8130       590
          15     0.7671    0.7368    0.7517        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.4279    0.4678    0.4470       419
          19     0.6442    0.5194    0.5751       129
          20     0.6538    0.6071    0.6296        28
          21     1.0000    0.7241    0.8400        29
          22     0.5909    0.2826    0.3824        46

    accuracy                         0.6871      4043
   macro avg     0.6777    0.5948    0.6187      4043
weighted avg     0.6887    0.6871    0.6831      4043

Macro average Test Precision, Recall and F1-Score...
(0.6777196748657451, 0.5947581188139988, 0.6187114560030532, None)
Micro average Test Precision, Recall and F1-Score...
(0.6871135295572595, 0.6871135295572595, 0.6871135295572595, None)
embeddings:
14157 3357 4043
[[ 0.44636968  0.4915773   0.42994452 ...  0.27165014  0.60545295
   0.4377896 ]
 [ 0.06334834  0.18720905  0.04481318 ... -0.01514984  0.32193044
   0.01131062]
 [ 0.39131957  0.31741562  0.08007945 ...  0.27188253  0.41757253
   0.35663623]
 ...
 [ 0.29533094  0.1569782   0.12664552 ...  0.09616251  0.1826078
   0.199941  ]
 [ 0.1289138   0.03059634  0.00838606 ...  0.12233895  0.08365046
   0.01759978]
 [ 0.12314351  0.2587431   0.28616345 ...  0.02535819  0.3373925
   0.22228628]]

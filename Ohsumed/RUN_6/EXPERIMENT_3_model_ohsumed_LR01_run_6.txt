(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13560 train_acc= 0.01489 val_loss= 2.94968 val_acc= 0.20597 time= 0.58795
Epoch: 0002 train_loss= 2.95090 train_acc= 0.17406 val_loss= 2.72401 val_acc= 0.20000 time= 0.29104
Epoch: 0003 train_loss= 2.73630 train_acc= 0.17240 val_loss= 2.73563 val_acc= 0.20597 time= 0.29103
Epoch: 0004 train_loss= 2.75018 train_acc= 0.17439 val_loss= 2.57248 val_acc= 0.27164 time= 0.28997
Epoch: 0005 train_loss= 2.55370 train_acc= 0.25017 val_loss= 2.51703 val_acc= 0.32537 time= 0.29386
Epoch: 0006 train_loss= 2.47854 train_acc= 0.34514 val_loss= 2.42980 val_acc= 0.40000 time= 0.28703
Epoch: 0007 train_loss= 2.37414 train_acc= 0.43216 val_loss= 2.27698 val_acc= 0.42388 time= 0.28900
Epoch: 0008 train_loss= 2.20505 train_acc= 0.45235 val_loss= 2.13252 val_acc= 0.40000 time= 0.28700
Epoch: 0009 train_loss= 2.04878 train_acc= 0.44904 val_loss= 2.03443 val_acc= 0.41194 time= 0.29297
Epoch: 0010 train_loss= 1.92201 train_acc= 0.45599 val_loss= 1.91188 val_acc= 0.47761 time= 0.29107
Epoch: 0011 train_loss= 1.75762 train_acc= 0.53144 val_loss= 1.80385 val_acc= 0.51642 time= 0.29200
Epoch: 0012 train_loss= 1.60127 train_acc= 0.59497 val_loss= 1.71399 val_acc= 0.55522 time= 0.28800
Epoch: 0013 train_loss= 1.46569 train_acc= 0.63501 val_loss= 1.63709 val_acc= 0.57313 time= 0.29600
Epoch: 0014 train_loss= 1.33916 train_acc= 0.66810 val_loss= 1.55751 val_acc= 0.57015 time= 0.29198
Epoch: 0015 train_loss= 1.22227 train_acc= 0.70351 val_loss= 1.48561 val_acc= 0.57612 time= 0.28700
Epoch: 0016 train_loss= 1.10594 train_acc= 0.70913 val_loss= 1.43021 val_acc= 0.57910 time= 0.28700
Epoch: 0017 train_loss= 1.00526 train_acc= 0.72899 val_loss= 1.38750 val_acc= 0.60299 time= 0.29797
Epoch: 0018 train_loss= 0.91073 train_acc= 0.75579 val_loss= 1.35453 val_acc= 0.58806 time= 0.29103
Epoch: 0019 train_loss= 0.81702 train_acc= 0.77664 val_loss= 1.33168 val_acc= 0.60597 time= 0.28700
Epoch: 0020 train_loss= 0.72627 train_acc= 0.81866 val_loss= 1.31665 val_acc= 0.61791 time= 0.28800
Epoch: 0021 train_loss= 0.64810 train_acc= 0.84116 val_loss= 1.27621 val_acc= 0.61493 time= 0.29801
Epoch: 0022 train_loss= 0.56429 train_acc= 0.84844 val_loss= 1.26072 val_acc= 0.62687 time= 0.28899
Epoch: 0023 train_loss= 0.49652 train_acc= 0.86300 val_loss= 1.25664 val_acc= 0.65075 time= 0.28500
Epoch: 0024 train_loss= 0.43609 train_acc= 0.88021 val_loss= 1.25879 val_acc= 0.67164 time= 0.29309
Epoch: 0025 train_loss= 0.38289 train_acc= 0.89940 val_loss= 1.26574 val_acc= 0.66567 time= 0.29399
Epoch: 0026 train_loss= 0.33951 train_acc= 0.91727 val_loss= 1.26194 val_acc= 0.67761 time= 0.28607
Epoch: 0027 train_loss= 0.29392 train_acc= 0.93316 val_loss= 1.26092 val_acc= 0.68657 time= 0.29000
Epoch: 0028 train_loss= 0.25508 train_acc= 0.94077 val_loss= 1.28051 val_acc= 0.67463 time= 0.28700
Epoch: 0029 train_loss= 0.22164 train_acc= 0.94606 val_loss= 1.31635 val_acc= 0.66567 time= 0.29208
Early stopping...
Optimization Finished!
Test set results: cost= 1.30377 accuracy= 0.67054 time= 0.12797
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6930    0.6667    0.6796       342
           1     0.6270    0.7670    0.6900       103
           2     0.7264    0.5500    0.6260       140
           3     0.5769    0.3797    0.4580        79
           4     0.6992    0.7045    0.7019       132
           5     0.7200    0.7476    0.7335       313
           6     0.6538    0.6667    0.6602       102
           7     0.5926    0.2286    0.3299        70
           8     0.5758    0.3800    0.4578        50
           9     0.6646    0.7032    0.6834       155
          10     0.8086    0.7005    0.7507       187
          11     0.6422    0.6061    0.6236       231
          12     0.7443    0.7360    0.7401       178
          13     0.7620    0.7950    0.7781       600
          14     0.8020    0.8102    0.8061       590
          15     0.7465    0.6974    0.7211        76
          16     0.6111    0.3235    0.4231        34
          17     0.5000    0.1000    0.1667        10
          18     0.3707    0.5370    0.4386       419
          19     0.6277    0.4574    0.5291       129
          20     0.7500    0.5357    0.6250        28
          21     0.9565    0.7586    0.8462        29
          22     0.4839    0.3261    0.3896        46

    accuracy                         0.6705      4043
   macro avg     0.6667    0.5729    0.6025      4043
weighted avg     0.6823    0.6705    0.6704      4043

Macro average Test Precision, Recall and F1-Score...
(0.6667340856453958, 0.5729310366298341, 0.6025346465070999, None)
Micro average Test Precision, Recall and F1-Score...
(0.6705416769725452, 0.6705416769725452, 0.6705416769725452, None)
embeddings:
14157 3357 4043
[[-0.3375726  -0.70758754  0.5552467  ...  0.40527314  0.45816267
   0.11268257]
 [-0.12848042 -0.18027407  0.10054203 ...  0.15940288  0.1024279
   0.07094896]
 [-0.26346877 -0.43237567  0.27885443 ...  0.01835327  0.7514311
   0.01712387]
 ...
 [-0.07956445 -0.22152486  0.29327354 ...  0.23035352  0.3610028
   0.09922981]
 [-0.09580909 -0.24310403 -0.13543251 ...  0.15056244  0.28853536
   0.6451169 ]
 [-0.05992804 -0.22474946  0.20986181 ...  0.35984173  0.07555451
   0.05504193]]

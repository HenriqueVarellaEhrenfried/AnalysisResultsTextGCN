(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.05361 val_loss= 3.11380 val_acc= 0.22687 time= 0.58643
Epoch: 0002 train_loss= 3.11399 train_acc= 0.19060 val_loss= 3.06499 val_acc= 0.20597 time= 0.28900
Epoch: 0003 train_loss= 3.06602 train_acc= 0.17737 val_loss= 2.99017 val_acc= 0.20299 time= 0.29397
Epoch: 0004 train_loss= 2.99249 train_acc= 0.17240 val_loss= 2.89768 val_acc= 0.20000 time= 0.29302
Epoch: 0005 train_loss= 2.90181 train_acc= 0.17141 val_loss= 2.80516 val_acc= 0.20000 time= 0.28800
Epoch: 0006 train_loss= 2.81126 train_acc= 0.17141 val_loss= 2.73103 val_acc= 0.20000 time= 0.29000
Epoch: 0007 train_loss= 2.73967 train_acc= 0.17141 val_loss= 2.69136 val_acc= 0.20000 time= 0.29387
Epoch: 0008 train_loss= 2.70217 train_acc= 0.17141 val_loss= 2.68929 val_acc= 0.20000 time= 0.29303
Epoch: 0009 train_loss= 2.70145 train_acc= 0.17141 val_loss= 2.69356 val_acc= 0.20000 time= 0.28962
Epoch: 0010 train_loss= 2.70353 train_acc= 0.17141 val_loss= 2.67807 val_acc= 0.20299 time= 0.28700
Epoch: 0011 train_loss= 2.68066 train_acc= 0.17207 val_loss= 2.64775 val_acc= 0.20597 time= 0.29397
Epoch: 0012 train_loss= 2.64000 train_acc= 0.17704 val_loss= 2.61644 val_acc= 0.21791 time= 0.29523
Epoch: 0013 train_loss= 2.59810 train_acc= 0.18862 val_loss= 2.59053 val_acc= 0.24478 time= 0.29200
Epoch: 0014 train_loss= 2.56357 train_acc= 0.20880 val_loss= 2.56848 val_acc= 0.25970 time= 0.29000
Epoch: 0015 train_loss= 2.53508 train_acc= 0.23627 val_loss= 2.54566 val_acc= 0.27761 time= 0.29800
Epoch: 0016 train_loss= 2.50782 train_acc= 0.26506 val_loss= 2.51837 val_acc= 0.29552 time= 0.29500
Epoch: 0017 train_loss= 2.47616 train_acc= 0.28987 val_loss= 2.48518 val_acc= 0.30746 time= 0.28700
Epoch: 0018 train_loss= 2.43958 train_acc= 0.30642 val_loss= 2.44680 val_acc= 0.31343 time= 0.29400
Epoch: 0019 train_loss= 2.39906 train_acc= 0.31800 val_loss= 2.40522 val_acc= 0.31343 time= 0.28997
Epoch: 0020 train_loss= 2.35277 train_acc= 0.32296 val_loss= 2.36261 val_acc= 0.31343 time= 0.29403
Epoch: 0021 train_loss= 2.30691 train_acc= 0.32528 val_loss= 2.32046 val_acc= 0.31940 time= 0.29199
Epoch: 0022 train_loss= 2.25796 train_acc= 0.33058 val_loss= 2.27900 val_acc= 0.31940 time= 0.28700
Epoch: 0023 train_loss= 2.20805 train_acc= 0.34613 val_loss= 2.23748 val_acc= 0.34328 time= 0.29000
Epoch: 0024 train_loss= 2.15896 train_acc= 0.36400 val_loss= 2.19511 val_acc= 0.34925 time= 0.29274
Epoch: 0025 train_loss= 2.10737 train_acc= 0.39146 val_loss= 2.15192 val_acc= 0.38507 time= 0.29003
Epoch: 0026 train_loss= 2.05241 train_acc= 0.42852 val_loss= 2.10874 val_acc= 0.39701 time= 0.28800
Epoch: 0027 train_loss= 1.99604 train_acc= 0.46559 val_loss= 2.06627 val_acc= 0.43881 time= 0.28602
Epoch: 0028 train_loss= 1.94040 train_acc= 0.51390 val_loss= 2.02434 val_acc= 0.47463 time= 0.29698
Epoch: 0029 train_loss= 1.88312 train_acc= 0.54798 val_loss= 1.98216 val_acc= 0.50746 time= 0.28700
Epoch: 0030 train_loss= 1.82706 train_acc= 0.57478 val_loss= 1.93907 val_acc= 0.52239 time= 0.28726
Epoch: 0031 train_loss= 1.76781 train_acc= 0.59861 val_loss= 1.89514 val_acc= 0.53134 time= 0.28958
Epoch: 0032 train_loss= 1.70953 train_acc= 0.60986 val_loss= 1.85123 val_acc= 0.52836 time= 0.29500
Epoch: 0033 train_loss= 1.65060 train_acc= 0.61582 val_loss= 1.80827 val_acc= 0.52836 time= 0.28704
Epoch: 0034 train_loss= 1.59280 train_acc= 0.62475 val_loss= 1.76685 val_acc= 0.54030 time= 0.29303
Epoch: 0035 train_loss= 1.53582 train_acc= 0.63666 val_loss= 1.72711 val_acc= 0.54328 time= 0.28900
Epoch: 0036 train_loss= 1.48105 train_acc= 0.64659 val_loss= 1.68885 val_acc= 0.54925 time= 0.29697
Epoch: 0037 train_loss= 1.42324 train_acc= 0.65950 val_loss= 1.65242 val_acc= 0.56716 time= 0.28900
Epoch: 0038 train_loss= 1.36968 train_acc= 0.67141 val_loss= 1.61772 val_acc= 0.57910 time= 0.29300
Epoch: 0039 train_loss= 1.31670 train_acc= 0.68266 val_loss= 1.58508 val_acc= 0.58209 time= 0.29003
Epoch: 0040 train_loss= 1.26396 train_acc= 0.69292 val_loss= 1.55402 val_acc= 0.57910 time= 0.29422
Epoch: 0041 train_loss= 1.21333 train_acc= 0.70582 val_loss= 1.52420 val_acc= 0.58507 time= 0.28708
Epoch: 0042 train_loss= 1.16247 train_acc= 0.71840 val_loss= 1.49534 val_acc= 0.59104 time= 0.28877
Epoch: 0043 train_loss= 1.11565 train_acc= 0.73428 val_loss= 1.46781 val_acc= 0.59104 time= 0.28699
Epoch: 0044 train_loss= 1.06725 train_acc= 0.74686 val_loss= 1.44213 val_acc= 0.59104 time= 0.29800
Epoch: 0045 train_loss= 1.01782 train_acc= 0.76373 val_loss= 1.41818 val_acc= 0.59702 time= 0.29215
Epoch: 0046 train_loss= 0.97630 train_acc= 0.77300 val_loss= 1.39558 val_acc= 0.59701 time= 0.28800
Epoch: 0047 train_loss= 0.93275 train_acc= 0.78524 val_loss= 1.37424 val_acc= 0.60896 time= 0.29000
Epoch: 0048 train_loss= 0.89141 train_acc= 0.79782 val_loss= 1.35429 val_acc= 0.61194 time= 0.29400
Epoch: 0049 train_loss= 0.85025 train_acc= 0.80841 val_loss= 1.33550 val_acc= 0.61791 time= 0.29100
Epoch: 0050 train_loss= 0.81099 train_acc= 0.81866 val_loss= 1.31808 val_acc= 0.61791 time= 0.28800
Epoch: 0051 train_loss= 0.77593 train_acc= 0.82627 val_loss= 1.30202 val_acc= 0.62090 time= 0.29000
Epoch: 0052 train_loss= 0.73664 train_acc= 0.83620 val_loss= 1.28703 val_acc= 0.62090 time= 0.29797
Epoch: 0053 train_loss= 0.70266 train_acc= 0.84348 val_loss= 1.27346 val_acc= 0.62985 time= 0.28903
Epoch: 0054 train_loss= 0.66851 train_acc= 0.84811 val_loss= 1.26185 val_acc= 0.62985 time= 0.29000
Epoch: 0055 train_loss= 0.63555 train_acc= 0.85970 val_loss= 1.25172 val_acc= 0.62687 time= 0.29300
Epoch: 0056 train_loss= 0.60468 train_acc= 0.86764 val_loss= 1.24294 val_acc= 0.62687 time= 0.29197
Epoch: 0057 train_loss= 0.57412 train_acc= 0.87591 val_loss= 1.23439 val_acc= 0.62985 time= 0.29003
Epoch: 0058 train_loss= 0.54678 train_acc= 0.88187 val_loss= 1.22574 val_acc= 0.63881 time= 0.28608
Epoch: 0059 train_loss= 0.52007 train_acc= 0.88815 val_loss= 1.21707 val_acc= 0.64179 time= 0.29300
Epoch: 0060 train_loss= 0.49305 train_acc= 0.89477 val_loss= 1.20895 val_acc= 0.65075 time= 0.29261
Epoch: 0061 train_loss= 0.46944 train_acc= 0.90437 val_loss= 1.20181 val_acc= 0.65672 time= 0.29603
Epoch: 0062 train_loss= 0.44425 train_acc= 0.90735 val_loss= 1.19576 val_acc= 0.65672 time= 0.29204
Epoch: 0063 train_loss= 0.42279 train_acc= 0.91165 val_loss= 1.19078 val_acc= 0.65672 time= 0.28700
Epoch: 0064 train_loss= 0.40303 train_acc= 0.91860 val_loss= 1.18728 val_acc= 0.65373 time= 0.29800
Epoch: 0065 train_loss= 0.38139 train_acc= 0.92389 val_loss= 1.18499 val_acc= 0.65970 time= 0.29700
Epoch: 0066 train_loss= 0.36233 train_acc= 0.93018 val_loss= 1.18308 val_acc= 0.65672 time= 0.28900
Epoch: 0067 train_loss= 0.34330 train_acc= 0.93349 val_loss= 1.18161 val_acc= 0.65672 time= 0.28900
Epoch: 0068 train_loss= 0.32677 train_acc= 0.94044 val_loss= 1.18002 val_acc= 0.65970 time= 0.29200
Epoch: 0069 train_loss= 0.31084 train_acc= 0.94375 val_loss= 1.17878 val_acc= 0.66269 time= 0.29900
Epoch: 0070 train_loss= 0.29499 train_acc= 0.94639 val_loss= 1.17779 val_acc= 0.66866 time= 0.28600
Epoch: 0071 train_loss= 0.27858 train_acc= 0.95003 val_loss= 1.17691 val_acc= 0.66567 time= 0.28900
Epoch: 0072 train_loss= 0.26534 train_acc= 0.95632 val_loss= 1.17585 val_acc= 0.66866 time= 0.29300
Epoch: 0073 train_loss= 0.25251 train_acc= 0.95665 val_loss= 1.17575 val_acc= 0.66567 time= 0.29600
Epoch: 0074 train_loss= 0.23808 train_acc= 0.96029 val_loss= 1.17672 val_acc= 0.66567 time= 0.28700
Epoch: 0075 train_loss= 0.22876 train_acc= 0.96327 val_loss= 1.17806 val_acc= 0.66866 time= 0.28600
Epoch: 0076 train_loss= 0.21800 train_acc= 0.96691 val_loss= 1.17957 val_acc= 0.67164 time= 0.29100
Early stopping...
Optimization Finished!
Test set results: cost= 1.16201 accuracy= 0.68786 time= 0.13200
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7262    0.6901    0.7076       342
           1     0.6870    0.7670    0.7248       103
           2     0.7217    0.5929    0.6510       140
           3     0.6140    0.4430    0.5147        79
           4     0.6781    0.7500    0.7122       132
           5     0.6814    0.7859    0.7300       313
           6     0.6893    0.6961    0.6927       102
           7     0.6774    0.3000    0.4158        70
           8     0.5556    0.4000    0.4651        50
           9     0.6032    0.7355    0.6628       155
          10     0.8367    0.6578    0.7365       187
          11     0.6224    0.6494    0.6356       231
          12     0.7683    0.7079    0.7368       178
          13     0.7650    0.8083    0.7861       600
          14     0.7773    0.8458    0.8101       590
          15     0.7467    0.7368    0.7417        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.2000    0.2857        10
          18     0.4390    0.4726    0.4552       419
          19     0.6415    0.5271    0.5787       129
          20     0.6333    0.6786    0.6552        28
          21     1.0000    0.7241    0.8400        29
          22     0.5455    0.3913    0.4557        46

    accuracy                         0.6879      4043
   macro avg     0.6789    0.6049    0.6289      4043
weighted avg     0.6899    0.6879    0.6842      4043

Macro average Test Precision, Recall and F1-Score...
(0.6789315067598916, 0.6049129142550631, 0.6288962055818953, None)
Micro average Test Precision, Recall and F1-Score...
(0.6878555528073212, 0.6878555528073212, 0.6878555528073212, None)
embeddings:
14157 3357 4043
[[ 0.60330373  0.59749275  0.40479738 ...  0.5003271   0.37472287
   0.20260577]
 [ 0.23039919  0.32706982  0.17581135 ...  0.08311374  0.14636166
  -0.00693021]
 [ 0.5936175   0.42476758  0.36836192 ...  0.30420113  0.23005559
   0.3158209 ]
 ...
 [ 0.40257072  0.21266767  0.21899685 ...  0.18626274  0.17739215
   0.13772179]
 [-0.07409672  0.11802642  0.07128699 ...  0.10145681  0.29514065
   0.14164707]
 [ 0.24582726  0.36022806  0.05282379 ...  0.09403495  0.08622354
   0.12255374]]

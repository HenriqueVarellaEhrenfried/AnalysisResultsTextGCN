(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13545 train_acc= 0.02747 val_loss= 3.11233 val_acc= 0.22090 time= 0.58717
Epoch: 0002 train_loss= 3.11259 train_acc= 0.18895 val_loss= 3.06327 val_acc= 0.20896 time= 0.29556
Epoch: 0003 train_loss= 3.06411 train_acc= 0.18531 val_loss= 2.98892 val_acc= 0.20597 time= 0.28800
Epoch: 0004 train_loss= 2.99066 train_acc= 0.18299 val_loss= 2.89736 val_acc= 0.20597 time= 0.29000
Epoch: 0005 train_loss= 2.90044 train_acc= 0.17935 val_loss= 2.80521 val_acc= 0.20597 time= 0.28800
Epoch: 0006 train_loss= 2.81061 train_acc= 0.17737 val_loss= 2.73193 val_acc= 0.20597 time= 0.29552
Epoch: 0007 train_loss= 2.73979 train_acc= 0.17538 val_loss= 2.69425 val_acc= 0.20597 time= 0.29300
Epoch: 0008 train_loss= 2.70446 train_acc= 0.17902 val_loss= 2.69070 val_acc= 0.20597 time= 0.29200
Epoch: 0009 train_loss= 2.70100 train_acc= 0.17340 val_loss= 2.69194 val_acc= 0.20000 time= 0.28900
Epoch: 0010 train_loss= 2.70082 train_acc= 0.17174 val_loss= 2.67618 val_acc= 0.20299 time= 0.29200
Epoch: 0011 train_loss= 2.67725 train_acc= 0.17207 val_loss= 2.64658 val_acc= 0.20597 time= 0.29600
Epoch: 0012 train_loss= 2.63615 train_acc= 0.17273 val_loss= 2.61543 val_acc= 0.20896 time= 0.28800
Epoch: 0013 train_loss= 2.59600 train_acc= 0.17670 val_loss= 2.58945 val_acc= 0.22388 time= 0.28700
Epoch: 0014 train_loss= 2.55893 train_acc= 0.19259 val_loss= 2.56742 val_acc= 0.23881 time= 0.29110
Epoch: 0015 train_loss= 2.53094 train_acc= 0.21145 val_loss= 2.54462 val_acc= 0.26866 time= 0.29500
Epoch: 0016 train_loss= 2.50120 train_acc= 0.24322 val_loss= 2.51708 val_acc= 0.28358 time= 0.28900
Epoch: 0017 train_loss= 2.47032 train_acc= 0.28226 val_loss= 2.48321 val_acc= 0.30149 time= 0.29100
Epoch: 0018 train_loss= 2.43037 train_acc= 0.31502 val_loss= 2.44361 val_acc= 0.30746 time= 0.28900
Epoch: 0019 train_loss= 2.38885 train_acc= 0.32793 val_loss= 2.40022 val_acc= 0.32239 time= 0.29600
Epoch: 0020 train_loss= 2.33945 train_acc= 0.34745 val_loss= 2.35514 val_acc= 0.33134 time= 0.28800
Epoch: 0021 train_loss= 2.29290 train_acc= 0.35572 val_loss= 2.30995 val_acc= 0.33731 time= 0.28900
Epoch: 0022 train_loss= 2.24020 train_acc= 0.36896 val_loss= 2.26530 val_acc= 0.35522 time= 0.28800
Epoch: 0023 train_loss= 2.19034 train_acc= 0.38087 val_loss= 2.22094 val_acc= 0.35821 time= 0.29700
Epoch: 0024 train_loss= 2.13670 train_acc= 0.39576 val_loss= 2.17624 val_acc= 0.36716 time= 0.29600
Epoch: 0025 train_loss= 2.08037 train_acc= 0.42422 val_loss= 2.13089 val_acc= 0.38209 time= 0.28911
Epoch: 0026 train_loss= 2.02226 train_acc= 0.45301 val_loss= 2.08535 val_acc= 0.42090 time= 0.28800
Epoch: 0027 train_loss= 1.96641 train_acc= 0.48941 val_loss= 2.04034 val_acc= 0.44776 time= 0.29049
Epoch: 0028 train_loss= 1.90687 train_acc= 0.52416 val_loss= 1.99630 val_acc= 0.48358 time= 0.29603
Epoch: 0029 train_loss= 1.84498 train_acc= 0.55857 val_loss= 1.95280 val_acc= 0.50746 time= 0.28762
Epoch: 0030 train_loss= 1.78998 train_acc= 0.58372 val_loss= 1.90861 val_acc= 0.52537 time= 0.28900
Epoch: 0031 train_loss= 1.73110 train_acc= 0.59960 val_loss= 1.86326 val_acc= 0.53134 time= 0.28902
Epoch: 0032 train_loss= 1.67122 train_acc= 0.61482 val_loss= 1.81753 val_acc= 0.54328 time= 0.29900
Epoch: 0033 train_loss= 1.61169 train_acc= 0.62475 val_loss= 1.77291 val_acc= 0.54627 time= 0.28671
Epoch: 0034 train_loss= 1.55363 train_acc= 0.63501 val_loss= 1.73104 val_acc= 0.53433 time= 0.28700
Epoch: 0035 train_loss= 1.49751 train_acc= 0.64428 val_loss= 1.69219 val_acc= 0.54328 time= 0.29100
Epoch: 0036 train_loss= 1.44051 train_acc= 0.65189 val_loss= 1.65554 val_acc= 0.55224 time= 0.29197
Epoch: 0037 train_loss= 1.38493 train_acc= 0.66678 val_loss= 1.62008 val_acc= 0.56418 time= 0.29103
Epoch: 0038 train_loss= 1.33091 train_acc= 0.67604 val_loss= 1.58589 val_acc= 0.56119 time= 0.28996
Epoch: 0039 train_loss= 1.28329 train_acc= 0.68630 val_loss= 1.55381 val_acc= 0.57015 time= 0.29000
Epoch: 0040 train_loss= 1.22854 train_acc= 0.69887 val_loss= 1.52378 val_acc= 0.57612 time= 0.29433
Epoch: 0041 train_loss= 1.18162 train_acc= 0.71277 val_loss= 1.49546 val_acc= 0.58209 time= 0.29403
Epoch: 0042 train_loss= 1.13698 train_acc= 0.72105 val_loss= 1.46904 val_acc= 0.57910 time= 0.29000
Epoch: 0043 train_loss= 1.09177 train_acc= 0.73660 val_loss= 1.44354 val_acc= 0.57910 time= 0.28800
Epoch: 0044 train_loss= 1.04404 train_acc= 0.75149 val_loss= 1.41961 val_acc= 0.59104 time= 0.29162
Epoch: 0045 train_loss= 0.99938 train_acc= 0.76770 val_loss= 1.39711 val_acc= 0.59104 time= 0.30003
Epoch: 0046 train_loss= 0.95860 train_acc= 0.77531 val_loss= 1.37646 val_acc= 0.60000 time= 0.28899
Epoch: 0047 train_loss= 0.91554 train_acc= 0.78392 val_loss= 1.35719 val_acc= 0.60299 time= 0.28997
Epoch: 0048 train_loss= 0.88055 train_acc= 0.79021 val_loss= 1.33872 val_acc= 0.60597 time= 0.29105
Epoch: 0049 train_loss= 0.84008 train_acc= 0.80146 val_loss= 1.32192 val_acc= 0.60597 time= 0.29595
Epoch: 0050 train_loss= 0.80126 train_acc= 0.81403 val_loss= 1.30671 val_acc= 0.60896 time= 0.28703
Epoch: 0051 train_loss= 0.76796 train_acc= 0.82296 val_loss= 1.29256 val_acc= 0.60597 time= 0.28800
Epoch: 0052 train_loss= 0.73143 train_acc= 0.83223 val_loss= 1.27966 val_acc= 0.60896 time= 0.29223
Epoch: 0053 train_loss= 0.69765 train_acc= 0.84150 val_loss= 1.26771 val_acc= 0.61493 time= 0.29500
Epoch: 0054 train_loss= 0.66662 train_acc= 0.84646 val_loss= 1.25690 val_acc= 0.62090 time= 0.29104
Epoch: 0055 train_loss= 0.63682 train_acc= 0.85572 val_loss= 1.24636 val_acc= 0.61791 time= 0.28800
Epoch: 0056 train_loss= 0.60654 train_acc= 0.86565 val_loss= 1.23690 val_acc= 0.62090 time= 0.28692
Epoch: 0057 train_loss= 0.57834 train_acc= 0.87426 val_loss= 1.22872 val_acc= 0.62090 time= 0.29500
Epoch: 0058 train_loss= 0.55356 train_acc= 0.88087 val_loss= 1.22226 val_acc= 0.62985 time= 0.29769
Epoch: 0059 train_loss= 0.52747 train_acc= 0.88385 val_loss= 1.21712 val_acc= 0.64179 time= 0.29200
Epoch: 0060 train_loss= 0.50171 train_acc= 0.89246 val_loss= 1.21178 val_acc= 0.63881 time= 0.28901
Epoch: 0061 train_loss= 0.47789 train_acc= 0.89610 val_loss= 1.20576 val_acc= 0.65373 time= 0.29001
Epoch: 0062 train_loss= 0.45564 train_acc= 0.90139 val_loss= 1.19922 val_acc= 0.66269 time= 0.29599
Epoch: 0063 train_loss= 0.43649 train_acc= 0.90801 val_loss= 1.19323 val_acc= 0.66567 time= 0.29000
Epoch: 0064 train_loss= 0.41314 train_acc= 0.91231 val_loss= 1.18858 val_acc= 0.66567 time= 0.28600
Epoch: 0065 train_loss= 0.39240 train_acc= 0.91860 val_loss= 1.18593 val_acc= 0.65672 time= 0.28808
Epoch: 0066 train_loss= 0.37447 train_acc= 0.92720 val_loss= 1.18315 val_acc= 0.66567 time= 0.29697
Epoch: 0067 train_loss= 0.35656 train_acc= 0.93382 val_loss= 1.17956 val_acc= 0.66269 time= 0.29303
Epoch: 0068 train_loss= 0.34038 train_acc= 0.93514 val_loss= 1.17743 val_acc= 0.66567 time= 0.28800
Epoch: 0069 train_loss= 0.32781 train_acc= 0.94044 val_loss= 1.17696 val_acc= 0.66567 time= 0.28897
Epoch: 0070 train_loss= 0.30927 train_acc= 0.94606 val_loss= 1.17691 val_acc= 0.66866 time= 0.29300
Epoch: 0071 train_loss= 0.29312 train_acc= 0.94639 val_loss= 1.17847 val_acc= 0.66567 time= 0.30003
Epoch: 0072 train_loss= 0.28456 train_acc= 0.94705 val_loss= 1.17847 val_acc= 0.66567 time= 0.29001
Epoch: 0073 train_loss= 0.27023 train_acc= 0.95202 val_loss= 1.17781 val_acc= 0.66567 time= 0.29099
Epoch: 0074 train_loss= 0.25429 train_acc= 0.95831 val_loss= 1.17781 val_acc= 0.66269 time= 0.28997
Epoch: 0075 train_loss= 0.24397 train_acc= 0.96029 val_loss= 1.17918 val_acc= 0.65970 time= 0.29653
Epoch: 0076 train_loss= 0.23323 train_acc= 0.96095 val_loss= 1.18275 val_acc= 0.65373 time= 0.29004
Early stopping...
Optimization Finished!
Test set results: cost= 1.17084 accuracy= 0.68810 time= 0.12799
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7204    0.6930    0.7064       342
           1     0.7170    0.7379    0.7273       103
           2     0.7203    0.6071    0.6589       140
           3     0.7083    0.4304    0.5354        79
           4     0.6623    0.7576    0.7067       132
           5     0.6882    0.7827    0.7324       313
           6     0.6726    0.7451    0.7070       102
           7     0.5526    0.3000    0.3889        70
           8     0.5806    0.3600    0.4444        50
           9     0.6319    0.7419    0.6825       155
          10     0.8333    0.6417    0.7251       187
          11     0.6198    0.6494    0.6342       231
          12     0.7875    0.7079    0.7456       178
          13     0.7826    0.8100    0.7961       600
          14     0.7827    0.8424    0.8114       590
          15     0.7313    0.6447    0.6853        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.4215    0.5060    0.4599       419
          19     0.6600    0.5116    0.5764       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.7241    0.8400        29
          22     0.5484    0.3696    0.4416        46

    accuracy                         0.6881      4043
   macro avg     0.6823    0.5939    0.6210      4043
weighted avg     0.6936    0.6881    0.6855      4043

Macro average Test Precision, Recall and F1-Score...
(0.6822621065340411, 0.5938634241489114, 0.6210336559058119, None)
Micro average Test Precision, Recall and F1-Score...
(0.6881028938906752, 0.6881028938906752, 0.6881028938906752, None)
embeddings:
14157 3357 4043
[[ 0.32772028  0.42092413  0.32367814 ...  0.42037994  0.505048
   0.5065686 ]
 [-0.06024158  0.15663657 -0.01140988 ...  0.12712674  0.21607026
   0.21669847]
 [ 0.15961538  0.07912942  0.04322315 ...  0.463549    0.48971307
   0.45899367]
 ...
 [ 0.10421777  0.20496449  0.13093814 ...  0.18731645  0.21605283
   0.2013161 ]
 [-0.02025895  0.2346927   0.23429215 ...  0.49414906 -0.01424969
   0.3719623 ]
 [ 0.16568898  0.2112442   0.24911226 ...  0.21003349  0.36873472
   0.15747681]]

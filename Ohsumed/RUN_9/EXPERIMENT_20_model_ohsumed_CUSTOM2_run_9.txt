(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.03342 val_loss= 2.91656 val_acc= 0.20000 time= 0.58250
Epoch: 0002 train_loss= 2.91775 train_acc= 0.17207 val_loss= 2.74117 val_acc= 0.20299 time= 0.29306
Epoch: 0003 train_loss= 2.76310 train_acc= 0.17207 val_loss= 2.67827 val_acc= 0.20000 time= 0.29600
Epoch: 0004 train_loss= 2.69814 train_acc= 0.17141 val_loss= 2.55472 val_acc= 0.24776 time= 0.29100
Epoch: 0005 train_loss= 2.53458 train_acc= 0.22105 val_loss= 2.47125 val_acc= 0.31940 time= 0.28752
Epoch: 0006 train_loss= 2.42608 train_acc= 0.32098 val_loss= 2.34488 val_acc= 0.38507 time= 0.28945
Epoch: 0007 train_loss= 2.28414 train_acc= 0.40437 val_loss= 2.18437 val_acc= 0.39701 time= 0.29400
Epoch: 0008 train_loss= 2.11242 train_acc= 0.43051 val_loss= 2.05313 val_acc= 0.40896 time= 0.29300
Epoch: 0009 train_loss= 1.96165 train_acc= 0.46757 val_loss= 1.94063 val_acc= 0.48358 time= 0.29508
Epoch: 0010 train_loss= 1.81208 train_acc= 0.53673 val_loss= 1.83183 val_acc= 0.51343 time= 0.29103
Epoch: 0011 train_loss= 1.65320 train_acc= 0.59398 val_loss= 1.74209 val_acc= 0.53731 time= 0.28954
Epoch: 0012 train_loss= 1.51525 train_acc= 0.62244 val_loss= 1.65807 val_acc= 0.55522 time= 0.29861
Epoch: 0013 train_loss= 1.38052 train_acc= 0.65189 val_loss= 1.57818 val_acc= 0.57015 time= 0.29100
Epoch: 0014 train_loss= 1.25738 train_acc= 0.69292 val_loss= 1.50691 val_acc= 0.58209 time= 0.28897
Epoch: 0015 train_loss= 1.14276 train_acc= 0.70351 val_loss= 1.45306 val_acc= 0.59701 time= 0.28903
Epoch: 0016 train_loss= 1.03094 train_acc= 0.71972 val_loss= 1.41214 val_acc= 0.60896 time= 0.29597
Epoch: 0017 train_loss= 0.92224 train_acc= 0.75314 val_loss= 1.38422 val_acc= 0.60299 time= 0.29304
Epoch: 0018 train_loss= 0.82200 train_acc= 0.78822 val_loss= 1.35848 val_acc= 0.60299 time= 0.28696
Epoch: 0019 train_loss= 0.73099 train_acc= 0.81469 val_loss= 1.33176 val_acc= 0.60597 time= 0.28900
Epoch: 0020 train_loss= 0.65026 train_acc= 0.83455 val_loss= 1.30521 val_acc= 0.60597 time= 0.29252
Epoch: 0021 train_loss= 0.56417 train_acc= 0.85771 val_loss= 1.29233 val_acc= 0.62090 time= 0.29604
Epoch: 0022 train_loss= 0.49090 train_acc= 0.86797 val_loss= 1.29902 val_acc= 0.64179 time= 0.28900
Epoch: 0023 train_loss= 0.42972 train_acc= 0.88683 val_loss= 1.31131 val_acc= 0.64478 time= 0.29297
Epoch: 0024 train_loss= 0.37510 train_acc= 0.90702 val_loss= 1.31821 val_acc= 0.65672 time= 0.28800
Early stopping...
Optimization Finished!
Test set results: cost= 1.32235 accuracy= 0.66683 time= 0.13303
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.6783    0.7398    0.7077       342
           1     0.6765    0.6699    0.6732       103
           2     0.6726    0.5429    0.6008       140
           3     0.3673    0.4557    0.4068        79
           4     0.6062    0.7348    0.6644       132
           5     0.6562    0.8051    0.7231       313
           6     0.6574    0.6961    0.6762       102
           7     0.6400    0.2286    0.3368        70
           8     0.5909    0.2600    0.3611        50
           9     0.6307    0.7161    0.6707       155
          10     0.7386    0.6952    0.7163       187
          11     0.5959    0.6320    0.6134       231
          12     0.7458    0.7416    0.7437       178
          13     0.7961    0.7550    0.7750       600
          14     0.7440    0.8475    0.7924       590
          15     0.7397    0.7105    0.7248        76
          16     0.5556    0.2941    0.3846        34
          17     0.0000    0.0000    0.0000        10
          18     0.4559    0.3699    0.4084       419
          19     0.5600    0.5426    0.5512       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.6552    0.7917        29
          22     0.4286    0.3261    0.3704        46

    accuracy                         0.6668      4043
   macro avg     0.6155    0.5679    0.5793      4043
weighted avg     0.6626    0.6668    0.6591      4043

Macro average Test Precision, Recall and F1-Score...
(0.6155256542039955, 0.5678929796758473, 0.5793133203549419, None)
Micro average Test Precision, Recall and F1-Score...
(0.6668315607222359, 0.6668315607222359, 0.6668315607222359, None)
embeddings:
14157 3357 4043
[[ 0.16360937  0.3114286   0.21438023 ...  0.29751658  0.13294877
  -0.46112016]
 [ 0.06556723  0.05672032  0.3117337  ...  0.00730466  0.18584028
  -0.09421791]
 [ 0.39263093  0.41730955  0.19667624 ...  0.15055372  0.34252387
  -0.2002065 ]
 ...
 [ 0.0514066   0.14356859  0.0655009  ...  0.16955815  0.05250997
  -0.10098889]
 [ 0.08311917 -0.01740976  0.39420003 ... -0.00888836  0.03994108
  -0.10017648]
 [ 0.07044578  0.09544271  0.06166918 ...  0.27508312  0.02733826
  -0.13361117]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13543 train_acc= 0.13501 val_loss= 3.10611 val_acc= 0.20896 time= 4.51403
Epoch: 0002 train_loss= 3.10639 train_acc= 0.18398 val_loss= 3.03272 val_acc= 0.20597 time= 4.33702
Epoch: 0003 train_loss= 3.03369 train_acc= 0.17604 val_loss= 2.92254 val_acc= 0.20597 time= 4.32073
Epoch: 0004 train_loss= 2.92532 train_acc= 0.17340 val_loss= 2.80533 val_acc= 0.20299 time= 4.37987
Epoch: 0005 train_loss= 2.81122 train_acc= 0.17240 val_loss= 2.71927 val_acc= 0.20299 time= 4.35898
Epoch: 0006 train_loss= 2.72878 train_acc= 0.17273 val_loss= 2.69262 val_acc= 0.20597 time= 4.33601
Epoch: 0007 train_loss= 2.70529 train_acc= 0.17340 val_loss= 2.70165 val_acc= 0.20299 time= 4.35239
Epoch: 0008 train_loss= 2.71500 train_acc= 0.17240 val_loss= 2.68837 val_acc= 0.20597 time= 4.33695
Epoch: 0009 train_loss= 2.69469 train_acc= 0.17439 val_loss= 2.64972 val_acc= 0.20896 time= 4.34594
Epoch: 0010 train_loss= 2.64282 train_acc= 0.18068 val_loss= 2.60984 val_acc= 0.22687 time= 4.34799
Epoch: 0011 train_loss= 2.58950 train_acc= 0.19788 val_loss= 2.57985 val_acc= 0.25075 time= 4.35730
Epoch: 0012 train_loss= 2.54928 train_acc= 0.22402 val_loss= 2.55354 val_acc= 0.27164 time= 4.35100
Epoch: 0013 train_loss= 2.51717 train_acc= 0.25381 val_loss= 2.52281 val_acc= 0.29552 time= 4.33601
Epoch: 0014 train_loss= 2.48221 train_acc= 0.28127 val_loss= 2.48374 val_acc= 0.31045 time= 4.37799
Epoch: 0015 train_loss= 2.43567 train_acc= 0.31205 val_loss= 2.43676 val_acc= 0.31642 time= 4.34700
Epoch: 0016 train_loss= 2.38638 train_acc= 0.32561 val_loss= 2.38460 val_acc= 0.31642 time= 4.33896
Epoch: 0017 train_loss= 2.32967 train_acc= 0.33322 val_loss= 2.33063 val_acc= 0.31940 time= 4.35395
Epoch: 0018 train_loss= 2.27434 train_acc= 0.34348 val_loss= 2.27728 val_acc= 0.32836 time= 4.35868
Epoch: 0019 train_loss= 2.20994 train_acc= 0.35043 val_loss= 2.22515 val_acc= 0.33731 time= 4.34501
Epoch: 0020 train_loss= 2.15097 train_acc= 0.36267 val_loss= 2.17348 val_acc= 0.35522 time= 4.36200
Epoch: 0021 train_loss= 2.08516 train_acc= 0.38782 val_loss= 2.12151 val_acc= 0.37910 time= 4.35199
Epoch: 0022 train_loss= 2.01991 train_acc= 0.42786 val_loss= 2.06990 val_acc= 0.41791 time= 4.34601
Epoch: 0023 train_loss= 1.95595 train_acc= 0.48015 val_loss= 2.02010 val_acc= 0.46269 time= 4.33300
Epoch: 0024 train_loss= 1.88986 train_acc= 0.52945 val_loss= 1.97197 val_acc= 0.49254 time= 4.35500
Epoch: 0025 train_loss= 1.82226 train_acc= 0.56817 val_loss= 1.92329 val_acc= 0.52239 time= 4.33800
Epoch: 0026 train_loss= 1.75647 train_acc= 0.59067 val_loss= 1.87225 val_acc= 0.53433 time= 4.33756
Epoch: 0027 train_loss= 1.68536 train_acc= 0.60920 val_loss= 1.81969 val_acc= 0.52836 time= 4.34505
Epoch: 0028 train_loss= 1.62030 train_acc= 0.60986 val_loss= 1.76757 val_acc= 0.53433 time= 4.33498
Epoch: 0029 train_loss= 1.55472 train_acc= 0.62012 val_loss= 1.71923 val_acc= 0.53731 time= 4.34801
Epoch: 0030 train_loss= 1.48870 train_acc= 0.63302 val_loss= 1.67636 val_acc= 0.54030 time= 4.33802
Epoch: 0031 train_loss= 1.43025 train_acc= 0.64097 val_loss= 1.63706 val_acc= 0.55821 time= 4.33500
Epoch: 0032 train_loss= 1.36716 train_acc= 0.66347 val_loss= 1.59962 val_acc= 0.55821 time= 4.40099
Epoch: 0033 train_loss= 1.30490 train_acc= 0.68134 val_loss= 1.56435 val_acc= 0.57015 time= 4.32300
Epoch: 0034 train_loss= 1.25068 train_acc= 0.69623 val_loss= 1.53134 val_acc= 0.58209 time= 4.34556
Epoch: 0035 train_loss= 1.19338 train_acc= 0.70549 val_loss= 1.50014 val_acc= 0.58209 time= 4.32301
Epoch: 0036 train_loss= 1.13814 train_acc= 0.71509 val_loss= 1.46866 val_acc= 0.58209 time= 4.34000
Epoch: 0037 train_loss= 1.09009 train_acc= 0.72601 val_loss= 1.43897 val_acc= 0.58507 time= 4.35400
Epoch: 0038 train_loss= 1.03122 train_acc= 0.74686 val_loss= 1.41163 val_acc= 0.59403 time= 4.33300
Epoch: 0039 train_loss= 0.98312 train_acc= 0.75778 val_loss= 1.38543 val_acc= 0.58806 time= 4.33542
Epoch: 0040 train_loss= 0.93988 train_acc= 0.76671 val_loss= 1.36106 val_acc= 0.60597 time= 4.34524
Epoch: 0041 train_loss= 0.89046 train_acc= 0.78359 val_loss= 1.33933 val_acc= 0.61194 time= 4.35505
Epoch: 0042 train_loss= 0.84775 train_acc= 0.79385 val_loss= 1.32032 val_acc= 0.62388 time= 4.36916
Epoch: 0043 train_loss= 0.80264 train_acc= 0.81205 val_loss= 1.30454 val_acc= 0.62388 time= 4.34798
Epoch: 0044 train_loss= 0.75891 train_acc= 0.81899 val_loss= 1.28988 val_acc= 0.62687 time= 4.34901
Epoch: 0045 train_loss= 0.72429 train_acc= 0.82925 val_loss= 1.27422 val_acc= 0.62388 time= 4.35701
Epoch: 0046 train_loss= 0.68236 train_acc= 0.84480 val_loss= 1.26007 val_acc= 0.62090 time= 4.35599
Epoch: 0047 train_loss= 0.64767 train_acc= 0.84646 val_loss= 1.24836 val_acc= 0.62985 time= 4.34100
Epoch: 0048 train_loss= 0.61049 train_acc= 0.85870 val_loss= 1.23633 val_acc= 0.63582 time= 4.33801
Epoch: 0049 train_loss= 0.57923 train_acc= 0.86135 val_loss= 1.22596 val_acc= 0.64179 time= 4.35200
Epoch: 0050 train_loss= 0.54709 train_acc= 0.87392 val_loss= 1.21703 val_acc= 0.64478 time= 4.35419
Epoch: 0051 train_loss= 0.51672 train_acc= 0.88484 val_loss= 1.20949 val_acc= 0.65075 time= 4.36400
Epoch: 0052 train_loss= 0.48674 train_acc= 0.89146 val_loss= 1.20563 val_acc= 0.65373 time= 4.35079
Epoch: 0053 train_loss= 0.46241 train_acc= 0.89907 val_loss= 1.19816 val_acc= 0.64179 time= 4.33300
Epoch: 0054 train_loss= 0.43524 train_acc= 0.90536 val_loss= 1.19231 val_acc= 0.65075 time= 4.34700
Epoch: 0055 train_loss= 0.41119 train_acc= 0.91264 val_loss= 1.18775 val_acc= 0.65373 time= 4.35398
Epoch: 0056 train_loss= 0.38862 train_acc= 0.91429 val_loss= 1.18228 val_acc= 0.65075 time= 4.35701
Epoch: 0057 train_loss= 0.36738 train_acc= 0.92224 val_loss= 1.18162 val_acc= 0.66269 time= 4.33493
Epoch: 0058 train_loss= 0.34481 train_acc= 0.93249 val_loss= 1.17846 val_acc= 0.65075 time= 4.34895
Epoch: 0059 train_loss= 0.32395 train_acc= 0.93746 val_loss= 1.17697 val_acc= 0.65970 time= 4.33402
Epoch: 0060 train_loss= 0.30943 train_acc= 0.94341 val_loss= 1.17856 val_acc= 0.65672 time= 4.35152
Epoch: 0061 train_loss= 0.29493 train_acc= 0.94044 val_loss= 1.17735 val_acc= 0.65672 time= 4.33296
Epoch: 0062 train_loss= 0.27398 train_acc= 0.94937 val_loss= 1.17906 val_acc= 0.65373 time= 4.34228
Epoch: 0063 train_loss= 0.25817 train_acc= 0.95367 val_loss= 1.18072 val_acc= 0.66269 time= 4.34400
Epoch: 0064 train_loss= 0.24796 train_acc= 0.95797 val_loss= 1.17674 val_acc= 0.65970 time= 4.39499
Epoch: 0065 train_loss= 0.23283 train_acc= 0.95897 val_loss= 1.17576 val_acc= 0.66866 time= 4.35607
Epoch: 0066 train_loss= 0.21987 train_acc= 0.96029 val_loss= 1.17771 val_acc= 0.67463 time= 4.34598
Epoch: 0067 train_loss= 0.20630 train_acc= 0.96327 val_loss= 1.18198 val_acc= 0.66866 time= 4.35800
Early stopping...
Optimization Finished!
Test set results: cost= 1.16526 accuracy= 0.68959 time= 1.48666
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7045    0.7251    0.7147       342
           1     0.7054    0.7670    0.7349       103
           2     0.7500    0.6000    0.6667       140
           3     0.5763    0.4304    0.4928        79
           4     0.6713    0.7273    0.6982       132
           5     0.6793    0.7987    0.7342       313
           6     0.6757    0.7353    0.7042       102
           7     0.5854    0.3429    0.4324        70
           8     0.5938    0.3800    0.4634        50
           9     0.6108    0.7290    0.6647       155
          10     0.8414    0.6524    0.7349       187
          11     0.6113    0.6537    0.6318       231
          12     0.7697    0.7135    0.7405       178
          13     0.7706    0.8117    0.7906       600
          14     0.7864    0.8424    0.8134       590
          15     0.7714    0.7105    0.7397        76
          16     0.7222    0.3824    0.5000        34
          17     0.5000    0.1000    0.1667        10
          18     0.4424    0.4582    0.4502       419
          19     0.6346    0.5116    0.5665       129
          20     0.6207    0.6429    0.6316        28
          21     1.0000    0.7241    0.8400        29
          22     0.5862    0.3696    0.4533        46

    accuracy                         0.6896      4043
   macro avg     0.6787    0.6004    0.6246      4043
weighted avg     0.6901    0.6896    0.6855      4043

Macro average Test Precision, Recall and F1-Score...
(0.6786689566306615, 0.600374114687837, 0.6245847433862499, None)
Micro average Test Precision, Recall and F1-Score...
(0.6895869403907989, 0.6895869403907989, 0.6895869403907989, None)
embeddings:
14157 3357 4043
[[ 0.27803484  0.27457228  0.2999516  ...  0.35002768  0.39269572
   0.33697525]
 [ 0.03386796  0.12055782 -0.01681272 ...  0.03172283  0.12830865
   0.07416654]
 [ 0.05200561  0.43517828  0.22967029 ...  0.21267654  0.27883893
   0.1297303 ]
 ...
 [ 0.10903786  0.1240718   0.14033037 ...  0.09663665  0.2033734
   0.14459659]
 [ 0.19148552  0.29666898  0.03565176 ...  0.13200374 -0.01291367
   0.18369314]
 [ 0.07874773  0.01947599  0.22799608 ...  0.24902833  0.2432569
   0.09716367]]

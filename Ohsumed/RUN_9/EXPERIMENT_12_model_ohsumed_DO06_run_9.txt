(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.01919 val_loss= 3.11476 val_acc= 0.21493 time= 0.58203
Epoch: 0002 train_loss= 3.11465 train_acc= 0.18862 val_loss= 3.06937 val_acc= 0.20597 time= 0.28901
Epoch: 0003 train_loss= 3.06940 train_acc= 0.17803 val_loss= 2.99954 val_acc= 0.20597 time= 0.28700
Epoch: 0004 train_loss= 3.00011 train_acc= 0.17538 val_loss= 2.91208 val_acc= 0.20597 time= 0.28700
Epoch: 0005 train_loss= 2.91376 train_acc= 0.17505 val_loss= 2.82245 val_acc= 0.20597 time= 0.29597
Epoch: 0006 train_loss= 2.82600 train_acc= 0.17472 val_loss= 2.74868 val_acc= 0.20597 time= 0.29203
Epoch: 0007 train_loss= 2.75422 train_acc= 0.17571 val_loss= 2.70428 val_acc= 0.20896 time= 0.28497
Epoch: 0008 train_loss= 2.71261 train_acc= 0.18001 val_loss= 2.69002 val_acc= 0.20896 time= 0.29004
Epoch: 0009 train_loss= 2.70086 train_acc= 0.18101 val_loss= 2.68722 val_acc= 0.20896 time= 0.28699
Epoch: 0010 train_loss= 2.69766 train_acc= 0.18398 val_loss= 2.67564 val_acc= 0.20896 time= 0.29401
Epoch: 0011 train_loss= 2.68390 train_acc= 0.18498 val_loss= 2.65029 val_acc= 0.22388 time= 0.28799
Epoch: 0012 train_loss= 2.64923 train_acc= 0.19358 val_loss= 2.61986 val_acc= 0.23582 time= 0.28897
Epoch: 0013 train_loss= 2.61297 train_acc= 0.20483 val_loss= 2.59217 val_acc= 0.25075 time= 0.28803
Epoch: 0014 train_loss= 2.57382 train_acc= 0.22336 val_loss= 2.56820 val_acc= 0.26567 time= 0.29600
Epoch: 0015 train_loss= 2.54506 train_acc= 0.23958 val_loss= 2.54530 val_acc= 0.28060 time= 0.29310
Epoch: 0016 train_loss= 2.51428 train_acc= 0.25943 val_loss= 2.51998 val_acc= 0.29254 time= 0.28800
Epoch: 0017 train_loss= 2.48227 train_acc= 0.27366 val_loss= 2.49024 val_acc= 0.30746 time= 0.28906
Epoch: 0018 train_loss= 2.45273 train_acc= 0.28888 val_loss= 2.45571 val_acc= 0.31045 time= 0.29461
Epoch: 0019 train_loss= 2.41749 train_acc= 0.29881 val_loss= 2.41720 val_acc= 0.31045 time= 0.29200
Epoch: 0020 train_loss= 2.37168 train_acc= 0.31469 val_loss= 2.37597 val_acc= 0.31343 time= 0.29401
Epoch: 0021 train_loss= 2.32624 train_acc= 0.31966 val_loss= 2.33326 val_acc= 0.31343 time= 0.28600
Epoch: 0022 train_loss= 2.28002 train_acc= 0.32892 val_loss= 2.28996 val_acc= 0.31940 time= 0.29100
Epoch: 0023 train_loss= 2.23518 train_acc= 0.34017 val_loss= 2.24647 val_acc= 0.33433 time= 0.29600
Epoch: 0024 train_loss= 2.18101 train_acc= 0.35738 val_loss= 2.20324 val_acc= 0.34030 time= 0.28800
Epoch: 0025 train_loss= 2.13230 train_acc= 0.37756 val_loss= 2.16052 val_acc= 0.35821 time= 0.29000
Epoch: 0026 train_loss= 2.07799 train_acc= 0.40602 val_loss= 2.11853 val_acc= 0.39701 time= 0.28700
Epoch: 0027 train_loss= 2.03218 train_acc= 0.43845 val_loss= 2.07751 val_acc= 0.42388 time= 0.29599
Epoch: 0028 train_loss= 1.97356 train_acc= 0.46889 val_loss= 2.03728 val_acc= 0.45672 time= 0.28701
Epoch: 0029 train_loss= 1.92347 train_acc= 0.51257 val_loss= 1.99744 val_acc= 0.48955 time= 0.28899
Epoch: 0030 train_loss= 1.86747 train_acc= 0.54335 val_loss= 1.95708 val_acc= 0.50149 time= 0.28600
Epoch: 0031 train_loss= 1.81703 train_acc= 0.56982 val_loss= 1.91564 val_acc= 0.51940 time= 0.29600
Epoch: 0032 train_loss= 1.75681 train_acc= 0.58670 val_loss= 1.87355 val_acc= 0.52537 time= 0.29200
Epoch: 0033 train_loss= 1.70039 train_acc= 0.59927 val_loss= 1.83081 val_acc= 0.52537 time= 0.28700
Epoch: 0034 train_loss= 1.64809 train_acc= 0.60854 val_loss= 1.78839 val_acc= 0.53433 time= 0.28600
Epoch: 0035 train_loss= 1.58769 train_acc= 0.61880 val_loss= 1.74785 val_acc= 0.53731 time= 0.29100
Epoch: 0036 train_loss= 1.54228 train_acc= 0.62508 val_loss= 1.70950 val_acc= 0.53731 time= 0.29500
Epoch: 0037 train_loss= 1.48115 train_acc= 0.63997 val_loss= 1.67357 val_acc= 0.55224 time= 0.28700
Epoch: 0038 train_loss= 1.43447 train_acc= 0.64560 val_loss= 1.63977 val_acc= 0.55821 time= 0.28600
Epoch: 0039 train_loss= 1.38340 train_acc= 0.65255 val_loss= 1.60891 val_acc= 0.56119 time= 0.28800
Epoch: 0040 train_loss= 1.33038 train_acc= 0.66545 val_loss= 1.58055 val_acc= 0.56418 time= 0.29700
Epoch: 0041 train_loss= 1.28115 train_acc= 0.68266 val_loss= 1.55293 val_acc= 0.57910 time= 0.28705
Epoch: 0042 train_loss= 1.24065 train_acc= 0.68365 val_loss= 1.52569 val_acc= 0.58209 time= 0.28600
Epoch: 0043 train_loss= 1.18794 train_acc= 0.70682 val_loss= 1.49815 val_acc= 0.59104 time= 0.29011
Epoch: 0044 train_loss= 1.14584 train_acc= 0.71377 val_loss= 1.47180 val_acc= 0.58507 time= 0.29286
Epoch: 0045 train_loss= 1.10653 train_acc= 0.72866 val_loss= 1.44597 val_acc= 0.59701 time= 0.29003
Epoch: 0046 train_loss= 1.06681 train_acc= 0.73825 val_loss= 1.42138 val_acc= 0.59701 time= 0.28800
Epoch: 0047 train_loss= 1.02441 train_acc= 0.74686 val_loss= 1.39775 val_acc= 0.60000 time= 0.28623
Epoch: 0048 train_loss= 0.98019 train_acc= 0.76704 val_loss= 1.37599 val_acc= 0.60896 time= 0.29266
Epoch: 0049 train_loss= 0.94109 train_acc= 0.76837 val_loss= 1.35683 val_acc= 0.61791 time= 0.29500
Epoch: 0050 train_loss= 0.90871 train_acc= 0.78028 val_loss= 1.34009 val_acc= 0.61791 time= 0.29196
Epoch: 0051 train_loss= 0.87187 train_acc= 0.78855 val_loss= 1.32365 val_acc= 0.61493 time= 0.29400
Epoch: 0052 train_loss= 0.83687 train_acc= 0.80046 val_loss= 1.30825 val_acc= 0.61791 time= 0.28600
Epoch: 0053 train_loss= 0.79965 train_acc= 0.81966 val_loss= 1.29365 val_acc= 0.62687 time= 0.29700
Epoch: 0054 train_loss= 0.76704 train_acc= 0.81701 val_loss= 1.28057 val_acc= 0.62985 time= 0.29339
Epoch: 0055 train_loss= 0.73767 train_acc= 0.82429 val_loss= 1.26826 val_acc= 0.62985 time= 0.28900
Epoch: 0056 train_loss= 0.70191 train_acc= 0.83190 val_loss= 1.25646 val_acc= 0.62985 time= 0.29000
Epoch: 0057 train_loss= 0.67703 train_acc= 0.84183 val_loss= 1.24556 val_acc= 0.63284 time= 0.29197
Epoch: 0058 train_loss= 0.65112 train_acc= 0.84216 val_loss= 1.23550 val_acc= 0.63582 time= 0.29203
Epoch: 0059 train_loss= 0.62393 train_acc= 0.85572 val_loss= 1.22589 val_acc= 0.65075 time= 0.28997
Epoch: 0060 train_loss= 0.60012 train_acc= 0.85771 val_loss= 1.21701 val_acc= 0.65373 time= 0.29003
Epoch: 0061 train_loss= 0.56561 train_acc= 0.87095 val_loss= 1.20994 val_acc= 0.65373 time= 0.29456
Epoch: 0062 train_loss= 0.54894 train_acc= 0.87657 val_loss= 1.20274 val_acc= 0.65373 time= 0.29303
Epoch: 0063 train_loss= 0.52526 train_acc= 0.88021 val_loss= 1.19521 val_acc= 0.65672 time= 0.29101
Epoch: 0064 train_loss= 0.50000 train_acc= 0.88948 val_loss= 1.18779 val_acc= 0.65970 time= 0.28696
Epoch: 0065 train_loss= 0.48266 train_acc= 0.88782 val_loss= 1.18370 val_acc= 0.65373 time= 0.29011
Epoch: 0066 train_loss= 0.45407 train_acc= 0.89676 val_loss= 1.18136 val_acc= 0.65970 time= 0.30500
Epoch: 0067 train_loss= 0.44026 train_acc= 0.90238 val_loss= 1.17857 val_acc= 0.65672 time= 0.29300
Epoch: 0068 train_loss= 0.42241 train_acc= 0.90933 val_loss= 1.17508 val_acc= 0.65672 time= 0.29101
Epoch: 0069 train_loss= 0.40223 train_acc= 0.91760 val_loss= 1.17284 val_acc= 0.65970 time= 0.28700
Epoch: 0070 train_loss= 0.38572 train_acc= 0.92389 val_loss= 1.16951 val_acc= 0.65672 time= 0.29475
Epoch: 0071 train_loss= 0.37946 train_acc= 0.92654 val_loss= 1.16489 val_acc= 0.65373 time= 0.29300
Epoch: 0072 train_loss= 0.35699 train_acc= 0.92952 val_loss= 1.16092 val_acc= 0.65672 time= 0.28700
Epoch: 0073 train_loss= 0.33951 train_acc= 0.93051 val_loss= 1.15846 val_acc= 0.65672 time= 0.28898
Epoch: 0074 train_loss= 0.32886 train_acc= 0.93613 val_loss= 1.15768 val_acc= 0.66567 time= 0.29881
Epoch: 0075 train_loss= 0.31443 train_acc= 0.93613 val_loss= 1.15812 val_acc= 0.67164 time= 0.29203
Epoch: 0076 train_loss= 0.29933 train_acc= 0.94176 val_loss= 1.16034 val_acc= 0.66866 time= 0.28619
Epoch: 0077 train_loss= 0.28812 train_acc= 0.94705 val_loss= 1.16464 val_acc= 0.66269 time= 0.28803
Epoch: 0078 train_loss= 0.27833 train_acc= 0.94904 val_loss= 1.16731 val_acc= 0.65970 time= 0.28697
Early stopping...
Optimization Finished!
Test set results: cost= 1.16093 accuracy= 0.68563 time= 0.13200
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7143    0.7164    0.7153       342
           1     0.6842    0.7573    0.7189       103
           2     0.6825    0.6143    0.6466       140
           3     0.6889    0.3924    0.5000        79
           4     0.6601    0.7652    0.7088       132
           5     0.6649    0.8051    0.7283       313
           6     0.6667    0.7451    0.7037       102
           7     0.5758    0.2714    0.3689        70
           8     0.5806    0.3600    0.4444        50
           9     0.6033    0.7161    0.6549       155
          10     0.8333    0.6684    0.7418       187
          11     0.6455    0.6147    0.6297       231
          12     0.7602    0.7303    0.7450       178
          13     0.7662    0.8083    0.7867       600
          14     0.7916    0.8305    0.8106       590
          15     0.7500    0.7105    0.7297        76
          16     0.8462    0.3235    0.4681        34
          17     0.5000    0.1000    0.1667        10
          18     0.4255    0.4773    0.4499       419
          19     0.6346    0.5116    0.5665       129
          20     0.6667    0.5714    0.6154        28
          21     1.0000    0.7241    0.8400        29
          22     0.6364    0.3043    0.4118        46

    accuracy                         0.6856      4043
   macro avg     0.6860    0.5878    0.6153      4043
weighted avg     0.6890    0.6856    0.6811      4043

Macro average Test Precision, Recall and F1-Score...
(0.6859755994957676, 0.5877624874050613, 0.6152958188917748, None)
Micro average Test Precision, Recall and F1-Score...
(0.6856294830571358, 0.6856294830571358, 0.6856294830571358, None)
embeddings:
14157 3357 4043
[[0.28633195 0.25281128 0.39483416 ... 0.20021199 0.05766832 0.45389673]
 [0.01609452 0.04471124 0.0585563  ... 0.08823258 0.01025006 0.14308883]
 [0.00785134 0.11619442 0.13583048 ... 0.36448196 0.00629891 0.2506526 ]
 ...
 [0.09829855 0.11143973 0.25256675 ... 0.114062   0.1395458  0.15741748]
 [0.04494667 0.07267257 0.37280044 ... 0.17546903 0.17439656 0.345048  ]
 [0.21264002 0.15223975 0.10919402 ... 0.23004188 0.04760754 0.36003447]]

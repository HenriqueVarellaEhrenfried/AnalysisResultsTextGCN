(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13553 train_acc= 0.02548 val_loss= 2.93208 val_acc= 0.23881 time= 0.58325
Epoch: 0002 train_loss= 2.93306 train_acc= 0.21013 val_loss= 2.73568 val_acc= 0.21194 time= 0.29310
Epoch: 0003 train_loss= 2.75096 train_acc= 0.18663 val_loss= 2.70370 val_acc= 0.20000 time= 0.29655
Epoch: 0004 train_loss= 2.72348 train_acc= 0.17141 val_loss= 2.54904 val_acc= 0.26567 time= 0.29003
Epoch: 0005 train_loss= 2.52877 train_acc= 0.24123 val_loss= 2.48972 val_acc= 0.34925 time= 0.28900
Epoch: 0006 train_loss= 2.44176 train_acc= 0.35407 val_loss= 2.38138 val_acc= 0.40597 time= 0.28635
Epoch: 0007 train_loss= 2.31336 train_acc= 0.42191 val_loss= 2.21502 val_acc= 0.43284 time= 0.29900
Epoch: 0008 train_loss= 2.13949 train_acc= 0.45930 val_loss= 2.06724 val_acc= 0.41194 time= 0.28900
Epoch: 0009 train_loss= 1.97873 train_acc= 0.44937 val_loss= 1.95649 val_acc= 0.42985 time= 0.29009
Epoch: 0010 train_loss= 1.84270 train_acc= 0.46923 val_loss= 1.84021 val_acc= 0.50746 time= 0.28900
Epoch: 0011 train_loss= 1.67944 train_acc= 0.56287 val_loss= 1.75568 val_acc= 0.53731 time= 0.29525
Epoch: 0012 train_loss= 1.53064 train_acc= 0.60026 val_loss= 1.67572 val_acc= 0.54328 time= 0.29581
Epoch: 0013 train_loss= 1.39688 train_acc= 0.63766 val_loss= 1.58817 val_acc= 0.54627 time= 0.28900
Epoch: 0014 train_loss= 1.28145 train_acc= 0.67869 val_loss= 1.50274 val_acc= 0.58209 time= 0.28700
Epoch: 0015 train_loss= 1.15535 train_acc= 0.70880 val_loss= 1.44181 val_acc= 0.58209 time= 0.29133
Epoch: 0016 train_loss= 1.05237 train_acc= 0.71807 val_loss= 1.38999 val_acc= 0.59403 time= 0.29400
Epoch: 0017 train_loss= 0.94432 train_acc= 0.74487 val_loss= 1.36750 val_acc= 0.60896 time= 0.28803
Epoch: 0018 train_loss= 0.85728 train_acc= 0.76837 val_loss= 1.33437 val_acc= 0.62687 time= 0.28800
Epoch: 0019 train_loss= 0.74955 train_acc= 0.80807 val_loss= 1.31083 val_acc= 0.61194 time= 0.29297
Epoch: 0020 train_loss= 0.67256 train_acc= 0.82263 val_loss= 1.27712 val_acc= 0.62090 time= 0.29803
Epoch: 0021 train_loss= 0.59077 train_acc= 0.85010 val_loss= 1.25065 val_acc= 0.63284 time= 0.29088
Epoch: 0022 train_loss= 0.52742 train_acc= 0.86201 val_loss= 1.22880 val_acc= 0.62985 time= 0.29116
Epoch: 0023 train_loss= 0.45879 train_acc= 0.87723 val_loss= 1.22537 val_acc= 0.63582 time= 0.28700
Epoch: 0024 train_loss= 0.40324 train_acc= 0.89212 val_loss= 1.23801 val_acc= 0.66269 time= 0.29700
Epoch: 0025 train_loss= 0.35075 train_acc= 0.91463 val_loss= 1.24308 val_acc= 0.66567 time= 0.28900
Epoch: 0026 train_loss= 0.30753 train_acc= 0.92621 val_loss= 1.26107 val_acc= 0.64776 time= 0.28800
Epoch: 0027 train_loss= 0.26940 train_acc= 0.93349 val_loss= 1.25054 val_acc= 0.66866 time= 0.28700
Epoch: 0028 train_loss= 0.23641 train_acc= 0.94143 val_loss= 1.24443 val_acc= 0.66866 time= 0.29197
Epoch: 0029 train_loss= 0.20352 train_acc= 0.95103 val_loss= 1.24901 val_acc= 0.65970 time= 0.29304
Epoch: 0030 train_loss= 0.17629 train_acc= 0.95764 val_loss= 1.28647 val_acc= 0.65970 time= 0.28699
Early stopping...
Optimization Finished!
Test set results: cost= 1.32282 accuracy= 0.67475 time= 0.12700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7315    0.6374    0.6813       342
           1     0.6446    0.7573    0.6964       103
           2     0.7653    0.5357    0.6303       140
           3     0.5789    0.4177    0.4853        79
           4     0.7143    0.7197    0.7170       132
           5     0.7071    0.7636    0.7343       313
           6     0.6916    0.7255    0.7081       102
           7     0.5814    0.3571    0.4425        70
           8     0.5500    0.4400    0.4889        50
           9     0.6457    0.7290    0.6848       155
          10     0.7914    0.6898    0.7371       187
          11     0.6481    0.6537    0.6509       231
          12     0.8014    0.6573    0.7222       178
          13     0.7776    0.7633    0.7704       600
          14     0.8099    0.8017    0.8058       590
          15     0.7656    0.6447    0.7000        76
          16     0.5357    0.4412    0.4839        34
          17     0.5000    0.1000    0.1667        10
          18     0.3816    0.5537    0.4518       419
          19     0.6000    0.5814    0.5906       129
          20     0.5806    0.6429    0.6102        28
          21     1.0000    0.7241    0.8400        29
          22     0.4359    0.3696    0.4000        46

    accuracy                         0.6747      4043
   macro avg     0.6625    0.5959    0.6173      4043
weighted avg     0.6907    0.6747    0.6779      4043

Macro average Test Precision, Recall and F1-Score...
(0.6625365646320162, 0.595934952021612, 0.617316713378712, None)
Micro average Test Precision, Recall and F1-Score...
(0.6747464753895622, 0.6747464753895622, 0.6747464753895622, None)
embeddings:
14157 3357 4043
[[ 0.4782715   0.16413806  0.44750524 ...  0.25731888  0.56571865
   0.5682275 ]
 [ 0.22872348  0.04140573  0.0748672  ...  0.03186022  0.21576855
   0.04708937]
 [ 0.62462443  0.04035681  0.4148291  ...  0.07152119  0.68605965
   0.42677438]
 ...
 [ 0.38774925  0.15318404  0.17631918 ...  0.14349021  0.1778199
   0.26905122]
 [ 0.6262735  -0.08623658  0.41494155 ... -0.1792409   0.3503289
  -0.0069038 ]
 [ 0.0512798   0.35027727  0.11377206 ...  0.34605303  0.07524637
   0.21056513]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13548 train_acc= 0.06023 val_loss= 3.11386 val_acc= 0.20000 time= 0.58417
Epoch: 0002 train_loss= 3.11392 train_acc= 0.17141 val_loss= 3.06788 val_acc= 0.20000 time= 0.29397
Epoch: 0003 train_loss= 3.06796 train_acc= 0.17141 val_loss= 2.99754 val_acc= 0.20000 time= 0.28897
Epoch: 0004 train_loss= 2.99818 train_acc= 0.17141 val_loss= 2.90984 val_acc= 0.20000 time= 0.28708
Epoch: 0005 train_loss= 2.91094 train_acc= 0.17141 val_loss= 2.81976 val_acc= 0.20000 time= 0.28902
Epoch: 0006 train_loss= 2.82270 train_acc= 0.17141 val_loss= 2.74383 val_acc= 0.20000 time= 0.29198
Epoch: 0007 train_loss= 2.74915 train_acc= 0.17141 val_loss= 2.69711 val_acc= 0.20000 time= 0.29100
Epoch: 0008 train_loss= 2.70668 train_acc= 0.17141 val_loss= 2.68518 val_acc= 0.20000 time= 0.29100
Epoch: 0009 train_loss= 2.69753 train_acc= 0.17141 val_loss= 2.68705 val_acc= 0.20000 time= 0.29100
Epoch: 0010 train_loss= 2.70097 train_acc= 0.17141 val_loss= 2.67631 val_acc= 0.20597 time= 0.29300
Epoch: 0011 train_loss= 2.68433 train_acc= 0.17373 val_loss= 2.64941 val_acc= 0.20896 time= 0.29000
Epoch: 0012 train_loss= 2.64708 train_acc= 0.18134 val_loss= 2.61707 val_acc= 0.22985 time= 0.29097
Epoch: 0013 train_loss= 2.60677 train_acc= 0.19722 val_loss= 2.58823 val_acc= 0.25075 time= 0.29511
Epoch: 0014 train_loss= 2.56653 train_acc= 0.21641 val_loss= 2.56397 val_acc= 0.26567 time= 0.28773
Epoch: 0015 train_loss= 2.53223 train_acc= 0.24123 val_loss= 2.54020 val_acc= 0.28358 time= 0.28903
Epoch: 0016 train_loss= 2.50082 train_acc= 0.26671 val_loss= 2.51263 val_acc= 0.29552 time= 0.28597
Epoch: 0017 train_loss= 2.46780 train_acc= 0.29351 val_loss= 2.47901 val_acc= 0.30149 time= 0.29303
Epoch: 0018 train_loss= 2.43081 train_acc= 0.31899 val_loss= 2.43978 val_acc= 0.31940 time= 0.28600
Epoch: 0019 train_loss= 2.38582 train_acc= 0.33752 val_loss= 2.39657 val_acc= 0.33433 time= 0.28700
Epoch: 0020 train_loss= 2.33796 train_acc= 0.35142 val_loss= 2.35101 val_acc= 0.33731 time= 0.28797
Epoch: 0021 train_loss= 2.28974 train_acc= 0.36036 val_loss= 2.30426 val_acc= 0.34925 time= 0.29203
Epoch: 0022 train_loss= 2.23971 train_acc= 0.36929 val_loss= 2.25683 val_acc= 0.35224 time= 0.28900
Epoch: 0023 train_loss= 2.18312 train_acc= 0.38154 val_loss= 2.20927 val_acc= 0.36119 time= 0.28900
Epoch: 0024 train_loss= 2.12755 train_acc= 0.39444 val_loss= 2.16220 val_acc= 0.37313 time= 0.28801
Epoch: 0025 train_loss= 2.07558 train_acc= 0.41827 val_loss= 2.11607 val_acc= 0.39104 time= 0.29537
Epoch: 0026 train_loss= 2.01753 train_acc= 0.45036 val_loss= 2.07101 val_acc= 0.44179 time= 0.28900
Epoch: 0027 train_loss= 1.95861 train_acc= 0.48544 val_loss= 2.02708 val_acc= 0.48060 time= 0.28697
Epoch: 0028 train_loss= 1.90320 train_acc= 0.52118 val_loss= 1.98385 val_acc= 0.49851 time= 0.29003
Epoch: 0029 train_loss= 1.84094 train_acc= 0.55592 val_loss= 1.94088 val_acc= 0.51343 time= 0.29755
Epoch: 0030 train_loss= 1.78151 train_acc= 0.58537 val_loss= 1.89810 val_acc= 0.51940 time= 0.28900
Epoch: 0031 train_loss= 1.72042 train_acc= 0.60026 val_loss= 1.85531 val_acc= 0.53134 time= 0.29000
Epoch: 0032 train_loss= 1.66159 train_acc= 0.60721 val_loss= 1.81316 val_acc= 0.53433 time= 0.29259
Epoch: 0033 train_loss= 1.60312 train_acc= 0.61714 val_loss= 1.77197 val_acc= 0.53731 time= 0.29003
Epoch: 0034 train_loss= 1.54883 train_acc= 0.62773 val_loss= 1.73232 val_acc= 0.53731 time= 0.28997
Epoch: 0035 train_loss= 1.49341 train_acc= 0.64196 val_loss= 1.69437 val_acc= 0.54328 time= 0.29303
Epoch: 0036 train_loss= 1.44591 train_acc= 0.65354 val_loss= 1.65801 val_acc= 0.55522 time= 0.29200
Epoch: 0037 train_loss= 1.39207 train_acc= 0.66942 val_loss= 1.62326 val_acc= 0.56119 time= 0.29000
Epoch: 0038 train_loss= 1.33748 train_acc= 0.67803 val_loss= 1.59055 val_acc= 0.56716 time= 0.28800
Epoch: 0039 train_loss= 1.28525 train_acc= 0.68862 val_loss= 1.55982 val_acc= 0.56716 time= 0.29100
Epoch: 0040 train_loss= 1.23772 train_acc= 0.69259 val_loss= 1.53111 val_acc= 0.57313 time= 0.29800
Epoch: 0041 train_loss= 1.18819 train_acc= 0.70549 val_loss= 1.50375 val_acc= 0.57612 time= 0.28800
Epoch: 0042 train_loss= 1.14151 train_acc= 0.71244 val_loss= 1.47812 val_acc= 0.57612 time= 0.29203
Epoch: 0043 train_loss= 1.09880 train_acc= 0.72105 val_loss= 1.45357 val_acc= 0.58507 time= 0.28900
Epoch: 0044 train_loss= 1.05228 train_acc= 0.74156 val_loss= 1.43048 val_acc= 0.58507 time= 0.29301
Epoch: 0045 train_loss= 1.01104 train_acc= 0.75083 val_loss= 1.40829 val_acc= 0.60299 time= 0.29000
Epoch: 0046 train_loss= 0.96819 train_acc= 0.76340 val_loss= 1.38695 val_acc= 0.60299 time= 0.28800
Epoch: 0047 train_loss= 0.93126 train_acc= 0.77068 val_loss= 1.36707 val_acc= 0.61493 time= 0.29000
Epoch: 0048 train_loss= 0.89014 train_acc= 0.78623 val_loss= 1.34859 val_acc= 0.61493 time= 0.29100
Epoch: 0049 train_loss= 0.85254 train_acc= 0.79947 val_loss= 1.33155 val_acc= 0.61791 time= 0.28896
Epoch: 0050 train_loss= 0.81446 train_acc= 0.81204 val_loss= 1.31656 val_acc= 0.63284 time= 0.29100
Epoch: 0051 train_loss= 0.78167 train_acc= 0.81436 val_loss= 1.30306 val_acc= 0.62687 time= 0.29000
Epoch: 0052 train_loss= 0.74545 train_acc= 0.82462 val_loss= 1.28937 val_acc= 0.63582 time= 0.29700
Epoch: 0053 train_loss= 0.71190 train_acc= 0.83223 val_loss= 1.27688 val_acc= 0.64179 time= 0.29103
Epoch: 0054 train_loss= 0.68171 train_acc= 0.84613 val_loss= 1.26583 val_acc= 0.63582 time= 0.29000
Epoch: 0055 train_loss= 0.65311 train_acc= 0.84977 val_loss= 1.25582 val_acc= 0.63284 time= 0.29014
Epoch: 0056 train_loss= 0.61582 train_acc= 0.86201 val_loss= 1.24671 val_acc= 0.63881 time= 0.29203
Epoch: 0057 train_loss= 0.59580 train_acc= 0.86797 val_loss= 1.23790 val_acc= 0.64478 time= 0.28800
Epoch: 0058 train_loss= 0.56679 train_acc= 0.87525 val_loss= 1.22988 val_acc= 0.65373 time= 0.28800
Epoch: 0059 train_loss= 0.54246 train_acc= 0.88054 val_loss= 1.22201 val_acc= 0.65373 time= 0.29097
Epoch: 0060 train_loss= 0.51582 train_acc= 0.89312 val_loss= 1.21476 val_acc= 0.65373 time= 0.29303
Epoch: 0061 train_loss= 0.49194 train_acc= 0.89775 val_loss= 1.20840 val_acc= 0.65075 time= 0.29000
Epoch: 0062 train_loss= 0.47105 train_acc= 0.89775 val_loss= 1.20185 val_acc= 0.64776 time= 0.28928
Epoch: 0063 train_loss= 0.44359 train_acc= 0.91032 val_loss= 1.19474 val_acc= 0.64776 time= 0.29461
Epoch: 0064 train_loss= 0.42405 train_acc= 0.91430 val_loss= 1.18833 val_acc= 0.65672 time= 0.28947
Epoch: 0065 train_loss= 0.40874 train_acc= 0.91297 val_loss= 1.18416 val_acc= 0.66567 time= 0.28800
Epoch: 0066 train_loss= 0.38720 train_acc= 0.92191 val_loss= 1.18316 val_acc= 0.65373 time= 0.29000
Epoch: 0067 train_loss= 0.36912 train_acc= 0.92753 val_loss= 1.18409 val_acc= 0.65075 time= 0.29200
Epoch: 0068 train_loss= 0.35398 train_acc= 0.92819 val_loss= 1.18577 val_acc= 0.65075 time= 0.28800
Epoch: 0069 train_loss= 0.33334 train_acc= 0.93547 val_loss= 1.18576 val_acc= 0.65075 time= 0.28800
Epoch: 0070 train_loss= 0.32082 train_acc= 0.93977 val_loss= 1.18367 val_acc= 0.65075 time= 0.29100
Epoch: 0071 train_loss= 0.30267 train_acc= 0.94507 val_loss= 1.17988 val_acc= 0.65672 time= 0.29051
Epoch: 0072 train_loss= 0.28896 train_acc= 0.94639 val_loss= 1.17600 val_acc= 0.66269 time= 0.29000
Epoch: 0073 train_loss= 0.27794 train_acc= 0.95202 val_loss= 1.17252 val_acc= 0.66567 time= 0.29100
Epoch: 0074 train_loss= 0.26806 train_acc= 0.95136 val_loss= 1.17294 val_acc= 0.65970 time= 0.28800
Epoch: 0075 train_loss= 0.25642 train_acc= 0.95433 val_loss= 1.17550 val_acc= 0.66567 time= 0.29367
Epoch: 0076 train_loss= 0.24472 train_acc= 0.96029 val_loss= 1.17728 val_acc= 0.66567 time= 0.29211
Epoch: 0077 train_loss= 0.23203 train_acc= 0.96492 val_loss= 1.18103 val_acc= 0.66866 time= 0.29200
Early stopping...
Optimization Finished!
Test set results: cost= 1.16094 accuracy= 0.68884 time= 0.12800
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7110    0.7193    0.7151       342
           1     0.6695    0.7670    0.7149       103
           2     0.7522    0.6071    0.6719       140
           3     0.6522    0.3797    0.4800        79
           4     0.6410    0.7576    0.6944       132
           5     0.6915    0.8019    0.7426       313
           6     0.6887    0.7157    0.7019       102
           7     0.6471    0.3143    0.4231        70
           8     0.5429    0.3800    0.4471        50
           9     0.6032    0.7355    0.6628       155
          10     0.8276    0.6417    0.7229       187
          11     0.6116    0.6407    0.6258       231
          12     0.7679    0.7247    0.7457       178
          13     0.7700    0.8200    0.7942       600
          14     0.7836    0.8407    0.8111       590
          15     0.7465    0.6974    0.7211        76
          16     0.6316    0.3529    0.4528        34
          17     0.5000    0.1000    0.1667        10
          18     0.4324    0.4654    0.4483       419
          19     0.6804    0.5116    0.5841       129
          20     0.6786    0.6786    0.6786        28
          21     1.0000    0.7241    0.8400        29
          22     0.6667    0.3043    0.4179        46

    accuracy                         0.6888      4043
   macro avg     0.6824    0.5948    0.6201      4043
weighted avg     0.6911    0.6888    0.6841      4043

Macro average Test Precision, Recall and F1-Score...
(0.6824232060776292, 0.5947963566310923, 0.6201284644521238, None)
Micro average Test Precision, Recall and F1-Score...
(0.688844917140737, 0.688844917140737, 0.688844917140737, None)
embeddings:
14157 3357 4043
[[ 0.4223891   0.30248728  0.39607438 ...  0.3015186   0.5392527
   0.45692396]
 [ 0.15944672  0.08556631  0.00372994 ...  0.01384319  0.4162662
   0.09824251]
 [ 0.28295767  0.2063377   0.21688105 ...  0.07203987  0.43179458
   0.32841134]
 ...
 [ 0.13351406  0.0613348   0.17950164 ...  0.1134682   0.2458328
   0.23648305]
 [ 0.22140479  0.20785733 -0.06003967 ...  0.04672265  0.08335315
  -0.07492404]
 [ 0.2327631   0.12524442  0.15131877 ...  0.18039356  0.30910373
   0.14896342]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13549 train_acc= 0.01952 val_loss= 2.92789 val_acc= 0.25075 time= 0.60003
Epoch: 0002 train_loss= 2.92885 train_acc= 0.22071 val_loss= 2.74073 val_acc= 0.24776 time= 0.29401
Epoch: 0003 train_loss= 2.76251 train_acc= 0.21906 val_loss= 2.68271 val_acc= 0.20000 time= 0.29796
Epoch: 0004 train_loss= 2.69896 train_acc= 0.17141 val_loss= 2.55836 val_acc= 0.24776 time= 0.29703
Epoch: 0005 train_loss= 2.53529 train_acc= 0.21741 val_loss= 2.49905 val_acc= 0.32836 time= 0.29500
Epoch: 0006 train_loss= 2.44962 train_acc= 0.32793 val_loss= 2.38049 val_acc= 0.37313 time= 0.29500
Epoch: 0007 train_loss= 2.32313 train_acc= 0.39742 val_loss= 2.22383 val_acc= 0.40597 time= 0.30001
Epoch: 0008 train_loss= 2.14956 train_acc= 0.43117 val_loss= 2.09018 val_acc= 0.39104 time= 0.29399
Epoch: 0009 train_loss= 2.00017 train_acc= 0.44341 val_loss= 1.97239 val_acc= 0.43881 time= 0.29600
Epoch: 0010 train_loss= 1.85158 train_acc= 0.48875 val_loss= 1.85863 val_acc= 0.51343 time= 0.29700
Epoch: 0011 train_loss= 1.69341 train_acc= 0.57379 val_loss= 1.76655 val_acc= 0.53433 time= 0.30200
Epoch: 0012 train_loss= 1.54836 train_acc= 0.60589 val_loss= 1.67257 val_acc= 0.53433 time= 0.29200
Epoch: 0013 train_loss= 1.40315 train_acc= 0.64825 val_loss= 1.59164 val_acc= 0.55224 time= 0.29505
Epoch: 0014 train_loss= 1.27813 train_acc= 0.68862 val_loss= 1.51876 val_acc= 0.58507 time= 0.29800
Epoch: 0015 train_loss= 1.16487 train_acc= 0.71178 val_loss= 1.44338 val_acc= 0.58806 time= 0.29800
Epoch: 0016 train_loss= 1.04679 train_acc= 0.72303 val_loss= 1.39358 val_acc= 0.60299 time= 0.29300
Epoch: 0017 train_loss= 0.94058 train_acc= 0.74355 val_loss= 1.37362 val_acc= 0.60299 time= 0.29600
Epoch: 0018 train_loss= 0.85064 train_acc= 0.77267 val_loss= 1.34097 val_acc= 0.61791 time= 0.29719
Epoch: 0019 train_loss= 0.74564 train_acc= 0.80708 val_loss= 1.31737 val_acc= 0.63582 time= 0.29600
Epoch: 0020 train_loss= 0.66254 train_acc= 0.83124 val_loss= 1.29829 val_acc= 0.62090 time= 0.29600
Epoch: 0021 train_loss= 0.58724 train_acc= 0.84811 val_loss= 1.27892 val_acc= 0.62985 time= 0.29700
Epoch: 0022 train_loss= 0.51982 train_acc= 0.86300 val_loss= 1.26846 val_acc= 0.62388 time= 0.29745
Epoch: 0023 train_loss= 0.45312 train_acc= 0.87326 val_loss= 1.26727 val_acc= 0.65075 time= 0.29500
Epoch: 0024 train_loss= 0.39590 train_acc= 0.89543 val_loss= 1.27837 val_acc= 0.66269 time= 0.29500
Epoch: 0025 train_loss= 0.34481 train_acc= 0.91827 val_loss= 1.28344 val_acc= 0.66567 time= 0.29200
Epoch: 0026 train_loss= 0.29371 train_acc= 0.92885 val_loss= 1.29899 val_acc= 0.65373 time= 0.29700
Epoch: 0027 train_loss= 0.26325 train_acc= 0.92985 val_loss= 1.29366 val_acc= 0.67164 time= 0.29900
Epoch: 0028 train_loss= 0.22518 train_acc= 0.94937 val_loss= 1.30805 val_acc= 0.68060 time= 0.29400
Early stopping...
Optimization Finished!
Test set results: cost= 1.31853 accuracy= 0.68192 time= 0.13000
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7101    0.7018    0.7059       342
           1     0.6279    0.7864    0.6983       103
           2     0.6929    0.6286    0.6592       140
           3     0.6034    0.4430    0.5109        79
           4     0.6392    0.7652    0.6966       132
           5     0.7109    0.7700    0.7393       313
           6     0.5594    0.7843    0.6531       102
           7     0.6286    0.3143    0.4190        70
           8     0.5833    0.2800    0.3784        50
           9     0.6270    0.7484    0.6824       155
          10     0.8194    0.6791    0.7427       187
          11     0.5512    0.7229    0.6255       231
          12     0.7914    0.7247    0.7566       178
          13     0.7733    0.7733    0.7733       600
          14     0.7745    0.8441    0.8078       590
          15     0.7606    0.7105    0.7347        76
          16     0.5909    0.3824    0.4643        34
          17     0.5000    0.1000    0.1667        10
          18     0.4657    0.3890    0.4239       419
          19     0.6216    0.5349    0.5750       129
          20     0.5862    0.6071    0.5965        28
          21     0.9524    0.6897    0.8000        29
          22     0.4595    0.3696    0.4096        46

    accuracy                         0.6819      4043
   macro avg     0.6535    0.5978    0.6095      4043
weighted avg     0.6807    0.6819    0.6758      4043

Macro average Test Precision, Recall and F1-Score...
(0.6534547692692497, 0.5977927696793609, 0.6095442497865732, None)
Micro average Test Precision, Recall and F1-Score...
(0.6819193668068266, 0.6819193668068266, 0.6819193668068266, None)
embeddings:
14157 3357 4043
[[ 6.45549238e-01 -8.70079249e-02  2.01695412e-01 ...  3.30927193e-01
   5.68373859e-01  1.94068789e-01]
 [ 2.35009938e-04  2.11981684e-03  7.43113458e-04 ... -5.80814816e-02
   4.85491827e-02  2.98078299e-01]
 [ 1.02246478e-01  8.01371410e-03  2.23833740e-01 ...  2.89652273e-02
   3.95359904e-01  4.02576745e-01]
 ...
 [ 3.26601058e-01  5.77705316e-02  1.92229301e-01 ...  1.02736734e-01
   3.24588418e-01  1.12886928e-01]
 [-1.14535689e-01  5.94727173e-02 -1.01938799e-01 ...  7.55410045e-02
  -5.88734597e-02  5.89292228e-01]
 [ 5.73054433e-01 -2.03005224e-02  2.76097059e-01 ...  3.17928404e-01
   4.15507287e-01 -1.46032441e-02]]

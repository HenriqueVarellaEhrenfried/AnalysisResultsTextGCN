(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13561 train_acc= 0.02349 val_loss= 3.10715 val_acc= 0.21791 time= 5.84904
Epoch: 0002 train_loss= 3.10733 train_acc= 0.18829 val_loss= 3.02908 val_acc= 0.20597 time= 5.68900
Epoch: 0003 train_loss= 3.03001 train_acc= 0.17770 val_loss= 2.90889 val_acc= 0.20597 time= 5.69800
Epoch: 0004 train_loss= 2.91168 train_acc= 0.17472 val_loss= 2.78519 val_acc= 0.20597 time= 5.70000
Epoch: 0005 train_loss= 2.79112 train_acc= 0.17439 val_loss= 2.70724 val_acc= 0.20597 time= 5.69598
Epoch: 0006 train_loss= 2.71705 train_acc= 0.17340 val_loss= 2.69993 val_acc= 0.20597 time= 5.73302
Epoch: 0007 train_loss= 2.71133 train_acc= 0.17373 val_loss= 2.70465 val_acc= 0.20299 time= 5.70398
Epoch: 0008 train_loss= 2.71468 train_acc= 0.17174 val_loss= 2.67449 val_acc= 0.20597 time= 5.70698
Epoch: 0009 train_loss= 2.67287 train_acc= 0.17340 val_loss= 2.62860 val_acc= 0.20896 time= 5.71104
Epoch: 0010 train_loss= 2.61272 train_acc= 0.18034 val_loss= 2.58992 val_acc= 0.22985 time= 5.73100
Epoch: 0011 train_loss= 2.55896 train_acc= 0.20251 val_loss= 2.55854 val_acc= 0.26269 time= 5.70404
Epoch: 0012 train_loss= 2.51841 train_acc= 0.24222 val_loss= 2.52449 val_acc= 0.28955 time= 5.70900
Epoch: 0013 train_loss= 2.47615 train_acc= 0.29186 val_loss= 2.48127 val_acc= 0.31642 time= 5.71300
Epoch: 0014 train_loss= 2.42767 train_acc= 0.35242 val_loss= 2.42809 val_acc= 0.34627 time= 5.69400
Epoch: 0015 train_loss= 2.36659 train_acc= 0.39212 val_loss= 2.36749 val_acc= 0.35522 time= 5.70600
Epoch: 0016 train_loss= 2.30001 train_acc= 0.40801 val_loss= 2.30340 val_acc= 0.36716 time= 5.68400
Epoch: 0017 train_loss= 2.22931 train_acc= 0.41198 val_loss= 2.23899 val_acc= 0.36716 time= 5.73798
Epoch: 0018 train_loss= 2.15864 train_acc= 0.41264 val_loss= 2.17585 val_acc= 0.36716 time= 5.71906
Epoch: 0019 train_loss= 2.08250 train_acc= 0.42522 val_loss= 2.11392 val_acc= 0.40299 time= 5.72500
Epoch: 0020 train_loss= 2.00894 train_acc= 0.44672 val_loss= 2.05262 val_acc= 0.42090 time= 5.72701
Epoch: 0021 train_loss= 1.92834 train_acc= 0.48511 val_loss= 1.99268 val_acc= 0.45970 time= 5.69898
Epoch: 0022 train_loss= 1.84618 train_acc= 0.52879 val_loss= 1.93539 val_acc= 0.49851 time= 5.70309
Epoch: 0023 train_loss= 1.77091 train_acc= 0.56916 val_loss= 1.87965 val_acc= 0.53134 time= 5.70699
Epoch: 0024 train_loss= 1.69148 train_acc= 0.59861 val_loss= 1.82282 val_acc= 0.53731 time= 5.70399
Epoch: 0025 train_loss= 1.61520 train_acc= 0.61383 val_loss= 1.76472 val_acc= 0.54030 time= 5.72101
Epoch: 0026 train_loss= 1.53605 train_acc= 0.63203 val_loss= 1.70834 val_acc= 0.54925 time= 5.67212
Epoch: 0027 train_loss= 1.46447 train_acc= 0.64328 val_loss= 1.65694 val_acc= 0.54627 time= 5.70799
Epoch: 0028 train_loss= 1.39314 train_acc= 0.66082 val_loss= 1.61072 val_acc= 0.54627 time= 5.72601
Epoch: 0029 train_loss= 1.32517 train_acc= 0.67604 val_loss= 1.56823 val_acc= 0.55522 time= 5.70699
Epoch: 0030 train_loss= 1.25731 train_acc= 0.68796 val_loss= 1.52837 val_acc= 0.58209 time= 5.68802
Epoch: 0031 train_loss= 1.19232 train_acc= 0.70152 val_loss= 1.49168 val_acc= 0.58507 time= 5.74021
Epoch: 0032 train_loss= 1.13059 train_acc= 0.71211 val_loss= 1.45752 val_acc= 0.58806 time= 5.71900
Epoch: 0033 train_loss= 1.07017 train_acc= 0.72800 val_loss= 1.42424 val_acc= 0.59104 time= 5.68103
Epoch: 0034 train_loss= 1.01577 train_acc= 0.74454 val_loss= 1.39293 val_acc= 0.59104 time= 5.71799
Epoch: 0035 train_loss= 0.95580 train_acc= 0.76208 val_loss= 1.36552 val_acc= 0.60000 time= 5.69901
Epoch: 0036 train_loss= 0.90354 train_acc= 0.77895 val_loss= 1.34069 val_acc= 0.60597 time= 5.70398
Epoch: 0037 train_loss= 0.84844 train_acc= 0.78789 val_loss= 1.31763 val_acc= 0.61791 time= 5.69802
Epoch: 0038 train_loss= 0.79892 train_acc= 0.80278 val_loss= 1.29692 val_acc= 0.62090 time= 5.70900
Epoch: 0039 train_loss= 0.75000 train_acc= 0.81569 val_loss= 1.27877 val_acc= 0.63284 time= 5.71007
Epoch: 0040 train_loss= 0.70665 train_acc= 0.82892 val_loss= 1.26184 val_acc= 0.63284 time= 5.70999
Epoch: 0041 train_loss= 0.66378 train_acc= 0.84381 val_loss= 1.24653 val_acc= 0.62388 time= 5.72499
Epoch: 0042 train_loss= 0.62276 train_acc= 0.85175 val_loss= 1.23338 val_acc= 0.62687 time= 5.70202
Epoch: 0043 train_loss= 0.58131 train_acc= 0.86367 val_loss= 1.22367 val_acc= 0.64179 time= 5.70099
Epoch: 0044 train_loss= 0.54664 train_acc= 0.86797 val_loss= 1.21454 val_acc= 0.64179 time= 5.69244
Epoch: 0045 train_loss= 0.51007 train_acc= 0.88286 val_loss= 1.20630 val_acc= 0.63284 time= 5.71800
Epoch: 0046 train_loss= 0.47917 train_acc= 0.88948 val_loss= 1.19879 val_acc= 0.64478 time= 5.69598
Epoch: 0047 train_loss= 0.44668 train_acc= 0.89973 val_loss= 1.19106 val_acc= 0.65075 time= 5.71301
Epoch: 0048 train_loss= 0.41495 train_acc= 0.90701 val_loss= 1.18485 val_acc= 0.65672 time= 5.73899
Epoch: 0049 train_loss= 0.38913 train_acc= 0.91827 val_loss= 1.17994 val_acc= 0.64478 time= 5.70402
Epoch: 0050 train_loss= 0.36127 train_acc= 0.92091 val_loss= 1.17567 val_acc= 0.66567 time= 5.69800
Epoch: 0051 train_loss= 0.33760 train_acc= 0.93051 val_loss= 1.17320 val_acc= 0.66269 time= 5.69799
Epoch: 0052 train_loss= 0.31695 train_acc= 0.93547 val_loss= 1.17221 val_acc= 0.66567 time= 5.70600
Epoch: 0053 train_loss= 0.29630 train_acc= 0.93977 val_loss= 1.17282 val_acc= 0.66866 time= 5.71899
Epoch: 0054 train_loss= 0.27256 train_acc= 0.94275 val_loss= 1.17404 val_acc= 0.66567 time= 5.70802
Epoch: 0055 train_loss= 0.25622 train_acc= 0.95003 val_loss= 1.17608 val_acc= 0.67164 time= 5.72900
Epoch: 0056 train_loss= 0.24028 train_acc= 0.95566 val_loss= 1.17755 val_acc= 0.67463 time= 5.70796
Epoch: 0057 train_loss= 0.22353 train_acc= 0.95963 val_loss= 1.17954 val_acc= 0.68060 time= 5.70404
Early stopping...
Optimization Finished!
Test set results: cost= 1.17037 accuracy= 0.69107 time= 1.96599
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7126    0.7105    0.7116       342
           1     0.7156    0.7573    0.7358       103
           2     0.7288    0.6143    0.6667       140
           3     0.5574    0.4304    0.4857        79
           4     0.6667    0.7576    0.7092       132
           5     0.6880    0.7891    0.7351       313
           6     0.6818    0.7353    0.7075       102
           7     0.5714    0.3429    0.4286        70
           8     0.5833    0.4200    0.4884        50
           9     0.6369    0.7355    0.6826       155
          10     0.8356    0.6524    0.7327       187
          11     0.6234    0.6450    0.6340       231
          12     0.7862    0.7022    0.7418       178
          13     0.7778    0.8050    0.7912       600
          14     0.7802    0.8424    0.8101       590
          15     0.7826    0.7105    0.7448        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4374    0.4916    0.4629       419
          19     0.6476    0.5271    0.5812       129
          20     0.6429    0.6429    0.6429        28
          21     1.0000    0.7241    0.8400        29
          22     0.6957    0.3478    0.4638        46

    accuracy                         0.6911      4043
   macro avg     0.6851    0.6016    0.6276      4043
weighted avg     0.6940    0.6911    0.6881      4043

Macro average Test Precision, Recall and F1-Score...
(0.6851188109783741, 0.6016059542672217, 0.6275635834872036, None)
Micro average Test Precision, Recall and F1-Score...
(0.6910709868909226, 0.6910709868909226, 0.6910709868909226, None)
embeddings:
14157 3357 4043
[[ 0.4203562   0.2433188   0.31029856 ...  0.27201337  0.14889102
   0.21751858]
 [ 0.22983152 -0.0705534   0.1740283  ...  0.0914925   0.10933113
   0.09174693]
 [ 0.17624949  0.09716939  0.29380217 ...  0.14452454  0.18194944
   0.12203688]
 ...
 [ 0.17300382  0.11892144  0.08911259 ...  0.17160268  0.13978015
   0.08680992]
 [ 0.05330661 -0.00244706  0.08157103 ...  0.31676847  0.06900394
   0.01789193]
 [ 0.22185524  0.11625246  0.17583513 ...  0.15332243  0.03117951
   0.0956303 ]]

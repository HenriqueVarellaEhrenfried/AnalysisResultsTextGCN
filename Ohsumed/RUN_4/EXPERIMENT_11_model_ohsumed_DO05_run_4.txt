(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.01555 val_loss= 3.11272 val_acc= 0.20597 time= 0.58096
Epoch: 0002 train_loss= 3.11325 train_acc= 0.17406 val_loss= 3.06312 val_acc= 0.20299 time= 0.29100
Epoch: 0003 train_loss= 3.06482 train_acc= 0.17273 val_loss= 2.98812 val_acc= 0.20000 time= 0.29000
Epoch: 0004 train_loss= 2.99098 train_acc= 0.17174 val_loss= 2.89619 val_acc= 0.20000 time= 0.28997
Epoch: 0005 train_loss= 2.90075 train_acc= 0.17141 val_loss= 2.80493 val_acc= 0.20000 time= 0.30703
Epoch: 0006 train_loss= 2.81130 train_acc= 0.17141 val_loss= 2.73417 val_acc= 0.20000 time= 0.30000
Epoch: 0007 train_loss= 2.74106 train_acc= 0.17141 val_loss= 2.69895 val_acc= 0.20000 time= 0.30700
Epoch: 0008 train_loss= 2.70821 train_acc= 0.17174 val_loss= 2.69633 val_acc= 0.20000 time= 0.30400
Epoch: 0009 train_loss= 2.70702 train_acc= 0.17174 val_loss= 2.69729 val_acc= 0.20000 time= 0.30200
Epoch: 0010 train_loss= 2.70507 train_acc= 0.17141 val_loss= 2.68243 val_acc= 0.20000 time= 0.30300
Epoch: 0011 train_loss= 2.68394 train_acc= 0.17141 val_loss= 2.65421 val_acc= 0.20299 time= 0.30200
Epoch: 0012 train_loss= 2.64568 train_acc= 0.17174 val_loss= 2.62449 val_acc= 0.20597 time= 0.29825
Epoch: 0013 train_loss= 2.60842 train_acc= 0.17637 val_loss= 2.59959 val_acc= 0.21791 time= 0.31502
Epoch: 0014 train_loss= 2.57586 train_acc= 0.18531 val_loss= 2.57825 val_acc= 0.23582 time= 0.30104
Epoch: 0015 train_loss= 2.54730 train_acc= 0.20781 val_loss= 2.55601 val_acc= 0.25672 time= 0.30108
Epoch: 0016 train_loss= 2.51882 train_acc= 0.23197 val_loss= 2.52961 val_acc= 0.28060 time= 0.29846
Epoch: 0017 train_loss= 2.49046 train_acc= 0.26142 val_loss= 2.49798 val_acc= 0.29254 time= 0.32095
Epoch: 0018 train_loss= 2.45303 train_acc= 0.28425 val_loss= 2.46170 val_acc= 0.30149 time= 0.34351
Epoch: 0019 train_loss= 2.41684 train_acc= 0.29649 val_loss= 2.42220 val_acc= 0.31045 time= 0.31204
Epoch: 0020 train_loss= 2.37365 train_acc= 0.30741 val_loss= 2.38089 val_acc= 0.31642 time= 0.29800
Epoch: 0021 train_loss= 2.32829 train_acc= 0.32462 val_loss= 2.33881 val_acc= 0.31940 time= 0.29439
Epoch: 0022 train_loss= 2.28169 train_acc= 0.32859 val_loss= 2.29651 val_acc= 0.32239 time= 0.29497
Epoch: 0023 train_loss= 2.23446 train_acc= 0.33984 val_loss= 2.25406 val_acc= 0.33731 time= 0.32000
Epoch: 0024 train_loss= 2.18331 train_acc= 0.35870 val_loss= 2.21128 val_acc= 0.34328 time= 0.31303
Epoch: 0025 train_loss= 2.13058 train_acc= 0.38352 val_loss= 2.16823 val_acc= 0.36716 time= 0.30100
Epoch: 0026 train_loss= 2.07852 train_acc= 0.41760 val_loss= 2.12508 val_acc= 0.40299 time= 0.29900
Epoch: 0027 train_loss= 2.02254 train_acc= 0.45731 val_loss= 2.08236 val_acc= 0.43582 time= 0.29500
Epoch: 0028 train_loss= 1.97092 train_acc= 0.49835 val_loss= 2.04039 val_acc= 0.46269 time= 0.29100
Epoch: 0029 train_loss= 1.91514 train_acc= 0.53044 val_loss= 1.99882 val_acc= 0.48657 time= 0.29901
Epoch: 0030 train_loss= 1.85888 train_acc= 0.56287 val_loss= 1.95673 val_acc= 0.50448 time= 0.29200
Epoch: 0031 train_loss= 1.80121 train_acc= 0.58438 val_loss= 1.91336 val_acc= 0.52239 time= 0.29597
Epoch: 0032 train_loss= 1.74498 train_acc= 0.59762 val_loss= 1.86939 val_acc= 0.51940 time= 0.29604
Epoch: 0033 train_loss= 1.69123 train_acc= 0.60126 val_loss= 1.82579 val_acc= 0.52537 time= 0.29399
Epoch: 0034 train_loss= 1.63242 train_acc= 0.61251 val_loss= 1.78374 val_acc= 0.52537 time= 0.29900
Epoch: 0035 train_loss= 1.57265 train_acc= 0.61946 val_loss= 1.74360 val_acc= 0.53134 time= 0.30897
Epoch: 0036 train_loss= 1.52541 train_acc= 0.62343 val_loss= 1.70536 val_acc= 0.53134 time= 0.30303
Epoch: 0037 train_loss= 1.47094 train_acc= 0.63501 val_loss= 1.66941 val_acc= 0.54627 time= 0.30697
Epoch: 0038 train_loss= 1.41808 train_acc= 0.65354 val_loss= 1.63574 val_acc= 0.56119 time= 0.30400
Epoch: 0039 train_loss= 1.36830 train_acc= 0.66314 val_loss= 1.60407 val_acc= 0.56716 time= 0.30603
Epoch: 0040 train_loss= 1.32737 train_acc= 0.66545 val_loss= 1.57448 val_acc= 0.57612 time= 0.29197
Epoch: 0041 train_loss= 1.26447 train_acc= 0.68034 val_loss= 1.54632 val_acc= 0.58209 time= 0.29503
Epoch: 0042 train_loss= 1.22497 train_acc= 0.69093 val_loss= 1.51785 val_acc= 0.58507 time= 0.29497
Epoch: 0043 train_loss= 1.17766 train_acc= 0.70417 val_loss= 1.49069 val_acc= 0.58507 time= 0.29504
Epoch: 0044 train_loss= 1.13178 train_acc= 0.71641 val_loss= 1.46559 val_acc= 0.59104 time= 0.29700
Epoch: 0045 train_loss= 1.08255 train_acc= 0.73660 val_loss= 1.44136 val_acc= 0.59104 time= 0.29097
Epoch: 0046 train_loss= 1.04689 train_acc= 0.74289 val_loss= 1.41762 val_acc= 0.59403 time= 0.30000
Epoch: 0047 train_loss= 1.00647 train_acc= 0.75645 val_loss= 1.39456 val_acc= 0.60896 time= 0.29203
Epoch: 0048 train_loss= 0.96599 train_acc= 0.76506 val_loss= 1.37250 val_acc= 0.62090 time= 0.29100
Epoch: 0049 train_loss= 0.92695 train_acc= 0.77796 val_loss= 1.35166 val_acc= 0.61493 time= 0.29200
Epoch: 0050 train_loss= 0.89342 train_acc= 0.78756 val_loss= 1.33283 val_acc= 0.62090 time= 0.29001
Epoch: 0051 train_loss= 0.85608 train_acc= 0.79186 val_loss= 1.31605 val_acc= 0.61493 time= 0.30099
Epoch: 0052 train_loss= 0.81585 train_acc= 0.81072 val_loss= 1.30136 val_acc= 0.61493 time= 0.29397
Epoch: 0053 train_loss= 0.78375 train_acc= 0.81734 val_loss= 1.28778 val_acc= 0.62090 time= 0.30400
Epoch: 0054 train_loss= 0.74997 train_acc= 0.82164 val_loss= 1.27466 val_acc= 0.62687 time= 0.30780
Epoch: 0055 train_loss= 0.71658 train_acc= 0.83289 val_loss= 1.26217 val_acc= 0.62687 time= 0.30397
Epoch: 0056 train_loss= 0.68873 train_acc= 0.84083 val_loss= 1.24944 val_acc= 0.63284 time= 0.31900
Epoch: 0057 train_loss= 0.65812 train_acc= 0.84811 val_loss= 1.23786 val_acc= 0.62985 time= 0.31147
Epoch: 0058 train_loss= 0.62969 train_acc= 0.85672 val_loss= 1.22777 val_acc= 0.63582 time= 0.30503
Epoch: 0059 train_loss= 0.59890 train_acc= 0.86168 val_loss= 1.21799 val_acc= 0.64179 time= 0.29300
Epoch: 0060 train_loss= 0.57522 train_acc= 0.86830 val_loss= 1.20872 val_acc= 0.63881 time= 0.29897
Epoch: 0061 train_loss= 0.55020 train_acc= 0.87723 val_loss= 1.20012 val_acc= 0.64478 time= 0.30703
Epoch: 0062 train_loss= 0.52579 train_acc= 0.88120 val_loss= 1.19353 val_acc= 0.64478 time= 0.30500
Epoch: 0063 train_loss= 0.50355 train_acc= 0.89543 val_loss= 1.18778 val_acc= 0.64776 time= 0.30497
Epoch: 0064 train_loss= 0.47778 train_acc= 0.89775 val_loss= 1.18213 val_acc= 0.64478 time= 0.31500
Epoch: 0065 train_loss= 0.46285 train_acc= 0.90205 val_loss= 1.17661 val_acc= 0.64478 time= 0.32559
Epoch: 0066 train_loss= 0.44309 train_acc= 0.90966 val_loss= 1.17138 val_acc= 0.64179 time= 0.29800
Epoch: 0067 train_loss= 0.42888 train_acc= 0.90999 val_loss= 1.16688 val_acc= 0.64776 time= 0.29700
Epoch: 0068 train_loss= 0.40497 train_acc= 0.91959 val_loss= 1.16341 val_acc= 0.65373 time= 0.29896
Epoch: 0069 train_loss= 0.38514 train_acc= 0.92191 val_loss= 1.16049 val_acc= 0.65373 time= 0.30100
Epoch: 0070 train_loss= 0.36963 train_acc= 0.92224 val_loss= 1.15815 val_acc= 0.65672 time= 0.30301
Epoch: 0071 train_loss= 0.35501 train_acc= 0.93051 val_loss= 1.15567 val_acc= 0.65970 time= 0.30137
Epoch: 0072 train_loss= 0.34087 train_acc= 0.93415 val_loss= 1.15297 val_acc= 0.65672 time= 0.29900
Epoch: 0073 train_loss= 0.32731 train_acc= 0.93977 val_loss= 1.15106 val_acc= 0.65970 time= 0.30401
Epoch: 0074 train_loss= 0.31193 train_acc= 0.94341 val_loss= 1.14970 val_acc= 0.65672 time= 0.30199
Epoch: 0075 train_loss= 0.29655 train_acc= 0.94375 val_loss= 1.14999 val_acc= 0.66567 time= 0.29603
Epoch: 0076 train_loss= 0.28434 train_acc= 0.94970 val_loss= 1.15112 val_acc= 0.66567 time= 0.29497
Epoch: 0077 train_loss= 0.26953 train_acc= 0.95433 val_loss= 1.15300 val_acc= 0.65672 time= 0.29503
Epoch: 0078 train_loss= 0.26225 train_acc= 0.95301 val_loss= 1.15262 val_acc= 0.66269 time= 0.29601
Epoch: 0079 train_loss= 0.24592 train_acc= 0.95764 val_loss= 1.15299 val_acc= 0.65970 time= 0.29200
Epoch: 0080 train_loss= 0.23378 train_acc= 0.96029 val_loss= 1.15386 val_acc= 0.66567 time= 0.29249
Early stopping...
Optimization Finished!
Test set results: cost= 1.15366 accuracy= 0.68884 time= 0.13200
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7043    0.7105    0.7074       342
           1     0.6777    0.7961    0.7321       103
           2     0.7350    0.6143    0.6693       140
           3     0.6111    0.4177    0.4962        79
           4     0.6352    0.7652    0.6942       132
           5     0.6833    0.7859    0.7311       313
           6     0.6789    0.7255    0.7014       102
           7     0.6047    0.3714    0.4602        70
           8     0.6129    0.3800    0.4691        50
           9     0.6292    0.7226    0.6727       155
          10     0.8200    0.6578    0.7300       187
          11     0.6309    0.6364    0.6336       231
          12     0.7665    0.7191    0.7420       178
          13     0.7883    0.8067    0.7974       600
          14     0.7747    0.8508    0.8110       590
          15     0.7500    0.7105    0.7297        76
          16     0.7059    0.3529    0.4706        34
          17     0.5000    0.1000    0.1667        10
          18     0.4334    0.4582    0.4455       419
          19     0.6381    0.5194    0.5726       129
          20     0.6071    0.6071    0.6071        28
          21     1.0000    0.7586    0.8627        29
          22     0.5600    0.3043    0.3944        46

    accuracy                         0.6888      4043
   macro avg     0.6760    0.5987    0.6216      4043
weighted avg     0.6892    0.6888    0.6846      4043

Macro average Test Precision, Recall and F1-Score...
(0.6759682777923784, 0.5987464769138561, 0.6216096349039324, None)
Micro average Test Precision, Recall and F1-Score...
(0.688844917140737, 0.688844917140737, 0.688844917140737, None)
embeddings:
14157 3357 4043
[[ 0.39532605  0.5030026   0.3056497  ...  0.36013398  0.5558814
   0.47717348]
 [ 0.06261276  0.0064601   0.00594716 ...  0.06711806  0.30410263
   0.34917027]
 [ 0.16070333  0.39672804  0.15363266 ...  0.30952352  0.43031162
   0.4633832 ]
 ...
 [ 0.09347663  0.20057845  0.10797024 ...  0.10899273  0.29101038
   0.27732173]
 [ 0.2653601   0.0254873  -0.05567917 ... -0.01778865  0.12803686
  -0.07725136]
 [ 0.13231727  0.31032458  0.12042965 ...  0.23111054  0.28053296
   0.25247592]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13544 train_acc= 0.09001 val_loss= 3.11281 val_acc= 0.20000 time= 0.58100
Epoch: 0002 train_loss= 3.11312 train_acc= 0.17207 val_loss= 3.06295 val_acc= 0.20000 time= 0.29303
Epoch: 0003 train_loss= 3.06375 train_acc= 0.17174 val_loss= 2.98756 val_acc= 0.20000 time= 0.28900
Epoch: 0004 train_loss= 2.98916 train_acc= 0.17174 val_loss= 2.89496 val_acc= 0.20000 time= 0.29000
Epoch: 0005 train_loss= 2.89910 train_acc= 0.17141 val_loss= 2.80274 val_acc= 0.20000 time= 0.29100
Epoch: 0006 train_loss= 2.80951 train_acc= 0.17141 val_loss= 2.73054 val_acc= 0.20000 time= 0.29200
Epoch: 0007 train_loss= 2.73828 train_acc= 0.17141 val_loss= 2.69350 val_acc= 0.20000 time= 0.28700
Epoch: 0008 train_loss= 2.70556 train_acc= 0.17141 val_loss= 2.68998 val_acc= 0.20000 time= 0.28800
Epoch: 0009 train_loss= 2.70332 train_acc= 0.17141 val_loss= 2.69186 val_acc= 0.20000 time= 0.29500
Epoch: 0010 train_loss= 2.70249 train_acc= 0.17141 val_loss= 2.67668 val_acc= 0.20000 time= 0.28700
Epoch: 0011 train_loss= 2.67958 train_acc= 0.17174 val_loss= 2.64775 val_acc= 0.20597 time= 0.28900
Epoch: 0012 train_loss= 2.64045 train_acc= 0.17836 val_loss= 2.61736 val_acc= 0.22985 time= 0.29609
Epoch: 0013 train_loss= 2.60082 train_acc= 0.19557 val_loss= 2.59233 val_acc= 0.24776 time= 0.29200
Epoch: 0014 train_loss= 2.56676 train_acc= 0.21476 val_loss= 2.57101 val_acc= 0.26269 time= 0.28797
Epoch: 0015 train_loss= 2.53677 train_acc= 0.24454 val_loss= 2.54863 val_acc= 0.28060 time= 0.29030
Epoch: 0016 train_loss= 2.50836 train_acc= 0.26109 val_loss= 2.52139 val_acc= 0.29254 time= 0.29100
Epoch: 0017 train_loss= 2.47767 train_acc= 0.28458 val_loss= 2.48824 val_acc= 0.30149 time= 0.29300
Epoch: 0018 train_loss= 2.44111 train_acc= 0.31271 val_loss= 2.45018 val_acc= 0.31343 time= 0.28930
Epoch: 0019 train_loss= 2.40023 train_acc= 0.33091 val_loss= 2.40915 val_acc= 0.31642 time= 0.28697
Epoch: 0020 train_loss= 2.35626 train_acc= 0.33719 val_loss= 2.36708 val_acc= 0.32239 time= 0.29403
Epoch: 0021 train_loss= 2.31222 train_acc= 0.34249 val_loss= 2.32509 val_acc= 0.33134 time= 0.28897
Epoch: 0022 train_loss= 2.26397 train_acc= 0.35506 val_loss= 2.28338 val_acc= 0.32836 time= 0.28904
Epoch: 0023 train_loss= 2.21585 train_acc= 0.36201 val_loss= 2.24161 val_acc= 0.33731 time= 0.29307
Epoch: 0024 train_loss= 2.16617 train_acc= 0.38418 val_loss= 2.19933 val_acc= 0.36418 time= 0.29333
Epoch: 0025 train_loss= 2.11005 train_acc= 0.40867 val_loss= 2.15655 val_acc= 0.37910 time= 0.28888
Epoch: 0026 train_loss= 2.05444 train_acc= 0.44275 val_loss= 2.11368 val_acc= 0.42388 time= 0.28896
Epoch: 0027 train_loss= 2.00013 train_acc= 0.47750 val_loss= 2.07115 val_acc= 0.45373 time= 0.29303
Epoch: 0028 train_loss= 1.94443 train_acc= 0.51621 val_loss= 2.02911 val_acc= 0.46567 time= 0.29200
Epoch: 0029 train_loss= 1.88979 train_acc= 0.54600 val_loss= 1.98689 val_acc= 0.48358 time= 0.28797
Epoch: 0030 train_loss= 1.83388 train_acc= 0.57015 val_loss= 1.94394 val_acc= 0.51045 time= 0.29208
Epoch: 0031 train_loss= 1.77457 train_acc= 0.59067 val_loss= 1.89966 val_acc= 0.51940 time= 0.29300
Epoch: 0032 train_loss= 1.71887 train_acc= 0.59497 val_loss= 1.85476 val_acc= 0.52537 time= 0.28800
Epoch: 0033 train_loss= 1.66062 train_acc= 0.60821 val_loss= 1.81053 val_acc= 0.52836 time= 0.28700
Epoch: 0034 train_loss= 1.60815 train_acc= 0.61946 val_loss= 1.76835 val_acc= 0.53433 time= 0.29300
Epoch: 0035 train_loss= 1.55037 train_acc= 0.62343 val_loss= 1.72885 val_acc= 0.54030 time= 0.29200
Epoch: 0036 train_loss= 1.49469 train_acc= 0.63733 val_loss= 1.69220 val_acc= 0.53134 time= 0.29100
Epoch: 0037 train_loss= 1.44024 train_acc= 0.64924 val_loss= 1.65754 val_acc= 0.54925 time= 0.28900
Epoch: 0038 train_loss= 1.38682 train_acc= 0.66479 val_loss= 1.62400 val_acc= 0.55522 time= 0.29600
Epoch: 0039 train_loss= 1.33821 train_acc= 0.67075 val_loss= 1.59177 val_acc= 0.56119 time= 0.29300
Epoch: 0040 train_loss= 1.28855 train_acc= 0.68001 val_loss= 1.56085 val_acc= 0.57612 time= 0.29400
Epoch: 0041 train_loss= 1.23702 train_acc= 0.69788 val_loss= 1.53082 val_acc= 0.57910 time= 0.28900
Epoch: 0042 train_loss= 1.19230 train_acc= 0.71509 val_loss= 1.50224 val_acc= 0.58209 time= 0.29400
Epoch: 0043 train_loss= 1.14664 train_acc= 0.72303 val_loss= 1.47447 val_acc= 0.58507 time= 0.29000
Epoch: 0044 train_loss= 1.10270 train_acc= 0.73395 val_loss= 1.44811 val_acc= 0.59403 time= 0.28800
Epoch: 0045 train_loss= 1.05256 train_acc= 0.75281 val_loss= 1.42334 val_acc= 0.59701 time= 0.29500
Epoch: 0046 train_loss= 1.01031 train_acc= 0.75910 val_loss= 1.40010 val_acc= 0.60299 time= 0.29600
Epoch: 0047 train_loss= 0.97712 train_acc= 0.76373 val_loss= 1.37855 val_acc= 0.60597 time= 0.29000
Epoch: 0048 train_loss= 0.92796 train_acc= 0.77730 val_loss= 1.35915 val_acc= 0.61194 time= 0.28900
Epoch: 0049 train_loss= 0.89491 train_acc= 0.79285 val_loss= 1.34101 val_acc= 0.62687 time= 0.29500
Epoch: 0050 train_loss= 0.85737 train_acc= 0.79914 val_loss= 1.32432 val_acc= 0.62687 time= 0.29007
Epoch: 0051 train_loss= 0.82006 train_acc= 0.80874 val_loss= 1.30843 val_acc= 0.62090 time= 0.28599
Epoch: 0052 train_loss= 0.77920 train_acc= 0.82131 val_loss= 1.29331 val_acc= 0.62687 time= 0.28900
Epoch: 0053 train_loss= 0.74883 train_acc= 0.82694 val_loss= 1.27943 val_acc= 0.61791 time= 0.29317
Epoch: 0054 train_loss= 0.71496 train_acc= 0.83355 val_loss= 1.26622 val_acc= 0.62090 time= 0.28999
Epoch: 0055 train_loss= 0.68729 train_acc= 0.83819 val_loss= 1.25336 val_acc= 0.62090 time= 0.28777
Epoch: 0056 train_loss= 0.65980 train_acc= 0.84778 val_loss= 1.24201 val_acc= 0.61493 time= 0.29101
Epoch: 0057 train_loss= 0.63047 train_acc= 0.85572 val_loss= 1.23244 val_acc= 0.62985 time= 0.29112
Epoch: 0058 train_loss= 0.60143 train_acc= 0.86499 val_loss= 1.22342 val_acc= 0.63582 time= 0.28700
Epoch: 0059 train_loss= 0.57418 train_acc= 0.87095 val_loss= 1.21705 val_acc= 0.63284 time= 0.28800
Epoch: 0060 train_loss= 0.55050 train_acc= 0.87955 val_loss= 1.21137 val_acc= 0.63881 time= 0.29500
Epoch: 0061 train_loss= 0.52735 train_acc= 0.89014 val_loss= 1.20575 val_acc= 0.64179 time= 0.29000
Epoch: 0062 train_loss= 0.49864 train_acc= 0.89378 val_loss= 1.20036 val_acc= 0.64776 time= 0.28699
Epoch: 0063 train_loss= 0.47837 train_acc= 0.90007 val_loss= 1.19336 val_acc= 0.65075 time= 0.29035
Epoch: 0064 train_loss= 0.45614 train_acc= 0.90073 val_loss= 1.18640 val_acc= 0.65373 time= 0.29099
Epoch: 0065 train_loss= 0.44059 train_acc= 0.90602 val_loss= 1.18084 val_acc= 0.65672 time= 0.29901
Epoch: 0066 train_loss= 0.42518 train_acc= 0.90602 val_loss= 1.17578 val_acc= 0.65970 time= 0.28903
Epoch: 0067 train_loss= 0.40026 train_acc= 0.91926 val_loss= 1.17247 val_acc= 0.66269 time= 0.29100
Epoch: 0068 train_loss= 0.38208 train_acc= 0.92555 val_loss= 1.17095 val_acc= 0.66269 time= 0.29400
Epoch: 0069 train_loss= 0.36460 train_acc= 0.93547 val_loss= 1.17057 val_acc= 0.66567 time= 0.28800
Epoch: 0070 train_loss= 0.35019 train_acc= 0.93580 val_loss= 1.17024 val_acc= 0.65970 time= 0.28758
Epoch: 0071 train_loss= 0.33664 train_acc= 0.93249 val_loss= 1.16910 val_acc= 0.66269 time= 0.29400
Epoch: 0072 train_loss= 0.31973 train_acc= 0.93845 val_loss= 1.16902 val_acc= 0.66269 time= 0.29004
Epoch: 0073 train_loss= 0.30694 train_acc= 0.94341 val_loss= 1.16972 val_acc= 0.65672 time= 0.29181
Epoch: 0074 train_loss= 0.29181 train_acc= 0.94871 val_loss= 1.16587 val_acc= 0.66567 time= 0.29511
Epoch: 0075 train_loss= 0.28210 train_acc= 0.94672 val_loss= 1.16506 val_acc= 0.66567 time= 0.28897
Epoch: 0076 train_loss= 0.27156 train_acc= 0.95169 val_loss= 1.16503 val_acc= 0.65970 time= 0.29403
Epoch: 0077 train_loss= 0.25484 train_acc= 0.95566 val_loss= 1.16832 val_acc= 0.66269 time= 0.28397
Epoch: 0078 train_loss= 0.24857 train_acc= 0.95963 val_loss= 1.17600 val_acc= 0.65075 time= 0.29000
Early stopping...
Optimization Finished!
Test set results: cost= 1.16618 accuracy= 0.68464 time= 0.12803
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7325    0.6725    0.7012       342
           1     0.7064    0.7476    0.7264       103
           2     0.7391    0.6071    0.6667       140
           3     0.5849    0.3924    0.4697        79
           4     0.6831    0.7348    0.7080       132
           5     0.6910    0.7859    0.7354       313
           6     0.6549    0.7255    0.6884       102
           7     0.6176    0.3000    0.4038        70
           8     0.5588    0.3800    0.4524        50
           9     0.6343    0.7161    0.6727       155
          10     0.8380    0.6364    0.7234       187
          11     0.6575    0.6234    0.6400       231
          12     0.7806    0.6798    0.7267       178
          13     0.7678    0.8100    0.7883       600
          14     0.7808    0.8390    0.8088       590
          15     0.7746    0.7237    0.7483        76
          16     0.8000    0.3529    0.4898        34
          17     0.3333    0.1000    0.1538        10
          18     0.4099    0.5322    0.4631       419
          19     0.6204    0.5194    0.5654       129
          20     0.7083    0.6071    0.6538        28
          21     0.9565    0.7586    0.8462        29
          22     0.5556    0.3261    0.4110        46

    accuracy                         0.6846      4043
   macro avg     0.6777    0.5900    0.6193      4043
weighted avg     0.6923    0.6846    0.6828      4043

Macro average Test Precision, Recall and F1-Score...
(0.67765658490258, 0.590026932966044, 0.6192822944968739, None)
Micro average Test Precision, Recall and F1-Score...
(0.68464011872372, 0.68464011872372, 0.68464011872372, None)
embeddings:
14157 3357 4043
[[ 0.40260085  0.41510162  0.4532919  ...  0.43254662  0.5215088
   0.33601865]
 [ 0.13871677  0.18501168  0.1421239  ... -0.02396159  0.3349501
  -0.00588959]
 [ 0.29667372  0.2031435   0.32360253 ...  0.04344593  0.27353936
   0.05999438]
 ...
 [ 0.16445689  0.09508796  0.21150456 ...  0.22580744  0.16753577
   0.09795962]
 [ 0.31067085  0.2838683  -0.07715369 ...  0.18046264  0.35824195
   0.20510785]
 [ 0.10495779  0.2548125   0.242025   ...  0.20532103  0.3223024
   0.26178062]]

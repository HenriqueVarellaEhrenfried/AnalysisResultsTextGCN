(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.02879 val_loss= 3.12825 val_acc= 0.20597 time= 0.58745
Epoch: 0002 train_loss= 3.12852 train_acc= 0.17737 val_loss= 3.11400 val_acc= 0.20299 time= 0.29360
Epoch: 0003 train_loss= 3.11465 train_acc= 0.17373 val_loss= 3.09311 val_acc= 0.20299 time= 0.28710
Epoch: 0004 train_loss= 3.09388 train_acc= 0.17439 val_loss= 3.06531 val_acc= 0.20000 time= 0.30097
Epoch: 0005 train_loss= 3.06701 train_acc= 0.17803 val_loss= 3.03072 val_acc= 0.20000 time= 0.28703
Epoch: 0006 train_loss= 3.03241 train_acc= 0.17505 val_loss= 2.99004 val_acc= 0.20000 time= 0.28800
Epoch: 0007 train_loss= 2.99446 train_acc= 0.17207 val_loss= 2.94473 val_acc= 0.20000 time= 0.29397
Epoch: 0008 train_loss= 2.94707 train_acc= 0.17174 val_loss= 2.89680 val_acc= 0.20000 time= 0.29203
Epoch: 0009 train_loss= 2.90135 train_acc= 0.17273 val_loss= 2.84884 val_acc= 0.20000 time= 0.29300
Epoch: 0010 train_loss= 2.85765 train_acc= 0.17174 val_loss= 2.80361 val_acc= 0.20000 time= 0.28900
Epoch: 0011 train_loss= 2.81108 train_acc= 0.17373 val_loss= 2.76388 val_acc= 0.20000 time= 0.29799
Epoch: 0012 train_loss= 2.77139 train_acc= 0.17207 val_loss= 2.73181 val_acc= 0.20000 time= 0.29000
Epoch: 0013 train_loss= 2.74224 train_acc= 0.17306 val_loss= 2.70907 val_acc= 0.20000 time= 0.28704
Epoch: 0014 train_loss= 2.72024 train_acc= 0.17240 val_loss= 2.69557 val_acc= 0.20000 time= 0.29200
Epoch: 0015 train_loss= 2.70915 train_acc= 0.17273 val_loss= 2.68896 val_acc= 0.20000 time= 0.29548
Epoch: 0016 train_loss= 2.70092 train_acc= 0.17373 val_loss= 2.68543 val_acc= 0.20000 time= 0.28600
Epoch: 0017 train_loss= 2.70026 train_acc= 0.17273 val_loss= 2.68104 val_acc= 0.20000 time= 0.29000
Epoch: 0018 train_loss= 2.69360 train_acc= 0.17240 val_loss= 2.67389 val_acc= 0.20000 time= 0.29500
Epoch: 0019 train_loss= 2.68310 train_acc= 0.17406 val_loss= 2.66367 val_acc= 0.20299 time= 0.29327
Epoch: 0020 train_loss= 2.67240 train_acc= 0.17240 val_loss= 2.65114 val_acc= 0.20597 time= 0.28900
Epoch: 0021 train_loss= 2.65976 train_acc= 0.17604 val_loss= 2.63762 val_acc= 0.20597 time= 0.29008
Epoch: 0022 train_loss= 2.63983 train_acc= 0.17637 val_loss= 2.62441 val_acc= 0.20896 time= 0.29495
Epoch: 0023 train_loss= 2.62005 train_acc= 0.18034 val_loss= 2.61221 val_acc= 0.21493 time= 0.29200
Epoch: 0024 train_loss= 2.60320 train_acc= 0.18432 val_loss= 2.60069 val_acc= 0.22985 time= 0.29000
Epoch: 0025 train_loss= 2.59302 train_acc= 0.19590 val_loss= 2.58946 val_acc= 0.23284 time= 0.28907
Epoch: 0026 train_loss= 2.57348 train_acc= 0.20715 val_loss= 2.57792 val_acc= 0.24776 time= 0.29600
Epoch: 0027 train_loss= 2.55617 train_acc= 0.21873 val_loss= 2.56553 val_acc= 0.25373 time= 0.28900
Epoch: 0028 train_loss= 2.55117 train_acc= 0.23263 val_loss= 2.55201 val_acc= 0.26567 time= 0.28900
Epoch: 0029 train_loss= 2.53063 train_acc= 0.24818 val_loss= 2.53704 val_acc= 0.27761 time= 0.29300
Epoch: 0030 train_loss= 2.50950 train_acc= 0.26208 val_loss= 2.52064 val_acc= 0.28358 time= 0.29516
Epoch: 0031 train_loss= 2.49034 train_acc= 0.26936 val_loss= 2.50297 val_acc= 0.28955 time= 0.29100
Epoch: 0032 train_loss= 2.47914 train_acc= 0.27101 val_loss= 2.48422 val_acc= 0.29254 time= 0.29197
Epoch: 0033 train_loss= 2.45567 train_acc= 0.27995 val_loss= 2.46484 val_acc= 0.29552 time= 0.29254
Epoch: 0034 train_loss= 2.43591 train_acc= 0.28028 val_loss= 2.44485 val_acc= 0.29851 time= 0.29703
Epoch: 0035 train_loss= 2.41488 train_acc= 0.29054 val_loss= 2.42443 val_acc= 0.30746 time= 0.29101
Epoch: 0036 train_loss= 2.39199 train_acc= 0.29120 val_loss= 2.40378 val_acc= 0.31045 time= 0.28700
Epoch: 0037 train_loss= 2.36559 train_acc= 0.30113 val_loss= 2.38284 val_acc= 0.31045 time= 0.29700
Epoch: 0038 train_loss= 2.33856 train_acc= 0.29815 val_loss= 2.36173 val_acc= 0.31045 time= 0.28997
Epoch: 0039 train_loss= 2.32059 train_acc= 0.30708 val_loss= 2.34037 val_acc= 0.31343 time= 0.29111
Epoch: 0040 train_loss= 2.29100 train_acc= 0.31833 val_loss= 2.31870 val_acc= 0.31642 time= 0.29299
Epoch: 0041 train_loss= 2.26842 train_acc= 0.33091 val_loss= 2.29672 val_acc= 0.31940 time= 0.29456
Epoch: 0042 train_loss= 2.24930 train_acc= 0.33554 val_loss= 2.27441 val_acc= 0.32239 time= 0.28805
Epoch: 0043 train_loss= 2.21428 train_acc= 0.35010 val_loss= 2.25178 val_acc= 0.32836 time= 0.29400
Epoch: 0044 train_loss= 2.18647 train_acc= 0.37657 val_loss= 2.22904 val_acc= 0.36119 time= 0.29297
Epoch: 0045 train_loss= 2.16185 train_acc= 0.39080 val_loss= 2.20611 val_acc= 0.36716 time= 0.29603
Epoch: 0046 train_loss= 2.12580 train_acc= 0.42124 val_loss= 2.18277 val_acc= 0.39403 time= 0.28906
Epoch: 0047 train_loss= 2.11070 train_acc= 0.45831 val_loss= 2.15878 val_acc= 0.41493 time= 0.28900
Epoch: 0048 train_loss= 2.06843 train_acc= 0.46327 val_loss= 2.13417 val_acc= 0.43582 time= 0.30300
Epoch: 0049 train_loss= 2.05096 train_acc= 0.47386 val_loss= 2.10905 val_acc= 0.45075 time= 0.29200
Epoch: 0050 train_loss= 2.01292 train_acc= 0.48809 val_loss= 2.08359 val_acc= 0.45075 time= 0.28846
Epoch: 0051 train_loss= 1.99167 train_acc= 0.50695 val_loss= 2.05807 val_acc= 0.45672 time= 0.29103
Epoch: 0052 train_loss= 1.96323 train_acc= 0.50794 val_loss= 2.03251 val_acc= 0.46866 time= 0.29201
Epoch: 0053 train_loss= 1.92712 train_acc= 0.51721 val_loss= 2.00717 val_acc= 0.47164 time= 0.28899
Epoch: 0054 train_loss= 1.90219 train_acc= 0.51357 val_loss= 1.98230 val_acc= 0.47164 time= 0.28700
Epoch: 0055 train_loss= 1.86897 train_acc= 0.52713 val_loss= 1.95799 val_acc= 0.48060 time= 0.29097
Epoch: 0056 train_loss= 1.83631 train_acc= 0.54103 val_loss= 1.93416 val_acc= 0.48358 time= 0.29473
Epoch: 0057 train_loss= 1.81270 train_acc= 0.53441 val_loss= 1.91087 val_acc= 0.48955 time= 0.29000
Epoch: 0058 train_loss= 1.78967 train_acc= 0.55063 val_loss= 1.88794 val_acc= 0.49254 time= 0.28906
Epoch: 0059 train_loss= 1.75554 train_acc= 0.55559 val_loss= 1.86529 val_acc= 0.50149 time= 0.29559
Epoch: 0060 train_loss= 1.70968 train_acc= 0.57181 val_loss= 1.84297 val_acc= 0.51642 time= 0.29107
Epoch: 0061 train_loss= 1.68242 train_acc= 0.58570 val_loss= 1.82117 val_acc= 0.52537 time= 0.28700
Epoch: 0062 train_loss= 1.65878 train_acc= 0.58703 val_loss= 1.79973 val_acc= 0.52836 time= 0.29600
Epoch: 0063 train_loss= 1.63130 train_acc= 0.59365 val_loss= 1.77870 val_acc= 0.54328 time= 0.29511
Epoch: 0064 train_loss= 1.61956 train_acc= 0.59365 val_loss= 1.75759 val_acc= 0.54627 time= 0.28898
Epoch: 0065 train_loss= 1.57950 train_acc= 0.60986 val_loss= 1.73668 val_acc= 0.54030 time= 0.28797
Epoch: 0066 train_loss= 1.54660 train_acc= 0.61085 val_loss= 1.71626 val_acc= 0.54328 time= 0.29400
Epoch: 0067 train_loss= 1.53449 train_acc= 0.61516 val_loss= 1.69582 val_acc= 0.54627 time= 0.29865
Epoch: 0068 train_loss= 1.49930 train_acc= 0.61979 val_loss= 1.67571 val_acc= 0.55224 time= 0.28900
Epoch: 0069 train_loss= 1.46989 train_acc= 0.62475 val_loss= 1.65691 val_acc= 0.55522 time= 0.29000
Epoch: 0070 train_loss= 1.44503 train_acc= 0.63038 val_loss= 1.63928 val_acc= 0.55522 time= 0.29396
Epoch: 0071 train_loss= 1.42656 train_acc= 0.63501 val_loss= 1.62225 val_acc= 0.56119 time= 0.29503
Epoch: 0072 train_loss= 1.40993 train_acc= 0.64725 val_loss= 1.60535 val_acc= 0.56418 time= 0.28800
Epoch: 0073 train_loss= 1.39542 train_acc= 0.63997 val_loss= 1.58941 val_acc= 0.56418 time= 0.29200
Epoch: 0074 train_loss= 1.36262 train_acc= 0.66082 val_loss= 1.57429 val_acc= 0.56716 time= 0.29400
Epoch: 0075 train_loss= 1.34838 train_acc= 0.66181 val_loss= 1.56004 val_acc= 0.57313 time= 0.28900
Epoch: 0076 train_loss= 1.31843 train_acc= 0.67538 val_loss= 1.54575 val_acc= 0.57910 time= 0.28901
Epoch: 0077 train_loss= 1.29827 train_acc= 0.66711 val_loss= 1.53234 val_acc= 0.59104 time= 0.29100
Epoch: 0078 train_loss= 1.27192 train_acc= 0.68134 val_loss= 1.51858 val_acc= 0.58209 time= 0.29500
Epoch: 0079 train_loss= 1.26834 train_acc= 0.68068 val_loss= 1.50495 val_acc= 0.58507 time= 0.28800
Epoch: 0080 train_loss= 1.23783 train_acc= 0.68432 val_loss= 1.49054 val_acc= 0.58209 time= 0.28997
Epoch: 0081 train_loss= 1.20716 train_acc= 0.70218 val_loss= 1.47647 val_acc= 0.58806 time= 0.29703
Epoch: 0082 train_loss= 1.19070 train_acc= 0.70384 val_loss= 1.46323 val_acc= 0.59403 time= 0.29500
Epoch: 0083 train_loss= 1.17385 train_acc= 0.70053 val_loss= 1.45055 val_acc= 0.60000 time= 0.29000
Epoch: 0084 train_loss= 1.15250 train_acc= 0.70946 val_loss= 1.43819 val_acc= 0.60299 time= 0.29200
Epoch: 0085 train_loss= 1.12894 train_acc= 0.71310 val_loss= 1.42656 val_acc= 0.60597 time= 0.28968
Epoch: 0086 train_loss= 1.10914 train_acc= 0.72502 val_loss= 1.41533 val_acc= 0.60896 time= 0.29403
Epoch: 0087 train_loss= 1.10324 train_acc= 0.72303 val_loss= 1.40513 val_acc= 0.61493 time= 0.28997
Epoch: 0088 train_loss= 1.08159 train_acc= 0.73263 val_loss= 1.39619 val_acc= 0.60896 time= 0.29103
Epoch: 0089 train_loss= 1.05833 train_acc= 0.73494 val_loss= 1.38774 val_acc= 0.61194 time= 0.29588
Epoch: 0090 train_loss= 1.04513 train_acc= 0.73660 val_loss= 1.37976 val_acc= 0.61493 time= 0.28900
Epoch: 0091 train_loss= 1.03438 train_acc= 0.74123 val_loss= 1.37194 val_acc= 0.60896 time= 0.28747
Epoch: 0092 train_loss= 1.01604 train_acc= 0.74388 val_loss= 1.36384 val_acc= 0.61791 time= 0.29405
Epoch: 0093 train_loss= 1.00828 train_acc= 0.75281 val_loss= 1.35488 val_acc= 0.62687 time= 0.29200
Epoch: 0094 train_loss= 0.98914 train_acc= 0.75182 val_loss= 1.34619 val_acc= 0.62687 time= 0.29100
Epoch: 0095 train_loss= 0.97629 train_acc= 0.75513 val_loss= 1.33818 val_acc= 0.62985 time= 0.29600
Epoch: 0096 train_loss= 0.94580 train_acc= 0.76605 val_loss= 1.33003 val_acc= 0.63582 time= 0.28900
Epoch: 0097 train_loss= 0.94927 train_acc= 0.75745 val_loss= 1.32283 val_acc= 0.64179 time= 0.29300
Epoch: 0098 train_loss= 0.92405 train_acc= 0.76506 val_loss= 1.31630 val_acc= 0.63582 time= 0.28884
Epoch: 0099 train_loss= 0.93582 train_acc= 0.76638 val_loss= 1.30908 val_acc= 0.62985 time= 0.29097
Epoch: 0100 train_loss= 0.89810 train_acc= 0.76671 val_loss= 1.30149 val_acc= 0.63284 time= 0.29450
Epoch: 0101 train_loss= 0.89361 train_acc= 0.78127 val_loss= 1.29508 val_acc= 0.63582 time= 0.29068
Epoch: 0102 train_loss= 0.88319 train_acc= 0.77664 val_loss= 1.28878 val_acc= 0.63284 time= 0.28602
Epoch: 0103 train_loss= 0.85724 train_acc= 0.78690 val_loss= 1.28163 val_acc= 0.62388 time= 0.29400
Epoch: 0104 train_loss= 0.84731 train_acc= 0.78723 val_loss= 1.27451 val_acc= 0.62687 time= 0.29100
Epoch: 0105 train_loss= 0.83735 train_acc= 0.79881 val_loss= 1.26780 val_acc= 0.62388 time= 0.29000
Epoch: 0106 train_loss= 0.82546 train_acc= 0.79484 val_loss= 1.26214 val_acc= 0.62687 time= 0.29500
Epoch: 0107 train_loss= 0.82178 train_acc= 0.79318 val_loss= 1.25736 val_acc= 0.62687 time= 0.29100
Epoch: 0108 train_loss= 0.80148 train_acc= 0.80675 val_loss= 1.25362 val_acc= 0.62687 time= 0.29400
Epoch: 0109 train_loss= 0.79588 train_acc= 0.80311 val_loss= 1.25004 val_acc= 0.62985 time= 0.28794
Epoch: 0110 train_loss= 0.79160 train_acc= 0.80874 val_loss= 1.24626 val_acc= 0.62985 time= 0.29000
Epoch: 0111 train_loss= 0.78205 train_acc= 0.81006 val_loss= 1.24236 val_acc= 0.62388 time= 0.28900
Epoch: 0112 train_loss= 0.75566 train_acc= 0.80807 val_loss= 1.23905 val_acc= 0.62090 time= 0.29900
Epoch: 0113 train_loss= 0.73936 train_acc= 0.81966 val_loss= 1.23531 val_acc= 0.62687 time= 0.28800
Epoch: 0114 train_loss= 0.74089 train_acc= 0.81469 val_loss= 1.23127 val_acc= 0.62687 time= 0.29500
Epoch: 0115 train_loss= 0.72763 train_acc= 0.81602 val_loss= 1.22732 val_acc= 0.63582 time= 0.29153
Epoch: 0116 train_loss= 0.71709 train_acc= 0.81568 val_loss= 1.22265 val_acc= 0.63284 time= 0.28903
Epoch: 0117 train_loss= 0.70451 train_acc= 0.83587 val_loss= 1.21715 val_acc= 0.62985 time= 0.29700
Epoch: 0118 train_loss= 0.68739 train_acc= 0.83984 val_loss= 1.21245 val_acc= 0.63582 time= 0.28900
Epoch: 0119 train_loss= 0.68373 train_acc= 0.82495 val_loss= 1.20813 val_acc= 0.63582 time= 0.29208
Epoch: 0120 train_loss= 0.68653 train_acc= 0.83058 val_loss= 1.20451 val_acc= 0.63582 time= 0.28900
Epoch: 0121 train_loss= 0.66322 train_acc= 0.84514 val_loss= 1.20110 val_acc= 0.63284 time= 0.29200
Epoch: 0122 train_loss= 0.65946 train_acc= 0.84811 val_loss= 1.19713 val_acc= 0.63582 time= 0.28904
Epoch: 0123 train_loss= 0.64560 train_acc= 0.84547 val_loss= 1.19412 val_acc= 0.64478 time= 0.29200
Epoch: 0124 train_loss= 0.64241 train_acc= 0.85208 val_loss= 1.19278 val_acc= 0.64179 time= 0.28912
Epoch: 0125 train_loss= 0.62370 train_acc= 0.84944 val_loss= 1.19059 val_acc= 0.65075 time= 0.29308
Epoch: 0126 train_loss= 0.61932 train_acc= 0.85142 val_loss= 1.18637 val_acc= 0.64776 time= 0.29297
Epoch: 0127 train_loss= 0.62045 train_acc= 0.84878 val_loss= 1.18099 val_acc= 0.63881 time= 0.29211
Epoch: 0128 train_loss= 0.60311 train_acc= 0.86201 val_loss= 1.17639 val_acc= 0.64179 time= 0.29400
Epoch: 0129 train_loss= 0.61084 train_acc= 0.85473 val_loss= 1.17512 val_acc= 0.63881 time= 0.28700
Epoch: 0130 train_loss= 0.59746 train_acc= 0.85970 val_loss= 1.17346 val_acc= 0.64179 time= 0.29400
Epoch: 0131 train_loss= 0.58192 train_acc= 0.86300 val_loss= 1.17099 val_acc= 0.64776 time= 0.29106
Epoch: 0132 train_loss= 0.58896 train_acc= 0.86135 val_loss= 1.16867 val_acc= 0.65075 time= 0.29000
Epoch: 0133 train_loss= 0.57818 train_acc= 0.85341 val_loss= 1.16750 val_acc= 0.64478 time= 0.28700
Epoch: 0134 train_loss= 0.56778 train_acc= 0.86995 val_loss= 1.16686 val_acc= 0.64776 time= 0.29549
Epoch: 0135 train_loss= 0.56336 train_acc= 0.86664 val_loss= 1.16557 val_acc= 0.65075 time= 0.29000
Epoch: 0136 train_loss= 0.55374 train_acc= 0.87062 val_loss= 1.16418 val_acc= 0.65075 time= 0.29400
Epoch: 0137 train_loss= 0.53475 train_acc= 0.88319 val_loss= 1.16172 val_acc= 0.65075 time= 0.29100
Epoch: 0138 train_loss= 0.54113 train_acc= 0.87591 val_loss= 1.15884 val_acc= 0.65075 time= 0.29200
Epoch: 0139 train_loss= 0.52880 train_acc= 0.87227 val_loss= 1.15550 val_acc= 0.65373 time= 0.29300
Epoch: 0140 train_loss= 0.51915 train_acc= 0.88120 val_loss= 1.15306 val_acc= 0.65672 time= 0.28900
Epoch: 0141 train_loss= 0.52523 train_acc= 0.87459 val_loss= 1.15197 val_acc= 0.65970 time= 0.29400
Epoch: 0142 train_loss= 0.51664 train_acc= 0.87690 val_loss= 1.15164 val_acc= 0.65672 time= 0.29000
Epoch: 0143 train_loss= 0.51157 train_acc= 0.87591 val_loss= 1.15053 val_acc= 0.66269 time= 0.29300
Epoch: 0144 train_loss= 0.50544 train_acc= 0.88154 val_loss= 1.14898 val_acc= 0.65672 time= 0.28900
Epoch: 0145 train_loss= 0.49491 train_acc= 0.88220 val_loss= 1.14821 val_acc= 0.65970 time= 0.29500
Epoch: 0146 train_loss= 0.48440 train_acc= 0.88650 val_loss= 1.14735 val_acc= 0.65970 time= 0.29100
Epoch: 0147 train_loss= 0.47791 train_acc= 0.89212 val_loss= 1.14745 val_acc= 0.65970 time= 0.29900
Epoch: 0148 train_loss= 0.47649 train_acc= 0.88882 val_loss= 1.14722 val_acc= 0.65672 time= 0.28897
Epoch: 0149 train_loss= 0.47247 train_acc= 0.89378 val_loss= 1.14711 val_acc= 0.65672 time= 0.29100
Epoch: 0150 train_loss= 0.46975 train_acc= 0.89576 val_loss= 1.14606 val_acc= 0.66269 time= 0.29557
Epoch: 0151 train_loss= 0.46395 train_acc= 0.90040 val_loss= 1.14393 val_acc= 0.66567 time= 0.28900
Epoch: 0152 train_loss= 0.46316 train_acc= 0.89610 val_loss= 1.14130 val_acc= 0.66866 time= 0.29800
Epoch: 0153 train_loss= 0.44388 train_acc= 0.90503 val_loss= 1.13952 val_acc= 0.66567 time= 0.28866
Epoch: 0154 train_loss= 0.44533 train_acc= 0.90073 val_loss= 1.13854 val_acc= 0.66866 time= 0.29300
Epoch: 0155 train_loss= 0.44520 train_acc= 0.89312 val_loss= 1.13927 val_acc= 0.66866 time= 0.28899
Epoch: 0156 train_loss= 0.44123 train_acc= 0.89974 val_loss= 1.14163 val_acc= 0.66567 time= 0.29300
Early stopping...
Optimization Finished!
Test set results: cost= 1.17440 accuracy= 0.67178 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7078    0.6871    0.6973       342
           1     0.7048    0.7184    0.7115       103
           2     0.7624    0.5500    0.6390       140
           3     0.7059    0.3038    0.4248        79
           4     0.6452    0.7576    0.6969       132
           5     0.6613    0.7923    0.7209       313
           6     0.6226    0.6471    0.6346       102
           7     0.6296    0.2429    0.3505        70
           8     0.6429    0.1800    0.2812        50
           9     0.6096    0.7355    0.6667       155
          10     0.8456    0.6150    0.7121       187
          11     0.6092    0.6883    0.6463       231
          12     0.7922    0.6854    0.7349       178
          13     0.7574    0.8117    0.7836       600
          14     0.7707    0.8373    0.8026       590
          15     0.7812    0.6579    0.7143        76
          16     0.8182    0.2647    0.4000        34
          17     0.0000    0.0000    0.0000        10
          18     0.3822    0.5036    0.4346       419
          19     0.6854    0.4729    0.5596       129
          20     0.7500    0.4286    0.5455        28
          21     1.0000    0.7241    0.8400        29
          22     0.5789    0.2391    0.3385        46

    accuracy                         0.6718      4043
   macro avg     0.6723    0.5454    0.5798      4043
weighted avg     0.6839    0.6718    0.6662      4043

Macro average Test Precision, Recall and F1-Score...
(0.67231168349598, 0.5453568708894165, 0.5798032203099328, None)
Micro average Test Precision, Recall and F1-Score...
(0.6717783823893149, 0.6717783823893149, 0.6717783823893149, None)
embeddings:
14157 3357 4043
[[ 0.43282127  0.21979451  0.29858342 ...  0.30571532  0.28475666
   0.2612464 ]
 [ 0.21325296 -0.12994021  0.15602176 ...  0.12309634  0.22589865
   0.08878624]
 [ 0.14336215  0.07845653  0.2801547  ...  0.22118385  0.35374135
   0.14077334]
 ...
 [ 0.21623054  0.13104187  0.09613195 ...  0.16822387  0.10501332
   0.18257564]
 [ 0.02746322 -0.01035679  0.0358955  ...  0.26108205  0.0397725
   0.03571519]
 [ 0.2679009   0.15089488  0.20807807 ... -0.01635301  0.14545731
   0.25157133]]

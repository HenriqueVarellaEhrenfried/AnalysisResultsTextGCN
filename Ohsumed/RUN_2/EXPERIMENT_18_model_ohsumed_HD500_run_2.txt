(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13555 train_acc= 0.02747 val_loss= 3.10369 val_acc= 0.20597 time= 6.42600
Epoch: 0002 train_loss= 3.10355 train_acc= 0.17637 val_loss= 3.01943 val_acc= 0.20000 time= 6.21900
Epoch: 0003 train_loss= 3.01950 train_acc= 0.17306 val_loss= 2.89391 val_acc= 0.20000 time= 6.11700
Epoch: 0004 train_loss= 2.89477 train_acc= 0.17240 val_loss= 2.77102 val_acc= 0.20000 time= 6.01103
Epoch: 0005 train_loss= 2.77534 train_acc= 0.17174 val_loss= 2.70216 val_acc= 0.20299 time= 5.74300
Epoch: 0006 train_loss= 2.71243 train_acc= 0.17306 val_loss= 2.70281 val_acc= 0.20597 time= 5.73200
Epoch: 0007 train_loss= 2.71545 train_acc= 0.17340 val_loss= 2.70189 val_acc= 0.20597 time= 5.73800
Epoch: 0008 train_loss= 2.71258 train_acc= 0.17340 val_loss= 2.66400 val_acc= 0.20896 time= 5.87500
Epoch: 0009 train_loss= 2.66150 train_acc= 0.17770 val_loss= 2.61766 val_acc= 0.22687 time= 5.71600
Epoch: 0010 train_loss= 2.59973 train_acc= 0.19524 val_loss= 2.58248 val_acc= 0.25075 time= 5.75100
Epoch: 0011 train_loss= 2.55093 train_acc= 0.22469 val_loss= 2.55275 val_acc= 0.27761 time= 5.78997
Epoch: 0012 train_loss= 2.51110 train_acc= 0.25612 val_loss= 2.51737 val_acc= 0.29851 time= 5.75603
Epoch: 0013 train_loss= 2.46928 train_acc= 0.29749 val_loss= 2.47106 val_acc= 0.31642 time= 6.02997
Epoch: 0014 train_loss= 2.41888 train_acc= 0.33984 val_loss= 2.41468 val_acc= 0.33134 time= 5.94800
Epoch: 0015 train_loss= 2.35895 train_acc= 0.37194 val_loss= 2.35219 val_acc= 0.34627 time= 5.70403
Epoch: 0016 train_loss= 2.28915 train_acc= 0.38650 val_loss= 2.28783 val_acc= 0.35224 time= 5.69397
Epoch: 0017 train_loss= 2.21852 train_acc= 0.38915 val_loss= 2.22440 val_acc= 0.35821 time= 5.71203
Epoch: 0018 train_loss= 2.14900 train_acc= 0.39146 val_loss= 2.16240 val_acc= 0.37015 time= 5.79200
Epoch: 0019 train_loss= 2.07683 train_acc= 0.41496 val_loss= 2.10139 val_acc= 0.38507 time= 6.16065
Epoch: 0020 train_loss= 2.00310 train_acc= 0.44011 val_loss= 2.04152 val_acc= 0.41791 time= 6.29837
Epoch: 0021 train_loss= 1.92436 train_acc= 0.48676 val_loss= 1.98415 val_acc= 0.46866 time= 6.13198
Epoch: 0022 train_loss= 1.84928 train_acc= 0.52978 val_loss= 1.93026 val_acc= 0.50149 time= 5.88912
Epoch: 0023 train_loss= 1.77075 train_acc= 0.57512 val_loss= 1.87778 val_acc= 0.52537 time= 5.81600
Epoch: 0024 train_loss= 1.69942 train_acc= 0.59795 val_loss= 1.82360 val_acc= 0.54030 time= 5.85797
Epoch: 0025 train_loss= 1.62446 train_acc= 0.61317 val_loss= 1.76767 val_acc= 0.54627 time= 5.80624
Epoch: 0026 train_loss= 1.54732 train_acc= 0.62707 val_loss= 1.71300 val_acc= 0.54030 time= 5.70501
Epoch: 0027 train_loss= 1.47769 train_acc= 0.63600 val_loss= 1.66191 val_acc= 0.54328 time= 5.63699
Epoch: 0028 train_loss= 1.40582 train_acc= 0.64725 val_loss= 1.61632 val_acc= 0.56418 time= 5.69802
Epoch: 0029 train_loss= 1.34065 train_acc= 0.67075 val_loss= 1.57497 val_acc= 0.57612 time= 5.67396
Epoch: 0030 train_loss= 1.27741 train_acc= 0.68762 val_loss= 1.53566 val_acc= 0.58806 time= 5.63900
Epoch: 0031 train_loss= 1.21134 train_acc= 0.69921 val_loss= 1.49858 val_acc= 0.58507 time= 5.69803
Epoch: 0032 train_loss= 1.15300 train_acc= 0.71013 val_loss= 1.46430 val_acc= 0.58507 time= 5.86600
Epoch: 0033 train_loss= 1.08739 train_acc= 0.72436 val_loss= 1.43239 val_acc= 0.60299 time= 5.66900
Epoch: 0034 train_loss= 1.03011 train_acc= 0.74256 val_loss= 1.40205 val_acc= 0.60597 time= 5.67698
Epoch: 0035 train_loss= 0.97316 train_acc= 0.75844 val_loss= 1.37402 val_acc= 0.60597 time= 5.67101
Epoch: 0036 train_loss= 0.92021 train_acc= 0.77829 val_loss= 1.34666 val_acc= 0.61194 time= 6.00099
Epoch: 0037 train_loss= 0.87072 train_acc= 0.78723 val_loss= 1.32046 val_acc= 0.62090 time= 5.81003
Epoch: 0038 train_loss= 0.81853 train_acc= 0.80179 val_loss= 1.29870 val_acc= 0.62687 time= 5.95097
Epoch: 0039 train_loss= 0.76996 train_acc= 0.81734 val_loss= 1.28073 val_acc= 0.63881 time= 6.06715
Epoch: 0040 train_loss= 0.72408 train_acc= 0.82661 val_loss= 1.26505 val_acc= 0.62687 time= 5.87004
Epoch: 0041 train_loss= 0.68022 train_acc= 0.83984 val_loss= 1.25053 val_acc= 0.63881 time= 5.77996
Epoch: 0042 train_loss= 0.63825 train_acc= 0.84811 val_loss= 1.23680 val_acc= 0.63582 time= 5.93700
Epoch: 0043 train_loss= 0.59392 train_acc= 0.86234 val_loss= 1.22411 val_acc= 0.63582 time= 6.01703
Epoch: 0044 train_loss= 0.55753 train_acc= 0.87293 val_loss= 1.21237 val_acc= 0.65075 time= 6.00497
Epoch: 0045 train_loss= 0.52342 train_acc= 0.87889 val_loss= 1.20237 val_acc= 0.65373 time= 5.96503
Epoch: 0046 train_loss= 0.49033 train_acc= 0.88518 val_loss= 1.19353 val_acc= 0.66269 time= 5.92097
Epoch: 0047 train_loss= 0.45689 train_acc= 0.89345 val_loss= 1.18619 val_acc= 0.66567 time= 6.30600
Epoch: 0048 train_loss= 0.42919 train_acc= 0.90271 val_loss= 1.18109 val_acc= 0.65970 time= 6.06603
Epoch: 0049 train_loss= 0.39769 train_acc= 0.91396 val_loss= 1.17758 val_acc= 0.65672 time= 6.15197
Epoch: 0050 train_loss= 0.37378 train_acc= 0.91992 val_loss= 1.17486 val_acc= 0.66567 time= 5.82304
Epoch: 0051 train_loss= 0.34796 train_acc= 0.92753 val_loss= 1.17088 val_acc= 0.66567 time= 5.76700
Epoch: 0052 train_loss= 0.32575 train_acc= 0.93514 val_loss= 1.16778 val_acc= 0.66567 time= 5.78200
Epoch: 0053 train_loss= 0.30280 train_acc= 0.93680 val_loss= 1.16735 val_acc= 0.66866 time= 5.77300
Epoch: 0054 train_loss= 0.28542 train_acc= 0.94110 val_loss= 1.16840 val_acc= 0.67164 time= 5.77800
Epoch: 0055 train_loss= 0.26541 train_acc= 0.94871 val_loss= 1.16907 val_acc= 0.66269 time= 5.80000
Epoch: 0056 train_loss= 0.24775 train_acc= 0.95367 val_loss= 1.17129 val_acc= 0.65970 time= 5.73500
Epoch: 0057 train_loss= 0.23236 train_acc= 0.95632 val_loss= 1.17270 val_acc= 0.66269 time= 5.75700
Epoch: 0058 train_loss= 0.21865 train_acc= 0.96261 val_loss= 1.17540 val_acc= 0.66866 time= 5.76000
Early stopping...
Optimization Finished!
Test set results: cost= 1.17191 accuracy= 0.68637 time= 2.00700
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7237    0.7047    0.7141       342
           1     0.6814    0.7476    0.7130       103
           2     0.7107    0.6143    0.6590       140
           3     0.5692    0.4684    0.5139        79
           4     0.6644    0.7348    0.6978       132
           5     0.6784    0.8019    0.7350       313
           6     0.6875    0.7549    0.7196       102
           7     0.6111    0.3143    0.4151        70
           8     0.6000    0.4200    0.4941        50
           9     0.6010    0.7484    0.6667       155
          10     0.8414    0.6524    0.7349       187
          11     0.6282    0.6364    0.6323       231
          12     0.7558    0.7303    0.7429       178
          13     0.7694    0.8117    0.7899       600
          14     0.7695    0.8373    0.8019       590
          15     0.7361    0.6974    0.7162        76
          16     0.6875    0.3235    0.4400        34
          17     0.5000    0.1000    0.1667        10
          18     0.4376    0.4439    0.4408       419
          19     0.6262    0.5194    0.5678       129
          20     0.6071    0.6071    0.6071        28
          21     0.9545    0.7241    0.8235        29
          22     0.6667    0.3043    0.4179        46

    accuracy                         0.6864      4043
   macro avg     0.6742    0.5955    0.6178      4043
weighted avg     0.6863    0.6864    0.6814      4043

Macro average Test Precision, Recall and F1-Score...
(0.6742402610723931, 0.5955266878238507, 0.6178363362916246, None)
Micro average Test Precision, Recall and F1-Score...
(0.6863715063071977, 0.6863715063071977, 0.6863715063071977, None)
embeddings:
14157 3357 4043
[[ 0.2944353   0.18563013  0.25868708 ...  0.20584784  0.28915682
   0.17824417]
 [ 0.00698014  0.01344798  0.04920531 ...  0.08820186  0.01921111
   0.17061062]
 [ 0.2958642   0.09884594  0.22724195 ...  0.16929355  0.21881837
   0.2113003 ]
 ...
 [ 0.19631144  0.12397692  0.08721977 ...  0.07422956  0.13734895
   0.17431141]
 [ 0.1866039  -0.01993516  0.22015321 ...  0.21597742  0.01180328
  -0.046303  ]
 [ 0.03822157  0.18900415  0.07349785 ...  0.10725682  0.21414493
  -0.00274967]]

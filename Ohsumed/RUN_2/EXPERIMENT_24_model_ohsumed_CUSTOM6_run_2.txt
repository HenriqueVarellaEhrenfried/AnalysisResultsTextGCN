(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13545 train_acc= 0.04368 val_loss= 3.11372 val_acc= 0.24179 time= 0.58816
Epoch: 0002 train_loss= 3.11335 train_acc= 0.20715 val_loss= 3.06567 val_acc= 0.24478 time= 0.29300
Epoch: 0003 train_loss= 3.06484 train_acc= 0.21377 val_loss= 2.99185 val_acc= 0.25075 time= 0.29003
Epoch: 0004 train_loss= 2.99100 train_acc= 0.22105 val_loss= 2.90014 val_acc= 0.25075 time= 0.29074
Epoch: 0005 train_loss= 2.90019 train_acc= 0.22667 val_loss= 2.80781 val_acc= 0.25373 time= 0.29400
Epoch: 0006 train_loss= 2.80962 train_acc= 0.23428 val_loss= 2.73537 val_acc= 0.26269 time= 0.29000
Epoch: 0007 train_loss= 2.73949 train_acc= 0.24255 val_loss= 2.69791 val_acc= 0.26269 time= 0.28900
Epoch: 0008 train_loss= 2.70309 train_acc= 0.24355 val_loss= 2.69135 val_acc= 0.24776 time= 0.28812
Epoch: 0009 train_loss= 2.69707 train_acc= 0.21046 val_loss= 2.68932 val_acc= 0.20896 time= 0.29609
Epoch: 0010 train_loss= 2.69348 train_acc= 0.18630 val_loss= 2.67435 val_acc= 0.20896 time= 0.28899
Epoch: 0011 train_loss= 2.67313 train_acc= 0.17770 val_loss= 2.64640 val_acc= 0.20896 time= 0.28700
Epoch: 0012 train_loss= 2.63566 train_acc= 0.18001 val_loss= 2.61466 val_acc= 0.22090 time= 0.29500
Epoch: 0013 train_loss= 2.59413 train_acc= 0.18895 val_loss= 2.58581 val_acc= 0.23284 time= 0.29400
Epoch: 0014 train_loss= 2.55492 train_acc= 0.20285 val_loss= 2.56049 val_acc= 0.25373 time= 0.28801
Epoch: 0015 train_loss= 2.52184 train_acc= 0.22667 val_loss= 2.53526 val_acc= 0.27761 time= 0.28695
Epoch: 0016 train_loss= 2.48949 train_acc= 0.25612 val_loss= 2.50652 val_acc= 0.29254 time= 0.29397
Epoch: 0017 train_loss= 2.45605 train_acc= 0.29120 val_loss= 2.47256 val_acc= 0.31343 time= 0.29303
Epoch: 0018 train_loss= 2.41844 train_acc= 0.32429 val_loss= 2.43355 val_acc= 0.31940 time= 0.28700
Epoch: 0019 train_loss= 2.37393 train_acc= 0.35308 val_loss= 2.39076 val_acc= 0.32537 time= 0.29197
Epoch: 0020 train_loss= 2.32799 train_acc= 0.37359 val_loss= 2.34556 val_acc= 0.33433 time= 0.30003
Epoch: 0021 train_loss= 2.27803 train_acc= 0.38352 val_loss= 2.29919 val_acc= 0.34030 time= 0.29000
Epoch: 0022 train_loss= 2.22585 train_acc= 0.38882 val_loss= 2.25276 val_acc= 0.35224 time= 0.28700
Epoch: 0023 train_loss= 2.17262 train_acc= 0.39510 val_loss= 2.20691 val_acc= 0.36418 time= 0.29500
Epoch: 0024 train_loss= 2.11840 train_acc= 0.40007 val_loss= 2.16186 val_acc= 0.37313 time= 0.29300
Epoch: 0025 train_loss= 2.06163 train_acc= 0.41827 val_loss= 2.11738 val_acc= 0.39104 time= 0.29300
Epoch: 0026 train_loss= 2.00337 train_acc= 0.44904 val_loss= 2.07348 val_acc= 0.42985 time= 0.28900
Epoch: 0027 train_loss= 1.94507 train_acc= 0.48709 val_loss= 2.03035 val_acc= 0.45672 time= 0.29297
Epoch: 0028 train_loss= 1.88551 train_acc= 0.52813 val_loss= 1.98793 val_acc= 0.48358 time= 0.29500
Epoch: 0029 train_loss= 1.82856 train_acc= 0.56486 val_loss= 1.94538 val_acc= 0.49552 time= 0.28903
Epoch: 0030 train_loss= 1.76732 train_acc= 0.59464 val_loss= 1.90153 val_acc= 0.53134 time= 0.29301
Epoch: 0031 train_loss= 1.71009 train_acc= 0.60457 val_loss= 1.85585 val_acc= 0.52836 time= 0.29712
Epoch: 0032 train_loss= 1.65096 train_acc= 0.61549 val_loss= 1.80944 val_acc= 0.53433 time= 0.29100
Epoch: 0033 train_loss= 1.59284 train_acc= 0.62343 val_loss= 1.76426 val_acc= 0.53134 time= 0.29200
Epoch: 0034 train_loss= 1.53169 train_acc= 0.63600 val_loss= 1.72196 val_acc= 0.53134 time= 0.29500
Epoch: 0035 train_loss= 1.47630 train_acc= 0.64659 val_loss= 1.68305 val_acc= 0.53731 time= 0.29585
Epoch: 0036 train_loss= 1.42112 train_acc= 0.65884 val_loss= 1.64687 val_acc= 0.54925 time= 0.29000
Epoch: 0037 train_loss= 1.36869 train_acc= 0.67373 val_loss= 1.61250 val_acc= 0.56716 time= 0.28800
Epoch: 0038 train_loss= 1.31559 train_acc= 0.68233 val_loss= 1.57996 val_acc= 0.57015 time= 0.29483
Epoch: 0039 train_loss= 1.26456 train_acc= 0.69821 val_loss= 1.54943 val_acc= 0.57015 time= 0.29911
Epoch: 0040 train_loss= 1.21337 train_acc= 0.70285 val_loss= 1.52059 val_acc= 0.58806 time= 0.28897
Epoch: 0041 train_loss= 1.16351 train_acc= 0.71575 val_loss= 1.49280 val_acc= 0.58806 time= 0.28900
Epoch: 0042 train_loss= 1.11635 train_acc= 0.72601 val_loss= 1.46582 val_acc= 0.59403 time= 0.30304
Epoch: 0043 train_loss= 1.06931 train_acc= 0.73925 val_loss= 1.43953 val_acc= 0.59104 time= 0.29000
Epoch: 0044 train_loss= 1.02384 train_acc= 0.75381 val_loss= 1.41469 val_acc= 0.59104 time= 0.29000
Epoch: 0045 train_loss= 0.98081 train_acc= 0.76605 val_loss= 1.39150 val_acc= 0.59701 time= 0.29300
Epoch: 0046 train_loss= 0.93660 train_acc= 0.77465 val_loss= 1.36992 val_acc= 0.60299 time= 0.29197
Epoch: 0047 train_loss= 0.89407 train_acc= 0.79451 val_loss= 1.35018 val_acc= 0.61493 time= 0.28910
Epoch: 0048 train_loss= 0.85511 train_acc= 0.80510 val_loss= 1.33230 val_acc= 0.62388 time= 0.28700
Epoch: 0049 train_loss= 0.81560 train_acc= 0.81370 val_loss= 1.31535 val_acc= 0.61791 time= 0.29400
Epoch: 0050 train_loss= 0.77724 train_acc= 0.82826 val_loss= 1.29890 val_acc= 0.61791 time= 0.29797
Epoch: 0051 train_loss= 0.74334 train_acc= 0.83388 val_loss= 1.28349 val_acc= 0.61791 time= 0.28898
Epoch: 0052 train_loss= 0.70794 train_acc= 0.83984 val_loss= 1.26899 val_acc= 0.61791 time= 0.28905
Epoch: 0053 train_loss= 0.67208 train_acc= 0.84745 val_loss= 1.25611 val_acc= 0.61791 time= 0.30306
Epoch: 0054 train_loss= 0.64161 train_acc= 0.85407 val_loss= 1.24488 val_acc= 0.62388 time= 0.29589
Epoch: 0055 train_loss= 0.61047 train_acc= 0.86267 val_loss= 1.23512 val_acc= 0.62687 time= 0.28814
Epoch: 0056 train_loss= 0.57863 train_acc= 0.87326 val_loss= 1.22625 val_acc= 0.62985 time= 0.29400
Epoch: 0057 train_loss= 0.55156 train_acc= 0.87856 val_loss= 1.21838 val_acc= 0.63284 time= 0.29539
Epoch: 0058 train_loss= 0.52512 train_acc= 0.88650 val_loss= 1.21063 val_acc= 0.63582 time= 0.28907
Epoch: 0059 train_loss= 0.49857 train_acc= 0.89411 val_loss= 1.20243 val_acc= 0.64478 time= 0.28600
Epoch: 0060 train_loss= 0.47318 train_acc= 0.90205 val_loss= 1.19435 val_acc= 0.64776 time= 0.29200
Epoch: 0061 train_loss= 0.44975 train_acc= 0.90470 val_loss= 1.18713 val_acc= 0.65373 time= 0.29307
Epoch: 0062 train_loss= 0.42697 train_acc= 0.91132 val_loss= 1.18125 val_acc= 0.65672 time= 0.29001
Epoch: 0063 train_loss= 0.40578 train_acc= 0.91727 val_loss= 1.17675 val_acc= 0.65970 time= 0.28703
Epoch: 0064 train_loss= 0.38328 train_acc= 0.92323 val_loss= 1.17397 val_acc= 0.66866 time= 0.29700
Epoch: 0065 train_loss= 0.36347 train_acc= 0.92720 val_loss= 1.17210 val_acc= 0.66567 time= 0.29903
Epoch: 0066 train_loss= 0.34622 train_acc= 0.93018 val_loss= 1.17058 val_acc= 0.66866 time= 0.29200
Epoch: 0067 train_loss= 0.32797 train_acc= 0.94044 val_loss= 1.16830 val_acc= 0.66567 time= 0.29307
Epoch: 0068 train_loss= 0.31242 train_acc= 0.94275 val_loss= 1.16566 val_acc= 0.67463 time= 0.29295
Epoch: 0069 train_loss= 0.29556 train_acc= 0.94705 val_loss= 1.16341 val_acc= 0.67761 time= 0.29100
Epoch: 0070 train_loss= 0.28055 train_acc= 0.94937 val_loss= 1.16191 val_acc= 0.67463 time= 0.29104
Epoch: 0071 train_loss= 0.26808 train_acc= 0.95136 val_loss= 1.16163 val_acc= 0.68358 time= 0.29099
Epoch: 0072 train_loss= 0.25306 train_acc= 0.95599 val_loss= 1.16300 val_acc= 0.68358 time= 0.29200
Epoch: 0073 train_loss= 0.24090 train_acc= 0.96095 val_loss= 1.16512 val_acc= 0.68060 time= 0.28800
Epoch: 0074 train_loss= 0.23038 train_acc= 0.96393 val_loss= 1.16727 val_acc= 0.68358 time= 0.28697
Early stopping...
Optimization Finished!
Test set results: cost= 1.16091 accuracy= 0.68711 time= 0.13403
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7173    0.6901    0.7034       342
           1     0.6937    0.7476    0.7196       103
           2     0.7321    0.5857    0.6508       140
           3     0.6667    0.4304    0.5231        79
           4     0.6623    0.7576    0.7067       132
           5     0.6831    0.7987    0.7364       313
           6     0.6796    0.6863    0.6829       102
           7     0.5946    0.3143    0.4112        70
           8     0.6774    0.4200    0.5185        50
           9     0.6120    0.7226    0.6627       155
          10     0.8345    0.6471    0.7289       187
          11     0.6183    0.6450    0.6314       231
          12     0.7679    0.7247    0.7457       178
          13     0.7665    0.8150    0.7900       600
          14     0.7858    0.8458    0.8147       590
          15     0.7333    0.7237    0.7285        76
          16     0.7857    0.3235    0.4583        34
          17     0.5000    0.1000    0.1667        10
          18     0.4224    0.4678    0.4439       419
          19     0.6415    0.5271    0.5787       129
          20     0.6296    0.6071    0.6182        28
          21     0.9565    0.7586    0.8462        29
          22     0.5484    0.3696    0.4416        46

    accuracy                         0.6871      4043
   macro avg     0.6830    0.5960    0.6221      4043
weighted avg     0.6900    0.6871    0.6834      4043

Macro average Test Precision, Recall and F1-Score...
(0.6830077649855772, 0.596007864336105, 0.6220889982963196, None)
Micro average Test Precision, Recall and F1-Score...
(0.6871135295572595, 0.6871135295572595, 0.6871135295572595, None)
embeddings:
14157 3357 4043
[[ 0.6291066   0.65278226  0.3496667  ...  0.46185416  0.4250696
   0.47698286]
 [ 0.15288238  0.428895    0.07640915 ...  0.03290739  0.25748745
   0.11573859]
 [ 0.44375533  0.44035676  0.02476656 ...  0.29381904  0.43824783
   0.4812544 ]
 ...
 [ 0.2518907   0.17596054  0.11157425 ...  0.31780928  0.24140339
   0.23659033]
 [ 0.06537834  0.58071375  0.21944135 ...  0.3692945   0.02601336
  -0.05635601]
 [ 0.2257206   0.3079256   0.21170515 ...  0.1971376   0.11920632
   0.1670222 ]]

(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13551 train_acc= 0.04070 val_loss= 3.11558 val_acc= 0.22090 time= 0.58503
Epoch: 0002 train_loss= 3.11610 train_acc= 0.18961 val_loss= 3.07102 val_acc= 0.20299 time= 0.28800
Epoch: 0003 train_loss= 3.07153 train_acc= 0.17935 val_loss= 3.00298 val_acc= 0.20000 time= 0.29200
Epoch: 0004 train_loss= 3.00490 train_acc= 0.17439 val_loss= 2.91784 val_acc= 0.20000 time= 0.29149
Epoch: 0005 train_loss= 2.92135 train_acc= 0.17273 val_loss= 2.82969 val_acc= 0.20000 time= 0.29197
Epoch: 0006 train_loss= 2.83703 train_acc= 0.17174 val_loss= 2.75737 val_acc= 0.20000 time= 0.28802
Epoch: 0007 train_loss= 2.76564 train_acc= 0.17340 val_loss= 2.71334 val_acc= 0.20000 time= 0.29597
Epoch: 0008 train_loss= 2.72756 train_acc= 0.17306 val_loss= 2.69985 val_acc= 0.20000 time= 0.29100
Epoch: 0009 train_loss= 2.71873 train_acc= 0.17273 val_loss= 2.69974 val_acc= 0.20000 time= 0.28807
Epoch: 0010 train_loss= 2.71949 train_acc= 0.17306 val_loss= 2.69385 val_acc= 0.20000 time= 0.29200
Epoch: 0011 train_loss= 2.71116 train_acc= 0.17141 val_loss= 2.67524 val_acc= 0.20299 time= 0.28800
Epoch: 0012 train_loss= 2.68877 train_acc= 0.17273 val_loss= 2.65050 val_acc= 0.20597 time= 0.29660
Epoch: 0013 train_loss= 2.65696 train_acc= 0.17571 val_loss= 2.62752 val_acc= 0.21493 time= 0.29000
Epoch: 0014 train_loss= 2.62582 train_acc= 0.18465 val_loss= 2.60881 val_acc= 0.22388 time= 0.29300
Epoch: 0015 train_loss= 2.59536 train_acc= 0.19689 val_loss= 2.59214 val_acc= 0.24776 time= 0.28903
Epoch: 0016 train_loss= 2.57036 train_acc= 0.21906 val_loss= 2.57386 val_acc= 0.25970 time= 0.29107
Epoch: 0017 train_loss= 2.55066 train_acc= 0.23064 val_loss= 2.55192 val_acc= 0.27761 time= 0.28680
Epoch: 0018 train_loss= 2.52879 train_acc= 0.25711 val_loss= 2.52535 val_acc= 0.28358 time= 0.29399
Epoch: 0019 train_loss= 2.48957 train_acc= 0.27796 val_loss= 2.49450 val_acc= 0.29552 time= 0.29400
Epoch: 0020 train_loss= 2.46769 train_acc= 0.28789 val_loss= 2.46048 val_acc= 0.30149 time= 0.28900
Epoch: 0021 train_loss= 2.43042 train_acc= 0.29749 val_loss= 2.42461 val_acc= 0.31045 time= 0.29300
Epoch: 0022 train_loss= 2.39367 train_acc= 0.29649 val_loss= 2.38814 val_acc= 0.31045 time= 0.28800
Epoch: 0023 train_loss= 2.35732 train_acc= 0.30510 val_loss= 2.35147 val_acc= 0.31045 time= 0.29500
Epoch: 0024 train_loss= 2.31715 train_acc= 0.30609 val_loss= 2.31509 val_acc= 0.31045 time= 0.28701
Epoch: 0025 train_loss= 2.27766 train_acc= 0.31403 val_loss= 2.27900 val_acc= 0.31343 time= 0.29299
Epoch: 0026 train_loss= 2.24201 train_acc= 0.32131 val_loss= 2.24305 val_acc= 0.32239 time= 0.28899
Epoch: 0027 train_loss= 2.19874 train_acc= 0.34778 val_loss= 2.20712 val_acc= 0.35224 time= 0.29200
Epoch: 0028 train_loss= 2.14281 train_acc= 0.38054 val_loss= 2.17151 val_acc= 0.37612 time= 0.28700
Epoch: 0029 train_loss= 2.11891 train_acc= 0.40304 val_loss= 2.13643 val_acc= 0.41493 time= 0.29601
Epoch: 0030 train_loss= 2.06706 train_acc= 0.43944 val_loss= 2.10180 val_acc= 0.43881 time= 0.29500
Epoch: 0031 train_loss= 2.01802 train_acc= 0.46691 val_loss= 2.06711 val_acc= 0.45672 time= 0.29000
Epoch: 0032 train_loss= 1.99382 train_acc= 0.49570 val_loss= 2.03093 val_acc= 0.47761 time= 0.29100
Epoch: 0033 train_loss= 1.93452 train_acc= 0.51191 val_loss= 1.99326 val_acc= 0.49254 time= 0.28801
Epoch: 0034 train_loss= 1.89812 train_acc= 0.52482 val_loss= 1.95511 val_acc= 0.50448 time= 0.29242
Epoch: 0035 train_loss= 1.83523 train_acc= 0.53574 val_loss= 1.91679 val_acc= 0.50746 time= 0.28900
Epoch: 0036 train_loss= 1.81071 train_acc= 0.53938 val_loss= 1.87909 val_acc= 0.51045 time= 0.29200
Epoch: 0037 train_loss= 1.75852 train_acc= 0.55328 val_loss= 1.84306 val_acc= 0.51642 time= 0.28955
Epoch: 0038 train_loss= 1.70750 train_acc= 0.56122 val_loss= 1.80858 val_acc= 0.51940 time= 0.29675
Epoch: 0039 train_loss= 1.67299 train_acc= 0.56585 val_loss= 1.77567 val_acc= 0.52239 time= 0.29200
Epoch: 0040 train_loss= 1.63605 train_acc= 0.58074 val_loss= 1.74387 val_acc= 0.52239 time= 0.29100
Epoch: 0041 train_loss= 1.58271 train_acc= 0.59067 val_loss= 1.71385 val_acc= 0.55522 time= 0.29300
Epoch: 0042 train_loss= 1.54554 train_acc= 0.59232 val_loss= 1.68518 val_acc= 0.56119 time= 0.28900
Epoch: 0043 train_loss= 1.50581 train_acc= 0.60556 val_loss= 1.65810 val_acc= 0.57612 time= 0.29200
Epoch: 0044 train_loss= 1.45653 train_acc= 0.61482 val_loss= 1.63064 val_acc= 0.57910 time= 0.29309
Epoch: 0045 train_loss= 1.42833 train_acc= 0.62674 val_loss= 1.60339 val_acc= 0.58507 time= 0.29303
Epoch: 0046 train_loss= 1.39084 train_acc= 0.64328 val_loss= 1.57617 val_acc= 0.57612 time= 0.28900
Epoch: 0047 train_loss= 1.34655 train_acc= 0.65222 val_loss= 1.54975 val_acc= 0.58507 time= 0.29400
Epoch: 0048 train_loss= 1.31027 train_acc= 0.65983 val_loss= 1.52535 val_acc= 0.58507 time= 0.28800
Epoch: 0049 train_loss= 1.27546 train_acc= 0.67472 val_loss= 1.50319 val_acc= 0.58507 time= 0.29400
Epoch: 0050 train_loss= 1.24427 train_acc= 0.68001 val_loss= 1.48251 val_acc= 0.58209 time= 0.28800
Epoch: 0051 train_loss= 1.20359 train_acc= 0.68696 val_loss= 1.46311 val_acc= 0.58507 time= 0.29400
Epoch: 0052 train_loss= 1.16079 train_acc= 0.68928 val_loss= 1.44609 val_acc= 0.59104 time= 0.29067
Epoch: 0053 train_loss= 1.13309 train_acc= 0.69656 val_loss= 1.42854 val_acc= 0.60000 time= 0.29204
Epoch: 0054 train_loss= 1.11517 train_acc= 0.72105 val_loss= 1.40916 val_acc= 0.61493 time= 0.28796
Epoch: 0055 train_loss= 1.07502 train_acc= 0.72005 val_loss= 1.38888 val_acc= 0.61791 time= 0.29303
Epoch: 0056 train_loss= 1.05654 train_acc= 0.73064 val_loss= 1.37169 val_acc= 0.61194 time= 0.29300
Epoch: 0057 train_loss= 1.01713 train_acc= 0.74553 val_loss= 1.35654 val_acc= 0.61493 time= 0.28900
Epoch: 0058 train_loss= 0.99577 train_acc= 0.75083 val_loss= 1.34142 val_acc= 0.62388 time= 0.29360
Epoch: 0059 train_loss= 0.96875 train_acc= 0.74123 val_loss= 1.32845 val_acc= 0.62687 time= 0.29000
Epoch: 0060 train_loss= 0.93516 train_acc= 0.75910 val_loss= 1.31793 val_acc= 0.62687 time= 0.29297
Epoch: 0061 train_loss= 0.92640 train_acc= 0.75513 val_loss= 1.30661 val_acc= 0.62985 time= 0.28903
Epoch: 0062 train_loss= 0.90969 train_acc= 0.76406 val_loss= 1.29514 val_acc= 0.63284 time= 0.29100
Epoch: 0063 train_loss= 0.85255 train_acc= 0.77598 val_loss= 1.28455 val_acc= 0.64179 time= 0.28696
Epoch: 0064 train_loss= 0.85404 train_acc= 0.77895 val_loss= 1.27464 val_acc= 0.63582 time= 0.29402
Epoch: 0065 train_loss= 0.82884 train_acc= 0.78458 val_loss= 1.26394 val_acc= 0.62985 time= 0.29101
Epoch: 0066 train_loss= 0.80465 train_acc= 0.79087 val_loss= 1.25269 val_acc= 0.63284 time= 0.29500
Epoch: 0067 train_loss= 0.76917 train_acc= 0.80543 val_loss= 1.24312 val_acc= 0.62687 time= 0.29401
Epoch: 0068 train_loss= 0.76495 train_acc= 0.80179 val_loss= 1.23675 val_acc= 0.62687 time= 0.29201
Epoch: 0069 train_loss= 0.73738 train_acc= 0.81039 val_loss= 1.23107 val_acc= 0.61493 time= 0.29296
Epoch: 0070 train_loss= 0.73682 train_acc= 0.81039 val_loss= 1.22433 val_acc= 0.62985 time= 0.28803
Epoch: 0071 train_loss= 0.69912 train_acc= 0.82528 val_loss= 1.21645 val_acc= 0.64478 time= 0.29355
Epoch: 0072 train_loss= 0.69003 train_acc= 0.83024 val_loss= 1.21289 val_acc= 0.63881 time= 0.29003
Epoch: 0073 train_loss= 0.66550 train_acc= 0.82925 val_loss= 1.20688 val_acc= 0.64478 time= 0.29198
Epoch: 0074 train_loss= 0.65605 train_acc= 0.83719 val_loss= 1.19940 val_acc= 0.64179 time= 0.29597
Epoch: 0075 train_loss= 0.62862 train_acc= 0.84050 val_loss= 1.19299 val_acc= 0.64179 time= 0.29303
Epoch: 0076 train_loss= 0.61425 train_acc= 0.84613 val_loss= 1.18868 val_acc= 0.64776 time= 0.28902
Epoch: 0077 train_loss= 0.60524 train_acc= 0.84646 val_loss= 1.18714 val_acc= 0.63881 time= 0.29200
Epoch: 0078 train_loss= 0.58571 train_acc= 0.85606 val_loss= 1.18433 val_acc= 0.63881 time= 0.29300
Epoch: 0079 train_loss= 0.57656 train_acc= 0.85440 val_loss= 1.17675 val_acc= 0.65373 time= 0.29000
Epoch: 0080 train_loss= 0.55884 train_acc= 0.85109 val_loss= 1.17096 val_acc= 0.66269 time= 0.29100
Epoch: 0081 train_loss= 0.54948 train_acc= 0.86466 val_loss= 1.16872 val_acc= 0.66269 time= 0.28800
Epoch: 0082 train_loss= 0.52811 train_acc= 0.88054 val_loss= 1.16548 val_acc= 0.66567 time= 0.29400
Epoch: 0083 train_loss= 0.52173 train_acc= 0.87723 val_loss= 1.16001 val_acc= 0.65672 time= 0.28800
Epoch: 0084 train_loss= 0.51319 train_acc= 0.87558 val_loss= 1.15869 val_acc= 0.65075 time= 0.29000
Epoch: 0085 train_loss= 0.49974 train_acc= 0.88286 val_loss= 1.16097 val_acc= 0.65075 time= 0.28900
Epoch: 0086 train_loss= 0.49018 train_acc= 0.88253 val_loss= 1.15941 val_acc= 0.66269 time= 0.29501
Epoch: 0087 train_loss= 0.48883 train_acc= 0.87889 val_loss= 1.15214 val_acc= 0.65672 time= 0.28703
Epoch: 0088 train_loss= 0.46138 train_acc= 0.89113 val_loss= 1.14839 val_acc= 0.65373 time= 0.29200
Epoch: 0089 train_loss= 0.46344 train_acc= 0.88815 val_loss= 1.14832 val_acc= 0.67164 time= 0.29100
Epoch: 0090 train_loss= 0.43491 train_acc= 0.89477 val_loss= 1.14690 val_acc= 0.68358 time= 0.29107
Epoch: 0091 train_loss= 0.43333 train_acc= 0.89411 val_loss= 1.14601 val_acc= 0.68657 time= 0.29500
Epoch: 0092 train_loss= 0.42290 train_acc= 0.90768 val_loss= 1.14536 val_acc= 0.68657 time= 0.28700
Epoch: 0093 train_loss= 0.41271 train_acc= 0.90470 val_loss= 1.14499 val_acc= 0.68657 time= 0.29400
Epoch: 0094 train_loss= 0.39838 train_acc= 0.91032 val_loss= 1.14409 val_acc= 0.67761 time= 0.28800
Epoch: 0095 train_loss= 0.39637 train_acc= 0.91595 val_loss= 1.14139 val_acc= 0.67164 time= 0.29700
Epoch: 0096 train_loss= 0.39561 train_acc= 0.90304 val_loss= 1.13906 val_acc= 0.67164 time= 0.29200
Epoch: 0097 train_loss= 0.37992 train_acc= 0.91032 val_loss= 1.13770 val_acc= 0.66866 time= 0.29400
Epoch: 0098 train_loss= 0.37176 train_acc= 0.91562 val_loss= 1.13966 val_acc= 0.66866 time= 0.28900
Epoch: 0099 train_loss= 0.36836 train_acc= 0.91529 val_loss= 1.14263 val_acc= 0.66866 time= 0.29597
Epoch: 0100 train_loss= 0.36280 train_acc= 0.91363 val_loss= 1.14703 val_acc= 0.66567 time= 0.29000
Early stopping...
Optimization Finished!
Test set results: cost= 1.16507 accuracy= 0.67771 time= 0.13103
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7191    0.6813    0.6997       342
           1     0.7170    0.7379    0.7273       103
           2     0.7912    0.5143    0.6234       140
           3     0.7368    0.3544    0.4786        79
           4     0.6809    0.7273    0.7033       132
           5     0.6427    0.7987    0.7123       313
           6     0.6698    0.6961    0.6827       102
           7     0.5882    0.2857    0.3846        70
           8     0.6429    0.3600    0.4615        50
           9     0.5897    0.7419    0.6571       155
          10     0.8456    0.6150    0.7121       187
          11     0.6419    0.6364    0.6391       231
          12     0.7471    0.7303    0.7386       178
          13     0.7669    0.8117    0.7887       600
          14     0.7792    0.8373    0.8072       590
          15     0.7812    0.6579    0.7143        76
          16     0.7500    0.2647    0.3913        34
          17     0.0000    0.0000    0.0000        10
          18     0.3949    0.5203    0.4490       419
          19     0.6598    0.4961    0.5664       129
          20     0.8000    0.4286    0.5581        28
          21     1.0000    0.7241    0.8400        29
          22     0.6364    0.3043    0.4118        46

    accuracy                         0.6777      4043
   macro avg     0.6775    0.5619    0.5977      4043
weighted avg     0.6897    0.6777    0.6742      4043

Macro average Test Precision, Recall and F1-Score...
(0.6774530332712309, 0.5619255089457984, 0.5977000077791202, None)
Micro average Test Precision, Recall and F1-Score...
(0.6777145683898096, 0.6777145683898096, 0.6777145683898096, None)
embeddings:
14157 3357 4043
[[ 0.43748114  0.3308268   0.28140372 ...  0.19979504  0.31552643
   0.34244016]
 [ 0.13997191  0.21636894  0.02634708 ...  0.2338955   0.10308133
   0.11905849]
 [ 0.4070466   0.29228592  0.0884326  ...  0.3479275   0.13148238
   0.2372683 ]
 ...
 [ 0.10559225  0.09467262  0.13677837 ...  0.11766097  0.10424568
   0.12460104]
 [-0.01831938  0.21809754 -0.08098221 ...  0.11796693  0.14285296
   0.02116153]
 [ 0.26568875  0.0985766   0.11556273 ...  0.01089452  0.20264958
   0.2273779 ]]

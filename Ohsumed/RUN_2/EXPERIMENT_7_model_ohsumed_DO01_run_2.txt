(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13550 train_acc= 0.01820 val_loss= 3.11452 val_acc= 0.25970 time= 0.58418
Epoch: 0002 train_loss= 3.11451 train_acc= 0.23759 val_loss= 3.06730 val_acc= 0.26269 time= 0.29000
Epoch: 0003 train_loss= 3.06755 train_acc= 0.23792 val_loss= 2.99471 val_acc= 0.26269 time= 0.28908
Epoch: 0004 train_loss= 2.99581 train_acc= 0.23329 val_loss= 2.90422 val_acc= 0.26567 time= 0.29497
Epoch: 0005 train_loss= 2.90666 train_acc= 0.24156 val_loss= 2.81270 val_acc= 0.27164 time= 0.29503
Epoch: 0006 train_loss= 2.81688 train_acc= 0.25050 val_loss= 2.74000 val_acc= 0.27164 time= 0.28800
Epoch: 0007 train_loss= 2.74680 train_acc= 0.25248 val_loss= 2.70224 val_acc= 0.26269 time= 0.28800
Epoch: 0008 train_loss= 2.71031 train_acc= 0.24289 val_loss= 2.69672 val_acc= 0.22687 time= 0.29497
Epoch: 0009 train_loss= 2.70582 train_acc= 0.19490 val_loss= 2.69610 val_acc= 0.20597 time= 0.29309
Epoch: 0010 train_loss= 2.70443 train_acc= 0.17340 val_loss= 2.68168 val_acc= 0.20000 time= 0.28900
Epoch: 0011 train_loss= 2.68460 train_acc= 0.17174 val_loss= 2.65410 val_acc= 0.20299 time= 0.29000
Epoch: 0012 train_loss= 2.64751 train_acc= 0.17207 val_loss= 2.62395 val_acc= 0.20597 time= 0.29600
Epoch: 0013 train_loss= 2.60790 train_acc= 0.17340 val_loss= 2.59782 val_acc= 0.21194 time= 0.28709
Epoch: 0014 train_loss= 2.57279 train_acc= 0.18167 val_loss= 2.57548 val_acc= 0.22985 time= 0.28700
Epoch: 0015 train_loss= 2.54287 train_acc= 0.19821 val_loss= 2.55314 val_acc= 0.25373 time= 0.29500
Epoch: 0016 train_loss= 2.51401 train_acc= 0.22502 val_loss= 2.52710 val_acc= 0.27761 time= 0.29244
Epoch: 0017 train_loss= 2.48301 train_acc= 0.25844 val_loss= 2.49581 val_acc= 0.28657 time= 0.29200
Epoch: 0018 train_loss= 2.44745 train_acc= 0.29153 val_loss= 2.45962 val_acc= 0.30448 time= 0.28800
Epoch: 0019 train_loss= 2.40605 train_acc= 0.31568 val_loss= 2.41980 val_acc= 0.31940 time= 0.29300
Epoch: 0020 train_loss= 2.36239 train_acc= 0.33223 val_loss= 2.37754 val_acc= 0.32537 time= 0.28900
Epoch: 0021 train_loss= 2.31487 train_acc= 0.34514 val_loss= 2.33371 val_acc= 0.32537 time= 0.29000
Epoch: 0022 train_loss= 2.26421 train_acc= 0.35440 val_loss= 2.28904 val_acc= 0.32537 time= 0.28900
Epoch: 0023 train_loss= 2.21130 train_acc= 0.36267 val_loss= 2.24417 val_acc= 0.33433 time= 0.29500
Epoch: 0024 train_loss= 2.15713 train_acc= 0.38418 val_loss= 2.19932 val_acc= 0.34328 time= 0.28800
Epoch: 0025 train_loss= 2.10191 train_acc= 0.40569 val_loss= 2.15450 val_acc= 0.38209 time= 0.28901
Epoch: 0026 train_loss= 2.04628 train_acc= 0.43911 val_loss= 2.10976 val_acc= 0.40597 time= 0.29099
Epoch: 0027 train_loss= 1.98838 train_acc= 0.47452 val_loss= 2.06517 val_acc= 0.45075 time= 0.29400
Epoch: 0028 train_loss= 1.93039 train_acc= 0.51754 val_loss= 2.02045 val_acc= 0.45970 time= 0.28800
Epoch: 0029 train_loss= 1.87180 train_acc= 0.54434 val_loss= 1.97511 val_acc= 0.49552 time= 0.28702
Epoch: 0030 train_loss= 1.81186 train_acc= 0.57677 val_loss= 1.92881 val_acc= 0.51045 time= 0.29853
Epoch: 0031 train_loss= 1.75113 train_acc= 0.59828 val_loss= 1.88173 val_acc= 0.52239 time= 0.29339
Epoch: 0032 train_loss= 1.69072 train_acc= 0.60854 val_loss= 1.83481 val_acc= 0.52537 time= 0.29411
Epoch: 0033 train_loss= 1.62754 train_acc= 0.62111 val_loss= 1.78939 val_acc= 0.52537 time= 0.28897
Epoch: 0034 train_loss= 1.56953 train_acc= 0.62972 val_loss= 1.74640 val_acc= 0.53433 time= 0.29600
Epoch: 0035 train_loss= 1.51084 train_acc= 0.63997 val_loss= 1.70604 val_acc= 0.53433 time= 0.29203
Epoch: 0036 train_loss= 1.45279 train_acc= 0.65586 val_loss= 1.66772 val_acc= 0.54627 time= 0.29000
Epoch: 0037 train_loss= 1.39675 train_acc= 0.66645 val_loss= 1.63092 val_acc= 0.55821 time= 0.29343
Epoch: 0038 train_loss= 1.34168 train_acc= 0.67803 val_loss= 1.59581 val_acc= 0.56119 time= 0.29495
Epoch: 0039 train_loss= 1.28869 train_acc= 0.69325 val_loss= 1.56271 val_acc= 0.57612 time= 0.29497
Epoch: 0040 train_loss= 1.23440 train_acc= 0.70020 val_loss= 1.53120 val_acc= 0.58209 time= 0.29503
Epoch: 0041 train_loss= 1.18445 train_acc= 0.70979 val_loss= 1.50064 val_acc= 0.58806 time= 0.29504
Epoch: 0042 train_loss= 1.13664 train_acc= 0.72402 val_loss= 1.47139 val_acc= 0.59403 time= 0.28800
Epoch: 0043 train_loss= 1.08425 train_acc= 0.74289 val_loss= 1.44437 val_acc= 0.60597 time= 0.28700
Epoch: 0044 train_loss= 1.03823 train_acc= 0.75381 val_loss= 1.42004 val_acc= 0.60896 time= 0.28908
Epoch: 0045 train_loss= 0.99293 train_acc= 0.76572 val_loss= 1.39787 val_acc= 0.60299 time= 0.29548
Epoch: 0046 train_loss= 0.94997 train_acc= 0.77829 val_loss= 1.37730 val_acc= 0.60299 time= 0.28799
Epoch: 0047 train_loss= 0.90512 train_acc= 0.78921 val_loss= 1.35790 val_acc= 0.60597 time= 0.29001
Epoch: 0048 train_loss= 0.86539 train_acc= 0.80013 val_loss= 1.33924 val_acc= 0.61194 time= 0.29904
Epoch: 0049 train_loss= 0.82444 train_acc= 0.81006 val_loss= 1.32165 val_acc= 0.61791 time= 0.29332
Epoch: 0050 train_loss= 0.78671 train_acc= 0.82429 val_loss= 1.30513 val_acc= 0.61791 time= 0.29008
Epoch: 0051 train_loss= 0.74827 train_acc= 0.83157 val_loss= 1.29004 val_acc= 0.62687 time= 0.29140
Epoch: 0052 train_loss= 0.71149 train_acc= 0.83918 val_loss= 1.27659 val_acc= 0.62687 time= 0.29732
Epoch: 0053 train_loss= 0.67791 train_acc= 0.85242 val_loss= 1.26519 val_acc= 0.63284 time= 0.29000
Epoch: 0054 train_loss= 0.64410 train_acc= 0.85539 val_loss= 1.25567 val_acc= 0.62985 time= 0.29000
Epoch: 0055 train_loss= 0.61426 train_acc= 0.86367 val_loss= 1.24760 val_acc= 0.63881 time= 0.28903
Epoch: 0056 train_loss= 0.58348 train_acc= 0.87426 val_loss= 1.24052 val_acc= 0.64179 time= 0.29601
Epoch: 0057 train_loss= 0.55078 train_acc= 0.88154 val_loss= 1.23353 val_acc= 0.63881 time= 0.28701
Epoch: 0058 train_loss= 0.52485 train_acc= 0.88617 val_loss= 1.22591 val_acc= 0.64478 time= 0.29004
Epoch: 0059 train_loss= 0.49809 train_acc= 0.89477 val_loss= 1.21849 val_acc= 0.65373 time= 0.29197
Epoch: 0060 train_loss= 0.47352 train_acc= 0.89907 val_loss= 1.21234 val_acc= 0.65075 time= 0.29420
Epoch: 0061 train_loss= 0.44975 train_acc= 0.90668 val_loss= 1.20649 val_acc= 0.65373 time= 0.28900
Epoch: 0062 train_loss= 0.42610 train_acc= 0.91264 val_loss= 1.20145 val_acc= 0.65373 time= 0.28600
Epoch: 0063 train_loss= 0.40431 train_acc= 0.91694 val_loss= 1.19726 val_acc= 0.65970 time= 0.29700
Epoch: 0064 train_loss= 0.38371 train_acc= 0.92323 val_loss= 1.19430 val_acc= 0.65970 time= 0.29297
Epoch: 0065 train_loss= 0.36337 train_acc= 0.92886 val_loss= 1.19327 val_acc= 0.67164 time= 0.29003
Epoch: 0066 train_loss= 0.34492 train_acc= 0.93448 val_loss= 1.19257 val_acc= 0.66866 time= 0.29100
Epoch: 0067 train_loss= 0.32657 train_acc= 0.94044 val_loss= 1.19148 val_acc= 0.65970 time= 0.29500
Epoch: 0068 train_loss= 0.30996 train_acc= 0.94341 val_loss= 1.19061 val_acc= 0.67164 time= 0.28800
Epoch: 0069 train_loss= 0.29476 train_acc= 0.94805 val_loss= 1.18982 val_acc= 0.67164 time= 0.29004
Epoch: 0070 train_loss= 0.27977 train_acc= 0.95268 val_loss= 1.18898 val_acc= 0.67164 time= 0.29693
Epoch: 0071 train_loss= 0.26437 train_acc= 0.95731 val_loss= 1.18857 val_acc= 0.67164 time= 0.29293
Epoch: 0072 train_loss= 0.25144 train_acc= 0.95996 val_loss= 1.18921 val_acc= 0.67761 time= 0.29097
Epoch: 0073 train_loss= 0.23912 train_acc= 0.96294 val_loss= 1.19125 val_acc= 0.68358 time= 0.29000
Epoch: 0074 train_loss= 0.22542 train_acc= 0.96525 val_loss= 1.19372 val_acc= 0.67761 time= 0.29603
Early stopping...
Optimization Finished!
Test set results: cost= 1.16383 accuracy= 0.68786 time= 0.12900
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7256    0.6959    0.7104       342
           1     0.6972    0.7379    0.7170       103
           2     0.7395    0.6286    0.6795       140
           3     0.6182    0.4304    0.5075        79
           4     0.6433    0.7652    0.6990       132
           5     0.6994    0.7955    0.7444       313
           6     0.6852    0.7255    0.7048       102
           7     0.6286    0.3143    0.4190        70
           8     0.5556    0.4000    0.4651        50
           9     0.6243    0.7290    0.6726       155
          10     0.8392    0.6417    0.7273       187
          11     0.6092    0.6277    0.6183       231
          12     0.7665    0.7191    0.7420       178
          13     0.7765    0.8050    0.7905       600
          14     0.7724    0.8458    0.8074       590
          15     0.7324    0.6842    0.7075        76
          16     0.6667    0.2941    0.4082        34
          17     0.5000    0.1000    0.1667        10
          18     0.4376    0.4940    0.4641       419
          19     0.6355    0.5271    0.5763       129
          20     0.6296    0.6071    0.6182        28
          21     1.0000    0.7586    0.8627        29
          22     0.5385    0.3043    0.3889        46

    accuracy                         0.6879      4043
   macro avg     0.6748    0.5927    0.6173      4043
weighted avg     0.6902    0.6879    0.6841      4043

Macro average Test Precision, Recall and F1-Score...
(0.6748282632104569, 0.59265625918088, 0.6172801945272014, None)
Micro average Test Precision, Recall and F1-Score...
(0.6878555528073212, 0.6878555528073212, 0.6878555528073212, None)
embeddings:
14157 3357 4043
[[ 0.56074387  0.4494336   0.44822714 ...  0.36437488  0.4515984
   0.33145124]
 [-0.03539068  0.28146634  0.16176279 ... -0.06791712  0.10776696
   0.23079777]
 [ 0.11301413  0.44084713  0.32833853 ...  0.10234981  0.31507424
   0.4296682 ]
 ...
 [ 0.25977424  0.2787166   0.16777915 ...  0.11570905  0.26278275
   0.1272259 ]
 [-0.0458292  -0.02029868 -0.05508579 ...  0.08704799  0.00452446
   0.33487242]
 [ 0.3895806   0.05052452  0.17945197 ...  0.23922034  0.32879767
   0.02311221]]

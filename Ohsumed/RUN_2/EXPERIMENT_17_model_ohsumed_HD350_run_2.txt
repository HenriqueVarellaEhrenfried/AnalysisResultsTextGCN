(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)
21557
  (0, 3359)	5.577030769017057
  (0, 3385)	26.455355949884314
  (0, 3421)	13.939666206452838
  (0, 3587)	5.220355825078324
  (0, 3645)	0.9703478889312359
  (0, 3738)	5.8647128414688385
  (0, 3917)	19.628143287419626
  (0, 4195)	5.651138741170779
  (0, 4329)	31.721429608653622
  (0, 4385)	4.52720864451838
  (0, 4428)	19.128403576588678
  (0, 4548)	3.6781266623376743
  (0, 4600)	2.5007064881327628
  (0, 4711)	10.730794439619183
  (0, 4779)	3.68348860547906
  (0, 4822)	6.076021935136045
  (0, 4886)	1.3033452781391397
  (0, 5046)	3.7387512841541093
  (0, 5108)	13.65958747502485
  (0, 5197)	4.565429857338577
  (0, 5737)	5.731181448844316
  (0, 5896)	3.147183896412084
  (0, 5900)	1.6815727804636063
  (0, 6074)	3.7107382479264355
  (0, 6125)	4.502516031928008
  :	:
  (21556, 12648)	4.03403795599111
  (21556, 12692)	22.56882275421711
  (21556, 12705)	2.9278210679377805
  (21556, 12894)	4.53978742672524
  (21556, 12960)	2.644863488059361
  (21556, 13102)	6.963325130136948
  (21556, 13281)	3.0484490557263952
  (21556, 14226)	5.651138741170779
  (21556, 14662)	14.483872993990904
  (21556, 14684)	9.23755167608774
  (21556, 14980)	3.56690102722745
  (21556, 15283)	3.071504832026321
  (21556, 15587)	1.6650077635889111
  (21556, 15636)	4.245796185080194
  (21556, 15690)	11.729425682937677
  (21556, 15849)	3.859379271942724
  (21556, 16287)	6.076021935136045
  (21556, 16491)	3.2567460989236108
  (21556, 16547)	2.7292186255396884
  (21556, 16819)	1.8953198043817334
  (21556, 17017)	5.337918868042983
  (21556, 17066)	6.018675851219539
  (21556, 17095)	6.1366465569524795
  (21556, 17311)	6.344285921730725
  (21556, 17421)	3.744449305268747
(21557, 21557)
(21557, 21557)
21557
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 23), dtype=float32)
Epoch: 0001 train_loss= 3.13547 train_acc= 0.05162 val_loss= 3.10862 val_acc= 0.20597 time= 4.57100
Epoch: 0002 train_loss= 3.10876 train_acc= 0.17737 val_loss= 3.04065 val_acc= 0.20597 time= 4.44800
Epoch: 0003 train_loss= 3.04131 train_acc= 0.17373 val_loss= 2.93598 val_acc= 0.20000 time= 4.42500
Epoch: 0004 train_loss= 2.93810 train_acc= 0.17240 val_loss= 2.81869 val_acc= 0.20000 time= 4.41600
Epoch: 0005 train_loss= 2.82369 train_acc= 0.17141 val_loss= 2.72813 val_acc= 0.20000 time= 4.45400
Epoch: 0006 train_loss= 2.73712 train_acc= 0.17141 val_loss= 2.69373 val_acc= 0.20000 time= 4.43900
Epoch: 0007 train_loss= 2.70646 train_acc= 0.17141 val_loss= 2.70053 val_acc= 0.20000 time= 4.44000
Epoch: 0008 train_loss= 2.71392 train_acc= 0.17141 val_loss= 2.69304 val_acc= 0.20000 time= 4.45300
Epoch: 0009 train_loss= 2.69802 train_acc= 0.17174 val_loss= 2.65891 val_acc= 0.20597 time= 4.44000
Epoch: 0010 train_loss= 2.65373 train_acc= 0.17373 val_loss= 2.61901 val_acc= 0.21194 time= 4.44600
Epoch: 0011 train_loss= 2.60065 train_acc= 0.18365 val_loss= 2.58732 val_acc= 0.23881 time= 4.44800
Epoch: 0012 train_loss= 2.55770 train_acc= 0.20682 val_loss= 2.56021 val_acc= 0.25970 time= 4.43400
Epoch: 0013 train_loss= 2.52015 train_acc= 0.23792 val_loss= 2.52946 val_acc= 0.28657 time= 4.44600
Epoch: 0014 train_loss= 2.48237 train_acc= 0.27929 val_loss= 2.49074 val_acc= 0.30448 time= 4.44500
Epoch: 0015 train_loss= 2.44010 train_acc= 0.32098 val_loss= 2.44406 val_acc= 0.31642 time= 4.41900
Epoch: 0016 train_loss= 2.38876 train_acc= 0.35572 val_loss= 2.39155 val_acc= 0.33433 time= 4.43800
Epoch: 0017 train_loss= 2.33098 train_acc= 0.37624 val_loss= 2.33573 val_acc= 0.33731 time= 4.44700
Epoch: 0018 train_loss= 2.27185 train_acc= 0.38187 val_loss= 2.27877 val_acc= 0.34328 time= 4.45800
Epoch: 0019 train_loss= 2.20784 train_acc= 0.38948 val_loss= 2.22166 val_acc= 0.34925 time= 4.41500
Epoch: 0020 train_loss= 2.13951 train_acc= 0.40768 val_loss= 2.16509 val_acc= 0.37015 time= 4.44500
Epoch: 0021 train_loss= 2.07440 train_acc= 0.42687 val_loss= 2.10917 val_acc= 0.39701 time= 4.46500
Epoch: 0022 train_loss= 2.00628 train_acc= 0.45731 val_loss= 2.05394 val_acc= 0.44776 time= 4.46000
Epoch: 0023 train_loss= 1.93446 train_acc= 0.50232 val_loss= 1.99998 val_acc= 0.47761 time= 4.45200
Epoch: 0024 train_loss= 1.86526 train_acc= 0.54103 val_loss= 1.94742 val_acc= 0.50746 time= 4.43100
Epoch: 0025 train_loss= 1.79490 train_acc= 0.57512 val_loss= 1.89552 val_acc= 0.53134 time= 4.46900
Epoch: 0026 train_loss= 1.72791 train_acc= 0.59332 val_loss= 1.84317 val_acc= 0.53731 time= 4.44000
Epoch: 0027 train_loss= 1.65748 train_acc= 0.60490 val_loss= 1.79056 val_acc= 0.54030 time= 4.46901
Epoch: 0028 train_loss= 1.58585 train_acc= 0.61847 val_loss= 1.73876 val_acc= 0.53433 time= 4.42699
Epoch: 0029 train_loss= 1.51907 train_acc= 0.63203 val_loss= 1.69071 val_acc= 0.54328 time= 4.46200
Epoch: 0030 train_loss= 1.45284 train_acc= 0.64428 val_loss= 1.64716 val_acc= 0.54925 time= 4.48500
Epoch: 0031 train_loss= 1.38971 train_acc= 0.66016 val_loss= 1.60668 val_acc= 0.56418 time= 4.46100
Epoch: 0032 train_loss= 1.32706 train_acc= 0.67538 val_loss= 1.56887 val_acc= 0.58507 time= 4.44600
Epoch: 0033 train_loss= 1.27317 train_acc= 0.68630 val_loss= 1.53374 val_acc= 0.57612 time= 4.63200
Epoch: 0034 train_loss= 1.21365 train_acc= 0.69821 val_loss= 1.50027 val_acc= 0.58209 time= 4.45500
Epoch: 0035 train_loss= 1.16106 train_acc= 0.70748 val_loss= 1.46780 val_acc= 0.58209 time= 4.44500
Epoch: 0036 train_loss= 1.10139 train_acc= 0.72171 val_loss= 1.43678 val_acc= 0.58507 time= 4.46200
Epoch: 0037 train_loss= 1.04942 train_acc= 0.73858 val_loss= 1.40906 val_acc= 0.59104 time= 4.44800
Epoch: 0038 train_loss= 0.99599 train_acc= 0.75745 val_loss= 1.38438 val_acc= 0.59403 time= 4.44400
Epoch: 0039 train_loss= 0.94860 train_acc= 0.76870 val_loss= 1.36056 val_acc= 0.59701 time= 4.42400
Epoch: 0040 train_loss= 0.90215 train_acc= 0.77631 val_loss= 1.33794 val_acc= 0.60896 time= 4.45000
Epoch: 0041 train_loss= 0.85234 train_acc= 0.79649 val_loss= 1.31630 val_acc= 0.61791 time= 4.48400
Epoch: 0042 train_loss= 0.81063 train_acc= 0.80377 val_loss= 1.29727 val_acc= 0.61791 time= 4.44200
Epoch: 0043 train_loss= 0.76839 train_acc= 0.81502 val_loss= 1.28053 val_acc= 0.62090 time= 4.46700
Epoch: 0044 train_loss= 0.73077 train_acc= 0.82495 val_loss= 1.26573 val_acc= 0.61493 time= 4.46800
Epoch: 0045 train_loss= 0.69017 train_acc= 0.83852 val_loss= 1.25309 val_acc= 0.62687 time= 4.48000
Epoch: 0046 train_loss= 0.64991 train_acc= 0.84778 val_loss= 1.24236 val_acc= 0.62687 time= 4.45400
Epoch: 0047 train_loss= 0.61467 train_acc= 0.85705 val_loss= 1.23143 val_acc= 0.63284 time= 4.40800
Epoch: 0048 train_loss= 0.58056 train_acc= 0.86962 val_loss= 1.22064 val_acc= 0.63881 time= 4.45000
Epoch: 0049 train_loss= 0.55183 train_acc= 0.86962 val_loss= 1.21180 val_acc= 0.64776 time= 4.45200
Epoch: 0050 train_loss= 0.51672 train_acc= 0.88154 val_loss= 1.20320 val_acc= 0.65373 time= 4.46400
Epoch: 0051 train_loss= 0.49357 train_acc= 0.89014 val_loss= 1.19575 val_acc= 0.64179 time= 5.00900
Epoch: 0052 train_loss= 0.46366 train_acc= 0.89245 val_loss= 1.19010 val_acc= 0.64776 time= 4.96400
Epoch: 0053 train_loss= 0.43688 train_acc= 0.90337 val_loss= 1.18560 val_acc= 0.65075 time= 4.71600
Epoch: 0054 train_loss= 0.41296 train_acc= 0.91330 val_loss= 1.18111 val_acc= 0.66269 time= 4.70600
Epoch: 0055 train_loss= 0.38950 train_acc= 0.91694 val_loss= 1.17799 val_acc= 0.66269 time= 4.55303
Epoch: 0056 train_loss= 0.36936 train_acc= 0.91959 val_loss= 1.17646 val_acc= 0.65075 time= 4.48200
Epoch: 0057 train_loss= 0.34598 train_acc= 0.92819 val_loss= 1.17395 val_acc= 0.65970 time= 4.59600
Epoch: 0058 train_loss= 0.32657 train_acc= 0.93613 val_loss= 1.17017 val_acc= 0.66567 time= 4.54500
Epoch: 0059 train_loss= 0.30834 train_acc= 0.93878 val_loss= 1.16776 val_acc= 0.67164 time= 4.74897
Epoch: 0060 train_loss= 0.28894 train_acc= 0.94408 val_loss= 1.16742 val_acc= 0.66269 time= 4.55000
Epoch: 0061 train_loss= 0.27274 train_acc= 0.95003 val_loss= 1.16885 val_acc= 0.66866 time= 4.95900
Epoch: 0062 train_loss= 0.25719 train_acc= 0.95169 val_loss= 1.17081 val_acc= 0.65373 time= 4.71600
Epoch: 0063 train_loss= 0.24566 train_acc= 0.95301 val_loss= 1.17384 val_acc= 0.65672 time= 4.53204
Epoch: 0064 train_loss= 0.23176 train_acc= 0.95599 val_loss= 1.17572 val_acc= 0.65970 time= 4.82200
Early stopping...
Optimization Finished!
Test set results: cost= 1.17032 accuracy= 0.68736 time= 1.70597
21557
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7180    0.7222    0.7201       342
           1     0.6916    0.7184    0.7048       103
           2     0.7478    0.6143    0.6745       140
           3     0.5517    0.4051    0.4672        79
           4     0.6600    0.7500    0.7021       132
           5     0.6756    0.8051    0.7347       313
           6     0.6822    0.7157    0.6986       102
           7     0.6857    0.3429    0.4571        70
           8     0.6667    0.3600    0.4675        50
           9     0.6333    0.7355    0.6806       155
          10     0.8462    0.6471    0.7333       187
          11     0.6024    0.6623    0.6309       231
          12     0.7222    0.7303    0.7263       178
          13     0.7728    0.8050    0.7886       600
          14     0.7729    0.8424    0.8062       590
          15     0.7606    0.7105    0.7347        76
          16     0.7500    0.3529    0.4800        34
          17     0.5000    0.1000    0.1667        10
          18     0.4318    0.4535    0.4424       419
          19     0.6569    0.5194    0.5801       129
          20     0.6296    0.6071    0.6182        28
          21     1.0000    0.7586    0.8627        29
          22     0.5909    0.2826    0.3824        46

    accuracy                         0.6874      4043
   macro avg     0.6847    0.5931    0.6200      4043
weighted avg     0.6891    0.6874    0.6826      4043

Macro average Test Precision, Recall and F1-Score...
(0.6847384089966885, 0.5930845036991796, 0.6199806663434926, None)
Micro average Test Precision, Recall and F1-Score...
(0.6873608706406134, 0.6873608706406134, 0.6873608706406134, None)
embeddings:
14157 3357 4043
[[ 0.2504753   0.2790098   0.25623843 ...  0.2549306   0.32486853
   0.32464477]
 [ 0.10898851 -0.00465674  0.08085738 ... -0.02072915  0.15893671
  -0.01273989]
 [ 0.26370913  0.16660793  0.1373176  ...  0.01364657  0.22486843
   0.26258582]
 ...
 [ 0.1376193   0.10107569  0.10280854 ...  0.0876048   0.11774253
   0.17870948]
 [-0.04270562 -0.06373263  0.14525257 ...  0.04771288  0.24670798
  -0.01596773]
 [ 0.12217867  0.15927267  0.14669922 ...  0.15935981  0.16665946
   0.11556996]]

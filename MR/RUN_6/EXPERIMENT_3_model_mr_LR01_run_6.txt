(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49906 val_loss= 0.68277 val_acc= 0.51127 time= 0.37680
Epoch: 0002 train_loss= 0.67407 train_acc= 0.54595 val_loss= 0.66433 val_acc= 0.52113 time= 0.07603
Epoch: 0003 train_loss= 0.61407 train_acc= 0.56565 val_loss= 0.59497 val_acc= 0.70704 time= 0.07600
Epoch: 0004 train_loss= 0.51869 train_acc= 0.83542 val_loss= 0.54001 val_acc= 0.73662 time= 0.07305
Epoch: 0005 train_loss= 0.41148 train_acc= 0.86730 val_loss= 0.52118 val_acc= 0.75211 time= 0.08603
Epoch: 0006 train_loss= 0.33137 train_acc= 0.86918 val_loss= 0.50461 val_acc= 0.76620 time= 0.07401
Epoch: 0007 train_loss= 0.25580 train_acc= 0.89934 val_loss= 0.54616 val_acc= 0.76056 time= 0.07099
Epoch: 0008 train_loss= 0.21577 train_acc= 0.91044 val_loss= 0.60476 val_acc= 0.76056 time= 0.07201
Epoch: 0009 train_loss= 0.19767 train_acc= 0.91544 val_loss= 0.64915 val_acc= 0.76479 time= 0.07210
Epoch: 0010 train_loss= 0.15404 train_acc= 0.93842 val_loss= 0.71601 val_acc= 0.75775 time= 0.07100
Epoch: 0011 train_loss= 0.13123 train_acc= 0.95030 val_loss= 0.78026 val_acc= 0.76620 time= 0.08900
Epoch: 0012 train_loss= 0.11586 train_acc= 0.95389 val_loss= 0.84276 val_acc= 0.75352 time= 0.07200
Early stopping...
Optimization Finished!
Test set results: cost= 0.82846 accuracy= 0.75549 time= 0.03001
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7506    0.7653    0.7579      1777
           1     0.7606    0.7456    0.7531      1777

    accuracy                         0.7555      3554
   macro avg     0.7556    0.7555    0.7555      3554
weighted avg     0.7556    0.7555    0.7555      3554

Macro average Test Precision, Recall and F1-Score...
(0.7555859267087892, 0.7554867754642656, 0.7554630592308236, None)
Micro average Test Precision, Recall and F1-Score...
(0.7554867754642656, 0.7554867754642656, 0.7554867754642657, None)
embeddings:
18764 7108 3554
[[-7.3882237e-02 -7.3838539e-02  5.5531017e-02 ...  1.4126341e-01
  -6.8657883e-02 -5.4861542e-02]
 [-1.0391577e-01 -1.6665909e-01 -2.5480807e-02 ... -6.5226667e-02
   1.7314506e-01  1.7801473e-02]
 [-8.7912254e-02 -1.0248430e-01  3.1137067e-01 ...  2.6257369e-01
  -8.2515642e-02 -5.4480948e-02]
 ...
 [-8.3217109e-03  3.6454827e-01  5.5239558e-01 ... -4.7022483e-04
  -2.3052869e-03 -1.6047378e-01]
 [-1.3824958e-01 -1.4524235e-01  1.2833805e-02 ...  1.2547312e-02
   1.8527338e-01 -4.3813732e-02]
 [-3.4435624e-01 -3.2347274e-01  1.6998212e-01 ... -4.0242516e-02
   1.9800881e-01 -6.0196660e-02]]

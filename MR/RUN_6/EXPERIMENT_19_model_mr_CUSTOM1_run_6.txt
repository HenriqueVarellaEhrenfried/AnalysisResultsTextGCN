(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50375 val_loss= 0.69233 val_acc= 0.73239 time= 0.36987
Epoch: 0002 train_loss= 0.69168 train_acc= 0.80306 val_loss= 0.69067 val_acc= 0.72817 time= 0.08303
Epoch: 0003 train_loss= 0.68896 train_acc= 0.81557 val_loss= 0.68805 val_acc= 0.72817 time= 0.07500
Epoch: 0004 train_loss= 0.68465 train_acc= 0.81088 val_loss= 0.68459 val_acc= 0.73662 time= 0.07499
Epoch: 0005 train_loss= 0.67887 train_acc= 0.82307 val_loss= 0.68034 val_acc= 0.73803 time= 0.07600
Epoch: 0006 train_loss= 0.67207 train_acc= 0.83182 val_loss= 0.67526 val_acc= 0.73662 time= 0.07403
Epoch: 0007 train_loss= 0.66392 train_acc= 0.83026 val_loss= 0.66937 val_acc= 0.74225 time= 0.07194
Epoch: 0008 train_loss= 0.65397 train_acc= 0.83542 val_loss= 0.66267 val_acc= 0.74648 time= 0.07700
Epoch: 0009 train_loss= 0.64320 train_acc= 0.83557 val_loss= 0.65522 val_acc= 0.75211 time= 0.07603
Epoch: 0010 train_loss= 0.63088 train_acc= 0.84573 val_loss= 0.64705 val_acc= 0.75211 time= 0.07297
Epoch: 0011 train_loss= 0.61796 train_acc= 0.84917 val_loss= 0.63823 val_acc= 0.75352 time= 0.07425
Epoch: 0012 train_loss= 0.60263 train_acc= 0.85198 val_loss= 0.62885 val_acc= 0.75775 time= 0.07199
Epoch: 0013 train_loss= 0.58759 train_acc= 0.85902 val_loss= 0.61899 val_acc= 0.76056 time= 0.07300
Epoch: 0014 train_loss= 0.56985 train_acc= 0.86965 val_loss= 0.60879 val_acc= 0.76338 time= 0.07200
Epoch: 0015 train_loss= 0.55450 train_acc= 0.86621 val_loss= 0.59836 val_acc= 0.76761 time= 0.07457
Epoch: 0016 train_loss= 0.53595 train_acc= 0.87121 val_loss= 0.58782 val_acc= 0.77324 time= 0.07403
Epoch: 0017 train_loss= 0.51776 train_acc= 0.86871 val_loss= 0.57734 val_acc= 0.77183 time= 0.07201
Epoch: 0018 train_loss= 0.49864 train_acc= 0.87465 val_loss= 0.56707 val_acc= 0.76901 time= 0.07199
Epoch: 0019 train_loss= 0.48085 train_acc= 0.87887 val_loss= 0.55718 val_acc= 0.77042 time= 0.07215
Epoch: 0020 train_loss= 0.46061 train_acc= 0.87652 val_loss= 0.54777 val_acc= 0.77465 time= 0.07300
Epoch: 0021 train_loss= 0.44335 train_acc= 0.87590 val_loss= 0.53895 val_acc= 0.77746 time= 0.07200
Epoch: 0022 train_loss= 0.42734 train_acc= 0.88309 val_loss= 0.53087 val_acc= 0.77324 time= 0.07404
Epoch: 0023 train_loss= 0.40871 train_acc= 0.88512 val_loss= 0.52360 val_acc= 0.77324 time= 0.07600
Epoch: 0024 train_loss= 0.39242 train_acc= 0.88778 val_loss= 0.51718 val_acc= 0.77183 time= 0.07302
Epoch: 0025 train_loss= 0.37476 train_acc= 0.88903 val_loss= 0.51166 val_acc= 0.77606 time= 0.07300
Epoch: 0026 train_loss= 0.36177 train_acc= 0.89184 val_loss= 0.50711 val_acc= 0.77887 time= 0.07300
Epoch: 0027 train_loss= 0.34625 train_acc= 0.89262 val_loss= 0.50347 val_acc= 0.78169 time= 0.07256
Epoch: 0028 train_loss= 0.33277 train_acc= 0.89512 val_loss= 0.50077 val_acc= 0.78169 time= 0.07400
Epoch: 0029 train_loss= 0.31972 train_acc= 0.89778 val_loss= 0.49898 val_acc= 0.78169 time= 0.07500
Epoch: 0030 train_loss= 0.30893 train_acc= 0.89778 val_loss= 0.49809 val_acc= 0.78310 time= 0.07204
Epoch: 0031 train_loss= 0.29753 train_acc= 0.90247 val_loss= 0.49796 val_acc= 0.78169 time= 0.07204
Epoch: 0032 train_loss= 0.28382 train_acc= 0.90653 val_loss= 0.49861 val_acc= 0.78028 time= 0.07304
Epoch: 0033 train_loss= 0.27509 train_acc= 0.91091 val_loss= 0.50002 val_acc= 0.78310 time= 0.07304
Early stopping...
Optimization Finished!
Test set results: cost= 0.50213 accuracy= 0.76168 time= 0.03103
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7589    0.7670    0.7629      1777
           1     0.7645    0.7563    0.7604      1777

    accuracy                         0.7617      3554
   macro avg     0.7617    0.7617    0.7617      3554
weighted avg     0.7617    0.7617    0.7617      3554

Macro average Test Precision, Recall and F1-Score...
(0.7617069027113723, 0.7616769836803601, 0.7616701720549888, None)
Micro average Test Precision, Recall and F1-Score...
(0.7616769836803602, 0.7616769836803602, 0.7616769836803602, None)
embeddings:
18764 7108 3554
[[ 0.06881476  0.0224985  -0.00949783 ... -0.00807263 -0.01765241
   0.01000624]
 [ 0.02263434  0.09596128  0.08785202 ...  0.10863823  0.06570078
   0.11196287]
 [ 0.13453151 -0.03437304 -0.03771135 ... -0.02224999 -0.04522768
  -0.0469347 ]
 ...
 [-0.00431865 -0.04542114 -0.04618882 ... -0.04201258 -0.04407615
  -0.02953899]
 [ 0.03817115  0.09637436  0.10398502 ...  0.08974169  0.12452229
   0.09212471]
 [ 0.06976552  0.18739249  0.2064496  ...  0.16978353  0.16546644
   0.18179314]]

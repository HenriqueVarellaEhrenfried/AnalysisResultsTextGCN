(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50391 val_loss= 0.69181 val_acc= 0.69155 time= 0.37310
Epoch: 0002 train_loss= 0.69059 train_acc= 0.83088 val_loss= 0.68887 val_acc= 0.68451 time= 0.07700
Epoch: 0003 train_loss= 0.68565 train_acc= 0.79572 val_loss= 0.68487 val_acc= 0.69155 time= 0.07500
Epoch: 0004 train_loss= 0.67880 train_acc= 0.80416 val_loss= 0.67988 val_acc= 0.69577 time= 0.07400
Epoch: 0005 train_loss= 0.67027 train_acc= 0.81010 val_loss= 0.67381 val_acc= 0.71690 time= 0.08811
Epoch: 0006 train_loss= 0.65991 train_acc= 0.82604 val_loss= 0.66668 val_acc= 0.71972 time= 0.07200
Epoch: 0007 train_loss= 0.64775 train_acc= 0.83542 val_loss= 0.65852 val_acc= 0.73380 time= 0.07199
Epoch: 0008 train_loss= 0.63393 train_acc= 0.84511 val_loss= 0.64940 val_acc= 0.73944 time= 0.07397
Epoch: 0009 train_loss= 0.61830 train_acc= 0.85464 val_loss= 0.63942 val_acc= 0.74225 time= 0.07504
Epoch: 0010 train_loss= 0.60111 train_acc= 0.86214 val_loss= 0.62866 val_acc= 0.74507 time= 0.07299
Epoch: 0011 train_loss= 0.58249 train_acc= 0.86527 val_loss= 0.61725 val_acc= 0.74930 time= 0.08700
Epoch: 0012 train_loss= 0.56239 train_acc= 0.87387 val_loss= 0.60538 val_acc= 0.75915 time= 0.07201
Epoch: 0013 train_loss= 0.54131 train_acc= 0.87996 val_loss= 0.59324 val_acc= 0.76479 time= 0.07296
Epoch: 0014 train_loss= 0.51956 train_acc= 0.88762 val_loss= 0.58105 val_acc= 0.76761 time= 0.07304
Epoch: 0015 train_loss= 0.49730 train_acc= 0.88903 val_loss= 0.56907 val_acc= 0.77183 time= 0.07206
Epoch: 0016 train_loss= 0.47469 train_acc= 0.89262 val_loss= 0.55753 val_acc= 0.77183 time= 0.08891
Epoch: 0017 train_loss= 0.45196 train_acc= 0.89575 val_loss= 0.54669 val_acc= 0.77324 time= 0.07201
Epoch: 0018 train_loss= 0.42943 train_acc= 0.89966 val_loss= 0.53676 val_acc= 0.77606 time= 0.07101
Epoch: 0019 train_loss= 0.40716 train_acc= 0.90309 val_loss= 0.52794 val_acc= 0.77465 time= 0.08100
Epoch: 0020 train_loss= 0.38616 train_acc= 0.90513 val_loss= 0.52035 val_acc= 0.77606 time= 0.07173
Epoch: 0021 train_loss= 0.36547 train_acc= 0.90778 val_loss= 0.51411 val_acc= 0.77887 time= 0.07201
Epoch: 0022 train_loss= 0.34617 train_acc= 0.90778 val_loss= 0.50929 val_acc= 0.78310 time= 0.07199
Epoch: 0023 train_loss= 0.32831 train_acc= 0.90982 val_loss= 0.50592 val_acc= 0.78028 time= 0.07496
Epoch: 0024 train_loss= 0.31134 train_acc= 0.91591 val_loss= 0.50399 val_acc= 0.78169 time= 0.08404
Epoch: 0025 train_loss= 0.29469 train_acc= 0.91560 val_loss= 0.50349 val_acc= 0.78028 time= 0.07199
Epoch: 0026 train_loss= 0.27846 train_acc= 0.92013 val_loss= 0.50430 val_acc= 0.78169 time= 0.07101
Epoch: 0027 train_loss= 0.26470 train_acc= 0.92404 val_loss= 0.50631 val_acc= 0.78169 time= 0.07300
Epoch: 0028 train_loss= 0.25048 train_acc= 0.92545 val_loss= 0.50944 val_acc= 0.78028 time= 0.07300
Epoch: 0029 train_loss= 0.23847 train_acc= 0.92982 val_loss= 0.51365 val_acc= 0.77887 time= 0.08803
Early stopping...
Optimization Finished!
Test set results: cost= 0.51179 accuracy= 0.75774 time= 0.03200
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7468    0.7800    0.7630      1777
           1     0.7697    0.7355    0.7522      1777

    accuracy                         0.7577      3554
   macro avg     0.7582    0.7577    0.7576      3554
weighted avg     0.7582    0.7577    0.7576      3554

Macro average Test Precision, Recall and F1-Score...
(0.7582481672149791, 0.7577377602701182, 0.7576179980870794, None)
Micro average Test Precision, Recall and F1-Score...
(0.7577377602701182, 0.7577377602701182, 0.7577377602701182, None)
embeddings:
18764 7108 3554
[[-0.0047945   0.00387688  0.08709646 ...  0.07575605 -0.00133037
   0.06994561]
 [ 0.09617092  0.11986152  0.01112495 ...  0.0025635   0.13253051
   0.00545977]
 [-0.02950024 -0.03902026  0.13923767 ...  0.13348624 -0.04079387
   0.13439868]
 ...
 [-0.0555717  -0.03965917  0.14710768 ...  0.14537252 -0.03586428
   0.18221033]
 [ 0.1061426   0.12877467  0.02806611 ...  0.02227592  0.14017457
   0.01624095]
 [ 0.18825996  0.23379716  0.10585745 ...  0.0825125   0.23886837
   0.07739583]]

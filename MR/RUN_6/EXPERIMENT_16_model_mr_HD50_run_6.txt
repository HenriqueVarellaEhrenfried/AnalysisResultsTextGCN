(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69313 train_acc= 0.50063 val_loss= 0.69144 val_acc= 0.63380 time= 0.29540
Epoch: 0002 train_loss= 0.69005 train_acc= 0.72601 val_loss= 0.68774 val_acc= 0.55352 time= 0.05503
Epoch: 0003 train_loss= 0.68377 train_acc= 0.59956 val_loss= 0.68337 val_acc= 0.54225 time= 0.05997
Epoch: 0004 train_loss= 0.67579 train_acc= 0.59597 val_loss= 0.67807 val_acc= 0.56056 time= 0.06000
Epoch: 0005 train_loss= 0.66617 train_acc= 0.61910 val_loss= 0.67178 val_acc= 0.57746 time= 0.05403
Epoch: 0006 train_loss= 0.65469 train_acc= 0.64661 val_loss= 0.66455 val_acc= 0.60141 time= 0.05400
Epoch: 0007 train_loss= 0.64193 train_acc= 0.68115 val_loss= 0.65642 val_acc= 0.62535 time= 0.05697
Epoch: 0008 train_loss= 0.62749 train_acc= 0.71475 val_loss= 0.64742 val_acc= 0.64648 time= 0.05403
Epoch: 0009 train_loss= 0.61109 train_acc= 0.75821 val_loss= 0.63759 val_acc= 0.67746 time= 0.05700
Epoch: 0010 train_loss= 0.59398 train_acc= 0.78884 val_loss= 0.62701 val_acc= 0.70000 time= 0.05400
Epoch: 0011 train_loss= 0.57569 train_acc= 0.81635 val_loss= 0.61581 val_acc= 0.73239 time= 0.05642
Epoch: 0012 train_loss= 0.55511 train_acc= 0.84073 val_loss= 0.60415 val_acc= 0.74366 time= 0.05400
Epoch: 0013 train_loss= 0.53434 train_acc= 0.86043 val_loss= 0.59231 val_acc= 0.75352 time= 0.05902
Epoch: 0014 train_loss= 0.51367 train_acc= 0.87527 val_loss= 0.58056 val_acc= 0.76056 time= 0.06098
Epoch: 0015 train_loss= 0.49199 train_acc= 0.88793 val_loss= 0.56924 val_acc= 0.76620 time= 0.05301
Epoch: 0016 train_loss= 0.46807 train_acc= 0.89637 val_loss= 0.55852 val_acc= 0.76479 time= 0.05800
Epoch: 0017 train_loss= 0.44672 train_acc= 0.89622 val_loss= 0.54861 val_acc= 0.75915 time= 0.05200
Epoch: 0018 train_loss= 0.42480 train_acc= 0.90012 val_loss= 0.53964 val_acc= 0.75634 time= 0.05799
Epoch: 0019 train_loss= 0.40251 train_acc= 0.90216 val_loss= 0.53175 val_acc= 0.75634 time= 0.05300
Epoch: 0020 train_loss= 0.38278 train_acc= 0.90778 val_loss= 0.52508 val_acc= 0.75352 time= 0.05800
Epoch: 0021 train_loss= 0.36181 train_acc= 0.90888 val_loss= 0.51973 val_acc= 0.75634 time= 0.05200
Epoch: 0022 train_loss= 0.34216 train_acc= 0.91075 val_loss= 0.51572 val_acc= 0.75915 time= 0.05800
Epoch: 0023 train_loss= 0.32658 train_acc= 0.91294 val_loss= 0.51316 val_acc= 0.75915 time= 0.05597
Epoch: 0024 train_loss= 0.30884 train_acc= 0.91654 val_loss= 0.51209 val_acc= 0.76197 time= 0.05903
Epoch: 0025 train_loss= 0.29309 train_acc= 0.91701 val_loss= 0.51260 val_acc= 0.76056 time= 0.05300
Epoch: 0026 train_loss= 0.27733 train_acc= 0.92185 val_loss= 0.51454 val_acc= 0.75775 time= 0.05799
Epoch: 0027 train_loss= 0.26415 train_acc= 0.92341 val_loss= 0.51711 val_acc= 0.75915 time= 0.05300
Epoch: 0028 train_loss= 0.25057 train_acc= 0.92498 val_loss= 0.52053 val_acc= 0.76056 time= 0.05800
Early stopping...
Optimization Finished!
Test set results: cost= 0.51231 accuracy= 0.75858 time= 0.02501
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7375    0.8030    0.7689      1777
           1     0.7838    0.7141    0.7473      1777

    accuracy                         0.7586      3554
   macro avg     0.7606    0.7586    0.7581      3554
weighted avg     0.7606    0.7586    0.7581      3554

Macro average Test Precision, Recall and F1-Score...
(0.7606424356758327, 0.7585818795723129, 0.7581037909711221, None)
Micro average Test Precision, Recall and F1-Score...
(0.7585818795723129, 0.7585818795723129, 0.7585818795723129, None)
embeddings:
18764 7108 3554
[[ 0.1520922   0.14912903  0.02427343 ...  0.02148248  0.16219838
   0.03880259]
 [ 0.00851651 -0.00229241  0.23768297 ...  0.27507707  0.00437406
   0.33870414]
 [ 0.30025372  0.2900127  -0.04902237 ... -0.05429107  0.28884256
  -0.05013048]
 ...
 [ 0.23675418 -0.00743478 -0.09998015 ... -0.09402395  0.33723867
  -0.09946626]
 [ 0.01634743  0.01443551  0.2764542  ...  0.27455205  0.01546596
   0.31341177]
 [ 0.0838638   0.10965686  0.5301361  ...  0.53933764  0.10024304
   0.585833  ]]

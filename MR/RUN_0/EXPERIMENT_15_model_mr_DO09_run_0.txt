(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.48703 val_loss= 0.69152 val_acc= 0.68873 time= 0.37576
Epoch: 0002 train_loss= 0.69019 train_acc= 0.77712 val_loss= 0.68742 val_acc= 0.67183 time= 0.08112
Epoch: 0003 train_loss= 0.68336 train_acc= 0.75617 val_loss= 0.68061 val_acc= 0.67606 time= 0.08105
Epoch: 0004 train_loss= 0.67202 train_acc= 0.76446 val_loss= 0.67130 val_acc= 0.69718 time= 0.07370
Epoch: 0005 train_loss= 0.65675 train_acc= 0.77931 val_loss= 0.65941 val_acc= 0.70845 time= 0.07413
Epoch: 0006 train_loss= 0.63662 train_acc= 0.80041 val_loss= 0.64502 val_acc= 0.73803 time= 0.07200
Epoch: 0007 train_loss= 0.61296 train_acc= 0.82369 val_loss= 0.62841 val_acc= 0.73662 time= 0.07300
Epoch: 0008 train_loss= 0.58532 train_acc= 0.84136 val_loss= 0.61009 val_acc= 0.75634 time= 0.07323
Epoch: 0009 train_loss= 0.55651 train_acc= 0.85480 val_loss= 0.59093 val_acc= 0.76056 time= 0.07297
Epoch: 0010 train_loss= 0.52136 train_acc= 0.86339 val_loss= 0.57182 val_acc= 0.77465 time= 0.09010
Epoch: 0011 train_loss= 0.49016 train_acc= 0.87480 val_loss= 0.55372 val_acc= 0.78169 time= 0.07206
Epoch: 0012 train_loss= 0.45690 train_acc= 0.87434 val_loss= 0.53727 val_acc= 0.78310 time= 0.07150
Epoch: 0013 train_loss= 0.42449 train_acc= 0.87715 val_loss= 0.52309 val_acc= 0.77887 time= 0.07212
Epoch: 0014 train_loss= 0.39270 train_acc= 0.88246 val_loss= 0.51175 val_acc= 0.77465 time= 0.07201
Epoch: 0015 train_loss= 0.36823 train_acc= 0.88543 val_loss= 0.50357 val_acc= 0.77746 time= 0.07399
Epoch: 0016 train_loss= 0.34232 train_acc= 0.88762 val_loss= 0.49863 val_acc= 0.77746 time= 0.09197
Epoch: 0017 train_loss= 0.31568 train_acc= 0.89528 val_loss= 0.49684 val_acc= 0.77887 time= 0.07204
Epoch: 0018 train_loss= 0.29441 train_acc= 0.90012 val_loss= 0.49803 val_acc= 0.78028 time= 0.07200
Epoch: 0019 train_loss= 0.28087 train_acc= 0.89747 val_loss= 0.50157 val_acc= 0.77606 time= 0.07100
Epoch: 0020 train_loss= 0.26257 train_acc= 0.90325 val_loss= 0.50775 val_acc= 0.77324 time= 0.07299
Epoch: 0021 train_loss= 0.24696 train_acc= 0.90982 val_loss= 0.51659 val_acc= 0.77183 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51652 accuracy= 0.76393 time= 0.03200
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7476    0.7968    0.7715      1777
           1     0.7825    0.7310    0.7559      1777

    accuracy                         0.7639      3554
   macro avg     0.7651    0.7639    0.7637      3554
weighted avg     0.7651    0.7639    0.7637      3554

Macro average Test Precision, Recall and F1-Score...
(0.7650770982557473, 0.7639279684862128, 0.7636718432663533, None)
Micro average Test Precision, Recall and F1-Score...
(0.7639279684862127, 0.7639279684862127, 0.7639279684862127, None)
embeddings:
18764 7108 3554
[[ 0.08142287 -0.00651419  0.05789001 ... -0.01242664  0.07777476
  -0.00307675]
 [-0.00709119  0.11087594  0.0241945  ...  0.05793022  0.03351909
   0.12238944]
 [ 0.16129512 -0.04606004  0.15500249 ... -0.03611405  0.15577789
  -0.03733522]
 ...
 [ 0.1467115  -0.10276883  0.09111928 ... -0.07113446 -0.00828343
  -0.08186098]
 [ 0.03730242  0.13205102  0.03374933 ...  0.111663    0.05121772
   0.12571831]
 [ 0.08546504  0.18667749  0.05390911 ...  0.21317394  0.10602635
   0.23929708]]

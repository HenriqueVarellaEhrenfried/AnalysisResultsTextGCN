(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50141 val_loss= 0.68226 val_acc= 0.54930 time= 0.37970
Epoch: 0002 train_loss= 0.67329 train_acc= 0.58612 val_loss= 0.66013 val_acc= 0.53099 time= 0.07822
Epoch: 0003 train_loss= 0.60943 train_acc= 0.57971 val_loss= 0.59172 val_acc= 0.71549 time= 0.07608
Epoch: 0004 train_loss= 0.51320 train_acc= 0.84433 val_loss= 0.53739 val_acc= 0.74085 time= 0.07596
Epoch: 0005 train_loss= 0.40632 train_acc= 0.87058 val_loss= 0.51660 val_acc= 0.75915 time= 0.08400
Epoch: 0006 train_loss= 0.32537 train_acc= 0.87402 val_loss= 0.50922 val_acc= 0.76761 time= 0.07400
Epoch: 0007 train_loss= 0.25425 train_acc= 0.89966 val_loss= 0.54277 val_acc= 0.76620 time= 0.07203
Epoch: 0008 train_loss= 0.21083 train_acc= 0.91404 val_loss= 0.60141 val_acc= 0.77324 time= 0.07300
Epoch: 0009 train_loss= 0.18484 train_acc= 0.92169 val_loss= 0.65464 val_acc= 0.77183 time= 0.07212
Epoch: 0010 train_loss= 0.14796 train_acc= 0.94029 val_loss= 0.72351 val_acc= 0.77183 time= 0.08699
Epoch: 0011 train_loss= 0.12292 train_acc= 0.95358 val_loss= 0.80058 val_acc= 0.76479 time= 0.07303
Epoch: 0012 train_loss= 0.10329 train_acc= 0.96046 val_loss= 0.88209 val_acc= 0.75352 time= 0.07201
Early stopping...
Optimization Finished!
Test set results: cost= 0.86761 accuracy= 0.75098 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7434    0.7665    0.7548      1777
           1     0.7590    0.7355    0.7471      1777

    accuracy                         0.7510      3554
   macro avg     0.7512    0.7510    0.7509      3554
weighted avg     0.7512    0.7510    0.7509      3554

Macro average Test Precision, Recall and F1-Score...
(0.7512254715497872, 0.7509848058525606, 0.7509251544854164, None)
Micro average Test Precision, Recall and F1-Score...
(0.7509848058525604, 0.7509848058525604, 0.7509848058525604, None)
embeddings:
18764 7108 3554
[[ 0.02668881 -0.0492044  -0.04637317 ... -0.08559883  0.2531159
   0.13064988]
 [ 0.00906437 -0.05157153  0.17410941 ...  0.27621013  0.02106602
  -0.04259616]
 [ 0.40784726  0.30320594 -0.07161262 ... -0.08889398  0.35593003
   0.23192038]
 ...
 [ 0.45186335  0.35721034 -0.2366357  ... -0.2852041   0.31741115
  -0.01224996]
 [ 0.06929059 -0.05217727  0.19821429 ...  0.19496906  0.03344416
  -0.03817442]
 [ 0.23649266 -0.10916001  0.33568013 ...  0.27754676  0.17767228
   0.1928101 ]]

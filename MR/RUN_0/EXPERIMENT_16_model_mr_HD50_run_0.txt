(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69312 train_acc= 0.50750 val_loss= 0.69132 val_acc= 0.68873 time= 0.29092
Epoch: 0002 train_loss= 0.68994 train_acc= 0.79150 val_loss= 0.68748 val_acc= 0.62958 time= 0.05440
Epoch: 0003 train_loss= 0.68358 train_acc= 0.72085 val_loss= 0.68285 val_acc= 0.63380 time= 0.05700
Epoch: 0004 train_loss= 0.67579 train_acc= 0.73085 val_loss= 0.67718 val_acc= 0.64085 time= 0.05354
Epoch: 0005 train_loss= 0.66615 train_acc= 0.75148 val_loss= 0.67040 val_acc= 0.66761 time= 0.07400
Epoch: 0006 train_loss= 0.65435 train_acc= 0.77993 val_loss= 0.66255 val_acc= 0.68873 time= 0.05300
Epoch: 0007 train_loss= 0.64114 train_acc= 0.80744 val_loss= 0.65367 val_acc= 0.71549 time= 0.05300
Epoch: 0008 train_loss= 0.62593 train_acc= 0.82635 val_loss= 0.64384 val_acc= 0.72535 time= 0.05300
Epoch: 0009 train_loss= 0.60930 train_acc= 0.83995 val_loss= 0.63315 val_acc= 0.74085 time= 0.05500
Epoch: 0010 train_loss= 0.59092 train_acc= 0.84980 val_loss= 0.62174 val_acc= 0.74507 time= 0.05301
Epoch: 0011 train_loss= 0.57221 train_acc= 0.86418 val_loss= 0.60976 val_acc= 0.75352 time= 0.05500
Epoch: 0012 train_loss= 0.55106 train_acc= 0.87324 val_loss= 0.59741 val_acc= 0.76056 time= 0.07300
Epoch: 0013 train_loss= 0.52901 train_acc= 0.88309 val_loss= 0.58496 val_acc= 0.76901 time= 0.05208
Epoch: 0014 train_loss= 0.50624 train_acc= 0.88887 val_loss= 0.57266 val_acc= 0.77465 time= 0.05196
Epoch: 0015 train_loss= 0.48271 train_acc= 0.89231 val_loss= 0.56080 val_acc= 0.77606 time= 0.05294
Epoch: 0016 train_loss= 0.46005 train_acc= 0.89778 val_loss= 0.54965 val_acc= 0.77324 time= 0.05300
Epoch: 0017 train_loss= 0.43639 train_acc= 0.89997 val_loss= 0.53939 val_acc= 0.77606 time= 0.05322
Epoch: 0018 train_loss= 0.41458 train_acc= 0.89950 val_loss= 0.53024 val_acc= 0.77606 time= 0.05411
Epoch: 0019 train_loss= 0.39256 train_acc= 0.90388 val_loss= 0.52238 val_acc= 0.77887 time= 0.07204
Epoch: 0020 train_loss= 0.37171 train_acc= 0.90653 val_loss= 0.51594 val_acc= 0.77465 time= 0.05300
Epoch: 0021 train_loss= 0.35148 train_acc= 0.90560 val_loss= 0.51097 val_acc= 0.77465 time= 0.05410
Epoch: 0022 train_loss= 0.33218 train_acc= 0.90950 val_loss= 0.50747 val_acc= 0.77465 time= 0.05342
Epoch: 0023 train_loss= 0.31508 train_acc= 0.90982 val_loss= 0.50548 val_acc= 0.77606 time= 0.05296
Epoch: 0024 train_loss= 0.29894 train_acc= 0.91622 val_loss= 0.50495 val_acc= 0.77746 time= 0.05304
Epoch: 0025 train_loss= 0.28241 train_acc= 0.91279 val_loss= 0.50586 val_acc= 0.77887 time= 0.07201
Epoch: 0026 train_loss= 0.26789 train_acc= 0.92201 val_loss= 0.50808 val_acc= 0.77887 time= 0.05395
Epoch: 0027 train_loss= 0.25505 train_acc= 0.92076 val_loss= 0.51168 val_acc= 0.77887 time= 0.05404
Epoch: 0028 train_loss= 0.24283 train_acc= 0.92763 val_loss= 0.51650 val_acc= 0.78028 time= 0.05301
Early stopping...
Optimization Finished!
Test set results: cost= 0.51513 accuracy= 0.75914 time= 0.02512
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7443    0.7895    0.7662      1777
           1     0.7759    0.7288    0.7516      1777

    accuracy                         0.7591      3554
   macro avg     0.7601    0.7591    0.7589      3554
weighted avg     0.7601    0.7591    0.7589      3554

Macro average Test Precision, Recall and F1-Score...
(0.7601054015095048, 0.759144625773776, 0.758922002667036, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[ 0.1947746  -0.01091714  0.15458435 ... -0.00334193  0.00801919
   0.19176556]
 [ 0.00330941  0.21336132  0.02906497 ...  0.28785315  0.25185084
   0.03049189]
 [ 0.31833106 -0.08771324  0.28622758 ... -0.07834835 -0.08885051
   0.31274837]
 ...
 [ 0.30570555 -0.00474693 -0.00726582 ... -0.00437245 -0.10246287
   0.27397087]
 [ 0.03438299  0.26995122  0.05482917 ...  0.29542592  0.26888222
   0.07523868]
 [ 0.21301612  0.4779445   0.18557408 ...  0.5097703   0.4992432
   0.16715454]]

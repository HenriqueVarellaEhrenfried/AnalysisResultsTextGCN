(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49656 val_loss= 0.69190 val_acc= 0.76338 time= 0.37303
Epoch: 0002 train_loss= 0.69074 train_acc= 0.86933 val_loss= 0.68903 val_acc= 0.76197 time= 0.07800
Epoch: 0003 train_loss= 0.68605 train_acc= 0.86043 val_loss= 0.68508 val_acc= 0.77324 time= 0.08997
Epoch: 0004 train_loss= 0.67950 train_acc= 0.86511 val_loss= 0.68013 val_acc= 0.77465 time= 0.07404
Epoch: 0005 train_loss= 0.67112 train_acc= 0.87137 val_loss= 0.67411 val_acc= 0.77324 time= 0.07200
Epoch: 0006 train_loss= 0.66113 train_acc= 0.87621 val_loss= 0.66699 val_acc= 0.77183 time= 0.07252
Epoch: 0007 train_loss= 0.64920 train_acc= 0.87793 val_loss= 0.65881 val_acc= 0.76901 time= 0.08504
Epoch: 0008 train_loss= 0.63560 train_acc= 0.87996 val_loss= 0.64965 val_acc= 0.76901 time= 0.07300
Epoch: 0009 train_loss= 0.61975 train_acc= 0.88184 val_loss= 0.63957 val_acc= 0.77042 time= 0.07400
Epoch: 0010 train_loss= 0.60273 train_acc= 0.88496 val_loss= 0.62871 val_acc= 0.77042 time= 0.07497
Epoch: 0011 train_loss= 0.58439 train_acc= 0.88496 val_loss= 0.61720 val_acc= 0.77606 time= 0.07103
Epoch: 0012 train_loss= 0.56443 train_acc= 0.88356 val_loss= 0.60522 val_acc= 0.77887 time= 0.07344
Epoch: 0013 train_loss= 0.54345 train_acc= 0.88559 val_loss= 0.59297 val_acc= 0.77887 time= 0.07499
Epoch: 0014 train_loss= 0.52171 train_acc= 0.89122 val_loss= 0.58069 val_acc= 0.77887 time= 0.07397
Epoch: 0015 train_loss= 0.49884 train_acc= 0.89247 val_loss= 0.56858 val_acc= 0.77887 time= 0.08404
Epoch: 0016 train_loss= 0.47646 train_acc= 0.89403 val_loss= 0.55691 val_acc= 0.77887 time= 0.07210
Epoch: 0017 train_loss= 0.45356 train_acc= 0.89747 val_loss= 0.54591 val_acc= 0.77746 time= 0.07200
Epoch: 0018 train_loss= 0.43107 train_acc= 0.89731 val_loss= 0.53578 val_acc= 0.77746 time= 0.07321
Epoch: 0019 train_loss= 0.40891 train_acc= 0.89887 val_loss= 0.52669 val_acc= 0.77606 time= 0.08501
Epoch: 0020 train_loss= 0.38781 train_acc= 0.90153 val_loss= 0.51880 val_acc= 0.77606 time= 0.07199
Epoch: 0021 train_loss= 0.36748 train_acc= 0.90403 val_loss= 0.51222 val_acc= 0.77746 time= 0.07200
Epoch: 0022 train_loss= 0.34790 train_acc= 0.90653 val_loss= 0.50703 val_acc= 0.77887 time= 0.07297
Epoch: 0023 train_loss= 0.32984 train_acc= 0.90919 val_loss= 0.50328 val_acc= 0.78028 time= 0.08503
Epoch: 0024 train_loss= 0.31293 train_acc= 0.91294 val_loss= 0.50097 val_acc= 0.77887 time= 0.07301
Epoch: 0025 train_loss= 0.29647 train_acc= 0.91419 val_loss= 0.50008 val_acc= 0.77887 time= 0.07299
Epoch: 0026 train_loss= 0.28147 train_acc= 0.91763 val_loss= 0.50054 val_acc= 0.78169 time= 0.07299
Epoch: 0027 train_loss= 0.26733 train_acc= 0.91982 val_loss= 0.50226 val_acc= 0.78451 time= 0.07444
Epoch: 0028 train_loss= 0.25338 train_acc= 0.92201 val_loss= 0.50520 val_acc= 0.78592 time= 0.08502
Epoch: 0029 train_loss= 0.24146 train_acc= 0.92591 val_loss= 0.50925 val_acc= 0.78451 time= 0.07206
Early stopping...
Optimization Finished!
Test set results: cost= 0.51329 accuracy= 0.76111 time= 0.03101
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7558    0.7715    0.7636      1777
           1     0.7667    0.7507    0.7586      1777

    accuracy                         0.7611      3554
   macro avg     0.7612    0.7611    0.7611      3554
weighted avg     0.7612    0.7611    0.7611      3554

Macro average Test Precision, Recall and F1-Score...
(0.7612274898934215, 0.7611142374788971, 0.7610883430735876, None)
Micro average Test Precision, Recall and F1-Score...
(0.7611142374788971, 0.7611142374788971, 0.7611142374788971, None)
embeddings:
18764 7108 3554
[[ 0.08641844  0.09286842 -0.00755092 ... -0.00437187  0.10494266
   0.1034126 ]
 [ 0.01781109  0.03195684  0.10457282 ...  0.09633637  0.01781556
   0.03009634]
 [ 0.13837755  0.1540445  -0.0344391  ... -0.03350414  0.16228339
   0.1610844 ]
 ...
 [ 0.16297603  0.1085548  -0.0089194  ... -0.0098331  -0.00880979
   0.15839471]
 [ 0.03430583  0.04290637  0.10504507 ...  0.09915143  0.03979104
   0.03998279]
 [ 0.12130006  0.12744059  0.17851928 ...  0.17695221  0.11990578
   0.12724668]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50641 val_loss= 0.69069 val_acc= 0.74366 time= 0.37188
Epoch: 0002 train_loss= 0.68861 train_acc= 0.84386 val_loss= 0.68408 val_acc= 0.69859 time= 0.07602
Epoch: 0003 train_loss= 0.67754 train_acc= 0.80822 val_loss= 0.67444 val_acc= 0.71690 time= 0.08201
Epoch: 0004 train_loss= 0.66113 train_acc= 0.82635 val_loss= 0.66132 val_acc= 0.73662 time= 0.07799
Epoch: 0005 train_loss= 0.63885 train_acc= 0.84714 val_loss= 0.64476 val_acc= 0.75634 time= 0.07308
Epoch: 0006 train_loss= 0.61065 train_acc= 0.86465 val_loss= 0.62528 val_acc= 0.76338 time= 0.07110
Epoch: 0007 train_loss= 0.57785 train_acc= 0.87668 val_loss= 0.60377 val_acc= 0.76761 time= 0.08058
Epoch: 0008 train_loss= 0.54048 train_acc= 0.88418 val_loss= 0.58140 val_acc= 0.77746 time= 0.07203
Epoch: 0009 train_loss= 0.50001 train_acc= 0.88950 val_loss= 0.55951 val_acc= 0.77465 time= 0.07300
Epoch: 0010 train_loss= 0.46048 train_acc= 0.89465 val_loss= 0.53956 val_acc= 0.77324 time= 0.07610
Epoch: 0011 train_loss= 0.41892 train_acc= 0.89481 val_loss= 0.52274 val_acc= 0.77324 time= 0.07199
Epoch: 0012 train_loss= 0.38051 train_acc= 0.90091 val_loss= 0.51009 val_acc= 0.78028 time= 0.07300
Epoch: 0013 train_loss= 0.34519 train_acc= 0.90341 val_loss= 0.50218 val_acc= 0.77746 time= 0.07996
Epoch: 0014 train_loss= 0.31390 train_acc= 0.90591 val_loss= 0.49919 val_acc= 0.77746 time= 0.07724
Epoch: 0015 train_loss= 0.28594 train_acc= 0.90982 val_loss= 0.50104 val_acc= 0.77887 time= 0.07295
Epoch: 0016 train_loss= 0.25941 train_acc= 0.91482 val_loss= 0.50735 val_acc= 0.78451 time= 0.08004
Epoch: 0017 train_loss= 0.23687 train_acc= 0.91904 val_loss= 0.51765 val_acc= 0.78592 time= 0.07400
Epoch: 0018 train_loss= 0.21792 train_acc= 0.92779 val_loss= 0.53139 val_acc= 0.78451 time= 0.07799
Epoch: 0019 train_loss= 0.20008 train_acc= 0.93201 val_loss= 0.54841 val_acc= 0.77606 time= 0.07101
Epoch: 0020 train_loss= 0.18329 train_acc= 0.94029 val_loss= 0.56735 val_acc= 0.77606 time= 0.07395
Epoch: 0021 train_loss= 0.16853 train_acc= 0.94373 val_loss= 0.58782 val_acc= 0.77746 time= 0.07905
Epoch: 0022 train_loss= 0.15588 train_acc= 0.94827 val_loss= 0.60991 val_acc= 0.77183 time= 0.07299
Epoch: 0023 train_loss= 0.14409 train_acc= 0.95295 val_loss= 0.63280 val_acc= 0.77746 time= 0.07400
Epoch: 0024 train_loss= 0.13346 train_acc= 0.95608 val_loss= 0.65702 val_acc= 0.77183 time= 0.07600
Epoch: 0025 train_loss= 0.12252 train_acc= 0.95999 val_loss= 0.68232 val_acc= 0.76620 time= 0.07276
Epoch: 0026 train_loss= 0.11400 train_acc= 0.96296 val_loss= 0.71003 val_acc= 0.75775 time= 0.08000
Epoch: 0027 train_loss= 0.10429 train_acc= 0.96874 val_loss= 0.73748 val_acc= 0.75634 time= 0.07200
Epoch: 0028 train_loss= 0.09769 train_acc= 0.97140 val_loss= 0.76306 val_acc= 0.75211 time= 0.07600
Epoch: 0029 train_loss= 0.08995 train_acc= 0.97546 val_loss= 0.78781 val_acc= 0.74648 time= 0.08101
Epoch: 0030 train_loss= 0.08279 train_acc= 0.97734 val_loss= 0.81397 val_acc= 0.74225 time= 0.07117
Epoch: 0031 train_loss= 0.07804 train_acc= 0.98062 val_loss= 0.84029 val_acc= 0.73803 time= 0.07200
Epoch: 0032 train_loss= 0.07173 train_acc= 0.98265 val_loss= 0.86687 val_acc= 0.73662 time= 0.07211
Epoch: 0033 train_loss= 0.06653 train_acc= 0.98531 val_loss= 0.89371 val_acc= 0.73099 time= 0.07302
Epoch: 0034 train_loss= 0.06179 train_acc= 0.98703 val_loss= 0.91888 val_acc= 0.72817 time= 0.08003
Epoch: 0035 train_loss= 0.05758 train_acc= 0.98828 val_loss= 0.94266 val_acc= 0.73380 time= 0.07201
Epoch: 0036 train_loss= 0.05371 train_acc= 0.98937 val_loss= 0.96641 val_acc= 0.72958 time= 0.07396
Epoch: 0037 train_loss= 0.05017 train_acc= 0.99062 val_loss= 0.99030 val_acc= 0.73239 time= 0.08104
Epoch: 0038 train_loss= 0.04724 train_acc= 0.99187 val_loss= 1.01434 val_acc= 0.73099 time= 0.07222
Epoch: 0039 train_loss= 0.04385 train_acc= 0.99187 val_loss= 1.03750 val_acc= 0.73239 time= 0.07296
Epoch: 0040 train_loss= 0.04137 train_acc= 0.99375 val_loss= 1.05941 val_acc= 0.72676 time= 0.08004
Epoch: 0041 train_loss= 0.03817 train_acc= 0.99453 val_loss= 1.08032 val_acc= 0.72958 time= 0.07222
Epoch: 0042 train_loss= 0.03584 train_acc= 0.99531 val_loss= 1.10121 val_acc= 0.72817 time= 0.07596
Epoch: 0043 train_loss= 0.03366 train_acc= 0.99594 val_loss= 1.12157 val_acc= 0.73099 time= 0.07803
Epoch: 0044 train_loss= 0.03177 train_acc= 0.99562 val_loss= 1.14236 val_acc= 0.73521 time= 0.07700
Epoch: 0045 train_loss= 0.02982 train_acc= 0.99609 val_loss= 1.16377 val_acc= 0.73803 time= 0.07244
Epoch: 0046 train_loss= 0.02851 train_acc= 0.99641 val_loss= 1.18216 val_acc= 0.73944 time= 0.07204
Epoch: 0047 train_loss= 0.02641 train_acc= 0.99750 val_loss= 1.19936 val_acc= 0.73662 time= 0.08015
Epoch: 0048 train_loss= 0.02496 train_acc= 0.99797 val_loss= 1.21632 val_acc= 0.73803 time= 0.07251
Epoch: 0049 train_loss= 0.02402 train_acc= 0.99781 val_loss= 1.23283 val_acc= 0.73803 time= 0.07404
Epoch: 0050 train_loss= 0.02305 train_acc= 0.99781 val_loss= 1.25057 val_acc= 0.73803 time= 0.08103
Epoch: 0051 train_loss= 0.02100 train_acc= 0.99875 val_loss= 1.26989 val_acc= 0.73944 time= 0.07101
Epoch: 0052 train_loss= 0.02010 train_acc= 0.99859 val_loss= 1.28902 val_acc= 0.73662 time= 0.07200
Epoch: 0053 train_loss= 0.01944 train_acc= 0.99891 val_loss= 1.30365 val_acc= 0.73803 time= 0.07700
Epoch: 0054 train_loss= 0.01899 train_acc= 0.99875 val_loss= 1.31456 val_acc= 0.73803 time= 0.07300
Epoch: 0055 train_loss= 0.01726 train_acc= 0.99906 val_loss= 1.32776 val_acc= 0.73803 time= 0.07296
Epoch: 0056 train_loss= 0.01710 train_acc= 0.99891 val_loss= 1.34259 val_acc= 0.73803 time= 0.08203
Epoch: 0057 train_loss= 0.01602 train_acc= 0.99922 val_loss= 1.35866 val_acc= 0.73803 time= 0.07597
Epoch: 0058 train_loss= 0.01533 train_acc= 0.99922 val_loss= 1.37472 val_acc= 0.74085 time= 0.07204
Epoch: 0059 train_loss= 0.01433 train_acc= 0.99875 val_loss= 1.39115 val_acc= 0.73944 time= 0.07199
Epoch: 0060 train_loss= 0.01402 train_acc= 0.99953 val_loss= 1.40551 val_acc= 0.74085 time= 0.07997
Epoch: 0061 train_loss= 0.01319 train_acc= 0.99984 val_loss= 1.41940 val_acc= 0.73944 time= 0.07211
Epoch: 0062 train_loss= 0.01273 train_acc= 0.99953 val_loss= 1.43044 val_acc= 0.74085 time= 0.07301
Epoch: 0063 train_loss= 0.01252 train_acc= 0.99922 val_loss= 1.44041 val_acc= 0.73662 time= 0.08204
Epoch: 0064 train_loss= 0.01186 train_acc= 0.99984 val_loss= 1.45162 val_acc= 0.73662 time= 0.07099
Epoch: 0065 train_loss= 0.01170 train_acc= 0.99984 val_loss= 1.46390 val_acc= 0.73803 time= 0.07304
Epoch: 0066 train_loss= 0.01143 train_acc= 0.99937 val_loss= 1.47771 val_acc= 0.73803 time= 0.07910
Epoch: 0067 train_loss= 0.01060 train_acc= 0.99984 val_loss= 1.49240 val_acc= 0.74225 time= 0.07100
Epoch: 0068 train_loss= 0.01014 train_acc= 0.99953 val_loss= 1.50615 val_acc= 0.73944 time= 0.07196
Epoch: 0069 train_loss= 0.01048 train_acc= 0.99953 val_loss= 1.51556 val_acc= 0.74225 time= 0.08003
Epoch: 0070 train_loss= 0.00997 train_acc= 0.99984 val_loss= 1.52361 val_acc= 0.73944 time= 0.07110
Epoch: 0071 train_loss= 0.00931 train_acc= 1.00000 val_loss= 1.53224 val_acc= 0.73944 time= 0.07799
Epoch: 0072 train_loss= 0.00925 train_acc= 0.99984 val_loss= 1.54161 val_acc= 0.73662 time= 0.07500
Epoch: 0073 train_loss= 0.00862 train_acc= 1.00000 val_loss= 1.55210 val_acc= 0.73662 time= 0.07296
Epoch: 0074 train_loss= 0.00857 train_acc= 0.99969 val_loss= 1.56309 val_acc= 0.73803 time= 0.07940
Epoch: 0075 train_loss= 0.00838 train_acc= 1.00000 val_loss= 1.57505 val_acc= 0.73521 time= 0.07400
Epoch: 0076 train_loss= 0.00815 train_acc= 1.00000 val_loss= 1.58577 val_acc= 0.73521 time= 0.07355
Epoch: 0077 train_loss= 0.00790 train_acc= 1.00000 val_loss= 1.59557 val_acc= 0.73662 time= 0.07903
Epoch: 0078 train_loss= 0.00770 train_acc= 1.00000 val_loss= 1.60471 val_acc= 0.73662 time= 0.07197
Epoch: 0079 train_loss= 0.00758 train_acc= 0.99969 val_loss= 1.61346 val_acc= 0.73521 time= 0.07407
Epoch: 0080 train_loss= 0.00738 train_acc= 0.99984 val_loss= 1.62263 val_acc= 0.73803 time= 0.07900
Epoch: 0081 train_loss= 0.00723 train_acc= 0.99969 val_loss= 1.63150 val_acc= 0.73803 time= 0.07236
Epoch: 0082 train_loss= 0.00681 train_acc= 1.00000 val_loss= 1.64025 val_acc= 0.73803 time= 0.07299
Epoch: 0083 train_loss= 0.00674 train_acc= 1.00000 val_loss= 1.64731 val_acc= 0.73803 time= 0.07940
Epoch: 0084 train_loss= 0.00647 train_acc= 1.00000 val_loss= 1.65450 val_acc= 0.73380 time= 0.07800
Epoch: 0085 train_loss= 0.00644 train_acc= 0.99984 val_loss= 1.66174 val_acc= 0.73380 time= 0.07400
Epoch: 0086 train_loss= 0.00631 train_acc= 0.99984 val_loss= 1.66934 val_acc= 0.73521 time= 0.07500
Epoch: 0087 train_loss= 0.00621 train_acc= 1.00000 val_loss= 1.67711 val_acc= 0.73521 time= 0.07997
Epoch: 0088 train_loss= 0.00610 train_acc= 1.00000 val_loss= 1.68561 val_acc= 0.73521 time= 0.07200
Epoch: 0089 train_loss= 0.00587 train_acc= 1.00000 val_loss= 1.69444 val_acc= 0.73662 time= 0.07403
Epoch: 0090 train_loss= 0.00569 train_acc= 0.99984 val_loss= 1.70338 val_acc= 0.73521 time= 0.07996
Epoch: 0091 train_loss= 0.00575 train_acc= 1.00000 val_loss= 1.71097 val_acc= 0.73662 time= 0.07308
Epoch: 0092 train_loss= 0.00540 train_acc= 1.00000 val_loss= 1.71873 val_acc= 0.73521 time= 0.07207
Epoch: 0093 train_loss= 0.00529 train_acc= 1.00000 val_loss= 1.72507 val_acc= 0.73521 time= 0.08199
Epoch: 0094 train_loss= 0.00521 train_acc= 1.00000 val_loss= 1.73178 val_acc= 0.73521 time= 0.07300
Epoch: 0095 train_loss= 0.00536 train_acc= 1.00000 val_loss= 1.73880 val_acc= 0.73380 time= 0.07300
Epoch: 0096 train_loss= 0.00499 train_acc= 1.00000 val_loss= 1.74512 val_acc= 0.73239 time= 0.07901
Epoch: 0097 train_loss= 0.00486 train_acc= 1.00000 val_loss= 1.75197 val_acc= 0.73239 time= 0.07800
Epoch: 0098 train_loss= 0.00486 train_acc= 1.00000 val_loss= 1.75905 val_acc= 0.73239 time= 0.07200
Epoch: 0099 train_loss= 0.00469 train_acc= 1.00000 val_loss= 1.76601 val_acc= 0.73380 time= 0.07297
Epoch: 0100 train_loss= 0.00467 train_acc= 1.00000 val_loss= 1.77293 val_acc= 0.73099 time= 0.07602
Epoch: 0101 train_loss= 0.00460 train_acc= 1.00000 val_loss= 1.78016 val_acc= 0.73099 time= 0.07309
Epoch: 0102 train_loss= 0.00449 train_acc= 1.00000 val_loss= 1.78769 val_acc= 0.73099 time= 0.07397
Early stopping...
Optimization Finished!
Test set results: cost= 1.89882 accuracy= 0.72454 time= 0.03903
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7157    0.7451    0.7301      1777
           1     0.7342    0.7040    0.7188      1777

    accuracy                         0.7245      3554
   macro avg     0.7249    0.7245    0.7244      3554
weighted avg     0.7249    0.7245    0.7244      3554

Macro average Test Precision, Recall and F1-Score...
(0.7249153026265702, 0.7245357343837929, 0.7244194665958898, None)
Micro average Test Precision, Recall and F1-Score...
(0.7245357343837929, 0.7245357343837929, 0.7245357343837929, None)
embeddings:
18764 7108 3554
[[ 0.19178946  0.02831497  0.18518068 ...  0.1935571   0.19181404
   0.04694195]
 [ 0.1269055   0.13318107  0.10897277 ...  0.12096325  0.0768264
   0.13208757]
 [ 0.32827747 -0.07960698  0.34136868 ...  0.34321412  0.34581667
  -0.07011697]
 ...
 [ 0.22461598 -0.04889929  0.26418406 ...  0.25469115  0.21354604
  -0.00694217]
 [ 0.05853664  0.27137986  0.1046311  ...  0.08168246  0.08844936
   0.27059868]
 [ 0.5093695   0.14348905  0.56065536 ...  0.5026139   0.5621577
   0.09577681]]

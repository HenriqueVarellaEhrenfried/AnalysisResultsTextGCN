(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49969 val_loss= 0.69236 val_acc= 0.73803 time= 0.38198
Epoch: 0002 train_loss= 0.69177 train_acc= 0.80259 val_loss= 0.69078 val_acc= 0.73803 time= 0.07624
Epoch: 0003 train_loss= 0.68928 train_acc= 0.81400 val_loss= 0.68830 val_acc= 0.73662 time= 0.08701
Epoch: 0004 train_loss= 0.68526 train_acc= 0.81479 val_loss= 0.68501 val_acc= 0.73662 time= 0.07602
Epoch: 0005 train_loss= 0.67997 train_acc= 0.81604 val_loss= 0.68093 val_acc= 0.74085 time= 0.07304
Epoch: 0006 train_loss= 0.67334 train_acc= 0.83042 val_loss= 0.67605 val_acc= 0.74366 time= 0.07299
Epoch: 0007 train_loss= 0.66545 train_acc= 0.83198 val_loss= 0.67037 val_acc= 0.74225 time= 0.07401
Epoch: 0008 train_loss= 0.65588 train_acc= 0.83870 val_loss= 0.66387 val_acc= 0.74789 time= 0.07900
Epoch: 0009 train_loss= 0.64527 train_acc= 0.83761 val_loss= 0.65661 val_acc= 0.75352 time= 0.07100
Epoch: 0010 train_loss= 0.63358 train_acc= 0.84605 val_loss= 0.64861 val_acc= 0.75915 time= 0.07200
Epoch: 0011 train_loss= 0.62046 train_acc= 0.85120 val_loss= 0.63994 val_acc= 0.75775 time= 0.07501
Epoch: 0012 train_loss= 0.60625 train_acc= 0.85480 val_loss= 0.63066 val_acc= 0.75915 time= 0.07443
Epoch: 0013 train_loss= 0.59027 train_acc= 0.85792 val_loss= 0.62088 val_acc= 0.76197 time= 0.08602
Epoch: 0014 train_loss= 0.57556 train_acc= 0.86261 val_loss= 0.61072 val_acc= 0.76620 time= 0.07216
Epoch: 0015 train_loss= 0.55790 train_acc= 0.86652 val_loss= 0.60032 val_acc= 0.76479 time= 0.07100
Epoch: 0016 train_loss= 0.53953 train_acc= 0.86715 val_loss= 0.58980 val_acc= 0.76620 time= 0.08501
Epoch: 0017 train_loss= 0.51996 train_acc= 0.87246 val_loss= 0.57931 val_acc= 0.77183 time= 0.07196
Epoch: 0018 train_loss= 0.50367 train_acc= 0.87355 val_loss= 0.56901 val_acc= 0.77042 time= 0.07211
Epoch: 0019 train_loss= 0.48438 train_acc= 0.87371 val_loss= 0.55902 val_acc= 0.76901 time= 0.07197
Epoch: 0020 train_loss= 0.46617 train_acc= 0.87731 val_loss= 0.54949 val_acc= 0.77042 time= 0.07200
Epoch: 0021 train_loss= 0.44670 train_acc= 0.87684 val_loss= 0.54056 val_acc= 0.76761 time= 0.08700
Epoch: 0022 train_loss= 0.43101 train_acc= 0.88028 val_loss= 0.53233 val_acc= 0.77183 time= 0.07203
Epoch: 0023 train_loss= 0.41237 train_acc= 0.88465 val_loss= 0.52489 val_acc= 0.77324 time= 0.07097
Epoch: 0024 train_loss= 0.39640 train_acc= 0.88621 val_loss= 0.51833 val_acc= 0.77606 time= 0.07200
Epoch: 0025 train_loss= 0.38129 train_acc= 0.88278 val_loss= 0.51269 val_acc= 0.77606 time= 0.07200
Epoch: 0026 train_loss= 0.36687 train_acc= 0.89137 val_loss= 0.50798 val_acc= 0.77887 time= 0.09300
Epoch: 0027 train_loss= 0.34966 train_acc= 0.88887 val_loss= 0.50421 val_acc= 0.77887 time= 0.07200
Epoch: 0028 train_loss= 0.33511 train_acc= 0.89559 val_loss= 0.50142 val_acc= 0.78028 time= 0.07103
Epoch: 0029 train_loss= 0.32158 train_acc= 0.90012 val_loss= 0.49955 val_acc= 0.78169 time= 0.07197
Epoch: 0030 train_loss= 0.31170 train_acc= 0.89934 val_loss= 0.49851 val_acc= 0.78169 time= 0.08110
Epoch: 0031 train_loss= 0.29795 train_acc= 0.90325 val_loss= 0.49828 val_acc= 0.78169 time= 0.07200
Epoch: 0032 train_loss= 0.28911 train_acc= 0.90388 val_loss= 0.49881 val_acc= 0.78169 time= 0.07100
Epoch: 0033 train_loss= 0.27792 train_acc= 0.90388 val_loss= 0.50007 val_acc= 0.78169 time= 0.07200
Early stopping...
Optimization Finished!
Test set results: cost= 0.50251 accuracy= 0.76055 time= 0.03200
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7544    0.7727    0.7634      1777
           1     0.7670    0.7485    0.7576      1777

    accuracy                         0.7606      3554
   macro avg     0.7607    0.7606    0.7605      3554
weighted avg     0.7607    0.7606    0.7605      3554

Macro average Test Precision, Recall and F1-Score...
(0.7607041459117583, 0.7605514912774338, 0.7605164340326962, None)
Micro average Test Precision, Recall and F1-Score...
(0.7605514912774338, 0.7605514912774338, 0.7605514912774338, None)
embeddings:
18764 7108 3554
[[-0.01034335  0.07710177  0.00546031 ... -0.00282017  0.00604353
  -0.00466523]
 [ 0.08831218 -0.00212076  0.06883591 ...  0.06156937  0.09556649
   0.11889324]
 [-0.03326198  0.13830155 -0.02619901 ... -0.02082128 -0.03671423
  -0.02707697]
 ...
 [-0.04634504  0.08202794 -0.03060223 ... -0.03144227 -0.0089978
  -0.013194  ]
 [ 0.0860379   0.03709964  0.10657145 ...  0.0806667   0.09768643
   0.10202321]
 [ 0.1677506   0.08744449  0.13745995 ...  0.13981584  0.14460474
   0.18007061]]

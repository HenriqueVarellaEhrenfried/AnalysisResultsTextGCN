(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49828 val_loss= 0.69187 val_acc= 0.76056 time= 0.36921
Epoch: 0002 train_loss= 0.69073 train_acc= 0.86840 val_loss= 0.68900 val_acc= 0.74930 time= 0.07645
Epoch: 0003 train_loss= 0.68602 train_acc= 0.84870 val_loss= 0.68505 val_acc= 0.75070 time= 0.09000
Epoch: 0004 train_loss= 0.67940 train_acc= 0.85324 val_loss= 0.68012 val_acc= 0.75775 time= 0.07406
Epoch: 0005 train_loss= 0.67116 train_acc= 0.86043 val_loss= 0.67411 val_acc= 0.76338 time= 0.07400
Epoch: 0006 train_loss= 0.66104 train_acc= 0.86386 val_loss= 0.66700 val_acc= 0.76338 time= 0.08000
Epoch: 0007 train_loss= 0.64889 train_acc= 0.86777 val_loss= 0.65882 val_acc= 0.76338 time= 0.07299
Epoch: 0008 train_loss= 0.63524 train_acc= 0.86871 val_loss= 0.64966 val_acc= 0.76620 time= 0.07230
Epoch: 0009 train_loss= 0.61968 train_acc= 0.87746 val_loss= 0.63960 val_acc= 0.76338 time= 0.08097
Epoch: 0010 train_loss= 0.60267 train_acc= 0.87934 val_loss= 0.62875 val_acc= 0.76338 time= 0.07307
Epoch: 0011 train_loss= 0.58392 train_acc= 0.88199 val_loss= 0.61724 val_acc= 0.76620 time= 0.07303
Epoch: 0012 train_loss= 0.56379 train_acc= 0.88528 val_loss= 0.60525 val_acc= 0.77183 time= 0.07397
Epoch: 0013 train_loss= 0.54310 train_acc= 0.88856 val_loss= 0.59298 val_acc= 0.77183 time= 0.08256
Epoch: 0014 train_loss= 0.52164 train_acc= 0.88825 val_loss= 0.58066 val_acc= 0.77465 time= 0.07301
Epoch: 0015 train_loss= 0.49866 train_acc= 0.89137 val_loss= 0.56855 val_acc= 0.77606 time= 0.07296
Epoch: 0016 train_loss= 0.47636 train_acc= 0.89590 val_loss= 0.55686 val_acc= 0.77465 time= 0.08004
Epoch: 0017 train_loss= 0.45323 train_acc= 0.89528 val_loss= 0.54584 val_acc= 0.77606 time= 0.07599
Epoch: 0018 train_loss= 0.43098 train_acc= 0.89997 val_loss= 0.53569 val_acc= 0.77746 time= 0.07600
Epoch: 0019 train_loss= 0.40864 train_acc= 0.89794 val_loss= 0.52661 val_acc= 0.77746 time= 0.07915
Epoch: 0020 train_loss= 0.38785 train_acc= 0.90403 val_loss= 0.51877 val_acc= 0.77746 time= 0.07100
Epoch: 0021 train_loss= 0.36732 train_acc= 0.90622 val_loss= 0.51227 val_acc= 0.78169 time= 0.07403
Epoch: 0022 train_loss= 0.34846 train_acc= 0.90794 val_loss= 0.50719 val_acc= 0.78310 time= 0.08001
Epoch: 0023 train_loss= 0.32897 train_acc= 0.90950 val_loss= 0.50355 val_acc= 0.78310 time= 0.07200
Epoch: 0024 train_loss= 0.31265 train_acc= 0.91044 val_loss= 0.50135 val_acc= 0.77746 time= 0.07200
Epoch: 0025 train_loss= 0.29640 train_acc= 0.91622 val_loss= 0.50055 val_acc= 0.78028 time= 0.07500
Epoch: 0026 train_loss= 0.28079 train_acc= 0.91747 val_loss= 0.50111 val_acc= 0.78451 time= 0.08000
Epoch: 0027 train_loss= 0.26618 train_acc= 0.92107 val_loss= 0.50293 val_acc= 0.78310 time= 0.07301
Epoch: 0028 train_loss= 0.25331 train_acc= 0.92357 val_loss= 0.50599 val_acc= 0.78451 time= 0.07308
Epoch: 0029 train_loss= 0.24129 train_acc= 0.92810 val_loss= 0.51009 val_acc= 0.78451 time= 0.07900
Early stopping...
Optimization Finished!
Test set results: cost= 0.51261 accuracy= 0.75971 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7534    0.7721    0.7626      1777
           1     0.7663    0.7473    0.7567      1777

    accuracy                         0.7597      3554
   macro avg     0.7599    0.7597    0.7597      3554
weighted avg     0.7599    0.7597    0.7597      3554

Macro average Test Precision, Recall and F1-Score...
(0.7598666959461536, 0.7597073719752392, 0.7596705355460218, None)
Micro average Test Precision, Recall and F1-Score...
(0.7597073719752392, 0.7597073719752392, 0.7597073719752392, None)
embeddings:
18764 7108 3554
[[-0.00254831 -0.00161764 -0.00356804 ... -0.00517843  0.09622361
  -0.00724445]
 [ 0.13051227  0.10525584  0.09349733 ...  0.12826772  0.00652356
   0.10432508]
 [-0.0423774  -0.03372883 -0.0354629  ... -0.03892654  0.15200216
  -0.03576137]
 ...
 [-0.05362598 -0.04272453 -0.01934163 ... -0.02031628  0.13460197
  -0.03883471]
 [ 0.1314472   0.10959689  0.11489328 ...  0.10734379  0.04456976
   0.10041611]
 [ 0.21761616  0.19223593  0.19444643 ...  0.20128617  0.12468605
   0.16778143]]

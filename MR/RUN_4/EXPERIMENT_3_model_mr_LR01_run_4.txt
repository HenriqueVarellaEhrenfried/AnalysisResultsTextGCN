(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50406 val_loss= 0.68266 val_acc= 0.55915 time= 0.38111
Epoch: 0002 train_loss= 0.67385 train_acc= 0.60706 val_loss= 0.66108 val_acc= 0.52676 time= 0.07608
Epoch: 0003 train_loss= 0.61085 train_acc= 0.57893 val_loss= 0.58948 val_acc= 0.72817 time= 0.07797
Epoch: 0004 train_loss= 0.51151 train_acc= 0.85120 val_loss= 0.53543 val_acc= 0.74085 time= 0.08449
Epoch: 0005 train_loss= 0.40393 train_acc= 0.87090 val_loss= 0.51877 val_acc= 0.75211 time= 0.07301
Epoch: 0006 train_loss= 0.32167 train_acc= 0.87574 val_loss= 0.50962 val_acc= 0.76761 time= 0.07499
Epoch: 0007 train_loss= 0.25143 train_acc= 0.90122 val_loss= 0.54711 val_acc= 0.76620 time= 0.07501
Epoch: 0008 train_loss= 0.21026 train_acc= 0.91325 val_loss= 0.61010 val_acc= 0.76620 time= 0.07300
Epoch: 0009 train_loss= 0.18682 train_acc= 0.91857 val_loss= 0.66080 val_acc= 0.76338 time= 0.07516
Epoch: 0010 train_loss= 0.14828 train_acc= 0.94045 val_loss= 0.72737 val_acc= 0.76338 time= 0.07901
Epoch: 0011 train_loss= 0.12348 train_acc= 0.95327 val_loss= 0.79845 val_acc= 0.76761 time= 0.07311
Epoch: 0012 train_loss= 0.10924 train_acc= 0.95796 val_loss= 0.86788 val_acc= 0.76056 time= 0.07699
Early stopping...
Optimization Finished!
Test set results: cost= 0.85847 accuracy= 0.75183 time= 0.03297
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7436    0.7687    0.7559      1777
           1     0.7606    0.7349    0.7476      1777

    accuracy                         0.7518      3554
   macro avg     0.7521    0.7518    0.7518      3554
weighted avg     0.7521    0.7518    0.7518      3554

Macro average Test Precision, Recall and F1-Score...
(0.7521163528822061, 0.7518289251547552, 0.7517581725205895, None)
Micro average Test Precision, Recall and F1-Score...
(0.7518289251547552, 0.7518289251547552, 0.7518289251547551, None)
embeddings:
18764 7108 3554
[[ 0.18200181 -0.07672002  0.2188794  ... -0.05889828 -0.04789654
   0.15881644]
 [ 0.08992181 -0.18728633 -0.05696255 ...  0.26385388  0.2679916
   0.05238023]
 [ 0.3454279  -0.10769127  0.07374074 ... -0.05965765 -0.1092267
   0.32564047]
 ...
 [ 0.39064282 -0.3797711  -0.18746775 ... -0.23460983 -0.21935326
  -0.00474591]
 [ 0.093748   -0.16684099 -0.02343285 ...  0.12439169  0.198804
   0.01221688]
 [ 0.30844235 -0.34323     0.08102099 ...  0.21437626  0.35049328
   0.22516102]]

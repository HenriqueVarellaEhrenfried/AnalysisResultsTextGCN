(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69313 train_acc= 0.50141 val_loss= 0.69140 val_acc= 0.69577 time= 0.29007
Epoch: 0002 train_loss= 0.69003 train_acc= 0.79681 val_loss= 0.68759 val_acc= 0.64085 time= 0.05203
Epoch: 0003 train_loss= 0.68369 train_acc= 0.72366 val_loss= 0.68290 val_acc= 0.63662 time= 0.07104
Epoch: 0004 train_loss= 0.67546 train_acc= 0.73304 val_loss= 0.67715 val_acc= 0.64507 time= 0.05399
Epoch: 0005 train_loss= 0.66544 train_acc= 0.74680 val_loss= 0.67028 val_acc= 0.66338 time= 0.05300
Epoch: 0006 train_loss= 0.65370 train_acc= 0.77696 val_loss= 0.66234 val_acc= 0.67887 time= 0.06200
Epoch: 0007 train_loss= 0.64030 train_acc= 0.79228 val_loss= 0.65342 val_acc= 0.70563 time= 0.05221
Epoch: 0008 train_loss= 0.62518 train_acc= 0.81041 val_loss= 0.64360 val_acc= 0.72113 time= 0.05199
Epoch: 0009 train_loss= 0.60800 train_acc= 0.83620 val_loss= 0.63297 val_acc= 0.73239 time= 0.05401
Epoch: 0010 train_loss= 0.58937 train_acc= 0.84558 val_loss= 0.62164 val_acc= 0.74225 time= 0.05355
Epoch: 0011 train_loss= 0.56993 train_acc= 0.85589 val_loss= 0.60979 val_acc= 0.74225 time= 0.07500
Epoch: 0012 train_loss= 0.54746 train_acc= 0.86965 val_loss= 0.59761 val_acc= 0.74507 time= 0.05600
Epoch: 0013 train_loss= 0.52692 train_acc= 0.87777 val_loss= 0.58535 val_acc= 0.75634 time= 0.05300
Epoch: 0014 train_loss= 0.50483 train_acc= 0.88637 val_loss= 0.57325 val_acc= 0.76901 time= 0.05301
Epoch: 0015 train_loss= 0.48241 train_acc= 0.89012 val_loss= 0.56161 val_acc= 0.77324 time= 0.05398
Epoch: 0016 train_loss= 0.45828 train_acc= 0.89497 val_loss= 0.55068 val_acc= 0.77183 time= 0.05400
Epoch: 0017 train_loss= 0.43597 train_acc= 0.89794 val_loss= 0.54068 val_acc= 0.77465 time= 0.06801
Epoch: 0018 train_loss= 0.41285 train_acc= 0.90263 val_loss= 0.53181 val_acc= 0.77324 time= 0.05300
Epoch: 0019 train_loss= 0.39227 train_acc= 0.90356 val_loss= 0.52421 val_acc= 0.76901 time= 0.05300
Epoch: 0020 train_loss= 0.37162 train_acc= 0.90231 val_loss= 0.51797 val_acc= 0.76761 time= 0.05300
Epoch: 0021 train_loss= 0.35146 train_acc= 0.90763 val_loss= 0.51316 val_acc= 0.76620 time= 0.05300
Epoch: 0022 train_loss= 0.33312 train_acc= 0.90622 val_loss= 0.50981 val_acc= 0.76901 time= 0.06900
Epoch: 0023 train_loss= 0.31392 train_acc= 0.91060 val_loss= 0.50790 val_acc= 0.77042 time= 0.05700
Epoch: 0024 train_loss= 0.29835 train_acc= 0.91419 val_loss= 0.50749 val_acc= 0.77465 time= 0.05300
Epoch: 0025 train_loss= 0.28347 train_acc= 0.91872 val_loss= 0.50846 val_acc= 0.77887 time= 0.05301
Epoch: 0026 train_loss= 0.26918 train_acc= 0.91935 val_loss= 0.51082 val_acc= 0.77606 time= 0.05300
Epoch: 0027 train_loss= 0.25594 train_acc= 0.92373 val_loss= 0.51461 val_acc= 0.77746 time= 0.05309
Epoch: 0028 train_loss= 0.24299 train_acc= 0.92716 val_loss= 0.51941 val_acc= 0.77746 time= 0.05398
Early stopping...
Optimization Finished!
Test set results: cost= 0.51439 accuracy= 0.75661 time= 0.04001
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7375    0.7968    0.7660      1777
           1     0.7791    0.7164    0.7464      1777

    accuracy                         0.7566      3554
   macro avg     0.7583    0.7566    0.7562      3554
weighted avg     0.7583    0.7566    0.7562      3554

Macro average Test Precision, Recall and F1-Score...
(0.7582848837209303, 0.7566122678671919, 0.7562175928932688, None)
Micro average Test Precision, Recall and F1-Score...
(0.7566122678671919, 0.7566122678671919, 0.7566122678671918, None)
embeddings:
18764 7108 3554
[[ 0.19551432  0.15273461  0.17710218 ...  0.2019645   0.18919443
   0.19882084]
 [-0.01400666 -0.0038793  -0.00067362 ...  0.0040639   0.01565812
  -0.00125517]
 [ 0.3081121   0.27757263  0.3004901  ...  0.30323246  0.29357612
   0.30374172]
 ...
 [ 0.23997656 -0.0076696   0.2829826  ...  0.3630169  -0.00275652
  -0.00471006]
 [ 0.03675766  0.01989642  0.04451455 ...  0.0350928   0.03286131
   0.00982786]
 [ 0.17579983  0.13801594  0.15291288 ...  0.15106916  0.15254812
   0.1434416 ]]

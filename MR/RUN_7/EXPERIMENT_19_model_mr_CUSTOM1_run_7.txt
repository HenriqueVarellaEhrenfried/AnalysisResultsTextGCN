(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49734 val_loss= 0.69237 val_acc= 0.75352 time= 0.36997
Epoch: 0002 train_loss= 0.69173 train_acc= 0.81869 val_loss= 0.69078 val_acc= 0.75775 time= 0.08204
Epoch: 0003 train_loss= 0.68912 train_acc= 0.82792 val_loss= 0.68824 val_acc= 0.75493 time= 0.07500
Epoch: 0004 train_loss= 0.68502 train_acc= 0.83651 val_loss= 0.68485 val_acc= 0.75915 time= 0.07400
Epoch: 0005 train_loss= 0.67955 train_acc= 0.84229 val_loss= 0.68066 val_acc= 0.76197 time= 0.07500
Epoch: 0006 train_loss= 0.67290 train_acc= 0.84261 val_loss= 0.67563 val_acc= 0.76479 time= 0.07301
Epoch: 0007 train_loss= 0.66462 train_acc= 0.84511 val_loss= 0.66979 val_acc= 0.76479 time= 0.07300
Epoch: 0008 train_loss= 0.65544 train_acc= 0.85308 val_loss= 0.66315 val_acc= 0.76197 time= 0.07200
Epoch: 0009 train_loss= 0.64424 train_acc= 0.85339 val_loss= 0.65573 val_acc= 0.76620 time= 0.07199
Epoch: 0010 train_loss= 0.63276 train_acc= 0.85324 val_loss= 0.64758 val_acc= 0.76761 time= 0.07296
Epoch: 0011 train_loss= 0.61872 train_acc= 0.85886 val_loss= 0.63877 val_acc= 0.76901 time= 0.07203
Epoch: 0012 train_loss= 0.60411 train_acc= 0.86027 val_loss= 0.62937 val_acc= 0.76901 time= 0.07401
Epoch: 0013 train_loss= 0.58946 train_acc= 0.86293 val_loss= 0.61950 val_acc= 0.77042 time= 0.07317
Epoch: 0014 train_loss= 0.57220 train_acc= 0.86324 val_loss= 0.60926 val_acc= 0.77324 time= 0.07366
Epoch: 0015 train_loss= 0.55561 train_acc= 0.86683 val_loss= 0.59883 val_acc= 0.77042 time= 0.07495
Epoch: 0016 train_loss= 0.53702 train_acc= 0.86511 val_loss= 0.58831 val_acc= 0.77042 time= 0.08000
Epoch: 0017 train_loss= 0.51872 train_acc= 0.87277 val_loss= 0.57785 val_acc= 0.76901 time= 0.07535
Epoch: 0018 train_loss= 0.49886 train_acc= 0.87355 val_loss= 0.56759 val_acc= 0.76901 time= 0.07399
Epoch: 0019 train_loss= 0.48153 train_acc= 0.87262 val_loss= 0.55768 val_acc= 0.76761 time= 0.07297
Epoch: 0020 train_loss= 0.46421 train_acc= 0.87949 val_loss= 0.54826 val_acc= 0.76901 time= 0.07304
Epoch: 0021 train_loss= 0.44451 train_acc= 0.87496 val_loss= 0.53947 val_acc= 0.77042 time= 0.07499
Epoch: 0022 train_loss= 0.43005 train_acc= 0.87965 val_loss= 0.53138 val_acc= 0.77183 time= 0.07500
Epoch: 0023 train_loss= 0.40951 train_acc= 0.88324 val_loss= 0.52408 val_acc= 0.77324 time= 0.07401
Epoch: 0024 train_loss= 0.39431 train_acc= 0.88762 val_loss= 0.51767 val_acc= 0.77887 time= 0.07203
Epoch: 0025 train_loss= 0.37577 train_acc= 0.89168 val_loss= 0.51216 val_acc= 0.78028 time= 0.07299
Epoch: 0026 train_loss= 0.36226 train_acc= 0.89215 val_loss= 0.50755 val_acc= 0.77606 time= 0.07400
Epoch: 0027 train_loss= 0.34802 train_acc= 0.89231 val_loss= 0.50388 val_acc= 0.77746 time= 0.07300
Epoch: 0028 train_loss= 0.33507 train_acc= 0.89215 val_loss= 0.50115 val_acc= 0.77887 time= 0.07300
Epoch: 0029 train_loss= 0.32209 train_acc= 0.89387 val_loss= 0.49931 val_acc= 0.77746 time= 0.07600
Epoch: 0030 train_loss= 0.30921 train_acc= 0.89669 val_loss= 0.49834 val_acc= 0.78028 time= 0.07598
Epoch: 0031 train_loss= 0.29821 train_acc= 0.89809 val_loss= 0.49819 val_acc= 0.78310 time= 0.07603
Epoch: 0032 train_loss= 0.28776 train_acc= 0.90278 val_loss= 0.49883 val_acc= 0.78028 time= 0.07300
Epoch: 0033 train_loss= 0.27897 train_acc= 0.91013 val_loss= 0.50017 val_acc= 0.78310 time= 0.07400
Early stopping...
Optimization Finished!
Test set results: cost= 0.50369 accuracy= 0.76083 time= 0.03200
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7548    0.7727    0.7636      1777
           1     0.7671    0.7490    0.7580      1777

    accuracy                         0.7608      3554
   macro avg     0.7610    0.7608    0.7608      3554
weighted avg     0.7610    0.7608    0.7608      3554

Macro average Test Precision, Recall and F1-Score...
(0.7609786547062467, 0.7608328643781654, 0.7607994582692489, None)
Micro average Test Precision, Recall and F1-Score...
(0.7608328643781654, 0.7608328643781654, 0.7608328643781654, None)
embeddings:
18764 7108 3554
[[-0.00773362 -0.00275253 -0.00981367 ...  0.08056973 -0.00990199
  -0.00575701]
 [ 0.11307434  0.0933864   0.06573028 ...  0.03906301  0.09386015
   0.1029249 ]
 [-0.03095629 -0.04159004 -0.02741879 ...  0.13398847 -0.03682037
  -0.04048596]
 ...
 [-0.01234096 -0.04526533 -0.02711844 ...  0.08542134 -0.01111291
  -0.03584373]
 [ 0.11350064  0.09306319  0.07611975 ...  0.0271444   0.10943661
   0.10074349]
 [ 0.15435037  0.1391402   0.1468053  ...  0.09145452  0.15929675
   0.18584469]]

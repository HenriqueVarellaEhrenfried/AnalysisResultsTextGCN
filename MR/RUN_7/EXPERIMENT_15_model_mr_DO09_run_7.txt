(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.49594 val_loss= 0.69154 val_acc= 0.75352 time= 0.37597
Epoch: 0002 train_loss= 0.69039 train_acc= 0.81635 val_loss= 0.68740 val_acc= 0.74789 time= 0.08500
Epoch: 0003 train_loss= 0.68386 train_acc= 0.82448 val_loss= 0.68064 val_acc= 0.74648 time= 0.08200
Epoch: 0004 train_loss= 0.67290 train_acc= 0.83339 val_loss= 0.67126 val_acc= 0.75352 time= 0.07200
Epoch: 0005 train_loss= 0.65798 train_acc= 0.83557 val_loss= 0.65926 val_acc= 0.75070 time= 0.07620
Epoch: 0006 train_loss= 0.63840 train_acc= 0.84636 val_loss= 0.64484 val_acc= 0.75493 time= 0.07292
Epoch: 0007 train_loss= 0.61451 train_acc= 0.84464 val_loss= 0.62832 val_acc= 0.76479 time= 0.07600
Epoch: 0008 train_loss= 0.58739 train_acc= 0.85574 val_loss= 0.61029 val_acc= 0.76620 time= 0.07301
Epoch: 0009 train_loss= 0.55691 train_acc= 0.85449 val_loss= 0.59147 val_acc= 0.76901 time= 0.07490
Epoch: 0010 train_loss= 0.52550 train_acc= 0.86261 val_loss= 0.57261 val_acc= 0.76761 time= 0.07399
Epoch: 0011 train_loss= 0.49136 train_acc= 0.86777 val_loss= 0.55450 val_acc= 0.76620 time= 0.07800
Epoch: 0012 train_loss= 0.45942 train_acc= 0.86996 val_loss= 0.53806 val_acc= 0.76620 time= 0.07306
Epoch: 0013 train_loss= 0.42818 train_acc= 0.87246 val_loss= 0.52391 val_acc= 0.76901 time= 0.07608
Epoch: 0014 train_loss= 0.39707 train_acc= 0.87887 val_loss= 0.51248 val_acc= 0.76761 time= 0.07099
Epoch: 0015 train_loss= 0.37100 train_acc= 0.88090 val_loss= 0.50412 val_acc= 0.77183 time= 0.07197
Epoch: 0016 train_loss= 0.34457 train_acc= 0.88653 val_loss= 0.49877 val_acc= 0.77606 time= 0.07807
Epoch: 0017 train_loss= 0.32196 train_acc= 0.88903 val_loss= 0.49661 val_acc= 0.77465 time= 0.07605
Epoch: 0018 train_loss= 0.30052 train_acc= 0.89559 val_loss= 0.49729 val_acc= 0.77465 time= 0.07700
Epoch: 0019 train_loss= 0.28277 train_acc= 0.89747 val_loss= 0.50077 val_acc= 0.77606 time= 0.07200
Epoch: 0020 train_loss= 0.26381 train_acc= 0.90247 val_loss= 0.50622 val_acc= 0.78028 time= 0.07600
Epoch: 0021 train_loss= 0.24864 train_acc= 0.90450 val_loss= 0.51377 val_acc= 0.77887 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51824 accuracy= 0.75886 time= 0.03199
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7519    0.7727    0.7621      1777
           1     0.7662    0.7451    0.7555      1777

    accuracy                         0.7589      3554
   macro avg     0.7591    0.7589    0.7588      3554
weighted avg     0.7591    0.7589    0.7588      3554

Macro average Test Precision, Recall and F1-Score...
(0.759060230822279, 0.7588632526730446, 0.7588174064804927, None)
Micro average Test Precision, Recall and F1-Score...
(0.7588632526730444, 0.7588632526730444, 0.7588632526730444, None)
embeddings:
18764 7108 3554
[[-2.41779238e-02  9.58865806e-02  6.29487075e-03 ...  7.04697371e-02
  -1.22678829e-02  9.13187861e-02]
 [ 1.61852673e-01  3.17968465e-02  4.88170832e-02 ...  4.72297380e-03
   1.02117822e-01  1.52101666e-02]
 [-5.82614094e-02  1.66760474e-01  1.72853366e-01 ...  1.67505249e-01
  -4.73139361e-02  1.27066493e-01]
 ...
 [-7.47807045e-03  1.55554786e-01  2.22493410e-02 ...  6.76331818e-02
  -3.81485443e-05  1.22196987e-01]
 [ 1.02935337e-01  1.19114704e-02  3.60226966e-02 ...  1.37486700e-02
   1.22281946e-01  2.17062868e-02]
 [ 1.62303820e-01  1.03229083e-01  9.39891636e-02 ...  1.05661817e-01
   1.66645318e-01  9.84964743e-02]]

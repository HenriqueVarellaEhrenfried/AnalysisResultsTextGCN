(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50625 val_loss= 0.69141 val_acc= 0.76338 time= 0.37323
Epoch: 0002 train_loss= 0.69020 train_acc= 0.81588 val_loss= 0.68713 val_acc= 0.75493 time= 0.07702
Epoch: 0003 train_loss= 0.68340 train_acc= 0.82244 val_loss= 0.68013 val_acc= 0.75070 time= 0.07758
Epoch: 0004 train_loss= 0.67246 train_acc= 0.83104 val_loss= 0.67052 val_acc= 0.75915 time= 0.07706
Epoch: 0005 train_loss= 0.65647 train_acc= 0.83667 val_loss= 0.65825 val_acc= 0.75775 time= 0.07750
Epoch: 0006 train_loss= 0.63655 train_acc= 0.84542 val_loss= 0.64354 val_acc= 0.76197 time= 0.07900
Epoch: 0007 train_loss= 0.61260 train_acc= 0.84886 val_loss= 0.62670 val_acc= 0.76338 time= 0.07396
Epoch: 0008 train_loss= 0.58507 train_acc= 0.85699 val_loss= 0.60835 val_acc= 0.76761 time= 0.07404
Epoch: 0009 train_loss= 0.55498 train_acc= 0.86230 val_loss= 0.58922 val_acc= 0.76761 time= 0.07500
Epoch: 0010 train_loss= 0.52355 train_acc= 0.86590 val_loss= 0.57019 val_acc= 0.77183 time= 0.07206
Epoch: 0011 train_loss= 0.48970 train_acc= 0.86168 val_loss= 0.55203 val_acc= 0.77183 time= 0.07401
Epoch: 0012 train_loss= 0.45772 train_acc= 0.87183 val_loss= 0.53563 val_acc= 0.77324 time= 0.07200
Epoch: 0013 train_loss= 0.42574 train_acc= 0.87152 val_loss= 0.52160 val_acc= 0.77183 time= 0.07500
Epoch: 0014 train_loss= 0.39673 train_acc= 0.87434 val_loss= 0.51044 val_acc= 0.77465 time= 0.07300
Epoch: 0015 train_loss= 0.36855 train_acc= 0.88418 val_loss= 0.50234 val_acc= 0.77324 time= 0.07503
Epoch: 0016 train_loss= 0.34171 train_acc= 0.88543 val_loss= 0.49742 val_acc= 0.77746 time= 0.07600
Epoch: 0017 train_loss= 0.31922 train_acc= 0.89262 val_loss= 0.49567 val_acc= 0.77465 time= 0.07209
Epoch: 0018 train_loss= 0.30062 train_acc= 0.89575 val_loss= 0.49691 val_acc= 0.77465 time= 0.07500
Epoch: 0019 train_loss= 0.28067 train_acc= 0.89622 val_loss= 0.50017 val_acc= 0.77746 time= 0.07807
Epoch: 0020 train_loss= 0.26274 train_acc= 0.90091 val_loss= 0.50650 val_acc= 0.78169 time= 0.07197
Epoch: 0021 train_loss= 0.24667 train_acc= 0.90919 val_loss= 0.51533 val_acc= 0.78169 time= 0.08034
Early stopping...
Optimization Finished!
Test set results: cost= 0.52135 accuracy= 0.76027 time= 0.03199
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7459    0.7895    0.7671      1777
           1     0.7764    0.7310    0.7530      1777

    accuracy                         0.7603      3554
   macro avg     0.7612    0.7603    0.7601      3554
weighted avg     0.7612    0.7603    0.7601      3554

Macro average Test Precision, Recall and F1-Score...
(0.7611646715368363, 0.7602701181767023, 0.7600646587586469, None)
Micro average Test Precision, Recall and F1-Score...
(0.7602701181767023, 0.7602701181767023, 0.7602701181767023, None)
embeddings:
18764 7108 3554
[[ 0.10799895  0.07428742  0.10515642 ...  0.09912442 -0.00204655
  -0.00868379]
 [-0.0248208   0.00789249 -0.01703792 ...  0.03817051  0.12496348
   0.15341492]
 [ 0.17916057  0.14727582  0.17204568 ...  0.19465223 -0.06009142
  -0.04733507]
 ...
 [ 0.12813297 -0.0115399   0.09437516 ...  0.09322614 -0.00930674
  -0.07346323]
 [ 0.042989    0.04901617  0.01124043 ...  0.05521317  0.10226061
   0.10741567]
 [ 0.11868462  0.10441218  0.13428907 ...  0.09427082  0.19382136
   0.22246756]]

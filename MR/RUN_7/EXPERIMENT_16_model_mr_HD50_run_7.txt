(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49484 val_loss= 0.69176 val_acc= 0.76479 time= 0.30076
Epoch: 0002 train_loss= 0.69071 train_acc= 0.86043 val_loss= 0.68838 val_acc= 0.76479 time= 0.05804
Epoch: 0003 train_loss= 0.68535 train_acc= 0.85746 val_loss= 0.68407 val_acc= 0.77324 time= 0.05800
Epoch: 0004 train_loss= 0.67817 train_acc= 0.86480 val_loss= 0.67869 val_acc= 0.76901 time= 0.05623
Epoch: 0005 train_loss= 0.66950 train_acc= 0.86902 val_loss= 0.67216 val_acc= 0.77042 time= 0.05403
Epoch: 0006 train_loss= 0.65866 train_acc= 0.87090 val_loss= 0.66450 val_acc= 0.77465 time= 0.05450
Epoch: 0007 train_loss= 0.64598 train_acc= 0.87856 val_loss= 0.65576 val_acc= 0.77465 time= 0.05700
Epoch: 0008 train_loss= 0.63097 train_acc= 0.87887 val_loss= 0.64599 val_acc= 0.77183 time= 0.05396
Epoch: 0009 train_loss= 0.61488 train_acc= 0.88121 val_loss= 0.63535 val_acc= 0.77042 time= 0.05700
Epoch: 0010 train_loss= 0.59690 train_acc= 0.88324 val_loss= 0.62394 val_acc= 0.77183 time= 0.05432
Epoch: 0011 train_loss= 0.57759 train_acc= 0.88481 val_loss= 0.61197 val_acc= 0.77042 time= 0.05700
Epoch: 0012 train_loss= 0.55722 train_acc= 0.88606 val_loss= 0.59963 val_acc= 0.77042 time= 0.05400
Epoch: 0013 train_loss= 0.53493 train_acc= 0.88965 val_loss= 0.58714 val_acc= 0.77042 time= 0.05700
Epoch: 0014 train_loss= 0.51218 train_acc= 0.88762 val_loss= 0.57472 val_acc= 0.77183 time= 0.05400
Epoch: 0015 train_loss= 0.49054 train_acc= 0.89043 val_loss= 0.56266 val_acc= 0.76901 time= 0.05701
Epoch: 0016 train_loss= 0.46631 train_acc= 0.89309 val_loss= 0.55118 val_acc= 0.77042 time= 0.05400
Epoch: 0017 train_loss= 0.44422 train_acc= 0.89465 val_loss= 0.54054 val_acc= 0.77183 time= 0.06099
Epoch: 0018 train_loss= 0.41964 train_acc= 0.89434 val_loss= 0.53091 val_acc= 0.77324 time= 0.05624
Epoch: 0019 train_loss= 0.40026 train_acc= 0.89841 val_loss= 0.52248 val_acc= 0.77465 time= 0.05701
Epoch: 0020 train_loss= 0.37739 train_acc= 0.89903 val_loss= 0.51542 val_acc= 0.77606 time= 0.05389
Epoch: 0021 train_loss= 0.35836 train_acc= 0.90309 val_loss= 0.50984 val_acc= 0.77746 time= 0.05700
Epoch: 0022 train_loss= 0.33925 train_acc= 0.90685 val_loss= 0.50575 val_acc= 0.77465 time= 0.05400
Epoch: 0023 train_loss= 0.32033 train_acc= 0.90903 val_loss= 0.50315 val_acc= 0.77465 time= 0.05701
Epoch: 0024 train_loss= 0.30433 train_acc= 0.91153 val_loss= 0.50205 val_acc= 0.77465 time= 0.05499
Epoch: 0025 train_loss= 0.28965 train_acc= 0.91341 val_loss= 0.50236 val_acc= 0.77606 time= 0.05701
Epoch: 0026 train_loss= 0.27576 train_acc= 0.91654 val_loss= 0.50401 val_acc= 0.78028 time= 0.05399
Epoch: 0027 train_loss= 0.26038 train_acc= 0.91732 val_loss= 0.50694 val_acc= 0.78169 time= 0.05707
Epoch: 0028 train_loss= 0.24987 train_acc= 0.92232 val_loss= 0.51098 val_acc= 0.78310 time= 0.05400
Early stopping...
Optimization Finished!
Test set results: cost= 0.51643 accuracy= 0.75914 time= 0.02900
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7548    0.7676    0.7612      1777
           1     0.7636    0.7507    0.7571      1777

    accuracy                         0.7591      3554
   macro avg     0.7592    0.7591    0.7591      3554
weighted avg     0.7592    0.7591    0.7591      3554

Macro average Test Precision, Recall and F1-Score...
(0.7592185069257791, 0.759144625773776, 0.7591274627047186, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[ 0.20361221 -0.00294012  0.19424297 ... -0.01864511 -0.00766823
   0.18816225]
 [ 0.01381843  0.21085787  0.02465815 ...  0.22681597  0.1876347
   0.02455039]
 [ 0.3157924  -0.0859082   0.31457332 ... -0.0955589  -0.09440441
   0.29848713]
 ...
 [-0.01228696 -0.00367905  0.29126316 ... -0.05589669 -0.10263267
   0.36229613]
 [ 0.08369984  0.223267    0.09207492 ...  0.24520686  0.2405278
   0.08585393]
 [ 0.28695083  0.3592586   0.31837377 ...  0.3579394   0.3971449
   0.24922615]]

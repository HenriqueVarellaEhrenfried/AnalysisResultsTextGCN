(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69313 train_acc= 0.50359 val_loss= 0.69187 val_acc= 0.71690 time= 0.37072
Epoch: 0002 train_loss= 0.69072 train_acc= 0.83620 val_loss= 0.68902 val_acc= 0.69577 time= 0.08999
Epoch: 0003 train_loss= 0.68596 train_acc= 0.80588 val_loss= 0.68510 val_acc= 0.68732 time= 0.07603
Epoch: 0004 train_loss= 0.67938 train_acc= 0.80322 val_loss= 0.68021 val_acc= 0.69577 time= 0.07650
Epoch: 0005 train_loss= 0.67088 train_acc= 0.81197 val_loss= 0.67424 val_acc= 0.70282 time= 0.07799
Epoch: 0006 train_loss= 0.66077 train_acc= 0.81963 val_loss= 0.66719 val_acc= 0.71690 time= 0.07500
Epoch: 0007 train_loss= 0.64863 train_acc= 0.83276 val_loss= 0.65910 val_acc= 0.72817 time= 0.07200
Epoch: 0008 train_loss= 0.63491 train_acc= 0.84073 val_loss= 0.65005 val_acc= 0.73944 time= 0.07200
Epoch: 0009 train_loss= 0.61943 train_acc= 0.84886 val_loss= 0.64009 val_acc= 0.74225 time= 0.07300
Epoch: 0010 train_loss= 0.60243 train_acc= 0.85730 val_loss= 0.62935 val_acc= 0.74366 time= 0.08307
Epoch: 0011 train_loss= 0.58396 train_acc= 0.86371 val_loss= 0.61795 val_acc= 0.74648 time= 0.07109
Epoch: 0012 train_loss= 0.56407 train_acc= 0.86949 val_loss= 0.60605 val_acc= 0.75634 time= 0.07134
Epoch: 0013 train_loss= 0.54267 train_acc= 0.87840 val_loss= 0.59386 val_acc= 0.76197 time= 0.07296
Epoch: 0014 train_loss= 0.52118 train_acc= 0.88653 val_loss= 0.58160 val_acc= 0.77042 time= 0.08294
Epoch: 0015 train_loss= 0.49835 train_acc= 0.89043 val_loss= 0.56955 val_acc= 0.77324 time= 0.07799
Epoch: 0016 train_loss= 0.47572 train_acc= 0.89356 val_loss= 0.55793 val_acc= 0.77183 time= 0.07200
Epoch: 0017 train_loss= 0.45274 train_acc= 0.89966 val_loss= 0.54700 val_acc= 0.77324 time= 0.07297
Epoch: 0018 train_loss= 0.43015 train_acc= 0.90028 val_loss= 0.53697 val_acc= 0.77324 time= 0.07803
Epoch: 0019 train_loss= 0.40892 train_acc= 0.90138 val_loss= 0.52806 val_acc= 0.77606 time= 0.08316
Epoch: 0020 train_loss= 0.38733 train_acc= 0.90591 val_loss= 0.52040 val_acc= 0.77606 time= 0.07199
Epoch: 0021 train_loss= 0.36638 train_acc= 0.90778 val_loss= 0.51409 val_acc= 0.78169 time= 0.07299
Epoch: 0022 train_loss= 0.34648 train_acc= 0.90982 val_loss= 0.50920 val_acc= 0.78169 time= 0.07483
Epoch: 0023 train_loss= 0.32865 train_acc= 0.91153 val_loss= 0.50575 val_acc= 0.77887 time= 0.07503
Epoch: 0024 train_loss= 0.31111 train_acc= 0.91529 val_loss= 0.50370 val_acc= 0.78169 time= 0.07200
Epoch: 0025 train_loss= 0.29454 train_acc= 0.91638 val_loss= 0.50306 val_acc= 0.78028 time= 0.07300
Epoch: 0026 train_loss= 0.27915 train_acc= 0.91872 val_loss= 0.50375 val_acc= 0.78028 time= 0.07297
Epoch: 0027 train_loss= 0.26576 train_acc= 0.92091 val_loss= 0.50576 val_acc= 0.78028 time= 0.08300
Epoch: 0028 train_loss= 0.25258 train_acc= 0.92545 val_loss= 0.50901 val_acc= 0.78028 time= 0.07200
Epoch: 0029 train_loss= 0.24012 train_acc= 0.92888 val_loss= 0.51335 val_acc= 0.77887 time= 0.07403
Early stopping...
Optimization Finished!
Test set results: cost= 0.51204 accuracy= 0.75914 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7448    0.7884    0.7660      1777
           1     0.7753    0.7299    0.7519      1777

    accuracy                         0.7591      3554
   macro avg     0.7600    0.7591    0.7589      3554
weighted avg     0.7600    0.7591    0.7589      3554

Macro average Test Precision, Recall and F1-Score...
(0.7600353107950553, 0.759144625773776, 0.7589382017575138, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[-0.00033035  0.09220167 -0.00443376 ...  0.08224026 -0.00355255
  -0.00348319]
 [ 0.14783259  0.0066977   0.11096764 ... -0.00190314  0.12914735
   0.1237756 ]
 [-0.04208387  0.14464812 -0.03326064 ...  0.14783232 -0.04032355
  -0.03834372]
 ...
 [-0.04525538  0.15223773 -0.05134537 ...  0.17815644 -0.04999092
  -0.04445532]
 [ 0.13474469  0.01454094  0.1030041  ...  0.0146382   0.14917076
   0.13228533]
 [ 0.24228969  0.096976    0.20042044 ...  0.08568611  0.25039986
   0.2353438 ]]

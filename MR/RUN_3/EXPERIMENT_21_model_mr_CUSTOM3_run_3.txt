(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49234 val_loss= 0.69151 val_acc= 0.71831 time= 0.37077
Epoch: 0002 train_loss= 0.69031 train_acc= 0.80416 val_loss= 0.68734 val_acc= 0.70704 time= 0.08262
Epoch: 0003 train_loss= 0.68349 train_acc= 0.79447 val_loss= 0.68058 val_acc= 0.70563 time= 0.07500
Epoch: 0004 train_loss= 0.67233 train_acc= 0.79556 val_loss= 0.67122 val_acc= 0.71831 time= 0.07899
Epoch: 0005 train_loss= 0.65701 train_acc= 0.80822 val_loss= 0.65925 val_acc= 0.73944 time= 0.07346
Epoch: 0006 train_loss= 0.63769 train_acc= 0.82729 val_loss= 0.64481 val_acc= 0.73803 time= 0.07800
Epoch: 0007 train_loss= 0.61364 train_acc= 0.83792 val_loss= 0.62823 val_acc= 0.74930 time= 0.08300
Epoch: 0008 train_loss= 0.58573 train_acc= 0.84651 val_loss= 0.61001 val_acc= 0.75634 time= 0.07206
Epoch: 0009 train_loss= 0.55556 train_acc= 0.85558 val_loss= 0.59087 val_acc= 0.77042 time= 0.07403
Epoch: 0010 train_loss= 0.52395 train_acc= 0.86511 val_loss= 0.57182 val_acc= 0.77606 time= 0.07900
Epoch: 0011 train_loss= 0.49187 train_acc= 0.86480 val_loss= 0.55359 val_acc= 0.78028 time= 0.07212
Epoch: 0012 train_loss= 0.45854 train_acc= 0.87090 val_loss= 0.53704 val_acc= 0.77887 time= 0.07301
Epoch: 0013 train_loss= 0.42534 train_acc= 0.87793 val_loss= 0.52279 val_acc= 0.77887 time= 0.07903
Epoch: 0014 train_loss= 0.39793 train_acc= 0.87918 val_loss= 0.51126 val_acc= 0.77606 time= 0.07700
Epoch: 0015 train_loss= 0.36751 train_acc= 0.88356 val_loss= 0.50280 val_acc= 0.77606 time= 0.07100
Epoch: 0016 train_loss= 0.34395 train_acc= 0.88856 val_loss= 0.49767 val_acc= 0.77606 time= 0.07100
Epoch: 0017 train_loss= 0.31853 train_acc= 0.89184 val_loss= 0.49552 val_acc= 0.77606 time= 0.08100
Epoch: 0018 train_loss= 0.29857 train_acc= 0.89278 val_loss= 0.49626 val_acc= 0.77606 time= 0.07109
Epoch: 0019 train_loss= 0.28302 train_acc= 0.89856 val_loss= 0.49997 val_acc= 0.77746 time= 0.07283
Epoch: 0020 train_loss= 0.26334 train_acc= 0.90591 val_loss= 0.50630 val_acc= 0.77746 time= 0.08299
Epoch: 0021 train_loss= 0.24768 train_acc= 0.91107 val_loss= 0.51368 val_acc= 0.77887 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51697 accuracy= 0.75914 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7435    0.7912    0.7666      1777
           1     0.7769    0.7271    0.7512      1777

    accuracy                         0.7591      3554
   macro avg     0.7602    0.7591    0.7589      3554
weighted avg     0.7602    0.7591    0.7589      3554

Macro average Test Precision, Recall and F1-Score...
(0.7602155731504073, 0.7591446257737761, 0.7588965534731557, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[ 0.07114313  0.09225955  0.00183369 ...  0.05865537  0.1038149
  -0.00768662]
 [ 0.00072598  0.00327023  0.07987607 ...  0.00236121 -0.00927596
   0.12629445]
 [ 0.16724503  0.16869767 -0.04242428 ...  0.14397901  0.15566121
  -0.04697537]
 ...
 [ 0.11066899 -0.01347589 -0.01050838 ...  0.12859862  0.08673626
   0.00361736]
 [ 0.02684856  0.00080323  0.12996875 ...  0.02267875  0.04103733
   0.12898347]
 [ 0.05113493  0.08017336  0.16892941 ...  0.08279882  0.09310412
   0.23669048]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49562 val_loss= 0.69142 val_acc= 0.69437 time= 0.36703
Epoch: 0002 train_loss= 0.69019 train_acc= 0.78243 val_loss= 0.68716 val_acc= 0.70282 time= 0.07648
Epoch: 0003 train_loss= 0.68351 train_acc= 0.80072 val_loss= 0.68021 val_acc= 0.71972 time= 0.08103
Epoch: 0004 train_loss= 0.67221 train_acc= 0.80400 val_loss= 0.67058 val_acc= 0.73662 time= 0.07297
Epoch: 0005 train_loss= 0.65642 train_acc= 0.82166 val_loss= 0.65824 val_acc= 0.72958 time= 0.08050
Epoch: 0006 train_loss= 0.63618 train_acc= 0.83323 val_loss= 0.64336 val_acc= 0.75352 time= 0.07200
Epoch: 0007 train_loss= 0.61230 train_acc= 0.84261 val_loss= 0.62635 val_acc= 0.75775 time= 0.08199
Epoch: 0008 train_loss= 0.58485 train_acc= 0.85042 val_loss= 0.60780 val_acc= 0.77042 time= 0.07201
Epoch: 0009 train_loss= 0.55442 train_acc= 0.86136 val_loss= 0.58849 val_acc= 0.77465 time= 0.07200
Epoch: 0010 train_loss= 0.52186 train_acc= 0.86652 val_loss= 0.56925 val_acc= 0.77324 time= 0.08100
Epoch: 0011 train_loss= 0.48915 train_acc= 0.86965 val_loss= 0.55098 val_acc= 0.77183 time= 0.07208
Epoch: 0012 train_loss= 0.45639 train_acc= 0.86996 val_loss= 0.53452 val_acc= 0.77042 time= 0.07201
Epoch: 0013 train_loss= 0.42346 train_acc= 0.87340 val_loss= 0.52046 val_acc= 0.77042 time= 0.08100
Epoch: 0014 train_loss= 0.39368 train_acc= 0.87949 val_loss= 0.50936 val_acc= 0.77465 time= 0.07130
Epoch: 0015 train_loss= 0.36370 train_acc= 0.88512 val_loss= 0.50140 val_acc= 0.77465 time= 0.07900
Epoch: 0016 train_loss= 0.33957 train_acc= 0.88887 val_loss= 0.49655 val_acc= 0.77606 time= 0.07200
Epoch: 0017 train_loss= 0.31535 train_acc= 0.88762 val_loss= 0.49484 val_acc= 0.78169 time= 0.07397
Epoch: 0018 train_loss= 0.29511 train_acc= 0.89419 val_loss= 0.49595 val_acc= 0.78028 time= 0.08211
Epoch: 0019 train_loss= 0.27702 train_acc= 0.90153 val_loss= 0.49948 val_acc= 0.78310 time= 0.07300
Epoch: 0020 train_loss= 0.26143 train_acc= 0.90059 val_loss= 0.50555 val_acc= 0.78310 time= 0.07200
Epoch: 0021 train_loss= 0.24590 train_acc= 0.90935 val_loss= 0.51364 val_acc= 0.78169 time= 0.08116
Early stopping...
Optimization Finished!
Test set results: cost= 0.51703 accuracy= 0.76196 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7582    0.7693    0.7637      1777
           1     0.7658    0.7546    0.7602      1777

    accuracy                         0.7620      3554
   macro avg     0.7620    0.7620    0.7619      3554
weighted avg     0.7620    0.7620    0.7619      3554

Macro average Test Precision, Recall and F1-Score...
(0.7620144482845236, 0.7619583567810917, 0.7619456162353209, None)
Micro average Test Precision, Recall and F1-Score...
(0.7619583567810917, 0.7619583567810917, 0.7619583567810917, None)
embeddings:
18764 7108 3554
[[ 0.03618752  0.0653572  -0.02044372 ...  0.02364567  0.0758383
  -0.01558945]
 [ 0.01001839  0.02099299  0.07536624 ...  0.1298257   0.00288638
   0.11421165]
 [ 0.15977508  0.15324636 -0.02585861 ... -0.0573175   0.14153887
  -0.0458249 ]
 ...
 [ 0.06486847 -0.00244674 -0.00038009 ... -0.04418979 -0.00316007
  -0.00064425]
 [ 0.02711802  0.0283558   0.0844176  ...  0.10822023  0.03948718
   0.11238176]
 [ 0.09516446  0.08132271  0.17278156 ...  0.19764297  0.12095103
   0.22417496]]

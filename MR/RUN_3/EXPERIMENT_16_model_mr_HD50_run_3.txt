(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50047 val_loss= 0.69159 val_acc= 0.74789 time= 0.29675
Epoch: 0002 train_loss= 0.69041 train_acc= 0.85808 val_loss= 0.68798 val_acc= 0.73521 time= 0.05882
Epoch: 0003 train_loss= 0.68466 train_acc= 0.83526 val_loss= 0.68344 val_acc= 0.73239 time= 0.05500
Epoch: 0004 train_loss= 0.67719 train_acc= 0.83182 val_loss= 0.67780 val_acc= 0.73380 time= 0.06600
Epoch: 0005 train_loss= 0.66758 train_acc= 0.84011 val_loss= 0.67095 val_acc= 0.73803 time= 0.05300
Epoch: 0006 train_loss= 0.65605 train_acc= 0.84323 val_loss= 0.66297 val_acc= 0.74225 time= 0.06101
Epoch: 0007 train_loss= 0.64262 train_acc= 0.85683 val_loss= 0.65393 val_acc= 0.74648 time= 0.05301
Epoch: 0008 train_loss= 0.62748 train_acc= 0.85839 val_loss= 0.64393 val_acc= 0.74507 time= 0.05300
Epoch: 0009 train_loss= 0.61054 train_acc= 0.86590 val_loss= 0.63306 val_acc= 0.75352 time= 0.06132
Epoch: 0010 train_loss= 0.59255 train_acc= 0.87027 val_loss= 0.62147 val_acc= 0.75352 time= 0.05304
Epoch: 0011 train_loss= 0.57290 train_acc= 0.87215 val_loss= 0.60932 val_acc= 0.75916 time= 0.05300
Epoch: 0012 train_loss= 0.55071 train_acc= 0.87902 val_loss= 0.59684 val_acc= 0.76479 time= 0.06300
Epoch: 0013 train_loss= 0.52860 train_acc= 0.88418 val_loss= 0.58425 val_acc= 0.76479 time= 0.05201
Epoch: 0014 train_loss= 0.50477 train_acc= 0.88590 val_loss= 0.57181 val_acc= 0.77465 time= 0.05310
Epoch: 0015 train_loss= 0.48272 train_acc= 0.89372 val_loss= 0.55977 val_acc= 0.77465 time= 0.06200
Epoch: 0016 train_loss= 0.45972 train_acc= 0.89356 val_loss= 0.54841 val_acc= 0.77465 time= 0.05300
Epoch: 0017 train_loss= 0.43622 train_acc= 0.89590 val_loss= 0.53797 val_acc= 0.77606 time= 0.05200
Epoch: 0018 train_loss= 0.41292 train_acc= 0.90122 val_loss= 0.52868 val_acc= 0.77887 time= 0.06496
Epoch: 0019 train_loss= 0.39099 train_acc= 0.90544 val_loss= 0.52072 val_acc= 0.77746 time= 0.05305
Epoch: 0020 train_loss= 0.37134 train_acc= 0.90450 val_loss= 0.51422 val_acc= 0.77887 time= 0.05203
Epoch: 0021 train_loss= 0.35177 train_acc= 0.90575 val_loss= 0.50927 val_acc= 0.78028 time= 0.06200
Epoch: 0022 train_loss= 0.33154 train_acc= 0.90778 val_loss= 0.50587 val_acc= 0.78028 time= 0.05361
Epoch: 0023 train_loss= 0.31367 train_acc= 0.91544 val_loss= 0.50396 val_acc= 0.78310 time= 0.05603
Epoch: 0024 train_loss= 0.29762 train_acc= 0.91310 val_loss= 0.50354 val_acc= 0.78169 time= 0.06414
Epoch: 0025 train_loss= 0.28169 train_acc= 0.91747 val_loss= 0.50456 val_acc= 0.78310 time= 0.05300
Epoch: 0026 train_loss= 0.26639 train_acc= 0.92216 val_loss= 0.50699 val_acc= 0.78310 time= 0.05496
Epoch: 0027 train_loss= 0.25409 train_acc= 0.92373 val_loss= 0.51074 val_acc= 0.78028 time= 0.06000
Epoch: 0028 train_loss= 0.24211 train_acc= 0.92451 val_loss= 0.51527 val_acc= 0.78028 time= 0.05304
Early stopping...
Optimization Finished!
Test set results: cost= 0.51528 accuracy= 0.75886 time= 0.02400
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7470    0.7828    0.7645      1777
           1     0.7719    0.7349    0.7530      1777

    accuracy                         0.7589      3554
   macro avg     0.7595    0.7589    0.7587      3554
weighted avg     0.7595    0.7589    0.7587      3554

Macro average Test Precision, Recall and F1-Score...
(0.7594568995944775, 0.7588632526730446, 0.7587252412943251, None)
Micro average Test Precision, Recall and F1-Score...
(0.7588632526730444, 0.7588632526730444, 0.7588632526730444, None)
embeddings:
18764 7108 3554
[[ 3.95940617e-04  1.58868626e-01 -7.29165971e-03 ... -2.58976035e-03
  -6.78880140e-03  1.74114972e-01]
 [ 1.96979210e-01  8.46333615e-03  2.53916681e-01 ...  2.66591251e-01
   2.31828064e-01 -1.09689385e-02]
 [-7.36319423e-02  2.83170819e-01 -7.89399967e-02 ... -7.63058960e-02
  -9.28782150e-02  3.15093338e-01]
 ...
 [-8.56280699e-03  3.11870635e-01 -6.39527366e-02 ... -5.19438880e-03
  -5.12822019e-03 -4.25022421e-03]
 [ 2.74066806e-01  6.09126128e-02  2.48441771e-01 ...  2.73049653e-01
   2.68168151e-01  5.32043613e-02]
 [ 4.67722476e-01  1.68972909e-01  4.76418614e-01 ...  4.87851381e-01
   4.71287519e-01  2.15158820e-01]]

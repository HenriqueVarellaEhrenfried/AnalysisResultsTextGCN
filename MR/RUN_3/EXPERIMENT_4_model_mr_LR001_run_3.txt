(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49437 val_loss= 0.69192 val_acc= 0.74789 time= 0.37501
Epoch: 0002 train_loss= 0.69079 train_acc= 0.86043 val_loss= 0.68915 val_acc= 0.73944 time= 0.07500
Epoch: 0003 train_loss= 0.68620 train_acc= 0.83870 val_loss= 0.68532 val_acc= 0.73944 time= 0.07414
Epoch: 0004 train_loss= 0.67979 train_acc= 0.84730 val_loss= 0.68051 val_acc= 0.74085 time= 0.07228
Epoch: 0005 train_loss= 0.67160 train_acc= 0.85105 val_loss= 0.67462 val_acc= 0.74648 time= 0.09001
Epoch: 0006 train_loss= 0.66159 train_acc= 0.85871 val_loss= 0.66763 val_acc= 0.75352 time= 0.07401
Epoch: 0007 train_loss= 0.64994 train_acc= 0.86277 val_loss= 0.65958 val_acc= 0.75352 time= 0.07200
Epoch: 0008 train_loss= 0.63635 train_acc= 0.86699 val_loss= 0.65052 val_acc= 0.75915 time= 0.07103
Epoch: 0009 train_loss= 0.62084 train_acc= 0.87043 val_loss= 0.64053 val_acc= 0.76197 time= 0.07597
Epoch: 0010 train_loss= 0.60383 train_acc= 0.87496 val_loss= 0.62973 val_acc= 0.76901 time= 0.07404
Epoch: 0011 train_loss= 0.58539 train_acc= 0.87949 val_loss= 0.61826 val_acc= 0.77042 time= 0.07917
Epoch: 0012 train_loss= 0.56544 train_acc= 0.88606 val_loss= 0.60628 val_acc= 0.76761 time= 0.07496
Epoch: 0013 train_loss= 0.54478 train_acc= 0.88278 val_loss= 0.59400 val_acc= 0.76901 time= 0.07114
Epoch: 0014 train_loss= 0.52242 train_acc= 0.88825 val_loss= 0.58163 val_acc= 0.77465 time= 0.07099
Epoch: 0015 train_loss= 0.50035 train_acc= 0.89184 val_loss= 0.56944 val_acc= 0.77465 time= 0.07205
Epoch: 0016 train_loss= 0.47802 train_acc= 0.89262 val_loss= 0.55767 val_acc= 0.77746 time= 0.07105
Epoch: 0017 train_loss= 0.45399 train_acc= 0.89747 val_loss= 0.54657 val_acc= 0.77887 time= 0.08900
Epoch: 0018 train_loss= 0.43182 train_acc= 0.90059 val_loss= 0.53637 val_acc= 0.77606 time= 0.07103
Epoch: 0019 train_loss= 0.40990 train_acc= 0.89919 val_loss= 0.52724 val_acc= 0.77606 time= 0.07207
Epoch: 0020 train_loss= 0.38831 train_acc= 0.90341 val_loss= 0.51932 val_acc= 0.77606 time= 0.08100
Epoch: 0021 train_loss= 0.36794 train_acc= 0.90560 val_loss= 0.51273 val_acc= 0.77887 time= 0.07206
Epoch: 0022 train_loss= 0.34887 train_acc= 0.90778 val_loss= 0.50757 val_acc= 0.78310 time= 0.07200
Epoch: 0023 train_loss= 0.33020 train_acc= 0.90825 val_loss= 0.50386 val_acc= 0.78169 time= 0.07203
Epoch: 0024 train_loss= 0.31305 train_acc= 0.91122 val_loss= 0.50161 val_acc= 0.78451 time= 0.07509
Epoch: 0025 train_loss= 0.29633 train_acc= 0.91497 val_loss= 0.50075 val_acc= 0.78310 time= 0.08109
Epoch: 0026 train_loss= 0.28121 train_acc= 0.91763 val_loss= 0.50126 val_acc= 0.78451 time= 0.07297
Epoch: 0027 train_loss= 0.26702 train_acc= 0.92029 val_loss= 0.50308 val_acc= 0.78451 time= 0.07104
Epoch: 0028 train_loss= 0.25423 train_acc= 0.92263 val_loss= 0.50608 val_acc= 0.78451 time= 0.07189
Epoch: 0029 train_loss= 0.24109 train_acc= 0.92826 val_loss= 0.51009 val_acc= 0.78451 time= 0.07200
Early stopping...
Optimization Finished!
Test set results: cost= 0.51252 accuracy= 0.75914 time= 0.03196
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7499    0.7777    0.7635      1777
           1     0.7691    0.7406    0.7546      1777

    accuracy                         0.7591      3554
   macro avg     0.7595    0.7591    0.7591      3554
weighted avg     0.7595    0.7591    0.7591      3554

Macro average Test Precision, Recall and F1-Score...
(0.7595026024514069, 0.7591446257737761, 0.7590615337827563, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[ 0.10107221 -0.00964738 -0.00116645 ...  0.08218886 -0.00621744
  -0.00841462]
 [ 0.01063046  0.09828915  0.09378808 ...  0.0225556   0.10997642
   0.11069043]
 [ 0.14980261 -0.03773668 -0.03078033 ...  0.13950421 -0.03412149
  -0.04434724]
 ...
 [ 0.12874015 -0.05028426 -0.00133906 ...  0.16983333 -0.0420995
  -0.04800409]
 [ 0.02697256  0.13679299  0.10738742 ...  0.03357666  0.12021466
   0.13704595]
 [ 0.12242398  0.21742834  0.17713152 ...  0.1046878   0.21209702
   0.23699893]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49422 val_loss= 0.69225 val_acc= 0.76338 time= 0.37012
Epoch: 0002 train_loss= 0.69162 train_acc= 0.82229 val_loss= 0.69045 val_acc= 0.74085 time= 0.08929
Epoch: 0003 train_loss= 0.68865 train_acc= 0.82588 val_loss= 0.68765 val_acc= 0.73944 time= 0.07501
Epoch: 0004 train_loss= 0.68437 train_acc= 0.82088 val_loss= 0.68398 val_acc= 0.74225 time= 0.08197
Epoch: 0005 train_loss= 0.67856 train_acc= 0.83010 val_loss= 0.67950 val_acc= 0.73944 time= 0.07086
Epoch: 0006 train_loss= 0.67119 train_acc= 0.82620 val_loss= 0.67420 val_acc= 0.73944 time= 0.07399
Epoch: 0007 train_loss= 0.66287 train_acc= 0.83401 val_loss= 0.66811 val_acc= 0.75211 time= 0.07708
Epoch: 0008 train_loss= 0.65238 train_acc= 0.83464 val_loss= 0.66121 val_acc= 0.74789 time= 0.07304
Epoch: 0009 train_loss= 0.64118 train_acc= 0.84542 val_loss= 0.65356 val_acc= 0.74648 time= 0.07503
Epoch: 0010 train_loss= 0.62833 train_acc= 0.85089 val_loss= 0.64520 val_acc= 0.75211 time= 0.07200
Epoch: 0011 train_loss= 0.61483 train_acc= 0.84948 val_loss= 0.63620 val_acc= 0.75634 time= 0.07800
Epoch: 0012 train_loss= 0.60048 train_acc= 0.85308 val_loss= 0.62665 val_acc= 0.76197 time= 0.07200
Epoch: 0013 train_loss= 0.58360 train_acc= 0.85589 val_loss= 0.61665 val_acc= 0.76197 time= 0.08400
Epoch: 0014 train_loss= 0.56726 train_acc= 0.86418 val_loss= 0.60629 val_acc= 0.76479 time= 0.07497
Epoch: 0015 train_loss= 0.54960 train_acc= 0.86855 val_loss= 0.59576 val_acc= 0.76901 time= 0.07209
Epoch: 0016 train_loss= 0.53286 train_acc= 0.86386 val_loss= 0.58521 val_acc= 0.77324 time= 0.07803
Epoch: 0017 train_loss= 0.51248 train_acc= 0.87230 val_loss= 0.57477 val_acc= 0.76761 time= 0.07287
Epoch: 0018 train_loss= 0.49437 train_acc= 0.87246 val_loss= 0.56461 val_acc= 0.76479 time= 0.08004
Epoch: 0019 train_loss= 0.47698 train_acc= 0.87402 val_loss= 0.55479 val_acc= 0.76761 time= 0.07101
Epoch: 0020 train_loss= 0.45855 train_acc= 0.88059 val_loss= 0.54552 val_acc= 0.77042 time= 0.07800
Epoch: 0021 train_loss= 0.43920 train_acc= 0.88137 val_loss= 0.53690 val_acc= 0.77324 time= 0.07300
Epoch: 0022 train_loss= 0.42115 train_acc= 0.88059 val_loss= 0.52896 val_acc= 0.77183 time= 0.07299
Epoch: 0023 train_loss= 0.40352 train_acc= 0.88793 val_loss= 0.52184 val_acc= 0.77042 time= 0.07702
Epoch: 0024 train_loss= 0.39012 train_acc= 0.88668 val_loss= 0.51561 val_acc= 0.77324 time= 0.07299
Epoch: 0025 train_loss= 0.37160 train_acc= 0.89168 val_loss= 0.51026 val_acc= 0.77887 time= 0.07197
Epoch: 0026 train_loss= 0.35833 train_acc= 0.89247 val_loss= 0.50586 val_acc= 0.77887 time= 0.07202
Epoch: 0027 train_loss= 0.34365 train_acc= 0.88981 val_loss= 0.50240 val_acc= 0.77746 time= 0.08000
Epoch: 0028 train_loss= 0.33263 train_acc= 0.89450 val_loss= 0.49986 val_acc= 0.78169 time= 0.07604
Epoch: 0029 train_loss= 0.31753 train_acc= 0.89825 val_loss= 0.49823 val_acc= 0.78028 time= 0.07400
Epoch: 0030 train_loss= 0.30756 train_acc= 0.90044 val_loss= 0.49738 val_acc= 0.78169 time= 0.07800
Epoch: 0031 train_loss= 0.29532 train_acc= 0.90216 val_loss= 0.49732 val_acc= 0.78028 time= 0.07299
Epoch: 0032 train_loss= 0.28338 train_acc= 0.90810 val_loss= 0.49809 val_acc= 0.78028 time= 0.07801
Epoch: 0033 train_loss= 0.27460 train_acc= 0.90825 val_loss= 0.49962 val_acc= 0.77606 time= 0.07208
Early stopping...
Optimization Finished!
Test set results: cost= 0.50296 accuracy= 0.76308 time= 0.03096
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7528    0.7833    0.7678      1777
           1     0.7742    0.7428    0.7582      1777

    accuracy                         0.7631      3554
   macro avg     0.7635    0.7631    0.7630      3554
weighted avg     0.7635    0.7631    0.7630      3554

Macro average Test Precision, Recall and F1-Score...
(0.7635164605104765, 0.763083849184018, 0.7629865737685546, None)
Micro average Test Precision, Recall and F1-Score...
(0.7630838491840181, 0.7630838491840181, 0.7630838491840181, None)
embeddings:
18764 7108 3554
[[ 0.07241426  0.06289019  0.00244692 ...  0.04694736 -0.00578573
   0.10120007]
 [ 0.00496983  0.03683168  0.09084753 ... -0.00736587  0.10134981
   0.00969846]
 [ 0.1341194   0.13063277 -0.03606461 ...  0.14314371 -0.04330392
   0.12812959]
 ...
 [ 0.05510388  0.0627239  -0.00414614 ...  0.08155314 -0.02649261
  -0.0092873 ]
 [ 0.01792126  0.01402645  0.11148077 ...  0.02427905  0.08759124
   0.04941018]
 [ 0.07554223  0.07113358  0.1611634  ...  0.06524631  0.16149703
   0.08645373]]

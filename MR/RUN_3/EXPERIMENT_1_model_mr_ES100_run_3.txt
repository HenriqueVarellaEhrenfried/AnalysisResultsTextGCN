(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50328 val_loss= 0.69082 val_acc= 0.73380 time= 0.37285
Epoch: 0002 train_loss= 0.68883 train_acc= 0.84089 val_loss= 0.68449 val_acc= 0.67746 time= 0.08201
Epoch: 0003 train_loss= 0.67818 train_acc= 0.78587 val_loss= 0.67522 val_acc= 0.69577 time= 0.07599
Epoch: 0004 train_loss= 0.66247 train_acc= 0.80869 val_loss= 0.66256 val_acc= 0.72535 time= 0.08201
Epoch: 0005 train_loss= 0.64074 train_acc= 0.83901 val_loss= 0.64652 val_acc= 0.74225 time= 0.07604
Epoch: 0006 train_loss= 0.61361 train_acc= 0.85746 val_loss= 0.62754 val_acc= 0.76056 time= 0.07503
Epoch: 0007 train_loss= 0.58102 train_acc= 0.87152 val_loss= 0.60644 val_acc= 0.76761 time= 0.08000
Epoch: 0008 train_loss= 0.54493 train_acc= 0.88262 val_loss= 0.58434 val_acc= 0.77746 time= 0.07208
Epoch: 0009 train_loss= 0.50571 train_acc= 0.88856 val_loss= 0.56255 val_acc= 0.77606 time= 0.08100
Epoch: 0010 train_loss= 0.46549 train_acc= 0.89184 val_loss= 0.54246 val_acc= 0.77042 time= 0.07199
Epoch: 0011 train_loss= 0.42510 train_acc= 0.89528 val_loss= 0.52535 val_acc= 0.77746 time= 0.07197
Epoch: 0012 train_loss= 0.38701 train_acc= 0.89997 val_loss= 0.51222 val_acc= 0.78169 time= 0.08004
Epoch: 0013 train_loss= 0.35039 train_acc= 0.90528 val_loss= 0.50368 val_acc= 0.78169 time= 0.07500
Epoch: 0014 train_loss= 0.31848 train_acc= 0.90794 val_loss= 0.50003 val_acc= 0.78028 time= 0.07199
Epoch: 0015 train_loss= 0.28846 train_acc= 0.91107 val_loss= 0.50115 val_acc= 0.78028 time= 0.07097
Epoch: 0016 train_loss= 0.26323 train_acc= 0.91466 val_loss= 0.50674 val_acc= 0.78169 time= 0.08103
Epoch: 0017 train_loss= 0.24097 train_acc= 0.91951 val_loss= 0.51630 val_acc= 0.78592 time= 0.07101
Epoch: 0018 train_loss= 0.21975 train_acc= 0.92716 val_loss= 0.52936 val_acc= 0.77887 time= 0.07507
Epoch: 0019 train_loss= 0.20277 train_acc= 0.93123 val_loss= 0.54539 val_acc= 0.78169 time= 0.08000
Epoch: 0020 train_loss= 0.18638 train_acc= 0.93842 val_loss= 0.56407 val_acc= 0.77606 time= 0.07705
Epoch: 0021 train_loss= 0.17082 train_acc= 0.94498 val_loss= 0.58467 val_acc= 0.77606 time= 0.07349
Epoch: 0022 train_loss= 0.15773 train_acc= 0.94780 val_loss= 0.60639 val_acc= 0.77465 time= 0.08101
Epoch: 0023 train_loss= 0.14501 train_acc= 0.95202 val_loss= 0.62928 val_acc= 0.77606 time= 0.07212
Epoch: 0024 train_loss= 0.13500 train_acc= 0.95639 val_loss= 0.65332 val_acc= 0.77324 time= 0.07208
Epoch: 0025 train_loss= 0.12330 train_acc= 0.96014 val_loss= 0.67841 val_acc= 0.76338 time= 0.07900
Epoch: 0026 train_loss= 0.11517 train_acc= 0.96218 val_loss= 0.70453 val_acc= 0.75634 time= 0.07197
Epoch: 0027 train_loss= 0.10582 train_acc= 0.96827 val_loss= 0.73025 val_acc= 0.75352 time= 0.07100
Epoch: 0028 train_loss= 0.09799 train_acc= 0.97155 val_loss= 0.75648 val_acc= 0.75211 time= 0.07200
Epoch: 0029 train_loss= 0.08995 train_acc= 0.97405 val_loss= 0.78342 val_acc= 0.74789 time= 0.08140
Epoch: 0030 train_loss= 0.08421 train_acc= 0.97687 val_loss= 0.81097 val_acc= 0.74366 time= 0.07048
Epoch: 0031 train_loss= 0.07860 train_acc= 0.97952 val_loss= 0.83792 val_acc= 0.74085 time= 0.07301
Epoch: 0032 train_loss= 0.07152 train_acc= 0.98218 val_loss= 0.86361 val_acc= 0.74085 time= 0.08100
Epoch: 0033 train_loss= 0.06673 train_acc= 0.98546 val_loss= 0.88970 val_acc= 0.74225 time= 0.07301
Epoch: 0034 train_loss= 0.06198 train_acc= 0.98546 val_loss= 0.91587 val_acc= 0.73662 time= 0.08299
Epoch: 0035 train_loss= 0.05774 train_acc= 0.98828 val_loss= 0.94347 val_acc= 0.73099 time= 0.07422
Epoch: 0036 train_loss= 0.05396 train_acc= 0.98922 val_loss= 0.96923 val_acc= 0.73099 time= 0.07300
Epoch: 0037 train_loss= 0.05131 train_acc= 0.98922 val_loss= 0.99136 val_acc= 0.73099 time= 0.07997
Epoch: 0038 train_loss= 0.04650 train_acc= 0.99203 val_loss= 1.01364 val_acc= 0.73099 time= 0.07203
Epoch: 0039 train_loss= 0.04372 train_acc= 0.99187 val_loss= 1.03650 val_acc= 0.73239 time= 0.07400
Epoch: 0040 train_loss= 0.04094 train_acc= 0.99172 val_loss= 1.05991 val_acc= 0.73099 time= 0.07900
Epoch: 0041 train_loss= 0.03796 train_acc= 0.99375 val_loss= 1.08547 val_acc= 0.73239 time= 0.07219
Epoch: 0042 train_loss= 0.03538 train_acc= 0.99531 val_loss= 1.10827 val_acc= 0.73803 time= 0.07404
Epoch: 0043 train_loss= 0.03407 train_acc= 0.99656 val_loss= 1.12588 val_acc= 0.73380 time= 0.07903
Epoch: 0044 train_loss= 0.03090 train_acc= 0.99609 val_loss= 1.14363 val_acc= 0.72958 time= 0.07197
Epoch: 0045 train_loss= 0.02943 train_acc= 0.99641 val_loss= 1.16295 val_acc= 0.72394 time= 0.07603
Epoch: 0046 train_loss= 0.02801 train_acc= 0.99656 val_loss= 1.18252 val_acc= 0.73099 time= 0.08008
Epoch: 0047 train_loss= 0.02661 train_acc= 0.99672 val_loss= 1.20345 val_acc= 0.73521 time= 0.07105
Epoch: 0048 train_loss= 0.02488 train_acc= 0.99719 val_loss= 1.22793 val_acc= 0.73380 time= 0.08300
Epoch: 0049 train_loss= 0.02362 train_acc= 0.99797 val_loss= 1.24903 val_acc= 0.73521 time= 0.07401
Epoch: 0050 train_loss= 0.02385 train_acc= 0.99797 val_loss= 1.26057 val_acc= 0.73662 time= 0.07161
Epoch: 0051 train_loss= 0.02094 train_acc= 0.99844 val_loss= 1.27328 val_acc= 0.73803 time= 0.08004
Epoch: 0052 train_loss= 0.02009 train_acc= 0.99859 val_loss= 1.28871 val_acc= 0.73521 time= 0.07200
Epoch: 0053 train_loss= 0.01973 train_acc= 0.99891 val_loss= 1.30465 val_acc= 0.73662 time= 0.07301
Epoch: 0054 train_loss= 0.01875 train_acc= 0.99812 val_loss= 1.32252 val_acc= 0.73944 time= 0.07196
Epoch: 0055 train_loss= 0.01738 train_acc= 0.99875 val_loss= 1.34452 val_acc= 0.74085 time= 0.07305
Epoch: 0056 train_loss= 0.01639 train_acc= 0.99922 val_loss= 1.36466 val_acc= 0.73803 time= 0.07901
Epoch: 0057 train_loss= 0.01768 train_acc= 0.99859 val_loss= 1.37448 val_acc= 0.73944 time= 0.07200
Epoch: 0058 train_loss= 0.01570 train_acc= 0.99937 val_loss= 1.38181 val_acc= 0.74085 time= 0.08195
Epoch: 0059 train_loss= 0.01437 train_acc= 0.99922 val_loss= 1.39289 val_acc= 0.73662 time= 0.07104
Epoch: 0060 train_loss= 0.01398 train_acc= 0.99953 val_loss= 1.40610 val_acc= 0.73803 time= 0.07199
Epoch: 0061 train_loss= 0.01354 train_acc= 0.99969 val_loss= 1.41961 val_acc= 0.73662 time= 0.08001
Epoch: 0062 train_loss= 0.01302 train_acc= 0.99953 val_loss= 1.43480 val_acc= 0.73803 time= 0.07200
Epoch: 0063 train_loss= 0.01224 train_acc= 0.99953 val_loss= 1.45122 val_acc= 0.74225 time= 0.07799
Epoch: 0064 train_loss= 0.01192 train_acc= 0.99953 val_loss= 1.46627 val_acc= 0.73944 time= 0.08105
Epoch: 0065 train_loss= 0.01164 train_acc= 1.00000 val_loss= 1.47687 val_acc= 0.74225 time= 0.07201
Epoch: 0066 train_loss= 0.01093 train_acc= 0.99953 val_loss= 1.48591 val_acc= 0.74366 time= 0.07807
Epoch: 0067 train_loss= 0.01079 train_acc= 0.99953 val_loss= 1.49430 val_acc= 0.73803 time= 0.07201
Epoch: 0068 train_loss= 0.01025 train_acc= 0.99984 val_loss= 1.50467 val_acc= 0.73803 time= 0.07301
Epoch: 0069 train_loss= 0.01039 train_acc= 0.99969 val_loss= 1.51594 val_acc= 0.73803 time= 0.07600
Epoch: 0070 train_loss= 0.00955 train_acc= 0.99984 val_loss= 1.52826 val_acc= 0.73944 time= 0.07201
Epoch: 0071 train_loss= 0.00944 train_acc= 0.99984 val_loss= 1.54229 val_acc= 0.74225 time= 0.08299
Epoch: 0072 train_loss= 0.00905 train_acc= 1.00000 val_loss= 1.55485 val_acc= 0.73944 time= 0.07200
Epoch: 0073 train_loss= 0.00881 train_acc= 0.99984 val_loss= 1.56469 val_acc= 0.73944 time= 0.07300
Epoch: 0074 train_loss= 0.00826 train_acc= 1.00000 val_loss= 1.57213 val_acc= 0.73803 time= 0.08101
Epoch: 0075 train_loss= 0.00819 train_acc= 0.99984 val_loss= 1.57875 val_acc= 0.73944 time= 0.07201
Epoch: 0076 train_loss= 0.00802 train_acc= 0.99984 val_loss= 1.58583 val_acc= 0.73803 time= 0.07200
Epoch: 0077 train_loss= 0.00783 train_acc= 1.00000 val_loss= 1.59425 val_acc= 0.73944 time= 0.07699
Epoch: 0078 train_loss= 0.00774 train_acc= 1.00000 val_loss= 1.60358 val_acc= 0.73944 time= 0.07301
Epoch: 0079 train_loss= 0.00729 train_acc= 1.00000 val_loss= 1.61319 val_acc= 0.73803 time= 0.07402
Epoch: 0080 train_loss= 0.00717 train_acc= 0.99969 val_loss= 1.62316 val_acc= 0.73944 time= 0.07806
Epoch: 0081 train_loss= 0.00678 train_acc= 1.00000 val_loss= 1.63240 val_acc= 0.73803 time= 0.07197
Epoch: 0082 train_loss= 0.00698 train_acc= 1.00000 val_loss= 1.64022 val_acc= 0.73803 time= 0.07297
Epoch: 0083 train_loss= 0.00661 train_acc= 1.00000 val_loss= 1.64616 val_acc= 0.73944 time= 0.07910
Epoch: 0084 train_loss= 0.00643 train_acc= 1.00000 val_loss= 1.65207 val_acc= 0.74085 time= 0.07296
Epoch: 0085 train_loss= 0.00661 train_acc= 1.00000 val_loss= 1.66001 val_acc= 0.73944 time= 0.07400
Epoch: 0086 train_loss= 0.00618 train_acc= 0.99984 val_loss= 1.66860 val_acc= 0.73803 time= 0.07984
Epoch: 0087 train_loss= 0.00600 train_acc= 1.00000 val_loss= 1.67756 val_acc= 0.73944 time= 0.07100
Epoch: 0088 train_loss= 0.00611 train_acc= 1.00000 val_loss= 1.68674 val_acc= 0.73944 time= 0.07299
Epoch: 0089 train_loss= 0.00585 train_acc= 1.00000 val_loss= 1.69463 val_acc= 0.73662 time= 0.07900
Epoch: 0090 train_loss= 0.00567 train_acc= 1.00000 val_loss= 1.70158 val_acc= 0.73803 time= 0.07200
Epoch: 0091 train_loss= 0.00547 train_acc= 1.00000 val_loss= 1.70745 val_acc= 0.73662 time= 0.07397
Epoch: 0092 train_loss= 0.00523 train_acc= 1.00000 val_loss= 1.71365 val_acc= 0.73662 time= 0.08003
Epoch: 0093 train_loss= 0.00533 train_acc= 1.00000 val_loss= 1.72025 val_acc= 0.73521 time= 0.07208
Epoch: 0094 train_loss= 0.00529 train_acc= 1.00000 val_loss= 1.72714 val_acc= 0.73521 time= 0.07100
Epoch: 0095 train_loss= 0.00511 train_acc= 1.00000 val_loss= 1.73497 val_acc= 0.73521 time= 0.07300
Epoch: 0096 train_loss= 0.00519 train_acc= 1.00000 val_loss= 1.74431 val_acc= 0.73662 time= 0.07900
Epoch: 0097 train_loss= 0.00492 train_acc= 0.99984 val_loss= 1.75298 val_acc= 0.73521 time= 0.07300
Epoch: 0098 train_loss= 0.00485 train_acc= 1.00000 val_loss= 1.75952 val_acc= 0.73521 time= 0.08100
Epoch: 0099 train_loss= 0.00468 train_acc= 1.00000 val_loss= 1.76507 val_acc= 0.73380 time= 0.07200
Epoch: 0100 train_loss= 0.00468 train_acc= 1.00000 val_loss= 1.77052 val_acc= 0.73662 time= 0.07200
Epoch: 0101 train_loss= 0.00460 train_acc= 1.00000 val_loss= 1.77579 val_acc= 0.73380 time= 0.08101
Epoch: 0102 train_loss= 0.00453 train_acc= 0.99984 val_loss= 1.78163 val_acc= 0.73099 time= 0.07200
Early stopping...
Optimization Finished!
Test set results: cost= 1.89525 accuracy= 0.72397 time= 0.03099
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7199    0.7333    0.7265      1777
           1     0.7282    0.7147    0.7214      1777

    accuracy                         0.7240      3554
   macro avg     0.7241    0.7240    0.7239      3554
weighted avg     0.7241    0.7240    0.7239      3554

Macro average Test Precision, Recall and F1-Score...
(0.7240502559683715, 0.7239729881823298, 0.7239491879025738, None)
Micro average Test Precision, Recall and F1-Score...
(0.7239729881823298, 0.7239729881823298, 0.7239729881823298, None)
embeddings:
18764 7108 3554
[[ 0.17371103  0.1339245  -0.00948182 ...  0.17778884  0.17416953
   0.17904583]
 [ 0.08416619  0.11373625  0.18612456 ...  0.08250623  0.09775636
   0.08610076]
 [ 0.35864037  0.3211158  -0.09421337 ...  0.35849452  0.34775203
   0.35195035]
 ...
 [ 0.2701136  -0.01066784 -0.07378902 ...  0.29163718 -0.00849555
   0.23539682]
 [ 0.0738667   0.08558089  0.2848816  ...  0.08778616  0.10521356
   0.07703025]
 [ 0.4605488   0.5301383   0.18188737 ...  0.606133    0.5617793
   0.44604608]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49390 val_loss= 0.69200 val_acc= 0.76197 time= 0.37987
Epoch: 0002 train_loss= 0.69097 train_acc= 0.86824 val_loss= 0.68940 val_acc= 0.74507 time= 0.08308
Epoch: 0003 train_loss= 0.68667 train_acc= 0.85605 val_loss= 0.68571 val_acc= 0.75352 time= 0.07500
Epoch: 0004 train_loss= 0.68060 train_acc= 0.85792 val_loss= 0.68105 val_acc= 0.76056 time= 0.07400
Epoch: 0005 train_loss= 0.67273 train_acc= 0.86246 val_loss= 0.67530 val_acc= 0.76197 time= 0.07700
Epoch: 0006 train_loss= 0.66316 train_acc= 0.86480 val_loss= 0.66845 val_acc= 0.76338 time= 0.07300
Epoch: 0007 train_loss= 0.65177 train_acc= 0.87480 val_loss= 0.66052 val_acc= 0.76197 time= 0.07497
Epoch: 0008 train_loss= 0.63844 train_acc= 0.87606 val_loss= 0.65158 val_acc= 0.76620 time= 0.07409
Epoch: 0009 train_loss= 0.62344 train_acc= 0.87887 val_loss= 0.64170 val_acc= 0.77042 time= 0.08703
Epoch: 0010 train_loss= 0.60663 train_acc= 0.88324 val_loss= 0.63099 val_acc= 0.77183 time= 0.07401
Epoch: 0011 train_loss= 0.58807 train_acc= 0.88324 val_loss= 0.61958 val_acc= 0.77465 time= 0.07200
Epoch: 0012 train_loss= 0.56855 train_acc= 0.88309 val_loss= 0.60764 val_acc= 0.77746 time= 0.08296
Epoch: 0013 train_loss= 0.54796 train_acc= 0.88606 val_loss= 0.59538 val_acc= 0.77606 time= 0.07226
Epoch: 0014 train_loss= 0.52635 train_acc= 0.88809 val_loss= 0.58302 val_acc= 0.77887 time= 0.07197
Epoch: 0015 train_loss= 0.50314 train_acc= 0.88965 val_loss= 0.57080 val_acc= 0.78028 time= 0.07103
Epoch: 0016 train_loss= 0.48171 train_acc= 0.89294 val_loss= 0.55897 val_acc= 0.78028 time= 0.07554
Epoch: 0017 train_loss= 0.45828 train_acc= 0.89512 val_loss= 0.54775 val_acc= 0.77887 time= 0.07201
Epoch: 0018 train_loss= 0.43520 train_acc= 0.89903 val_loss= 0.53738 val_acc= 0.77887 time= 0.07902
Epoch: 0019 train_loss= 0.41342 train_acc= 0.90169 val_loss= 0.52806 val_acc= 0.77746 time= 0.07300
Epoch: 0020 train_loss= 0.39191 train_acc= 0.90263 val_loss= 0.51997 val_acc= 0.77746 time= 0.07295
Epoch: 0021 train_loss= 0.37073 train_acc= 0.90544 val_loss= 0.51320 val_acc= 0.77606 time= 0.07400
Epoch: 0022 train_loss= 0.35101 train_acc= 0.90810 val_loss= 0.50783 val_acc= 0.77746 time= 0.08204
Epoch: 0023 train_loss= 0.33310 train_acc= 0.90982 val_loss= 0.50390 val_acc= 0.78310 time= 0.07299
Epoch: 0024 train_loss= 0.31515 train_acc= 0.91200 val_loss= 0.50143 val_acc= 0.78169 time= 0.07625
Epoch: 0025 train_loss= 0.29839 train_acc= 0.91575 val_loss= 0.50039 val_acc= 0.77887 time= 0.07334
Epoch: 0026 train_loss= 0.28356 train_acc= 0.91654 val_loss= 0.50074 val_acc= 0.78451 time= 0.08264
Epoch: 0027 train_loss= 0.26929 train_acc= 0.91935 val_loss= 0.50242 val_acc= 0.78592 time= 0.07197
Epoch: 0028 train_loss= 0.25525 train_acc= 0.92294 val_loss= 0.50530 val_acc= 0.78592 time= 0.07203
Epoch: 0029 train_loss= 0.24302 train_acc= 0.92748 val_loss= 0.50923 val_acc= 0.78169 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51269 accuracy= 0.76111 time= 0.03800
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7536    0.7760    0.7646      1777
           1     0.7691    0.7462    0.7575      1777

    accuracy                         0.7611      3554
   macro avg     0.7613    0.7611    0.7611      3554
weighted avg     0.7613    0.7611    0.7611      3554

Macro average Test Precision, Recall and F1-Score...
(0.761346721945406, 0.7611142374788971, 0.7610610996565044, None)
Micro average Test Precision, Recall and F1-Score...
(0.7611142374788971, 0.7611142374788971, 0.7611142374788971, None)
embeddings:
18764 7108 3554
[[ 0.09462475 -0.00711646  0.00164221 ... -0.00158137  0.09812485
   0.10303404]
 [ 0.01834126  0.09609643  0.11349075 ...  0.11463455  0.01820179
   0.00598893]
 [ 0.14992996 -0.03802177 -0.0456888  ... -0.04570518  0.15515663
   0.15201257]
 ...
 [ 0.13353047 -0.04840275 -0.05459105 ... -0.04359444  0.17742771
  -0.00950353]
 [ 0.03770806  0.10765885  0.12194738 ...  0.14111131  0.04341376
   0.04105522]
 [ 0.11374652  0.18534471  0.20903471 ...  0.22188021  0.1270788
   0.11984536]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49625 val_loss= 0.69164 val_acc= 0.70141 time= 0.36327
Epoch: 0002 train_loss= 0.69049 train_acc= 0.78462 val_loss= 0.68784 val_acc= 0.69437 time= 0.07708
Epoch: 0003 train_loss= 0.68426 train_acc= 0.76868 val_loss= 0.68136 val_acc= 0.70704 time= 0.07606
Epoch: 0004 train_loss= 0.67362 train_acc= 0.79337 val_loss= 0.67228 val_acc= 0.72817 time= 0.07403
Epoch: 0005 train_loss= 0.65935 train_acc= 0.81041 val_loss= 0.66055 val_acc= 0.73944 time= 0.07200
Epoch: 0006 train_loss= 0.63991 train_acc= 0.82698 val_loss= 0.64636 val_acc= 0.73944 time= 0.07197
Epoch: 0007 train_loss= 0.61657 train_acc= 0.84589 val_loss= 0.63001 val_acc= 0.75493 time= 0.07415
Epoch: 0008 train_loss= 0.58991 train_acc= 0.84808 val_loss= 0.61195 val_acc= 0.75634 time= 0.07700
Epoch: 0009 train_loss= 0.55960 train_acc= 0.85402 val_loss= 0.59293 val_acc= 0.77042 time= 0.07300
Epoch: 0010 train_loss= 0.52835 train_acc= 0.86261 val_loss= 0.57375 val_acc= 0.77465 time= 0.07200
Epoch: 0011 train_loss= 0.49378 train_acc= 0.86199 val_loss= 0.55534 val_acc= 0.78028 time= 0.07200
Epoch: 0012 train_loss= 0.46364 train_acc= 0.87340 val_loss= 0.53854 val_acc= 0.77746 time= 0.07103
Epoch: 0013 train_loss= 0.42856 train_acc= 0.87746 val_loss= 0.52398 val_acc= 0.77746 time= 0.07400
Epoch: 0014 train_loss= 0.39829 train_acc= 0.88293 val_loss= 0.51220 val_acc= 0.77465 time= 0.07100
Epoch: 0015 train_loss= 0.37271 train_acc= 0.88278 val_loss= 0.50347 val_acc= 0.77746 time= 0.07200
Epoch: 0016 train_loss= 0.34391 train_acc= 0.88762 val_loss= 0.49804 val_acc= 0.77887 time= 0.07200
Epoch: 0017 train_loss= 0.32068 train_acc= 0.88981 val_loss= 0.49584 val_acc= 0.78310 time= 0.07221
Epoch: 0018 train_loss= 0.30026 train_acc= 0.89544 val_loss= 0.49659 val_acc= 0.78451 time= 0.07100
Epoch: 0019 train_loss= 0.28334 train_acc= 0.89731 val_loss= 0.49991 val_acc= 0.78451 time= 0.07300
Epoch: 0020 train_loss= 0.26368 train_acc= 0.90575 val_loss= 0.50554 val_acc= 0.78028 time= 0.07103
Epoch: 0021 train_loss= 0.24608 train_acc= 0.90731 val_loss= 0.51313 val_acc= 0.78451 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51506 accuracy= 0.76393 time= 0.03300
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7554    0.7805    0.7678      1777
           1     0.7730    0.7473    0.7599      1777

    accuracy                         0.7639      3554
   macro avg     0.7642    0.7639    0.7639      3554
weighted avg     0.7642    0.7639    0.7639      3554

Macro average Test Precision, Recall and F1-Score...
(0.7642192370416023, 0.7639279684862127, 0.7638628906028817, None)
Micro average Test Precision, Recall and F1-Score...
(0.7639279684862127, 0.7639279684862127, 0.7639279684862127, None)
embeddings:
18764 7108 3554
[[ 0.01080639  0.07528219  0.03303545 ...  0.01516676  0.06419168
   0.07622315]
 [ 0.10666595  0.0115396   0.08414328 ...  0.1296792  -0.00303814
   0.04225557]
 [-0.04152228  0.14273663 -0.03179908 ... -0.03799566  0.15790063
   0.14708821]
 ...
 [-0.07377342 -0.01076995 -0.0115827  ... -0.09654005  0.16272232
   0.0065165 ]
 [ 0.13098359  0.04499716  0.10904013 ...  0.09950099  0.02440084
   0.0516401 ]
 [ 0.16727476  0.07703107  0.16267449 ...  0.19800678  0.10724218
   0.0803238 ]]

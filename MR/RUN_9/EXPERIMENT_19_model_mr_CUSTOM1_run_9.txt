(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.49390 val_loss= 0.69241 val_acc= 0.71831 time= 0.37018
Epoch: 0002 train_loss= 0.69177 train_acc= 0.80166 val_loss= 0.69086 val_acc= 0.72113 time= 0.07669
Epoch: 0003 train_loss= 0.68925 train_acc= 0.80228 val_loss= 0.68841 val_acc= 0.73099 time= 0.07500
Epoch: 0004 train_loss= 0.68538 train_acc= 0.80838 val_loss= 0.68517 val_acc= 0.74225 time= 0.07500
Epoch: 0005 train_loss= 0.68003 train_acc= 0.81729 val_loss= 0.68114 val_acc= 0.73803 time= 0.07517
Epoch: 0006 train_loss= 0.67353 train_acc= 0.82307 val_loss= 0.67631 val_acc= 0.73662 time= 0.07700
Epoch: 0007 train_loss= 0.66556 train_acc= 0.83573 val_loss= 0.67067 val_acc= 0.73380 time= 0.07200
Epoch: 0008 train_loss= 0.65636 train_acc= 0.84089 val_loss= 0.66424 val_acc= 0.74085 time= 0.07400
Epoch: 0009 train_loss= 0.64599 train_acc= 0.83729 val_loss= 0.65702 val_acc= 0.74930 time= 0.07300
Epoch: 0010 train_loss= 0.63449 train_acc= 0.84433 val_loss= 0.64908 val_acc= 0.75493 time= 0.07400
Epoch: 0011 train_loss= 0.62095 train_acc= 0.85214 val_loss= 0.64045 val_acc= 0.76056 time= 0.07199
Epoch: 0012 train_loss= 0.60655 train_acc= 0.85558 val_loss= 0.63122 val_acc= 0.76338 time= 0.07199
Epoch: 0013 train_loss= 0.59058 train_acc= 0.85792 val_loss= 0.62147 val_acc= 0.76197 time= 0.07200
Epoch: 0014 train_loss= 0.57627 train_acc= 0.86246 val_loss= 0.61131 val_acc= 0.76620 time= 0.07197
Epoch: 0015 train_loss= 0.55852 train_acc= 0.86308 val_loss= 0.60087 val_acc= 0.77183 time= 0.07207
Epoch: 0016 train_loss= 0.54016 train_acc= 0.86980 val_loss= 0.59029 val_acc= 0.77465 time= 0.07221
Epoch: 0017 train_loss= 0.52303 train_acc= 0.87012 val_loss= 0.57972 val_acc= 0.77324 time= 0.07100
Epoch: 0018 train_loss= 0.50255 train_acc= 0.87449 val_loss= 0.56932 val_acc= 0.77042 time= 0.07200
Epoch: 0019 train_loss= 0.48398 train_acc= 0.87949 val_loss= 0.55929 val_acc= 0.77465 time= 0.07401
Epoch: 0020 train_loss= 0.46784 train_acc= 0.87512 val_loss= 0.54971 val_acc= 0.77465 time= 0.07305
Epoch: 0021 train_loss= 0.44566 train_acc= 0.87887 val_loss= 0.54068 val_acc= 0.77606 time= 0.07181
Epoch: 0022 train_loss= 0.43078 train_acc= 0.87746 val_loss= 0.53236 val_acc= 0.77465 time= 0.07299
Epoch: 0023 train_loss= 0.41336 train_acc= 0.88278 val_loss= 0.52482 val_acc= 0.77606 time= 0.07214
Epoch: 0024 train_loss= 0.39637 train_acc= 0.88575 val_loss= 0.51815 val_acc= 0.77746 time= 0.07697
Epoch: 0025 train_loss= 0.38158 train_acc= 0.88934 val_loss= 0.51236 val_acc= 0.77746 time= 0.07508
Epoch: 0026 train_loss= 0.36479 train_acc= 0.89012 val_loss= 0.50746 val_acc= 0.77746 time= 0.07200
Epoch: 0027 train_loss= 0.35038 train_acc= 0.89450 val_loss= 0.50352 val_acc= 0.77887 time= 0.07200
Epoch: 0028 train_loss= 0.33620 train_acc= 0.89528 val_loss= 0.50052 val_acc= 0.77746 time= 0.07200
Epoch: 0029 train_loss= 0.32477 train_acc= 0.89575 val_loss= 0.49851 val_acc= 0.78028 time= 0.07200
Epoch: 0030 train_loss= 0.31094 train_acc= 0.89919 val_loss= 0.49744 val_acc= 0.78169 time= 0.07154
Epoch: 0031 train_loss= 0.30021 train_acc= 0.90169 val_loss= 0.49715 val_acc= 0.78310 time= 0.07300
Epoch: 0032 train_loss= 0.28973 train_acc= 0.90591 val_loss= 0.49738 val_acc= 0.78451 time= 0.07200
Epoch: 0033 train_loss= 0.27861 train_acc= 0.90435 val_loss= 0.49834 val_acc= 0.78310 time= 0.07697
Early stopping...
Optimization Finished!
Test set results: cost= 0.50220 accuracy= 0.76083 time= 0.03203
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7576    0.7670    0.7623      1777
           1     0.7641    0.7546    0.7593      1777

    accuracy                         0.7608      3554
   macro avg     0.7609    0.7608    0.7608      3554
weighted avg     0.7609    0.7608    0.7608      3554

Macro average Test Precision, Recall and F1-Score...
(0.7608728495888029, 0.7608328643781654, 0.7608236994585775, None)
Micro average Test Precision, Recall and F1-Score...
(0.7608328643781654, 0.7608328643781654, 0.7608328643781654, None)
embeddings:
18764 7108 3554
[[ 0.06420188 -0.00059083  0.07212859 ... -0.00946519  0.07479469
  -0.00572794]
 [ 0.00640701  0.06608506  0.0068893  ...  0.09520342  0.03983484
   0.07600981]
 [ 0.13266593 -0.03560719  0.11593271 ... -0.03263126  0.13140391
  -0.04296562]
 ...
 [ 0.11268011 -0.03997767  0.06548863 ... -0.03489232  0.10699373
  -0.00429085]
 [ 0.01585942  0.07574973  0.02472758 ...  0.09026014  0.01600845
   0.10961257]
 [ 0.07572095  0.13750923  0.06943418 ...  0.18406102  0.06324822
   0.14625788]]

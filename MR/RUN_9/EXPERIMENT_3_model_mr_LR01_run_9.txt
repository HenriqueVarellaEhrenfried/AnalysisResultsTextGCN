(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49875 val_loss= 0.68244 val_acc= 0.51408 time= 0.37115
Epoch: 0002 train_loss= 0.67329 train_acc= 0.54548 val_loss= 0.66871 val_acc= 0.51690 time= 0.08597
Epoch: 0003 train_loss= 0.61692 train_acc= 0.55658 val_loss= 0.59827 val_acc= 0.70282 time= 0.07854
Epoch: 0004 train_loss= 0.52545 train_acc= 0.83057 val_loss= 0.54948 val_acc= 0.72394 time= 0.07501
Epoch: 0005 train_loss= 0.42781 train_acc= 0.85073 val_loss= 0.51601 val_acc= 0.75775 time= 0.07823
Epoch: 0006 train_loss= 0.33776 train_acc= 0.87387 val_loss= 0.49790 val_acc= 0.77746 time= 0.07100
Epoch: 0007 train_loss= 0.26261 train_acc= 0.89716 val_loss= 0.55572 val_acc= 0.74930 time= 0.07300
Epoch: 0008 train_loss= 0.23577 train_acc= 0.89950 val_loss= 0.57568 val_acc= 0.78310 time= 0.07299
Epoch: 0009 train_loss= 0.19104 train_acc= 0.91935 val_loss= 0.62861 val_acc= 0.78451 time= 0.08700
Epoch: 0010 train_loss= 0.16124 train_acc= 0.93310 val_loss= 0.71115 val_acc= 0.75211 time= 0.07200
Epoch: 0011 train_loss= 0.14680 train_acc= 0.94092 val_loss= 0.75126 val_acc= 0.76761 time= 0.07200
Epoch: 0012 train_loss= 0.11207 train_acc= 0.95749 val_loss= 0.83015 val_acc= 0.76761 time= 0.07299
Early stopping...
Optimization Finished!
Test set results: cost= 0.81378 accuracy= 0.75070 time= 0.04201
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7628    0.7276    0.7448      1777
           1     0.7396    0.7738    0.7563      1777

    accuracy                         0.7507      3554
   macro avg     0.7512    0.7507    0.7506      3554
weighted avg     0.7512    0.7507    0.7506      3554

Macro average Test Precision, Recall and F1-Score...
(0.7512384144106404, 0.7507034327518289, 0.7505706503830107, None)
Micro average Test Precision, Recall and F1-Score...
(0.7507034327518289, 0.7507034327518289, 0.7507034327518289, None)
embeddings:
18764 7108 3554
[[-0.05185651 -0.08391359 -0.05451895 ...  0.23387936 -0.03419156
   0.21379569]
 [ 0.0696608  -0.12435205  0.11871977 ...  0.08704224  0.20798954
   0.0137284 ]
 [-0.05175721 -0.07816755 -0.08613878 ...  0.33523747 -0.12387051
   0.29999954]
 ...
 [-0.00119068 -0.38298273 -0.01322023 ...  0.603036   -0.412394
   0.34750545]
 [-0.04107475 -0.13940495 -0.05938144 ...  0.05401552  0.19265226
   0.03222438]
 [-0.10303535 -0.31186953 -0.09358133 ...  0.30024174  0.28226745
   0.09915353]]

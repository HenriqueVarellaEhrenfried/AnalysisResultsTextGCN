(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.51016 val_loss= 0.69198 val_acc= 0.75915 time= 0.38002
Epoch: 0002 train_loss= 0.69096 train_acc= 0.85964 val_loss= 0.68937 val_acc= 0.74366 time= 0.08115
Epoch: 0003 train_loss= 0.68667 train_acc= 0.84948 val_loss= 0.68579 val_acc= 0.74648 time= 0.07900
Epoch: 0004 train_loss= 0.68069 train_acc= 0.85120 val_loss= 0.68128 val_acc= 0.74366 time= 0.07365
Epoch: 0005 train_loss= 0.67305 train_acc= 0.85167 val_loss= 0.67575 val_acc= 0.74507 time= 0.07400
Epoch: 0006 train_loss= 0.66378 train_acc= 0.85621 val_loss= 0.66916 val_acc= 0.74648 time= 0.07900
Epoch: 0007 train_loss= 0.65262 train_acc= 0.85792 val_loss= 0.66153 val_acc= 0.75070 time= 0.07200
Epoch: 0008 train_loss= 0.63999 train_acc= 0.86261 val_loss= 0.65292 val_acc= 0.74930 time= 0.07305
Epoch: 0009 train_loss= 0.62514 train_acc= 0.86246 val_loss= 0.64338 val_acc= 0.75211 time= 0.07913
Epoch: 0010 train_loss= 0.60852 train_acc= 0.86996 val_loss= 0.63301 val_acc= 0.75493 time= 0.07208
Epoch: 0011 train_loss= 0.59087 train_acc= 0.87496 val_loss= 0.62193 val_acc= 0.75493 time= 0.07296
Epoch: 0012 train_loss= 0.57159 train_acc= 0.87871 val_loss= 0.61030 val_acc= 0.76197 time= 0.08100
Epoch: 0013 train_loss= 0.55148 train_acc= 0.88153 val_loss= 0.59827 val_acc= 0.76761 time= 0.07304
Epoch: 0014 train_loss= 0.52996 train_acc= 0.88575 val_loss= 0.58608 val_acc= 0.76761 time= 0.07396
Epoch: 0015 train_loss= 0.50825 train_acc= 0.89090 val_loss= 0.57397 val_acc= 0.77042 time= 0.07912
Epoch: 0016 train_loss= 0.48504 train_acc= 0.89137 val_loss= 0.56216 val_acc= 0.77324 time= 0.07300
Epoch: 0017 train_loss= 0.46259 train_acc= 0.89512 val_loss= 0.55090 val_acc= 0.77465 time= 0.07289
Epoch: 0018 train_loss= 0.44029 train_acc= 0.89825 val_loss= 0.54045 val_acc= 0.77042 time= 0.07865
Epoch: 0019 train_loss= 0.41831 train_acc= 0.89887 val_loss= 0.53099 val_acc= 0.77465 time= 0.07200
Epoch: 0020 train_loss= 0.39522 train_acc= 0.90481 val_loss= 0.52270 val_acc= 0.78028 time= 0.07299
Epoch: 0021 train_loss= 0.37483 train_acc= 0.90778 val_loss= 0.51572 val_acc= 0.77887 time= 0.07801
Epoch: 0022 train_loss= 0.35467 train_acc= 0.90857 val_loss= 0.51014 val_acc= 0.78028 time= 0.07300
Epoch: 0023 train_loss= 0.33644 train_acc= 0.90841 val_loss= 0.50601 val_acc= 0.77887 time= 0.07299
Epoch: 0024 train_loss= 0.31863 train_acc= 0.91185 val_loss= 0.50335 val_acc= 0.78028 time= 0.08101
Epoch: 0025 train_loss= 0.30112 train_acc= 0.91404 val_loss= 0.50211 val_acc= 0.78451 time= 0.07101
Epoch: 0026 train_loss= 0.28608 train_acc= 0.91857 val_loss= 0.50225 val_acc= 0.78310 time= 0.07125
Epoch: 0027 train_loss= 0.27131 train_acc= 0.91935 val_loss= 0.50368 val_acc= 0.78451 time= 0.07900
Epoch: 0028 train_loss= 0.25666 train_acc= 0.92576 val_loss= 0.50637 val_acc= 0.78310 time= 0.07108
Epoch: 0029 train_loss= 0.24483 train_acc= 0.92701 val_loss= 0.51017 val_acc= 0.78028 time= 0.07193
Epoch: 0030 train_loss= 0.23267 train_acc= 0.93060 val_loss= 0.51492 val_acc= 0.78028 time= 0.08200
Early stopping...
Optimization Finished!
Test set results: cost= 0.51482 accuracy= 0.75802 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7469    0.7805    0.7633      1777
           1     0.7702    0.7355    0.7524      1777

    accuracy                         0.7580      3554
   macro avg     0.7585    0.7580    0.7579      3554
weighted avg     0.7585    0.7580    0.7579      3554

Macro average Test Precision, Recall and F1-Score...
(0.7585431416396067, 0.7580191333708497, 0.7578964611395795, None)
Micro average Test Precision, Recall and F1-Score...
(0.7580191333708497, 0.7580191333708497, 0.7580191333708496, None)
embeddings:
18764 7108 3554
[[-0.00657544 -0.00557937 -0.00623859 ...  0.10071494  0.09546964
  -0.00473203]
 [ 0.10121794  0.10949758  0.12896726 ...  0.00227626  0.00331016
   0.1256919 ]
 [-0.03057748 -0.02984973 -0.04048302 ...  0.15926524  0.15064108
  -0.03524691]
 ...
 [-0.00827094 -0.05865482 -0.05078802 ...  0.11253189 -0.01119002
  -0.03683468]
 [ 0.10790808  0.11773239  0.13141339 ...  0.02831655  0.04135178
   0.13729483]
 [ 0.2134774   0.19533354  0.23264161 ...  0.10399386  0.10927522
   0.23476624]]

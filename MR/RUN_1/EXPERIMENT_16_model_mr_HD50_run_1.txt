(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49375 val_loss= 0.69180 val_acc= 0.75493 time= 0.28800
Epoch: 0002 train_loss= 0.69070 train_acc= 0.86089 val_loss= 0.68844 val_acc= 0.74507 time= 0.06100
Epoch: 0003 train_loss= 0.68527 train_acc= 0.85339 val_loss= 0.68406 val_acc= 0.76901 time= 0.07204
Epoch: 0004 train_loss= 0.67817 train_acc= 0.86339 val_loss= 0.67856 val_acc= 0.77324 time= 0.05299
Epoch: 0005 train_loss= 0.66900 train_acc= 0.86746 val_loss= 0.67186 val_acc= 0.77465 time= 0.05397
Epoch: 0006 train_loss= 0.65785 train_acc= 0.87418 val_loss= 0.66399 val_acc= 0.77183 time= 0.05303
Epoch: 0007 train_loss= 0.64490 train_acc= 0.87340 val_loss= 0.65504 val_acc= 0.77183 time= 0.05500
Epoch: 0008 train_loss= 0.62982 train_acc= 0.88106 val_loss= 0.64507 val_acc= 0.76901 time= 0.06900
Epoch: 0009 train_loss= 0.61323 train_acc= 0.87762 val_loss= 0.63422 val_acc= 0.77042 time= 0.05297
Epoch: 0010 train_loss= 0.59466 train_acc= 0.88403 val_loss= 0.62261 val_acc= 0.76901 time= 0.05300
Epoch: 0011 train_loss= 0.57472 train_acc= 0.88590 val_loss= 0.61043 val_acc= 0.76761 time= 0.05300
Epoch: 0012 train_loss= 0.55309 train_acc= 0.88199 val_loss= 0.59790 val_acc= 0.76901 time= 0.05300
Epoch: 0013 train_loss= 0.53134 train_acc= 0.89028 val_loss= 0.58525 val_acc= 0.77042 time= 0.05403
Epoch: 0014 train_loss= 0.50860 train_acc= 0.88887 val_loss= 0.57271 val_acc= 0.77465 time= 0.06800
Epoch: 0015 train_loss= 0.48519 train_acc= 0.89278 val_loss= 0.56057 val_acc= 0.77465 time= 0.05297
Epoch: 0016 train_loss= 0.46199 train_acc= 0.89637 val_loss= 0.54910 val_acc= 0.77324 time= 0.05303
Epoch: 0017 train_loss= 0.43805 train_acc= 0.89309 val_loss= 0.53853 val_acc= 0.77465 time= 0.05297
Epoch: 0018 train_loss= 0.41482 train_acc= 0.89981 val_loss= 0.52905 val_acc= 0.77606 time= 0.05400
Epoch: 0019 train_loss= 0.39509 train_acc= 0.90091 val_loss= 0.52086 val_acc= 0.77746 time= 0.06600
Epoch: 0020 train_loss= 0.37356 train_acc= 0.90372 val_loss= 0.51408 val_acc= 0.77887 time= 0.05500
Epoch: 0021 train_loss= 0.35381 train_acc= 0.90466 val_loss= 0.50879 val_acc= 0.77606 time= 0.05600
Epoch: 0022 train_loss= 0.33661 train_acc= 0.90497 val_loss= 0.50509 val_acc= 0.78028 time= 0.05400
Epoch: 0023 train_loss= 0.31760 train_acc= 0.90919 val_loss= 0.50294 val_acc= 0.77887 time= 0.05400
Epoch: 0024 train_loss= 0.30033 train_acc= 0.91200 val_loss= 0.50225 val_acc= 0.77887 time= 0.05500
Epoch: 0025 train_loss= 0.28384 train_acc= 0.91466 val_loss= 0.50298 val_acc= 0.77887 time= 0.06892
Epoch: 0026 train_loss= 0.27040 train_acc= 0.91919 val_loss= 0.50506 val_acc= 0.78310 time= 0.05599
Epoch: 0027 train_loss= 0.25648 train_acc= 0.92435 val_loss= 0.50840 val_acc= 0.78451 time= 0.05301
Epoch: 0028 train_loss= 0.24518 train_acc= 0.92216 val_loss= 0.51285 val_acc= 0.78451 time= 0.05300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51628 accuracy= 0.76055 time= 0.02500
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7561    0.7693    0.7626      1777
           1     0.7652    0.7518    0.7584      1777

    accuracy                         0.7606      3554
   macro avg     0.7606    0.7606    0.7605      3554
weighted avg     0.7606    0.7606    0.7605      3554

Macro average Test Precision, Recall and F1-Score...
(0.7606308097395817, 0.7605514912774338, 0.760533271892101, None)
Micro average Test Precision, Recall and F1-Score...
(0.7605514912774338, 0.7605514912774338, 0.7605514912774338, None)
embeddings:
18764 7108 3554
[[ 1.82625949e-01 -1.10482574e-02 -2.16325521e-02 ... -1.97347216e-02
   1.77809998e-01  2.23480314e-01]
 [ 2.39265934e-02  2.02927351e-01  1.83991358e-01 ...  2.62803555e-01
  -1.93637796e-04  1.81186683e-02]
 [ 3.08618367e-01 -8.42952952e-02 -8.43659341e-02 ... -1.00062236e-01
   3.08165580e-01  3.20947528e-01]
 ...
 [ 1.59598768e-01 -1.76970717e-02 -9.57966894e-02 ... -7.53722340e-02
   3.65678817e-01  3.14506590e-01]
 [ 9.12072584e-02  2.40110382e-01  2.76417375e-01 ...  2.89222926e-01
   6.30127490e-02  9.15154815e-02]
 [ 2.52772152e-01  3.54350001e-01  4.14534360e-01 ...  4.15240675e-01
   2.37482548e-01  2.46900246e-01]]

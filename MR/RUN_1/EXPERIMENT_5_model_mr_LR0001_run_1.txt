(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49625 val_loss= 0.69302 val_acc= 0.69718 time= 0.37230
Epoch: 0002 train_loss= 0.69288 train_acc= 0.77806 val_loss= 0.69285 val_acc= 0.72394 time= 0.08641
Epoch: 0003 train_loss= 0.69256 train_acc= 0.84480 val_loss= 0.69265 val_acc= 0.73803 time= 0.07545
Epoch: 0004 train_loss= 0.69219 train_acc= 0.85433 val_loss= 0.69241 val_acc= 0.74789 time= 0.08303
Epoch: 0005 train_loss= 0.69177 train_acc= 0.85589 val_loss= 0.69214 val_acc= 0.74648 time= 0.07400
Epoch: 0006 train_loss= 0.69129 train_acc= 0.85199 val_loss= 0.69182 val_acc= 0.74648 time= 0.07300
Epoch: 0007 train_loss= 0.69077 train_acc= 0.85355 val_loss= 0.69147 val_acc= 0.74648 time= 0.07114
Epoch: 0008 train_loss= 0.69019 train_acc= 0.85183 val_loss= 0.69109 val_acc= 0.75211 time= 0.07300
Epoch: 0009 train_loss= 0.68955 train_acc= 0.85073 val_loss= 0.69069 val_acc= 0.75352 time= 0.07298
Epoch: 0010 train_loss= 0.68890 train_acc= 0.85152 val_loss= 0.69026 val_acc= 0.75070 time= 0.07503
Epoch: 0011 train_loss= 0.68821 train_acc= 0.85558 val_loss= 0.68981 val_acc= 0.75352 time= 0.07901
Epoch: 0012 train_loss= 0.68745 train_acc= 0.85714 val_loss= 0.68934 val_acc= 0.75211 time= 0.07203
Epoch: 0013 train_loss= 0.68671 train_acc= 0.85683 val_loss= 0.68884 val_acc= 0.75070 time= 0.07300
Epoch: 0014 train_loss= 0.68587 train_acc= 0.85792 val_loss= 0.68832 val_acc= 0.75070 time= 0.08003
Epoch: 0015 train_loss= 0.68503 train_acc= 0.85777 val_loss= 0.68779 val_acc= 0.75070 time= 0.07305
Epoch: 0016 train_loss= 0.68413 train_acc= 0.85792 val_loss= 0.68723 val_acc= 0.75070 time= 0.07798
Epoch: 0017 train_loss= 0.68322 train_acc= 0.86136 val_loss= 0.68665 val_acc= 0.75070 time= 0.07903
Epoch: 0018 train_loss= 0.68227 train_acc= 0.86105 val_loss= 0.68604 val_acc= 0.74930 time= 0.07301
Epoch: 0019 train_loss= 0.68132 train_acc= 0.86089 val_loss= 0.68542 val_acc= 0.75352 time= 0.07102
Epoch: 0020 train_loss= 0.68027 train_acc= 0.86011 val_loss= 0.68477 val_acc= 0.75352 time= 0.07197
Epoch: 0021 train_loss= 0.67917 train_acc= 0.86621 val_loss= 0.68410 val_acc= 0.75493 time= 0.07303
Epoch: 0022 train_loss= 0.67809 train_acc= 0.86386 val_loss= 0.68341 val_acc= 0.75493 time= 0.07901
Epoch: 0023 train_loss= 0.67699 train_acc= 0.86527 val_loss= 0.68270 val_acc= 0.75634 time= 0.07296
Epoch: 0024 train_loss= 0.67580 train_acc= 0.86808 val_loss= 0.68197 val_acc= 0.75634 time= 0.07303
Epoch: 0025 train_loss= 0.67454 train_acc= 0.86808 val_loss= 0.68122 val_acc= 0.75775 time= 0.07896
Epoch: 0026 train_loss= 0.67338 train_acc= 0.86808 val_loss= 0.68044 val_acc= 0.76056 time= 0.07201
Epoch: 0027 train_loss= 0.67201 train_acc= 0.86887 val_loss= 0.67964 val_acc= 0.75775 time= 0.07196
Epoch: 0028 train_loss= 0.67074 train_acc= 0.87152 val_loss= 0.67882 val_acc= 0.75915 time= 0.07903
Epoch: 0029 train_loss= 0.66933 train_acc= 0.87090 val_loss= 0.67797 val_acc= 0.76056 time= 0.07100
Epoch: 0030 train_loss= 0.66799 train_acc= 0.87121 val_loss= 0.67711 val_acc= 0.76056 time= 0.08600
Epoch: 0031 train_loss= 0.66652 train_acc= 0.87465 val_loss= 0.67621 val_acc= 0.76197 time= 0.07401
Epoch: 0032 train_loss= 0.66502 train_acc= 0.87574 val_loss= 0.67530 val_acc= 0.76338 time= 0.07799
Epoch: 0033 train_loss= 0.66349 train_acc= 0.87605 val_loss= 0.67436 val_acc= 0.76197 time= 0.07106
Epoch: 0034 train_loss= 0.66207 train_acc= 0.87715 val_loss= 0.67340 val_acc= 0.76338 time= 0.07319
Epoch: 0035 train_loss= 0.66035 train_acc= 0.87574 val_loss= 0.67241 val_acc= 0.76338 time= 0.07900
Epoch: 0036 train_loss= 0.65872 train_acc= 0.87590 val_loss= 0.67140 val_acc= 0.76338 time= 0.07397
Epoch: 0037 train_loss= 0.65702 train_acc= 0.87824 val_loss= 0.67037 val_acc= 0.76761 time= 0.07351
Epoch: 0038 train_loss= 0.65541 train_acc= 0.88199 val_loss= 0.66931 val_acc= 0.76620 time= 0.07911
Epoch: 0039 train_loss= 0.65354 train_acc= 0.87981 val_loss= 0.66823 val_acc= 0.76620 time= 0.07098
Epoch: 0040 train_loss= 0.65166 train_acc= 0.88153 val_loss= 0.66713 val_acc= 0.76761 time= 0.07307
Epoch: 0041 train_loss= 0.64966 train_acc= 0.88012 val_loss= 0.66600 val_acc= 0.76901 time= 0.07802
Epoch: 0042 train_loss= 0.64783 train_acc= 0.88028 val_loss= 0.66484 val_acc= 0.76901 time= 0.07200
Epoch: 0043 train_loss= 0.64603 train_acc= 0.88199 val_loss= 0.66367 val_acc= 0.77042 time= 0.07299
Epoch: 0044 train_loss= 0.64410 train_acc= 0.88309 val_loss= 0.66247 val_acc= 0.76901 time= 0.08207
Epoch: 0045 train_loss= 0.64190 train_acc= 0.88496 val_loss= 0.66125 val_acc= 0.76901 time= 0.07597
Epoch: 0046 train_loss= 0.63979 train_acc= 0.88450 val_loss= 0.66000 val_acc= 0.76761 time= 0.07205
Epoch: 0047 train_loss= 0.63766 train_acc= 0.88418 val_loss= 0.65873 val_acc= 0.76479 time= 0.07198
Epoch: 0048 train_loss= 0.63561 train_acc= 0.88512 val_loss= 0.65744 val_acc= 0.76338 time= 0.08107
Epoch: 0049 train_loss= 0.63327 train_acc= 0.88371 val_loss= 0.65613 val_acc= 0.76338 time= 0.07300
Epoch: 0050 train_loss= 0.63122 train_acc= 0.88809 val_loss= 0.65479 val_acc= 0.76338 time= 0.07301
Epoch: 0051 train_loss= 0.62895 train_acc= 0.88637 val_loss= 0.65344 val_acc= 0.76197 time= 0.07301
Epoch: 0052 train_loss= 0.62673 train_acc= 0.88793 val_loss= 0.65206 val_acc= 0.76338 time= 0.07203
Epoch: 0053 train_loss= 0.62413 train_acc= 0.88825 val_loss= 0.65066 val_acc= 0.76338 time= 0.07236
Epoch: 0054 train_loss= 0.62213 train_acc= 0.88746 val_loss= 0.64924 val_acc= 0.76338 time= 0.07900
Epoch: 0055 train_loss= 0.61972 train_acc= 0.88872 val_loss= 0.64780 val_acc= 0.76197 time= 0.07100
Epoch: 0056 train_loss= 0.61720 train_acc= 0.88653 val_loss= 0.64635 val_acc= 0.76338 time= 0.07297
Epoch: 0057 train_loss= 0.61475 train_acc= 0.89028 val_loss= 0.64487 val_acc= 0.76338 time= 0.07909
Epoch: 0058 train_loss= 0.61215 train_acc= 0.88934 val_loss= 0.64337 val_acc= 0.76479 time= 0.07400
Epoch: 0059 train_loss= 0.60964 train_acc= 0.89106 val_loss= 0.64186 val_acc= 0.76901 time= 0.08200
Epoch: 0060 train_loss= 0.60701 train_acc= 0.89090 val_loss= 0.64033 val_acc= 0.76901 time= 0.07203
Epoch: 0061 train_loss= 0.60426 train_acc= 0.88981 val_loss= 0.63879 val_acc= 0.76901 time= 0.07313
Epoch: 0062 train_loss= 0.60158 train_acc= 0.89106 val_loss= 0.63723 val_acc= 0.76901 time= 0.07853
Epoch: 0063 train_loss= 0.59933 train_acc= 0.89153 val_loss= 0.63566 val_acc= 0.77042 time= 0.07500
Epoch: 0064 train_loss= 0.59659 train_acc= 0.89137 val_loss= 0.63407 val_acc= 0.77183 time= 0.07180
Epoch: 0065 train_loss= 0.59361 train_acc= 0.89231 val_loss= 0.63247 val_acc= 0.77183 time= 0.07905
Epoch: 0066 train_loss= 0.59082 train_acc= 0.89231 val_loss= 0.63085 val_acc= 0.77183 time= 0.07100
Epoch: 0067 train_loss= 0.58802 train_acc= 0.89122 val_loss= 0.62922 val_acc= 0.77042 time= 0.07300
Epoch: 0068 train_loss= 0.58507 train_acc= 0.89497 val_loss= 0.62758 val_acc= 0.77042 time= 0.07910
Epoch: 0069 train_loss= 0.58223 train_acc= 0.89512 val_loss= 0.62593 val_acc= 0.77042 time= 0.07100
Epoch: 0070 train_loss= 0.57992 train_acc= 0.89590 val_loss= 0.62427 val_acc= 0.77042 time= 0.07322
Epoch: 0071 train_loss= 0.57670 train_acc= 0.89669 val_loss= 0.62261 val_acc= 0.77042 time= 0.07833
Epoch: 0072 train_loss= 0.57412 train_acc= 0.89387 val_loss= 0.62093 val_acc= 0.77042 time= 0.07200
Epoch: 0073 train_loss= 0.57082 train_acc= 0.89465 val_loss= 0.61924 val_acc= 0.77183 time= 0.07299
Epoch: 0074 train_loss= 0.56798 train_acc= 0.89637 val_loss= 0.61755 val_acc= 0.77183 time= 0.07500
Epoch: 0075 train_loss= 0.56515 train_acc= 0.89465 val_loss= 0.61585 val_acc= 0.77183 time= 0.07902
Epoch: 0076 train_loss= 0.56212 train_acc= 0.89825 val_loss= 0.61415 val_acc= 0.77324 time= 0.07299
Epoch: 0077 train_loss= 0.55920 train_acc= 0.89887 val_loss= 0.61244 val_acc= 0.77324 time= 0.07300
Epoch: 0078 train_loss= 0.55573 train_acc= 0.89669 val_loss= 0.61073 val_acc= 0.77183 time= 0.07900
Epoch: 0079 train_loss= 0.55305 train_acc= 0.89919 val_loss= 0.60902 val_acc= 0.77324 time= 0.07200
Epoch: 0080 train_loss= 0.54953 train_acc= 0.89903 val_loss= 0.60730 val_acc= 0.77324 time= 0.07497
Epoch: 0081 train_loss= 0.54630 train_acc= 0.89872 val_loss= 0.60559 val_acc= 0.77324 time= 0.07903
Epoch: 0082 train_loss= 0.54399 train_acc= 0.89747 val_loss= 0.60387 val_acc= 0.77324 time= 0.07200
Epoch: 0083 train_loss= 0.54102 train_acc= 0.89778 val_loss= 0.60215 val_acc= 0.77324 time= 0.07496
Epoch: 0084 train_loss= 0.53742 train_acc= 0.89841 val_loss= 0.60043 val_acc= 0.77324 time= 0.08004
Epoch: 0085 train_loss= 0.53461 train_acc= 0.90122 val_loss= 0.59872 val_acc= 0.77324 time= 0.07796
Epoch: 0086 train_loss= 0.53153 train_acc= 0.89934 val_loss= 0.59700 val_acc= 0.77324 time= 0.07203
Epoch: 0087 train_loss= 0.52789 train_acc= 0.90012 val_loss= 0.59529 val_acc= 0.77324 time= 0.07200
Epoch: 0088 train_loss= 0.52542 train_acc= 0.90169 val_loss= 0.59359 val_acc= 0.77324 time= 0.07900
Epoch: 0089 train_loss= 0.52177 train_acc= 0.89809 val_loss= 0.59189 val_acc= 0.77465 time= 0.07297
Epoch: 0090 train_loss= 0.51870 train_acc= 0.89981 val_loss= 0.59019 val_acc= 0.77324 time= 0.07332
Epoch: 0091 train_loss= 0.51585 train_acc= 0.90075 val_loss= 0.58850 val_acc= 0.77324 time= 0.07900
Epoch: 0092 train_loss= 0.51275 train_acc= 0.90153 val_loss= 0.58682 val_acc= 0.77324 time= 0.07101
Epoch: 0093 train_loss= 0.50963 train_acc= 0.90169 val_loss= 0.58515 val_acc= 0.77465 time= 0.07299
Epoch: 0094 train_loss= 0.50593 train_acc= 0.90169 val_loss= 0.58348 val_acc= 0.77465 time= 0.07900
Epoch: 0095 train_loss= 0.50389 train_acc= 0.90325 val_loss= 0.58183 val_acc= 0.77465 time= 0.07200
Epoch: 0096 train_loss= 0.50021 train_acc= 0.90216 val_loss= 0.58019 val_acc= 0.77465 time= 0.07199
Epoch: 0097 train_loss= 0.49727 train_acc= 0.90372 val_loss= 0.57855 val_acc= 0.77465 time= 0.08007
Epoch: 0098 train_loss= 0.49383 train_acc= 0.90497 val_loss= 0.57693 val_acc= 0.77465 time= 0.07101
Epoch: 0099 train_loss= 0.49099 train_acc= 0.90466 val_loss= 0.57532 val_acc= 0.77465 time= 0.07799
Epoch: 0100 train_loss= 0.48748 train_acc= 0.90294 val_loss= 0.57372 val_acc= 0.77465 time= 0.07210
Epoch: 0101 train_loss= 0.48411 train_acc= 0.90419 val_loss= 0.57213 val_acc= 0.77606 time= 0.07184
Epoch: 0102 train_loss= 0.48178 train_acc= 0.90513 val_loss= 0.57056 val_acc= 0.77606 time= 0.08099
Epoch: 0103 train_loss= 0.47774 train_acc= 0.90466 val_loss= 0.56900 val_acc= 0.77606 time= 0.07701
Epoch: 0104 train_loss= 0.47532 train_acc= 0.90560 val_loss= 0.56745 val_acc= 0.77606 time= 0.07292
Epoch: 0105 train_loss= 0.47201 train_acc= 0.90544 val_loss= 0.56592 val_acc= 0.77606 time= 0.07904
Epoch: 0106 train_loss= 0.46889 train_acc= 0.90794 val_loss= 0.56440 val_acc= 0.77606 time= 0.07204
Epoch: 0107 train_loss= 0.46618 train_acc= 0.90606 val_loss= 0.56290 val_acc= 0.77606 time= 0.07400
Epoch: 0108 train_loss= 0.46334 train_acc= 0.90716 val_loss= 0.56141 val_acc= 0.77606 time= 0.07800
Epoch: 0109 train_loss= 0.46049 train_acc= 0.90716 val_loss= 0.55995 val_acc= 0.77606 time= 0.07204
Epoch: 0110 train_loss= 0.45703 train_acc= 0.90731 val_loss= 0.55850 val_acc= 0.77746 time= 0.07303
Epoch: 0111 train_loss= 0.45411 train_acc= 0.90685 val_loss= 0.55706 val_acc= 0.77746 time= 0.08000
Epoch: 0112 train_loss= 0.45147 train_acc= 0.90716 val_loss= 0.55565 val_acc= 0.77746 time= 0.07197
Epoch: 0113 train_loss= 0.44807 train_acc= 0.90966 val_loss= 0.55426 val_acc= 0.77746 time= 0.07200
Epoch: 0114 train_loss= 0.44501 train_acc= 0.90857 val_loss= 0.55288 val_acc= 0.77746 time= 0.07301
Epoch: 0115 train_loss= 0.44248 train_acc= 0.91028 val_loss= 0.55152 val_acc= 0.77746 time= 0.07897
Epoch: 0116 train_loss= 0.43980 train_acc= 0.91060 val_loss= 0.55018 val_acc= 0.77746 time= 0.07403
Epoch: 0117 train_loss= 0.43653 train_acc= 0.91122 val_loss= 0.54886 val_acc= 0.77746 time= 0.07599
Epoch: 0118 train_loss= 0.43326 train_acc= 0.91028 val_loss= 0.54756 val_acc= 0.77606 time= 0.08000
Epoch: 0119 train_loss= 0.43042 train_acc= 0.91091 val_loss= 0.54628 val_acc= 0.77606 time= 0.07100
Epoch: 0120 train_loss= 0.42786 train_acc= 0.91091 val_loss= 0.54502 val_acc= 0.77746 time= 0.07300
Epoch: 0121 train_loss= 0.42517 train_acc= 0.91060 val_loss= 0.54378 val_acc= 0.77746 time= 0.07900
Epoch: 0122 train_loss= 0.42187 train_acc= 0.91279 val_loss= 0.54256 val_acc= 0.77746 time= 0.07107
Epoch: 0123 train_loss= 0.41934 train_acc= 0.91122 val_loss= 0.54136 val_acc= 0.77887 time= 0.07204
Epoch: 0124 train_loss= 0.41662 train_acc= 0.91294 val_loss= 0.54018 val_acc= 0.78028 time= 0.07903
Epoch: 0125 train_loss= 0.41416 train_acc= 0.91310 val_loss= 0.53902 val_acc= 0.78028 time= 0.07700
Epoch: 0126 train_loss= 0.41085 train_acc= 0.91122 val_loss= 0.53788 val_acc= 0.78310 time= 0.07097
Epoch: 0127 train_loss= 0.40795 train_acc= 0.91388 val_loss= 0.53677 val_acc= 0.78451 time= 0.07200
Epoch: 0128 train_loss= 0.40563 train_acc= 0.91294 val_loss= 0.53568 val_acc= 0.78592 time= 0.08111
Epoch: 0129 train_loss= 0.40291 train_acc= 0.91325 val_loss= 0.53460 val_acc= 0.78592 time= 0.07296
Epoch: 0130 train_loss= 0.40067 train_acc= 0.91200 val_loss= 0.53355 val_acc= 0.78873 time= 0.07400
Epoch: 0131 train_loss= 0.39755 train_acc= 0.91513 val_loss= 0.53253 val_acc= 0.78732 time= 0.08404
Epoch: 0132 train_loss= 0.39506 train_acc= 0.91591 val_loss= 0.53152 val_acc= 0.78732 time= 0.07306
Epoch: 0133 train_loss= 0.39219 train_acc= 0.91622 val_loss= 0.53053 val_acc= 0.78732 time= 0.07297
Epoch: 0134 train_loss= 0.38989 train_acc= 0.91607 val_loss= 0.52957 val_acc= 0.78732 time= 0.07910
Epoch: 0135 train_loss= 0.38729 train_acc= 0.91622 val_loss= 0.52863 val_acc= 0.78732 time= 0.07130
Epoch: 0136 train_loss= 0.38452 train_acc= 0.91685 val_loss= 0.52771 val_acc= 0.78732 time= 0.07213
Epoch: 0137 train_loss= 0.38204 train_acc= 0.91701 val_loss= 0.52682 val_acc= 0.78592 time= 0.07933
Epoch: 0138 train_loss= 0.37940 train_acc= 0.91685 val_loss= 0.52595 val_acc= 0.78451 time= 0.07369
Epoch: 0139 train_loss= 0.37751 train_acc= 0.91747 val_loss= 0.52510 val_acc= 0.78451 time= 0.07797
Epoch: 0140 train_loss= 0.37383 train_acc= 0.91935 val_loss= 0.52427 val_acc= 0.78451 time= 0.07104
Epoch: 0141 train_loss= 0.37193 train_acc= 0.91857 val_loss= 0.52346 val_acc= 0.78451 time= 0.07299
Epoch: 0142 train_loss= 0.36974 train_acc= 0.91982 val_loss= 0.52267 val_acc= 0.78592 time= 0.07908
Epoch: 0143 train_loss= 0.36777 train_acc= 0.91966 val_loss= 0.52190 val_acc= 0.78592 time= 0.07297
Epoch: 0144 train_loss= 0.36480 train_acc= 0.91872 val_loss= 0.52116 val_acc= 0.78592 time= 0.07303
Epoch: 0145 train_loss= 0.36274 train_acc= 0.92076 val_loss= 0.52043 val_acc= 0.78592 time= 0.08097
Epoch: 0146 train_loss= 0.36058 train_acc= 0.91997 val_loss= 0.51972 val_acc= 0.78592 time= 0.07404
Epoch: 0147 train_loss= 0.35816 train_acc= 0.92154 val_loss= 0.51904 val_acc= 0.78592 time= 0.07299
Epoch: 0148 train_loss= 0.35582 train_acc= 0.91998 val_loss= 0.51838 val_acc= 0.78310 time= 0.08001
Epoch: 0149 train_loss= 0.35364 train_acc= 0.92248 val_loss= 0.51773 val_acc= 0.78310 time= 0.07100
Epoch: 0150 train_loss= 0.35176 train_acc= 0.92154 val_loss= 0.51711 val_acc= 0.78451 time= 0.07400
Epoch: 0151 train_loss= 0.34898 train_acc= 0.92232 val_loss= 0.51651 val_acc= 0.78310 time= 0.07900
Epoch: 0152 train_loss= 0.34715 train_acc= 0.92420 val_loss= 0.51593 val_acc= 0.78310 time= 0.07309
Epoch: 0153 train_loss= 0.34465 train_acc= 0.92294 val_loss= 0.51537 val_acc= 0.78169 time= 0.07206
Epoch: 0154 train_loss= 0.34336 train_acc= 0.92420 val_loss= 0.51482 val_acc= 0.78169 time= 0.07196
Epoch: 0155 train_loss= 0.34055 train_acc= 0.92138 val_loss= 0.51430 val_acc= 0.78028 time= 0.07804
Epoch: 0156 train_loss= 0.33841 train_acc= 0.92310 val_loss= 0.51380 val_acc= 0.78028 time= 0.07301
Epoch: 0157 train_loss= 0.33570 train_acc= 0.92498 val_loss= 0.51331 val_acc= 0.78028 time= 0.07197
Epoch: 0158 train_loss= 0.33436 train_acc= 0.92357 val_loss= 0.51285 val_acc= 0.78028 time= 0.08009
Epoch: 0159 train_loss= 0.33181 train_acc= 0.92607 val_loss= 0.51240 val_acc= 0.78028 time= 0.07426
Epoch: 0160 train_loss= 0.33025 train_acc= 0.92623 val_loss= 0.51198 val_acc= 0.78028 time= 0.08500
Epoch: 0161 train_loss= 0.32801 train_acc= 0.92795 val_loss= 0.51156 val_acc= 0.78028 time= 0.07197
Epoch: 0162 train_loss= 0.32586 train_acc= 0.92842 val_loss= 0.51117 val_acc= 0.78028 time= 0.07163
Epoch: 0163 train_loss= 0.32334 train_acc= 0.92857 val_loss= 0.51080 val_acc= 0.78028 time= 0.07807
Epoch: 0164 train_loss= 0.32136 train_acc= 0.92795 val_loss= 0.51045 val_acc= 0.78028 time= 0.07297
Epoch: 0165 train_loss= 0.31989 train_acc= 0.92857 val_loss= 0.51011 val_acc= 0.78028 time= 0.07803
Epoch: 0166 train_loss= 0.31823 train_acc= 0.92951 val_loss= 0.50980 val_acc= 0.78028 time= 0.07197
Epoch: 0167 train_loss= 0.31602 train_acc= 0.93060 val_loss= 0.50949 val_acc= 0.78169 time= 0.07208
Epoch: 0168 train_loss= 0.31370 train_acc= 0.92951 val_loss= 0.50920 val_acc= 0.78169 time= 0.07963
Epoch: 0169 train_loss= 0.31184 train_acc= 0.93060 val_loss= 0.50893 val_acc= 0.78028 time= 0.07300
Epoch: 0170 train_loss= 0.31035 train_acc= 0.93232 val_loss= 0.50867 val_acc= 0.78028 time= 0.07102
Epoch: 0171 train_loss= 0.30772 train_acc= 0.93123 val_loss= 0.50843 val_acc= 0.78028 time= 0.08003
Epoch: 0172 train_loss= 0.30624 train_acc= 0.93013 val_loss= 0.50821 val_acc= 0.78028 time= 0.07202
Epoch: 0173 train_loss= 0.30477 train_acc= 0.93092 val_loss= 0.50801 val_acc= 0.78028 time= 0.07204
Epoch: 0174 train_loss= 0.30345 train_acc= 0.93107 val_loss= 0.50782 val_acc= 0.78028 time= 0.08604
Epoch: 0175 train_loss= 0.30183 train_acc= 0.93342 val_loss= 0.50764 val_acc= 0.78028 time= 0.07199
Epoch: 0176 train_loss= 0.29920 train_acc= 0.93482 val_loss= 0.50748 val_acc= 0.78028 time= 0.07200
Epoch: 0177 train_loss= 0.29664 train_acc= 0.93373 val_loss= 0.50734 val_acc= 0.78028 time= 0.07901
Epoch: 0178 train_loss= 0.29598 train_acc= 0.93389 val_loss= 0.50721 val_acc= 0.78028 time= 0.07307
Epoch: 0179 train_loss= 0.29425 train_acc= 0.93420 val_loss= 0.50709 val_acc= 0.78028 time= 0.07200
Epoch: 0180 train_loss= 0.29179 train_acc= 0.93482 val_loss= 0.50699 val_acc= 0.78028 time= 0.07102
Epoch: 0181 train_loss= 0.29130 train_acc= 0.93576 val_loss= 0.50690 val_acc= 0.78028 time= 0.08000
Epoch: 0182 train_loss= 0.28944 train_acc= 0.93498 val_loss= 0.50683 val_acc= 0.78028 time= 0.07106
Epoch: 0183 train_loss= 0.28712 train_acc= 0.93435 val_loss= 0.50677 val_acc= 0.78028 time= 0.07303
Epoch: 0184 train_loss= 0.28605 train_acc= 0.93592 val_loss= 0.50672 val_acc= 0.78028 time= 0.08100
Epoch: 0185 train_loss= 0.28416 train_acc= 0.93811 val_loss= 0.50668 val_acc= 0.78028 time= 0.07200
Epoch: 0186 train_loss= 0.28322 train_acc= 0.94014 val_loss= 0.50666 val_acc= 0.78028 time= 0.07097
Epoch: 0187 train_loss= 0.28105 train_acc= 0.93732 val_loss= 0.50665 val_acc= 0.78169 time= 0.08003
Epoch: 0188 train_loss= 0.27995 train_acc= 0.93795 val_loss= 0.50666 val_acc= 0.78169 time= 0.07597
Epoch: 0189 train_loss= 0.27745 train_acc= 0.93904 val_loss= 0.50668 val_acc= 0.78310 time= 0.07572
Epoch: 0190 train_loss= 0.27628 train_acc= 0.93982 val_loss= 0.50672 val_acc= 0.78169 time= 0.08012
Epoch: 0191 train_loss= 0.27440 train_acc= 0.94029 val_loss= 0.50677 val_acc= 0.78169 time= 0.07200
Early stopping...
Optimization Finished!
Test set results: cost= 0.50940 accuracy= 0.76168 time= 0.03601
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7549    0.7749    0.7648      1777
           1     0.7688    0.7485    0.7585      1777

    accuracy                         0.7617      3554
   macro avg     0.7619    0.7617    0.7616      3554
weighted avg     0.7619    0.7617    0.7616      3554

Macro average Test Precision, Recall and F1-Score...
(0.7618601688469729, 0.7616769836803601, 0.7616352964717608, None)
Micro average Test Precision, Recall and F1-Score...
(0.7616769836803602, 0.7616769836803602, 0.7616769836803602, None)
embeddings:
18764 7108 3554
[[ 8.60515013e-02  8.34038183e-02  8.11747983e-02 ...  8.24209377e-02
   2.95536593e-05  8.33703801e-02]
 [ 1.29775070e-02  1.23978220e-02  1.07553387e-02 ...  6.24574069e-03
   8.50336552e-02  1.01554450e-02]
 [ 1.27599463e-01  1.26316383e-01  1.19965099e-01 ...  1.24759436e-01
  -3.52087505e-02  1.29825890e-01]
 ...
 [ 1.21365696e-01 -1.29627651e-02  9.71558541e-02 ...  1.02847345e-01
  -7.24412734e-03  1.16412088e-01]
 [ 3.05747483e-02  3.11976578e-02  2.91204043e-02 ...  3.40791717e-02
   1.01019263e-01  3.27802263e-02]
 [ 1.07546978e-01  1.05467416e-01  1.04803137e-01 ...  1.06236339e-01
   1.59448683e-01  1.04045466e-01]]

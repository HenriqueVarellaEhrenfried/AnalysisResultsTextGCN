(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49812 val_loss= 0.69002 val_acc= 0.67465 time= 0.47429
Epoch: 0002 train_loss= 0.68708 train_acc= 0.78603 val_loss= 0.68087 val_acc= 0.62113 time= 0.12205
Epoch: 0003 train_loss= 0.67084 train_acc= 0.70631 val_loss= 0.66636 val_acc= 0.64225 time= 0.11700
Epoch: 0004 train_loss= 0.64466 train_acc= 0.74633 val_loss= 0.64518 val_acc= 0.69859 time= 0.11606
Epoch: 0005 train_loss= 0.60806 train_acc= 0.81307 val_loss= 0.61780 val_acc= 0.74085 time= 0.13296
Epoch: 0006 train_loss= 0.56134 train_acc= 0.85699 val_loss= 0.58691 val_acc= 0.76901 time= 0.11700
Epoch: 0007 train_loss= 0.50788 train_acc= 0.88496 val_loss= 0.55638 val_acc= 0.77465 time= 0.11404
Epoch: 0008 train_loss= 0.45134 train_acc= 0.89262 val_loss= 0.52982 val_acc= 0.77042 time= 0.11424
Epoch: 0009 train_loss= 0.39587 train_acc= 0.89887 val_loss= 0.51032 val_acc= 0.77042 time= 0.11400
Epoch: 0010 train_loss= 0.34496 train_acc= 0.90325 val_loss= 0.50016 val_acc= 0.77183 time= 0.11601
Epoch: 0011 train_loss= 0.30031 train_acc= 0.90810 val_loss= 0.49999 val_acc= 0.77324 time= 0.12909
Epoch: 0012 train_loss= 0.26286 train_acc= 0.91294 val_loss= 0.50963 val_acc= 0.77183 time= 0.11401
Epoch: 0013 train_loss= 0.23184 train_acc= 0.91935 val_loss= 0.52809 val_acc= 0.77606 time= 0.11599
Epoch: 0014 train_loss= 0.20539 train_acc= 0.92982 val_loss= 0.55231 val_acc= 0.77465 time= 0.13196
Early stopping...
Optimization Finished!
Test set results: cost= 0.54805 accuracy= 0.76027 time= 0.04704
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7464    0.7884    0.7668      1777
           1     0.7758    0.7321    0.7533      1777

    accuracy                         0.7603      3554
   macro avg     0.7611    0.7603    0.7601      3554
weighted avg     0.7611    0.7603    0.7601      3554

Macro average Test Precision, Recall and F1-Score...
(0.7610969686399306, 0.7602701181767023, 0.760080171676921, None)
Micro average Test Precision, Recall and F1-Score...
(0.7602701181767023, 0.7602701181767023, 0.7602701181767023, None)
embeddings:
18764 7108 3554
[[ 3.3626322e-02  4.6627771e-02  3.7243618e-03 ...  3.8959060e-02
   7.1733482e-02  1.6423268e-03]
 [ 3.2223500e-03  1.7869953e-02  1.1215803e-01 ...  1.7197177e-02
  -9.9278986e-07  7.1154751e-02]
 [ 5.2159715e-02  8.7900512e-02 -3.0828293e-02 ...  1.1838999e-01
   1.1791095e-01 -2.3885692e-02]
 ...
 [ 1.0867063e-02  1.1052379e-01 -6.1967716e-02 ...  1.7036794e-01
  -1.7823088e-03 -1.0168871e-02]
 [ 1.1433096e-02  1.3003997e-02  1.0626734e-01 ...  1.3039582e-02
   2.0676455e-02  9.1043778e-02]
 [ 3.1481791e-02  4.9109064e-02  1.9348748e-01 ...  5.9230726e-02
   7.7924967e-02  1.6371858e-01]]

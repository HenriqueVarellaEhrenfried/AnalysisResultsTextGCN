(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.51110 val_loss= 0.69246 val_acc= 0.73803 time= 0.37339
Epoch: 0002 train_loss= 0.69187 train_acc= 0.80635 val_loss= 0.69100 val_acc= 0.73662 time= 0.08200
Epoch: 0003 train_loss= 0.68951 train_acc= 0.81697 val_loss= 0.68868 val_acc= 0.74225 time= 0.08600
Epoch: 0004 train_loss= 0.68582 train_acc= 0.82620 val_loss= 0.68558 val_acc= 0.74366 time= 0.07600
Epoch: 0005 train_loss= 0.68074 train_acc= 0.83698 val_loss= 0.68171 val_acc= 0.74789 time= 0.07200
Epoch: 0006 train_loss= 0.67461 train_acc= 0.83714 val_loss= 0.67704 val_acc= 0.75211 time= 0.07200
Epoch: 0007 train_loss= 0.66681 train_acc= 0.84308 val_loss= 0.67157 val_acc= 0.76197 time= 0.07201
Epoch: 0008 train_loss= 0.65797 train_acc= 0.84698 val_loss= 0.66530 val_acc= 0.76197 time= 0.07296
Epoch: 0009 train_loss= 0.64758 train_acc= 0.84651 val_loss= 0.65823 val_acc= 0.76197 time= 0.08605
Epoch: 0010 train_loss= 0.63633 train_acc= 0.85558 val_loss= 0.65044 val_acc= 0.76197 time= 0.07099
Epoch: 0011 train_loss= 0.62374 train_acc= 0.85667 val_loss= 0.64195 val_acc= 0.76338 time= 0.07196
Epoch: 0012 train_loss= 0.61019 train_acc= 0.86089 val_loss= 0.63287 val_acc= 0.76901 time= 0.07200
Epoch: 0013 train_loss= 0.59386 train_acc= 0.85714 val_loss= 0.62326 val_acc= 0.76761 time= 0.07716
Epoch: 0014 train_loss= 0.57815 train_acc= 0.86058 val_loss= 0.61325 val_acc= 0.76620 time= 0.07600
Epoch: 0015 train_loss= 0.56137 train_acc= 0.86996 val_loss= 0.60293 val_acc= 0.76761 time= 0.07197
Epoch: 0016 train_loss= 0.54371 train_acc= 0.87121 val_loss= 0.59248 val_acc= 0.76620 time= 0.07215
Epoch: 0017 train_loss= 0.52629 train_acc= 0.87058 val_loss= 0.58202 val_acc= 0.76901 time= 0.08300
Epoch: 0018 train_loss= 0.50937 train_acc= 0.86855 val_loss= 0.57171 val_acc= 0.76901 time= 0.07403
Epoch: 0019 train_loss= 0.48644 train_acc= 0.88231 val_loss= 0.56167 val_acc= 0.76761 time= 0.07200
Epoch: 0020 train_loss= 0.47072 train_acc= 0.87480 val_loss= 0.55207 val_acc= 0.77042 time= 0.07240
Epoch: 0021 train_loss= 0.45277 train_acc= 0.87621 val_loss= 0.54302 val_acc= 0.77042 time= 0.07100
Epoch: 0022 train_loss= 0.43330 train_acc= 0.87871 val_loss= 0.53466 val_acc= 0.76901 time= 0.08700
Epoch: 0023 train_loss= 0.41986 train_acc= 0.87918 val_loss= 0.52708 val_acc= 0.77042 time= 0.07099
Epoch: 0024 train_loss= 0.40017 train_acc= 0.88637 val_loss= 0.52031 val_acc= 0.77183 time= 0.07220
Epoch: 0025 train_loss= 0.38521 train_acc= 0.88700 val_loss= 0.51441 val_acc= 0.77324 time= 0.07223
Epoch: 0026 train_loss= 0.36913 train_acc= 0.88793 val_loss= 0.50949 val_acc= 0.77183 time= 0.07203
Epoch: 0027 train_loss= 0.35433 train_acc= 0.89090 val_loss= 0.50550 val_acc= 0.77324 time= 0.08900
Epoch: 0028 train_loss= 0.34024 train_acc= 0.89294 val_loss= 0.50245 val_acc= 0.77183 time= 0.07203
Epoch: 0029 train_loss= 0.32895 train_acc= 0.88997 val_loss= 0.50027 val_acc= 0.77606 time= 0.07200
Epoch: 0030 train_loss= 0.31397 train_acc= 0.89716 val_loss= 0.49889 val_acc= 0.77606 time= 0.07203
Epoch: 0031 train_loss= 0.30509 train_acc= 0.89716 val_loss= 0.49840 val_acc= 0.77746 time= 0.08400
Epoch: 0032 train_loss= 0.29404 train_acc= 0.90153 val_loss= 0.49870 val_acc= 0.77746 time= 0.07504
Epoch: 0033 train_loss= 0.27993 train_acc= 0.90544 val_loss= 0.49971 val_acc= 0.77746 time= 0.07296
Epoch: 0034 train_loss= 0.26852 train_acc= 0.91060 val_loss= 0.50144 val_acc= 0.78028 time= 0.07204
Early stopping...
Optimization Finished!
Test set results: cost= 0.50537 accuracy= 0.76027 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7551    0.7704    0.7627      1777
           1     0.7657    0.7501    0.7578      1777

    accuracy                         0.7603      3554
   macro avg     0.7604    0.7603    0.7602      3554
weighted avg     0.7604    0.7603    0.7602      3554

Macro average Test Precision, Recall and F1-Score...
(0.7603769824989157, 0.7602701181767023, 0.7602455180757617, None)
Micro average Test Precision, Recall and F1-Score...
(0.7602701181767023, 0.7602701181767023, 0.7602701181767023, None)
embeddings:
18764 7108 3554
[[-0.00909007 -0.00352825  0.0775309  ...  0.07313078  0.07446169
   0.07451745]
 [ 0.07463279  0.06470434  0.01873347 ...  0.01297032  0.02259976
   0.01163381]
 [-0.02729336 -0.04747101  0.13719763 ...  0.11998303  0.13775203
   0.1521884 ]
 ...
 [-0.00402415 -0.00446587  0.1422418  ...  0.07475092  0.06235132
  -0.00733564]
 [ 0.06834064  0.09055132  0.03497851 ...  0.02916803  0.03584411
   0.02473048]
 [ 0.13448025  0.15474896  0.08483904 ...  0.09431544  0.07424484
   0.10222484]]

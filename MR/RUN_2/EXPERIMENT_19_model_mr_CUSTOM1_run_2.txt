(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.48890 val_loss= 0.69233 val_acc= 0.69296 time= 0.36917
Epoch: 0002 train_loss= 0.69168 train_acc= 0.78071 val_loss= 0.69068 val_acc= 0.70282 time= 0.08373
Epoch: 0003 train_loss= 0.68893 train_acc= 0.79587 val_loss= 0.68810 val_acc= 0.70704 time= 0.07600
Epoch: 0004 train_loss= 0.68474 train_acc= 0.80291 val_loss= 0.68469 val_acc= 0.71268 time= 0.07500
Epoch: 0005 train_loss= 0.67924 train_acc= 0.80431 val_loss= 0.68048 val_acc= 0.72535 time= 0.08000
Epoch: 0006 train_loss= 0.67221 train_acc= 0.81166 val_loss= 0.67546 val_acc= 0.73239 time= 0.07300
Epoch: 0007 train_loss= 0.66467 train_acc= 0.82135 val_loss= 0.66965 val_acc= 0.73803 time= 0.07897
Epoch: 0008 train_loss= 0.65497 train_acc= 0.82760 val_loss= 0.66305 val_acc= 0.73662 time= 0.08010
Epoch: 0009 train_loss= 0.64421 train_acc= 0.83901 val_loss= 0.65570 val_acc= 0.74366 time= 0.07199
Epoch: 0010 train_loss= 0.63214 train_acc= 0.83276 val_loss= 0.64762 val_acc= 0.74930 time= 0.07793
Epoch: 0011 train_loss= 0.61835 train_acc= 0.84855 val_loss= 0.63887 val_acc= 0.75634 time= 0.07202
Epoch: 0012 train_loss= 0.60464 train_acc= 0.85214 val_loss= 0.62950 val_acc= 0.75775 time= 0.07302
Epoch: 0013 train_loss= 0.58966 train_acc= 0.86043 val_loss= 0.61966 val_acc= 0.75915 time= 0.07603
Epoch: 0014 train_loss= 0.57368 train_acc= 0.86199 val_loss= 0.60945 val_acc= 0.76479 time= 0.07300
Epoch: 0015 train_loss= 0.55421 train_acc= 0.86543 val_loss= 0.59897 val_acc= 0.77042 time= 0.07904
Epoch: 0016 train_loss= 0.53601 train_acc= 0.86840 val_loss= 0.58838 val_acc= 0.77183 time= 0.07331
Epoch: 0017 train_loss= 0.51763 train_acc= 0.87152 val_loss= 0.57783 val_acc= 0.76901 time= 0.07300
Epoch: 0018 train_loss= 0.50085 train_acc= 0.87590 val_loss= 0.56748 val_acc= 0.76901 time= 0.07697
Epoch: 0019 train_loss= 0.48324 train_acc= 0.87543 val_loss= 0.55750 val_acc= 0.77606 time= 0.07201
Epoch: 0020 train_loss= 0.46401 train_acc= 0.87918 val_loss= 0.54804 val_acc= 0.77606 time= 0.08003
Epoch: 0021 train_loss= 0.44358 train_acc= 0.88184 val_loss= 0.53918 val_acc= 0.77606 time= 0.07518
Epoch: 0022 train_loss= 0.42824 train_acc= 0.88231 val_loss= 0.53105 val_acc= 0.77465 time= 0.07697
Epoch: 0023 train_loss= 0.40839 train_acc= 0.88278 val_loss= 0.52372 val_acc= 0.77465 time= 0.07603
Epoch: 0024 train_loss= 0.39276 train_acc= 0.88653 val_loss= 0.51728 val_acc= 0.77324 time= 0.07700
Epoch: 0025 train_loss= 0.37534 train_acc= 0.89106 val_loss= 0.51175 val_acc= 0.77465 time= 0.07200
Epoch: 0026 train_loss= 0.36314 train_acc= 0.88965 val_loss= 0.50713 val_acc= 0.77465 time= 0.07802
Epoch: 0027 train_loss= 0.34853 train_acc= 0.89153 val_loss= 0.50345 val_acc= 0.77746 time= 0.07100
Epoch: 0028 train_loss= 0.33367 train_acc= 0.89434 val_loss= 0.50072 val_acc= 0.78028 time= 0.07305
Epoch: 0029 train_loss= 0.32298 train_acc= 0.89262 val_loss= 0.49889 val_acc= 0.77887 time= 0.07904
Epoch: 0030 train_loss= 0.30829 train_acc= 0.89606 val_loss= 0.49786 val_acc= 0.77887 time= 0.07100
Epoch: 0031 train_loss= 0.29687 train_acc= 0.90247 val_loss= 0.49769 val_acc= 0.78169 time= 0.07497
Epoch: 0032 train_loss= 0.28734 train_acc= 0.90372 val_loss= 0.49832 val_acc= 0.78028 time= 0.07303
Epoch: 0033 train_loss= 0.27573 train_acc= 0.90982 val_loss= 0.49964 val_acc= 0.78169 time= 0.08000
Early stopping...
Optimization Finished!
Test set results: cost= 0.50246 accuracy= 0.76224 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7530    0.7805    0.7665      1777
           1     0.7722    0.7440    0.7578      1777

    accuracy                         0.7622      3554
   macro avg     0.7626    0.7622    0.7622      3554
weighted avg     0.7626    0.7622    0.7622      3554

Macro average Test Precision, Recall and F1-Score...
(0.762591073294976, 0.7622397298818233, 0.7621601732393704, None)
Micro average Test Precision, Recall and F1-Score...
(0.7622397298818233, 0.7622397298818233, 0.7622397298818234, None)
embeddings:
18764 7108 3554
[[ 0.07194104  0.0171343   0.06938527 ...  0.07894839  0.00532905
   0.07712144]
 [ 0.00304555  0.09797037 -0.00784797 ...  0.01425099  0.07582733
   0.0003961 ]
 [ 0.13698304 -0.03745588  0.13887691 ...  0.13439845 -0.02747116
   0.12423815]
 ...
 [ 0.08724507 -0.04006464  0.10072817 ...  0.05901587 -0.02510171
   0.09115486]
 [ 0.0285982   0.1014028   0.00186353 ...  0.00972826  0.08579835
   0.02279191]
 [ 0.05609539  0.17941836  0.06973232 ...  0.08342937  0.14419854
   0.07856665]]

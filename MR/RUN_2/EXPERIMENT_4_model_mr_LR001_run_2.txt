(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.48859 val_loss= 0.69177 val_acc= 0.75211 time= 0.37897
Epoch: 0002 train_loss= 0.69055 train_acc= 0.85621 val_loss= 0.68869 val_acc= 0.73099 time= 0.07603
Epoch: 0003 train_loss= 0.68546 train_acc= 0.83651 val_loss= 0.68449 val_acc= 0.72817 time= 0.08197
Epoch: 0004 train_loss= 0.67841 train_acc= 0.83088 val_loss= 0.67928 val_acc= 0.72817 time= 0.07803
Epoch: 0005 train_loss= 0.66968 train_acc= 0.83932 val_loss= 0.67298 val_acc= 0.73662 time= 0.07201
Epoch: 0006 train_loss= 0.65897 train_acc= 0.84651 val_loss= 0.66557 val_acc= 0.73944 time= 0.07300
Epoch: 0007 train_loss= 0.64634 train_acc= 0.85214 val_loss= 0.65712 val_acc= 0.74225 time= 0.07301
Epoch: 0008 train_loss= 0.63190 train_acc= 0.85808 val_loss= 0.64770 val_acc= 0.75070 time= 0.07299
Epoch: 0009 train_loss= 0.61619 train_acc= 0.86418 val_loss= 0.63741 val_acc= 0.74930 time= 0.08701
Epoch: 0010 train_loss= 0.59843 train_acc= 0.86996 val_loss= 0.62635 val_acc= 0.75634 time= 0.07200
Epoch: 0011 train_loss= 0.57888 train_acc= 0.87480 val_loss= 0.61470 val_acc= 0.76197 time= 0.07200
Epoch: 0012 train_loss= 0.55907 train_acc= 0.88074 val_loss= 0.60262 val_acc= 0.76620 time= 0.07200
Epoch: 0013 train_loss= 0.53767 train_acc= 0.88371 val_loss= 0.59032 val_acc= 0.77042 time= 0.07199
Epoch: 0014 train_loss= 0.51562 train_acc= 0.88997 val_loss= 0.57802 val_acc= 0.76901 time= 0.08901
Epoch: 0015 train_loss= 0.49262 train_acc= 0.89278 val_loss= 0.56598 val_acc= 0.77183 time= 0.07109
Epoch: 0016 train_loss= 0.47003 train_acc= 0.89512 val_loss= 0.55446 val_acc= 0.77042 time= 0.07899
Epoch: 0017 train_loss= 0.44647 train_acc= 0.89794 val_loss= 0.54370 val_acc= 0.77183 time= 0.07400
Epoch: 0018 train_loss= 0.42460 train_acc= 0.89966 val_loss= 0.53391 val_acc= 0.77465 time= 0.07399
Epoch: 0019 train_loss= 0.40344 train_acc= 0.90419 val_loss= 0.52527 val_acc= 0.77887 time= 0.07200
Epoch: 0020 train_loss= 0.38082 train_acc= 0.90263 val_loss= 0.51792 val_acc= 0.77887 time= 0.07301
Epoch: 0021 train_loss= 0.36080 train_acc= 0.90857 val_loss= 0.51194 val_acc= 0.77887 time= 0.07300
Epoch: 0022 train_loss= 0.34181 train_acc= 0.90919 val_loss= 0.50738 val_acc= 0.77887 time= 0.08701
Epoch: 0023 train_loss= 0.32416 train_acc= 0.91169 val_loss= 0.50426 val_acc= 0.78028 time= 0.07100
Epoch: 0024 train_loss= 0.30734 train_acc= 0.91325 val_loss= 0.50260 val_acc= 0.78451 time= 0.07199
Epoch: 0025 train_loss= 0.29046 train_acc= 0.91654 val_loss= 0.50234 val_acc= 0.78451 time= 0.07197
Epoch: 0026 train_loss= 0.27601 train_acc= 0.91951 val_loss= 0.50339 val_acc= 0.78451 time= 0.07200
Epoch: 0027 train_loss= 0.26258 train_acc= 0.92107 val_loss= 0.50561 val_acc= 0.78451 time= 0.08204
Epoch: 0028 train_loss= 0.24982 train_acc= 0.92560 val_loss= 0.50895 val_acc= 0.78451 time= 0.07196
Epoch: 0029 train_loss= 0.23689 train_acc= 0.92967 val_loss= 0.51337 val_acc= 0.78028 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51258 accuracy= 0.75830 time= 0.03600
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7508    0.7732    0.7619      1777
           1     0.7662    0.7434    0.7546      1777

    accuracy                         0.7583      3554
   macro avg     0.7585    0.7583    0.7582      3554
weighted avg     0.7585    0.7583    0.7582      3554

Macro average Test Precision, Recall and F1-Score...
(0.7585304857175459, 0.7583005064715813, 0.758246742761999, None)
Micro average Test Precision, Recall and F1-Score...
(0.7583005064715813, 0.7583005064715813, 0.7583005064715813, None)
embeddings:
18764 7108 3554
[[ 1.0734207e-01  8.6309932e-02 -3.7936447e-03 ...  8.5635900e-02
  -3.9277291e-03  9.6780315e-02]
 [-2.4661003e-04 -6.9045462e-04  1.1991743e-01 ...  1.6905367e-04
   1.2524255e-01  9.1996249e-03]
 [ 1.5259242e-01  1.4891489e-01 -3.6153313e-02 ...  1.5313147e-01
  -4.1439969e-02  1.5318973e-01]
 ...
 [ 1.3528509e-01  1.6234182e-01 -4.1461359e-03 ...  1.4812095e-01
  -5.8195285e-02 -9.2872996e-03]
 [ 3.2226611e-02  2.1692717e-02  1.3505654e-01 ...  3.3887941e-02
   1.2916234e-01  3.3782955e-02]
 [ 1.1408751e-01  9.7130686e-02  2.2329742e-01 ...  1.0439825e-01
   2.3628320e-01  1.0646394e-01]]

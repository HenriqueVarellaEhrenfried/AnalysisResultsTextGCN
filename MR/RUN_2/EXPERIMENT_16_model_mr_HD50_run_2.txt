(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49547 val_loss= 0.69119 val_acc= 0.76338 time= 0.30024
Epoch: 0002 train_loss= 0.68979 train_acc= 0.85495 val_loss= 0.68675 val_acc= 0.74789 time= 0.05305
Epoch: 0003 train_loss= 0.68273 train_acc= 0.84902 val_loss= 0.68128 val_acc= 0.75915 time= 0.05548
Epoch: 0004 train_loss= 0.67363 train_acc= 0.85605 val_loss= 0.67467 val_acc= 0.76620 time= 0.06153
Epoch: 0005 train_loss= 0.66286 train_acc= 0.86511 val_loss= 0.66685 val_acc= 0.76761 time= 0.05299
Epoch: 0006 train_loss= 0.64998 train_acc= 0.86871 val_loss= 0.65789 val_acc= 0.76761 time= 0.05501
Epoch: 0007 train_loss= 0.63493 train_acc= 0.87840 val_loss= 0.64790 val_acc= 0.77042 time= 0.05200
Epoch: 0008 train_loss= 0.61897 train_acc= 0.87590 val_loss= 0.63699 val_acc= 0.77042 time= 0.05300
Epoch: 0009 train_loss= 0.60012 train_acc= 0.87590 val_loss= 0.62534 val_acc= 0.77042 time= 0.06269
Epoch: 0010 train_loss= 0.58088 train_acc= 0.87840 val_loss= 0.61311 val_acc= 0.77183 time= 0.05325
Epoch: 0011 train_loss= 0.55977 train_acc= 0.88481 val_loss= 0.60051 val_acc= 0.77183 time= 0.05304
Epoch: 0012 train_loss= 0.53761 train_acc= 0.88793 val_loss= 0.58776 val_acc= 0.77183 time= 0.06296
Epoch: 0013 train_loss= 0.51427 train_acc= 0.88746 val_loss= 0.57513 val_acc= 0.77465 time= 0.05303
Epoch: 0014 train_loss= 0.49178 train_acc= 0.89090 val_loss= 0.56288 val_acc= 0.77183 time= 0.05297
Epoch: 0015 train_loss= 0.46790 train_acc= 0.89169 val_loss= 0.55126 val_acc= 0.77606 time= 0.05403
Epoch: 0016 train_loss= 0.44375 train_acc= 0.89372 val_loss= 0.54047 val_acc= 0.77746 time= 0.06197
Epoch: 0017 train_loss= 0.42172 train_acc= 0.89700 val_loss= 0.53076 val_acc= 0.77606 time= 0.05904
Epoch: 0018 train_loss= 0.39958 train_acc= 0.89872 val_loss= 0.52228 val_acc= 0.77746 time= 0.05503
Epoch: 0019 train_loss= 0.37889 train_acc= 0.90309 val_loss= 0.51517 val_acc= 0.77746 time= 0.06400
Epoch: 0020 train_loss= 0.35936 train_acc= 0.90544 val_loss= 0.50957 val_acc= 0.78028 time= 0.05300
Epoch: 0021 train_loss= 0.34008 train_acc= 0.90794 val_loss= 0.50546 val_acc= 0.78169 time= 0.05297
Epoch: 0022 train_loss= 0.32118 train_acc= 0.90950 val_loss= 0.50284 val_acc= 0.78169 time= 0.06103
Epoch: 0023 train_loss= 0.30488 train_acc= 0.91185 val_loss= 0.50170 val_acc= 0.78028 time= 0.05400
Epoch: 0024 train_loss= 0.28963 train_acc= 0.91654 val_loss= 0.50194 val_acc= 0.78028 time= 0.05291
Epoch: 0025 train_loss= 0.27416 train_acc= 0.91607 val_loss= 0.50352 val_acc= 0.78310 time= 0.05300
Epoch: 0026 train_loss= 0.26100 train_acc= 0.91857 val_loss= 0.50645 val_acc= 0.78310 time= 0.05400
Epoch: 0027 train_loss= 0.24958 train_acc= 0.92294 val_loss= 0.51069 val_acc= 0.78028 time= 0.06201
Early stopping...
Optimization Finished!
Test set results: cost= 0.51590 accuracy= 0.75999 time= 0.02499
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7511    0.7777    0.7642      1777
           1     0.7695    0.7423    0.7557      1777

    accuracy                         0.7600      3554
   macro avg     0.7603    0.7600    0.7599      3554
weighted avg     0.7603    0.7600    0.7599      3554

Macro average Test Precision, Recall and F1-Score...
(0.7603159403378824, 0.7599887450759708, 0.7599133028751923, None)
Micro average Test Precision, Recall and F1-Score...
(0.7599887450759707, 0.7599887450759707, 0.7599887450759707, None)
embeddings:
18764 7108 3554
[[ 0.19928785 -0.01085165 -0.01130051 ...  0.19016272 -0.0121173
  -0.00888072]
 [ 0.00441071  0.25849164  0.18861836 ...  0.00285564  0.21941634
   0.21738398]
 [ 0.31426364 -0.0874593  -0.09347045 ...  0.29878747 -0.07123958
  -0.08502887]
 ...
 [ 0.3345636  -0.07124169 -0.00291798 ...  0.30886042 -0.04170633
  -0.09470947]
 [ 0.06803346  0.24679187  0.2370796  ...  0.06760302  0.19608621
   0.22666785]
 [ 0.26622382  0.41176388  0.39481992 ...  0.24633536  0.34306133
   0.3956819 ]]

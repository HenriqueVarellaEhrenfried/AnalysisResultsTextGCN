(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.50656 val_loss= 0.69148 val_acc= 0.75070 time= 0.37132
Epoch: 0002 train_loss= 0.69035 train_acc= 0.82213 val_loss= 0.68730 val_acc= 0.76197 time= 0.07558
Epoch: 0003 train_loss= 0.68373 train_acc= 0.82510 val_loss= 0.68040 val_acc= 0.76338 time= 0.07600
Epoch: 0004 train_loss= 0.67271 train_acc= 0.83323 val_loss= 0.67087 val_acc= 0.76761 time= 0.07303
Epoch: 0005 train_loss= 0.65780 train_acc= 0.84058 val_loss= 0.65868 val_acc= 0.76620 time= 0.08197
Epoch: 0006 train_loss= 0.63810 train_acc= 0.84761 val_loss= 0.64409 val_acc= 0.76901 time= 0.07300
Epoch: 0007 train_loss= 0.61399 train_acc= 0.84980 val_loss= 0.62750 val_acc= 0.77042 time= 0.07203
Epoch: 0008 train_loss= 0.58787 train_acc= 0.85480 val_loss= 0.60940 val_acc= 0.77465 time= 0.08200
Epoch: 0009 train_loss= 0.55789 train_acc= 0.85699 val_loss= 0.59052 val_acc= 0.77465 time= 0.07200
Epoch: 0010 train_loss= 0.52657 train_acc= 0.86246 val_loss= 0.57172 val_acc= 0.77606 time= 0.07200
Epoch: 0011 train_loss= 0.49282 train_acc= 0.86355 val_loss= 0.55385 val_acc= 0.77042 time= 0.08407
Epoch: 0012 train_loss= 0.46120 train_acc= 0.86902 val_loss= 0.53752 val_acc= 0.76901 time= 0.07236
Epoch: 0013 train_loss= 0.42814 train_acc= 0.87262 val_loss= 0.52336 val_acc= 0.77465 time= 0.07199
Epoch: 0014 train_loss= 0.39874 train_acc= 0.87262 val_loss= 0.51202 val_acc= 0.77324 time= 0.07300
Epoch: 0015 train_loss= 0.37093 train_acc= 0.88387 val_loss= 0.50383 val_acc= 0.77746 time= 0.08900
Epoch: 0016 train_loss= 0.34880 train_acc= 0.88090 val_loss= 0.49897 val_acc= 0.78028 time= 0.07203
Epoch: 0017 train_loss= 0.32282 train_acc= 0.88450 val_loss= 0.49694 val_acc= 0.77465 time= 0.07297
Epoch: 0018 train_loss= 0.29967 train_acc= 0.89247 val_loss= 0.49743 val_acc= 0.77746 time= 0.08306
Epoch: 0019 train_loss= 0.28383 train_acc= 0.89403 val_loss= 0.50058 val_acc= 0.77606 time= 0.07203
Epoch: 0020 train_loss= 0.26782 train_acc= 0.89950 val_loss= 0.50627 val_acc= 0.77746 time= 0.07301
Epoch: 0021 train_loss= 0.24954 train_acc= 0.90716 val_loss= 0.51372 val_acc= 0.77746 time= 0.07296
Early stopping...
Optimization Finished!
Test set results: cost= 0.51793 accuracy= 0.76027 time= 0.03604
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7597    0.7614    0.7605      1777
           1     0.7609    0.7591    0.7600      1777

    accuracy                         0.7603      3554
   macro avg     0.7603    0.7603    0.7603      3554
weighted avg     0.7603    0.7603    0.7603      3554

Macro average Test Precision, Recall and F1-Score...
(0.76027143695453, 0.7602701181767023, 0.7602698145025295, None)
Micro average Test Precision, Recall and F1-Score...
(0.7602701181767023, 0.7602701181767023, 0.7602701181767023, None)
embeddings:
18764 7108 3554
[[-0.01025208 -0.0063456   0.11443979 ... -0.01029687  0.0676851
   0.07241535]
 [ 0.10503863  0.15948102  0.03712029 ...  0.08561688  0.01677534
  -0.01412158]
 [-0.03106092 -0.04506435  0.1599292  ... -0.03887844  0.14547999
   0.15144037]
 ...
 [-0.08565667 -0.00431456  0.11401034 ... -0.00949908  0.15683326
  -0.00463255]
 [ 0.10856435  0.10038473  0.01179001 ...  0.09896323  0.00285868
   0.02642688]
 [ 0.1875665   0.18153156  0.11142948 ...  0.13450035  0.10966025
   0.10983362]]

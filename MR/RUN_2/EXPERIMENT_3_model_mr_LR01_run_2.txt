(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49312 val_loss= 0.68248 val_acc= 0.55352 time= 0.37984
Epoch: 0002 train_loss= 0.67341 train_acc= 0.59362 val_loss= 0.65954 val_acc= 0.53099 time= 0.08197
Epoch: 0003 train_loss= 0.60918 train_acc= 0.58190 val_loss= 0.58704 val_acc= 0.73803 time= 0.07400
Epoch: 0004 train_loss= 0.50653 train_acc= 0.86058 val_loss= 0.53348 val_acc= 0.73803 time= 0.08605
Epoch: 0005 train_loss= 0.39735 train_acc= 0.86949 val_loss= 0.51814 val_acc= 0.75634 time= 0.07200
Epoch: 0006 train_loss= 0.31670 train_acc= 0.87574 val_loss= 0.51559 val_acc= 0.76197 time= 0.07200
Epoch: 0007 train_loss= 0.24991 train_acc= 0.89903 val_loss= 0.54465 val_acc= 0.78310 time= 0.07181
Epoch: 0008 train_loss= 0.20320 train_acc= 0.91919 val_loss= 0.60318 val_acc= 0.77887 time= 0.07313
Epoch: 0009 train_loss= 0.17297 train_acc= 0.92998 val_loss= 0.67670 val_acc= 0.76761 time= 0.07300
Epoch: 0010 train_loss= 0.14437 train_acc= 0.94279 val_loss= 0.74160 val_acc= 0.76479 time= 0.09097
Epoch: 0011 train_loss= 0.11565 train_acc= 0.95624 val_loss= 0.82611 val_acc= 0.75211 time= 0.07400
Epoch: 0012 train_loss= 0.09543 train_acc= 0.96577 val_loss= 0.92062 val_acc= 0.75211 time= 0.07203
Early stopping...
Optimization Finished!
Test set results: cost= 0.91322 accuracy= 0.75127 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7339    0.7884    0.7602      1777
           1     0.7714    0.7141    0.7417      1777

    accuracy                         0.7513      3554
   macro avg     0.7527    0.7513    0.7509      3554
weighted avg     0.7527    0.7513    0.7509      3554

Macro average Test Precision, Recall and F1-Score...
(0.75266033076405, 0.7512661789532921, 0.750922583531983, None)
Micro average Test Precision, Recall and F1-Score...
(0.7512661789532921, 0.7512661789532921, 0.7512661789532921, None)
embeddings:
18764 7108 3554
[[ 0.23369583  0.08983381 -0.06953843 ... -0.04661844 -0.0344362
  -0.06216361]
 [-0.00807196 -0.11945192  0.33611965 ...  0.22903505 -0.16553912
   0.22482854]
 [ 0.2235819   0.24297777 -0.06452252 ... -0.08251127 -0.05751732
  -0.12926689]
 ...
 [-0.01213967  0.3561168  -0.00181594 ... -0.22726691 -0.00985837
  -0.3569066 ]
 [ 0.06030723 -0.04630259  0.26856774 ...  0.24763682 -0.17349462
   0.23747171]
 [ 0.20775183 -0.16123107  0.49651286 ...  0.34822083 -0.39271337
   0.44585624]]

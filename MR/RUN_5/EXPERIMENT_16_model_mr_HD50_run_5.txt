(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69313 train_acc= 0.50328 val_loss= 0.69186 val_acc= 0.71831 time= 0.29014
Epoch: 0002 train_loss= 0.69077 train_acc= 0.82151 val_loss= 0.68877 val_acc= 0.65493 time= 0.05810
Epoch: 0003 train_loss= 0.68560 train_acc= 0.75586 val_loss= 0.68483 val_acc= 0.63662 time= 0.05403
Epoch: 0004 train_loss= 0.67880 train_acc= 0.73804 val_loss= 0.67986 val_acc= 0.64507 time= 0.06900
Epoch: 0005 train_loss= 0.67013 train_acc= 0.75336 val_loss= 0.67378 val_acc= 0.65070 time= 0.05288
Epoch: 0006 train_loss= 0.65982 train_acc= 0.76321 val_loss= 0.66665 val_acc= 0.67183 time= 0.05400
Epoch: 0007 train_loss= 0.64769 train_acc= 0.79118 val_loss= 0.65850 val_acc= 0.68732 time= 0.05300
Epoch: 0008 train_loss= 0.63337 train_acc= 0.80681 val_loss= 0.64939 val_acc= 0.70845 time= 0.05300
Epoch: 0009 train_loss= 0.61727 train_acc= 0.82104 val_loss= 0.63937 val_acc= 0.72254 time= 0.07001
Epoch: 0010 train_loss= 0.60070 train_acc= 0.83510 val_loss= 0.62856 val_acc= 0.73662 time= 0.05278
Epoch: 0011 train_loss= 0.58132 train_acc= 0.84948 val_loss= 0.61705 val_acc= 0.74366 time= 0.05809
Epoch: 0012 train_loss= 0.56147 train_acc= 0.86168 val_loss= 0.60502 val_acc= 0.74225 time= 0.05400
Epoch: 0013 train_loss= 0.53985 train_acc= 0.87105 val_loss= 0.59267 val_acc= 0.74789 time= 0.05301
Epoch: 0014 train_loss= 0.51783 train_acc= 0.87699 val_loss= 0.58027 val_acc= 0.76056 time= 0.05399
Epoch: 0015 train_loss= 0.49541 train_acc= 0.88825 val_loss= 0.56817 val_acc= 0.77183 time= 0.05518
Epoch: 0016 train_loss= 0.47148 train_acc= 0.89450 val_loss= 0.55664 val_acc= 0.77324 time= 0.07201
Epoch: 0017 train_loss= 0.44958 train_acc= 0.89919 val_loss= 0.54594 val_acc= 0.76901 time= 0.05333
Epoch: 0018 train_loss= 0.42712 train_acc= 0.89747 val_loss= 0.53625 val_acc= 0.77465 time= 0.05303
Epoch: 0019 train_loss= 0.40320 train_acc= 0.90122 val_loss= 0.52775 val_acc= 0.77465 time= 0.05300
Epoch: 0020 train_loss= 0.38171 train_acc= 0.90497 val_loss= 0.52058 val_acc= 0.77465 time= 0.05403
Epoch: 0021 train_loss= 0.36299 train_acc= 0.90435 val_loss= 0.51485 val_acc= 0.77465 time= 0.06903
Epoch: 0022 train_loss= 0.34220 train_acc= 0.90794 val_loss= 0.51063 val_acc= 0.77606 time= 0.05297
Epoch: 0023 train_loss= 0.32495 train_acc= 0.90825 val_loss= 0.50797 val_acc= 0.77324 time= 0.05304
Epoch: 0024 train_loss= 0.30794 train_acc= 0.91247 val_loss= 0.50683 val_acc= 0.77042 time= 0.05296
Epoch: 0025 train_loss= 0.29022 train_acc= 0.91654 val_loss= 0.50712 val_acc= 0.77324 time= 0.05400
Epoch: 0026 train_loss= 0.27553 train_acc= 0.91935 val_loss= 0.50872 val_acc= 0.77465 time= 0.05500
Epoch: 0027 train_loss= 0.26291 train_acc= 0.92404 val_loss= 0.51156 val_acc= 0.77606 time= 0.06900
Epoch: 0028 train_loss= 0.24854 train_acc= 0.92466 val_loss= 0.51556 val_acc= 0.77465 time= 0.05800
Early stopping...
Optimization Finished!
Test set results: cost= 0.51192 accuracy= 0.75943 time= 0.02500
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7452    0.7884    0.7662      1777
           1     0.7754    0.7304    0.7522      1777

    accuracy                         0.7594      3554
   macro avg     0.7603    0.7594    0.7592      3554
weighted avg     0.7603    0.7594    0.7592      3554

Macro average Test Precision, Recall and F1-Score...
(0.7603005287373853, 0.7594259988745076, 0.7592237653172779, None)
Micro average Test Precision, Recall and F1-Score...
(0.7594259988745076, 0.7594259988745076, 0.7594259988745077, None)
embeddings:
18764 7108 3554
[[-7.1806107e-03  2.0451646e-01  1.7735493e-01 ...  1.1011021e-02
   1.6984372e-01  1.4376126e-02]
 [ 2.7348772e-01 -4.0488690e-04  8.7949215e-03 ...  2.5424781e-01
  -9.6974988e-03  2.6966929e-01]
 [-7.9607457e-02  2.9404733e-01  3.1780192e-01 ... -8.7189682e-02
   2.9268247e-01 -6.3953102e-02]
 ...
 [-9.6037313e-02  3.0197710e-01  2.5282496e-01 ... -6.8778679e-02
  -3.1059911e-03 -3.3310056e-02]
 [ 2.8570601e-01  4.2907432e-02  2.3473989e-02 ...  3.1856486e-01
   2.5143750e-02  2.5740606e-01]
 [ 5.2260166e-01  1.5141630e-01  1.6324736e-01 ...  5.3955352e-01
   1.4778346e-01  4.8846671e-01]]

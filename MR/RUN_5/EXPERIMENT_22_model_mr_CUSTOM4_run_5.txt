(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50547 val_loss= 0.69157 val_acc= 0.71268 time= 0.36754
Epoch: 0002 train_loss= 0.69033 train_acc= 0.79869 val_loss= 0.68752 val_acc= 0.69859 time= 0.07740
Epoch: 0003 train_loss= 0.68377 train_acc= 0.78993 val_loss= 0.68081 val_acc= 0.71268 time= 0.07608
Epoch: 0004 train_loss= 0.67289 train_acc= 0.79712 val_loss= 0.67152 val_acc= 0.73521 time= 0.07606
Epoch: 0005 train_loss= 0.65782 train_acc= 0.81010 val_loss= 0.65967 val_acc= 0.73380 time= 0.07697
Epoch: 0006 train_loss= 0.63815 train_acc= 0.82166 val_loss= 0.64533 val_acc= 0.74225 time= 0.08300
Epoch: 0007 train_loss= 0.61510 train_acc= 0.84198 val_loss= 0.62888 val_acc= 0.75211 time= 0.07300
Epoch: 0008 train_loss= 0.58745 train_acc= 0.85199 val_loss= 0.61082 val_acc= 0.75915 time= 0.07803
Epoch: 0009 train_loss= 0.55829 train_acc= 0.86136 val_loss= 0.59186 val_acc= 0.77042 time= 0.07200
Epoch: 0010 train_loss= 0.52576 train_acc= 0.86402 val_loss= 0.57282 val_acc= 0.77606 time= 0.08200
Epoch: 0011 train_loss= 0.49310 train_acc= 0.87090 val_loss= 0.55457 val_acc= 0.77887 time= 0.07297
Epoch: 0012 train_loss= 0.46024 train_acc= 0.87121 val_loss= 0.53796 val_acc= 0.78310 time= 0.07106
Epoch: 0013 train_loss= 0.42740 train_acc= 0.87512 val_loss= 0.52365 val_acc= 0.78028 time= 0.08012
Epoch: 0014 train_loss= 0.39906 train_acc= 0.87559 val_loss= 0.51222 val_acc= 0.77465 time= 0.07200
Epoch: 0015 train_loss= 0.36861 train_acc= 0.88356 val_loss= 0.50384 val_acc= 0.77324 time= 0.07252
Epoch: 0016 train_loss= 0.34543 train_acc= 0.88262 val_loss= 0.49829 val_acc= 0.77887 time= 0.07199
Epoch: 0017 train_loss= 0.32186 train_acc= 0.88559 val_loss= 0.49594 val_acc= 0.78028 time= 0.07299
Epoch: 0018 train_loss= 0.29958 train_acc= 0.89403 val_loss= 0.49691 val_acc= 0.78169 time= 0.08600
Epoch: 0019 train_loss= 0.28302 train_acc= 0.89606 val_loss= 0.50034 val_acc= 0.78310 time= 0.07101
Epoch: 0020 train_loss= 0.26141 train_acc= 0.90903 val_loss= 0.50604 val_acc= 0.78451 time= 0.07199
Epoch: 0021 train_loss= 0.24560 train_acc= 0.90872 val_loss= 0.51377 val_acc= 0.78451 time= 0.07302
Early stopping...
Optimization Finished!
Test set results: cost= 0.51519 accuracy= 0.76168 time= 0.03099
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7547    0.7755    0.7649      1777
           1     0.7691    0.7479    0.7583      1777

    accuracy                         0.7617      3554
   macro avg     0.7619    0.7617    0.7616      3554
weighted avg     0.7619    0.7617    0.7616      3554

Macro average Test Precision, Recall and F1-Score...
(0.7618761028964343, 0.7616769836803601, 0.7616316724492151, None)
Micro average Test Precision, Recall and F1-Score...
(0.7616769836803602, 0.7616769836803602, 0.7616769836803602, None)
embeddings:
18764 7108 3554
[[-0.00094471  0.01259572  0.03155883 ... -0.01990222  0.07797348
   0.01658115]
 [ 0.10006742  0.15742745  0.04257212 ...  0.07846496  0.02034331
   0.08765994]
 [-0.03329794 -0.04361641  0.17454216 ... -0.03927336  0.17150873
  -0.04223854]
 ...
 [-0.07459342 -0.08099786  0.18137553 ... -0.0500472   0.12753756
  -0.04602733]
 [ 0.09319795  0.10354941  0.00033944 ...  0.09246435  0.05228716
   0.13250872]
 [ 0.20884116  0.20620546  0.07993728 ...  0.14819244  0.08327058
   0.23083805]]

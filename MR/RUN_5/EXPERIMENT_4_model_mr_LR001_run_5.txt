(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50000 val_loss= 0.69197 val_acc= 0.72958 time= 0.37034
Epoch: 0002 train_loss= 0.69085 train_acc= 0.85027 val_loss= 0.68930 val_acc= 0.70563 time= 0.07997
Epoch: 0003 train_loss= 0.68638 train_acc= 0.82010 val_loss= 0.68560 val_acc= 0.70000 time= 0.08241
Epoch: 0004 train_loss= 0.68008 train_acc= 0.81854 val_loss= 0.68095 val_acc= 0.71268 time= 0.07299
Epoch: 0005 train_loss= 0.67218 train_acc= 0.82338 val_loss= 0.67522 val_acc= 0.71972 time= 0.07301
Epoch: 0006 train_loss= 0.66235 train_acc= 0.82838 val_loss= 0.66842 val_acc= 0.72958 time= 0.07400
Epoch: 0007 train_loss= 0.65094 train_acc= 0.84480 val_loss= 0.66058 val_acc= 0.74225 time= 0.07300
Epoch: 0008 train_loss= 0.63759 train_acc= 0.85152 val_loss= 0.65174 val_acc= 0.73944 time= 0.07199
Epoch: 0009 train_loss= 0.62253 train_acc= 0.85527 val_loss= 0.64197 val_acc= 0.74789 time= 0.07997
Epoch: 0010 train_loss= 0.60559 train_acc= 0.86433 val_loss= 0.63136 val_acc= 0.74930 time= 0.07211
Epoch: 0011 train_loss= 0.58750 train_acc= 0.86699 val_loss= 0.62006 val_acc= 0.75775 time= 0.07301
Epoch: 0012 train_loss= 0.56772 train_acc= 0.87762 val_loss= 0.60821 val_acc= 0.75915 time= 0.07900
Epoch: 0013 train_loss= 0.54721 train_acc= 0.88262 val_loss= 0.59603 val_acc= 0.76620 time= 0.07500
Epoch: 0014 train_loss= 0.52517 train_acc= 0.88653 val_loss= 0.58372 val_acc= 0.77183 time= 0.07228
Epoch: 0015 train_loss= 0.50291 train_acc= 0.88981 val_loss= 0.57156 val_acc= 0.77042 time= 0.07400
Epoch: 0016 train_loss= 0.48037 train_acc= 0.89247 val_loss= 0.55979 val_acc= 0.77465 time= 0.08097
Epoch: 0017 train_loss= 0.45703 train_acc= 0.89700 val_loss= 0.54864 val_acc= 0.77042 time= 0.07309
Epoch: 0018 train_loss= 0.43431 train_acc= 0.89981 val_loss= 0.53836 val_acc= 0.77465 time= 0.07310
Epoch: 0019 train_loss= 0.41294 train_acc= 0.90341 val_loss= 0.52913 val_acc= 0.77887 time= 0.07900
Epoch: 0020 train_loss= 0.39077 train_acc= 0.90278 val_loss= 0.52111 val_acc= 0.77887 time= 0.07300
Epoch: 0021 train_loss= 0.37074 train_acc= 0.90653 val_loss= 0.51442 val_acc= 0.77887 time= 0.07347
Epoch: 0022 train_loss= 0.35123 train_acc= 0.90763 val_loss= 0.50915 val_acc= 0.78169 time= 0.07910
Epoch: 0023 train_loss= 0.33266 train_acc= 0.90950 val_loss= 0.50533 val_acc= 0.78169 time= 0.07204
Epoch: 0024 train_loss= 0.31425 train_acc= 0.91185 val_loss= 0.50295 val_acc= 0.78169 time= 0.07300
Epoch: 0025 train_loss= 0.29748 train_acc= 0.91435 val_loss= 0.50200 val_acc= 0.78451 time= 0.07800
Epoch: 0026 train_loss= 0.28121 train_acc= 0.92013 val_loss= 0.50243 val_acc= 0.78451 time= 0.07200
Epoch: 0027 train_loss= 0.26825 train_acc= 0.91872 val_loss= 0.50420 val_acc= 0.78451 time= 0.07200
Epoch: 0028 train_loss= 0.25469 train_acc= 0.92326 val_loss= 0.50723 val_acc= 0.78451 time= 0.07300
Epoch: 0029 train_loss= 0.24199 train_acc= 0.92795 val_loss= 0.51136 val_acc= 0.78028 time= 0.07800
Early stopping...
Optimization Finished!
Test set results: cost= 0.51138 accuracy= 0.75858 time= 0.03099
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7456    0.7850    0.7648      1777
           1     0.7730    0.7321    0.7520      1777

    accuracy                         0.7586      3554
   macro avg     0.7593    0.7586    0.7584      3554
weighted avg     0.7593    0.7586    0.7584      3554

Macro average Test Precision, Recall and F1-Score...
(0.7593074772626444, 0.7585818795723129, 0.7584128764831153, None)
Micro average Test Precision, Recall and F1-Score...
(0.7585818795723129, 0.7585818795723129, 0.7585818795723129, None)
embeddings:
18764 7108 3554
[[-0.00414739 -0.00408417  0.09605841 ...  0.08718668 -0.00085868
   0.09069314]
 [ 0.0956338   0.09324776  0.00255537 ...  0.00685264  0.10865839
   0.01483069]
 [-0.03465372 -0.03489391  0.1522173  ...  0.15174532 -0.03339959
   0.14030518]
 ...
 [-0.05017284 -0.04083281  0.1543986  ...  0.14514595 -0.00792404
   0.12601131]
 [ 0.11991975  0.11353903  0.03896628 ...  0.02044273  0.1312229
   0.01828482]
 [ 0.20372617  0.19963807  0.1144956  ...  0.10416695  0.22272219
   0.09707139]]

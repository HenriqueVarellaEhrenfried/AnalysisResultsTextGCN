(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50672 val_loss= 0.69238 val_acc= 0.75493 time= 0.38989
Epoch: 0002 train_loss= 0.69181 train_acc= 0.80963 val_loss= 0.69084 val_acc= 0.74648 time= 0.10100
Epoch: 0003 train_loss= 0.68933 train_acc= 0.82088 val_loss= 0.68840 val_acc= 0.74648 time= 0.07600
Epoch: 0004 train_loss= 0.68544 train_acc= 0.83088 val_loss= 0.68517 val_acc= 0.75775 time= 0.08797
Epoch: 0005 train_loss= 0.68027 train_acc= 0.83182 val_loss= 0.68114 val_acc= 0.75775 time= 0.07300
Epoch: 0006 train_loss= 0.67390 train_acc= 0.84198 val_loss= 0.67632 val_acc= 0.75493 time= 0.07346
Epoch: 0007 train_loss= 0.66595 train_acc= 0.84183 val_loss= 0.67069 val_acc= 0.74930 time= 0.07298
Epoch: 0008 train_loss= 0.65683 train_acc= 0.84323 val_loss= 0.66425 val_acc= 0.75493 time= 0.07307
Epoch: 0009 train_loss= 0.64654 train_acc= 0.85105 val_loss= 0.65704 val_acc= 0.75915 time= 0.07700
Epoch: 0010 train_loss= 0.63476 train_acc= 0.84573 val_loss= 0.64910 val_acc= 0.76620 time= 0.07201
Epoch: 0011 train_loss= 0.62131 train_acc= 0.86058 val_loss= 0.64049 val_acc= 0.76761 time= 0.07300
Epoch: 0012 train_loss= 0.60761 train_acc= 0.85949 val_loss= 0.63128 val_acc= 0.76901 time= 0.07200
Epoch: 0013 train_loss= 0.59164 train_acc= 0.86730 val_loss= 0.62157 val_acc= 0.77042 time= 0.07340
Epoch: 0014 train_loss= 0.57429 train_acc= 0.86683 val_loss= 0.61148 val_acc= 0.77183 time= 0.08900
Epoch: 0015 train_loss= 0.55974 train_acc= 0.86402 val_loss= 0.60114 val_acc= 0.77183 time= 0.07200
Epoch: 0016 train_loss= 0.54142 train_acc= 0.87121 val_loss= 0.59067 val_acc= 0.77042 time= 0.07308
Epoch: 0017 train_loss= 0.52377 train_acc= 0.87215 val_loss= 0.58022 val_acc= 0.76901 time= 0.07597
Epoch: 0018 train_loss= 0.50390 train_acc= 0.87230 val_loss= 0.56991 val_acc= 0.76761 time= 0.07300
Epoch: 0019 train_loss= 0.48542 train_acc= 0.87043 val_loss= 0.55990 val_acc= 0.76620 time= 0.07303
Epoch: 0020 train_loss= 0.46854 train_acc= 0.87543 val_loss= 0.55035 val_acc= 0.76901 time= 0.07397
Epoch: 0021 train_loss= 0.45105 train_acc= 0.87918 val_loss= 0.54136 val_acc= 0.77183 time= 0.07280
Epoch: 0022 train_loss= 0.43112 train_acc= 0.88324 val_loss= 0.53309 val_acc= 0.77042 time= 0.07424
Epoch: 0023 train_loss= 0.41436 train_acc= 0.88184 val_loss= 0.52560 val_acc= 0.77606 time= 0.08584
Epoch: 0024 train_loss= 0.39636 train_acc= 0.88559 val_loss= 0.51896 val_acc= 0.77606 time= 0.07304
Epoch: 0025 train_loss= 0.38161 train_acc= 0.88637 val_loss= 0.51324 val_acc= 0.77606 time= 0.07201
Epoch: 0026 train_loss= 0.36481 train_acc= 0.88965 val_loss= 0.50845 val_acc= 0.77465 time= 0.07300
Epoch: 0027 train_loss= 0.35344 train_acc= 0.88934 val_loss= 0.50457 val_acc= 0.77465 time= 0.07311
Epoch: 0028 train_loss= 0.33743 train_acc= 0.89325 val_loss= 0.50155 val_acc= 0.77746 time= 0.08814
Epoch: 0029 train_loss= 0.32425 train_acc= 0.89731 val_loss= 0.49947 val_acc= 0.77746 time= 0.07299
Epoch: 0030 train_loss= 0.31362 train_acc= 0.89731 val_loss= 0.49828 val_acc= 0.78028 time= 0.07301
Epoch: 0031 train_loss= 0.30020 train_acc= 0.90231 val_loss= 0.49792 val_acc= 0.78310 time= 0.08138
Epoch: 0032 train_loss= 0.28976 train_acc= 0.90341 val_loss= 0.49840 val_acc= 0.78169 time= 0.07204
Epoch: 0033 train_loss= 0.27930 train_acc= 0.90653 val_loss= 0.49968 val_acc= 0.78028 time= 0.07207
Early stopping...
Optimization Finished!
Test set results: cost= 0.50377 accuracy= 0.75914 time= 0.03199
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7521    0.7732    0.7625      1777
           1     0.7666    0.7451    0.7557      1777

    accuracy                         0.7591      3554
   macro avg     0.7593    0.7591    0.7591      3554
weighted avg     0.7593    0.7591    0.7591      3554

Macro average Test Precision, Recall and F1-Score...
(0.7593499552647367, 0.7591446257737761, 0.7590969445415798, None)
Micro average Test Precision, Recall and F1-Score...
(0.7591446257737761, 0.7591446257737761, 0.759144625773776, None)
embeddings:
18764 7108 3554
[[ 0.0731096   0.07779881  0.0726254  ...  0.06803054  0.00841428
   0.00310834]
 [ 0.05070162  0.03089895  0.00401826 ...  0.01245317  0.06103817
   0.03810424]
 [ 0.13655278  0.13621043  0.11757565 ...  0.13961038 -0.03900228
  -0.02724116]
 ...
 [ 0.05252735  0.08284593  0.08981171 ...  0.06774008 -0.00685349
  -0.00972671]
 [ 0.01979536  0.02717874  0.01868698 ...  0.04636849  0.11005752
   0.05970191]
 [ 0.10845032  0.07592446  0.07356124 ...  0.09594966  0.14204247
   0.12830694]]

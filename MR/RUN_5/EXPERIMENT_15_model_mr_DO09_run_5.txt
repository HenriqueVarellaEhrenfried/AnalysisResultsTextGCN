(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.49375 val_loss= 0.69159 val_acc= 0.73803 time= 0.37979
Epoch: 0002 train_loss= 0.69043 train_acc= 0.82604 val_loss= 0.68759 val_acc= 0.73380 time= 0.07600
Epoch: 0003 train_loss= 0.68404 train_acc= 0.80963 val_loss= 0.68088 val_acc= 0.73662 time= 0.08417
Epoch: 0004 train_loss= 0.67321 train_acc= 0.82026 val_loss= 0.67152 val_acc= 0.74930 time= 0.07279
Epoch: 0005 train_loss= 0.65817 train_acc= 0.83823 val_loss= 0.65950 val_acc= 0.75775 time= 0.07300
Epoch: 0006 train_loss= 0.63787 train_acc= 0.84276 val_loss= 0.64500 val_acc= 0.75775 time= 0.07304
Epoch: 0007 train_loss= 0.61478 train_acc= 0.85355 val_loss= 0.62834 val_acc= 0.76761 time= 0.07316
Epoch: 0008 train_loss= 0.58696 train_acc= 0.85777 val_loss= 0.61011 val_acc= 0.76901 time= 0.07296
Epoch: 0009 train_loss= 0.55664 train_acc= 0.85855 val_loss= 0.59111 val_acc= 0.77183 time= 0.08603
Epoch: 0010 train_loss= 0.52577 train_acc= 0.86386 val_loss= 0.57213 val_acc= 0.77042 time= 0.07197
Epoch: 0011 train_loss= 0.49065 train_acc= 0.86746 val_loss= 0.55392 val_acc= 0.76901 time= 0.07500
Epoch: 0012 train_loss= 0.46015 train_acc= 0.87027 val_loss= 0.53739 val_acc= 0.76479 time= 0.07400
Epoch: 0013 train_loss= 0.42634 train_acc= 0.87512 val_loss= 0.52314 val_acc= 0.76479 time= 0.07312
Epoch: 0014 train_loss= 0.39778 train_acc= 0.87777 val_loss= 0.51172 val_acc= 0.76620 time= 0.08971
Epoch: 0015 train_loss= 0.36823 train_acc= 0.87918 val_loss= 0.50342 val_acc= 0.77042 time= 0.07109
Epoch: 0016 train_loss= 0.34346 train_acc= 0.88528 val_loss= 0.49829 val_acc= 0.77324 time= 0.07896
Epoch: 0017 train_loss= 0.31951 train_acc= 0.88621 val_loss= 0.49601 val_acc= 0.77606 time= 0.07209
Epoch: 0018 train_loss= 0.30120 train_acc= 0.89340 val_loss= 0.49711 val_acc= 0.78028 time= 0.07300
Epoch: 0019 train_loss= 0.28337 train_acc= 0.89450 val_loss= 0.50086 val_acc= 0.78169 time= 0.07204
Epoch: 0020 train_loss= 0.26511 train_acc= 0.90388 val_loss= 0.50593 val_acc= 0.77746 time= 0.07299
Epoch: 0021 train_loss= 0.24964 train_acc= 0.90763 val_loss= 0.51356 val_acc= 0.78169 time= 0.07212
Early stopping...
Optimization Finished!
Test set results: cost= 0.51725 accuracy= 0.76055 time= 0.04700
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7581    0.7653    0.7617      1777
           1     0.7631    0.7558    0.7594      1777

    accuracy                         0.7606      3554
   macro avg     0.7606    0.7606    0.7605      3554
weighted avg     0.7606    0.7606    0.7605      3554

Macro average Test Precision, Recall and F1-Score...
(0.7605753395155569, 0.7605514912774338, 0.760546012482199, None)
Micro average Test Precision, Recall and F1-Score...
(0.7605514912774338, 0.7605514912774338, 0.7605514912774338, None)
embeddings:
18764 7108 3554
[[ 0.00290908 -0.01038524  0.00928841 ...  0.00032538  0.04391818
   0.00528887]
 [ 0.11135855  0.11545382  0.09737799 ...  0.1283461   0.0065543
   0.11029663]
 [-0.05332689 -0.02996495 -0.05130544 ... -0.03863576  0.14101602
  -0.0437578 ]
 ...
 [-0.10377663 -0.05394591 -0.06551085 ... -0.05073228  0.05577127
  -0.07862517]
 [ 0.1098545   0.07803807  0.05871082 ...  0.09461511  0.03931631
   0.09630182]
 [ 0.19595101  0.20816645  0.18866079 ...  0.16088013  0.11424044
   0.18229155]]

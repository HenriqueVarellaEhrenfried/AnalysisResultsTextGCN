(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.48078 val_loss= 0.68051 val_acc= 0.52254 time= 0.37483
Epoch: 0002 train_loss= 0.66974 train_acc= 0.55017 val_loss= 0.67133 val_acc= 0.51408 time= 0.07504
Epoch: 0003 train_loss= 0.61545 train_acc= 0.55236 val_loss= 0.59975 val_acc= 0.70845 time= 0.07600
Epoch: 0004 train_loss= 0.52588 train_acc= 0.83698 val_loss= 0.56149 val_acc= 0.70141 time= 0.07607
Epoch: 0005 train_loss= 0.43775 train_acc= 0.83229 val_loss= 0.50643 val_acc= 0.76479 time= 0.09000
Epoch: 0006 train_loss= 0.33265 train_acc= 0.88434 val_loss= 0.51175 val_acc= 0.76056 time= 0.07231
Epoch: 0007 train_loss= 0.27482 train_acc= 0.88809 val_loss= 0.56125 val_acc= 0.74648 time= 0.07199
Epoch: 0008 train_loss= 0.23856 train_acc= 0.89747 val_loss= 0.57242 val_acc= 0.78169 time= 0.07802
Early stopping...
Optimization Finished!
Test set results: cost= 0.57189 accuracy= 0.76140 time= 0.03099
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7504    0.7833    0.7665      1777
           1     0.7734    0.7394    0.7560      1777

    accuracy                         0.7614      3554
   macro avg     0.7619    0.7614    0.7613      3554
weighted avg     0.7619    0.7614    0.7613      3554

Macro average Test Precision, Recall and F1-Score...
(0.7619002140152207, 0.7613956105796287, 0.7612806253580245, None)
Micro average Test Precision, Recall and F1-Score...
(0.7613956105796286, 0.7613956105796286, 0.7613956105796287, None)
embeddings:
18764 7108 3554
[[ 3.23731191e-02  8.45466480e-02 -2.59317122e-02 ...  1.16399415e-01
   1.77431896e-01 -2.94261612e-04]
 [-3.81600820e-02  7.05674943e-03 -8.46248493e-02 ... -4.49437741e-03
   1.68165714e-02  1.22822352e-01]
 [ 2.28719369e-01  2.13696331e-01 -5.44477254e-02 ...  2.16158926e-01
   2.33153880e-01 -9.20967534e-02]
 ...
 [ 3.55969727e-01 -1.34758893e-02 -7.07058981e-02 ...  4.10336018e-01
  -7.54756434e-03  2.29752855e-03]
 [ 8.27534590e-03 -2.84606256e-02 -9.35668945e-02 ...  5.01296367e-04
   2.10299548e-02  1.39004633e-01]
 [ 8.80114362e-03  5.73223047e-02 -2.20318720e-01 ...  1.22708138e-02
   6.24813847e-02  2.73676395e-01]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50141 val_loss= 0.69162 val_acc= 0.72676 time= 0.37217
Epoch: 0002 train_loss= 0.69046 train_acc= 0.80197 val_loss= 0.68770 val_acc= 0.70986 time= 0.08800
Epoch: 0003 train_loss= 0.68398 train_acc= 0.78712 val_loss= 0.68104 val_acc= 0.72817 time= 0.07580
Epoch: 0004 train_loss= 0.67338 train_acc= 0.80916 val_loss= 0.67179 val_acc= 0.73662 time= 0.07500
Epoch: 0005 train_loss= 0.65827 train_acc= 0.82198 val_loss= 0.65989 val_acc= 0.73662 time= 0.08399
Epoch: 0006 train_loss= 0.63871 train_acc= 0.83761 val_loss= 0.64550 val_acc= 0.75352 time= 0.07300
Epoch: 0007 train_loss= 0.61565 train_acc= 0.84745 val_loss= 0.62900 val_acc= 0.76056 time= 0.07300
Epoch: 0008 train_loss= 0.58712 train_acc= 0.85449 val_loss= 0.61092 val_acc= 0.77183 time= 0.07600
Epoch: 0009 train_loss= 0.55819 train_acc= 0.85902 val_loss= 0.59194 val_acc= 0.76901 time= 0.07300
Epoch: 0010 train_loss= 0.52731 train_acc= 0.86261 val_loss= 0.57300 val_acc= 0.77042 time= 0.08400
Epoch: 0011 train_loss= 0.49213 train_acc= 0.86949 val_loss= 0.55480 val_acc= 0.77324 time= 0.07200
Epoch: 0012 train_loss= 0.46325 train_acc= 0.86793 val_loss= 0.53823 val_acc= 0.77324 time= 0.07200
Epoch: 0013 train_loss= 0.42972 train_acc= 0.87418 val_loss= 0.52393 val_acc= 0.77183 time= 0.07600
Epoch: 0014 train_loss= 0.39933 train_acc= 0.87434 val_loss= 0.51244 val_acc= 0.77042 time= 0.08307
Epoch: 0015 train_loss= 0.36988 train_acc= 0.88278 val_loss= 0.50403 val_acc= 0.77183 time= 0.07200
Epoch: 0016 train_loss= 0.34398 train_acc= 0.88872 val_loss= 0.49870 val_acc= 0.77887 time= 0.07199
Epoch: 0017 train_loss= 0.32114 train_acc= 0.88856 val_loss= 0.49656 val_acc= 0.77606 time= 0.07297
Epoch: 0018 train_loss= 0.29872 train_acc= 0.89481 val_loss= 0.49728 val_acc= 0.77746 time= 0.08304
Epoch: 0019 train_loss= 0.28437 train_acc= 0.89387 val_loss= 0.50045 val_acc= 0.77606 time= 0.07700
Epoch: 0020 train_loss= 0.26548 train_acc= 0.90372 val_loss= 0.50611 val_acc= 0.77606 time= 0.07102
Epoch: 0021 train_loss= 0.24794 train_acc= 0.90638 val_loss= 0.51379 val_acc= 0.78028 time= 0.07199
Early stopping...
Optimization Finished!
Test set results: cost= 0.51735 accuracy= 0.76140 time= 0.03300
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7568    0.7704    0.7635      1777
           1     0.7662    0.7524    0.7592      1777

    accuracy                         0.7614      3554
   macro avg     0.7615    0.7614    0.7614      3554
weighted avg     0.7615    0.7614    0.7614      3554

Macro average Test Precision, Recall and F1-Score...
(0.7614804044090278, 0.7613956105796286, 0.7613762651335421, None)
Micro average Test Precision, Recall and F1-Score...
(0.7613956105796286, 0.7613956105796286, 0.7613956105796287, None)
embeddings:
18764 7108 3554
[[ 6.3916355e-02  7.6168545e-02  7.5597897e-02 ...  2.3270035e-03
   1.2965469e-02  1.0991574e-01]
 [ 4.3088803e-05  5.9481934e-02  8.1668198e-03 ...  1.0174555e-01
   1.3270749e-01  4.6223826e-03]
 [ 1.3857745e-01  1.6035014e-01  1.5429090e-01 ... -3.9938360e-02
  -3.7002049e-02  1.4805570e-01]
 ...
 [-1.3904256e-04 -1.3159732e-02 -3.1626553e-03 ... -9.9443421e-02
  -9.4910376e-02 -3.2536946e-03]
 [ 2.6554395e-02  2.5125068e-02  5.5953614e-02 ...  9.2527568e-02
   8.9649901e-02  3.9729964e-02]
 [ 1.1790481e-01  8.9338630e-02  6.7216896e-02 ...  1.7500331e-01
   1.8644829e-01  8.8022903e-02]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50328 val_loss= 0.69149 val_acc= 0.74366 time= 0.37088
Epoch: 0002 train_loss= 0.69032 train_acc= 0.80447 val_loss= 0.68740 val_acc= 0.73521 time= 0.07912
Epoch: 0003 train_loss= 0.68384 train_acc= 0.80306 val_loss= 0.68056 val_acc= 0.74085 time= 0.08697
Epoch: 0004 train_loss= 0.67250 train_acc= 0.81776 val_loss= 0.67101 val_acc= 0.74225 time= 0.07800
Epoch: 0005 train_loss= 0.65789 train_acc= 0.82260 val_loss= 0.65879 val_acc= 0.74789 time= 0.07575
Epoch: 0006 train_loss= 0.63747 train_acc= 0.83917 val_loss= 0.64405 val_acc= 0.75493 time= 0.08503
Epoch: 0007 train_loss= 0.61463 train_acc= 0.84214 val_loss= 0.62720 val_acc= 0.76338 time= 0.07297
Epoch: 0008 train_loss= 0.58633 train_acc= 0.84933 val_loss= 0.60881 val_acc= 0.76338 time= 0.07203
Epoch: 0009 train_loss= 0.55556 train_acc= 0.85980 val_loss= 0.58966 val_acc= 0.76620 time= 0.07100
Epoch: 0010 train_loss= 0.52243 train_acc= 0.86121 val_loss= 0.57053 val_acc= 0.77183 time= 0.07201
Epoch: 0011 train_loss= 0.49244 train_acc= 0.86949 val_loss= 0.55232 val_acc= 0.76901 time= 0.08300
Epoch: 0012 train_loss= 0.45775 train_acc= 0.87043 val_loss= 0.53581 val_acc= 0.76761 time= 0.07100
Epoch: 0013 train_loss= 0.42529 train_acc= 0.87340 val_loss= 0.52156 val_acc= 0.76901 time= 0.07212
Epoch: 0014 train_loss= 0.39573 train_acc= 0.87699 val_loss= 0.51007 val_acc= 0.77324 time= 0.07205
Epoch: 0015 train_loss= 0.36935 train_acc= 0.87996 val_loss= 0.50172 val_acc= 0.77606 time= 0.08597
Epoch: 0016 train_loss= 0.34153 train_acc= 0.88356 val_loss= 0.49657 val_acc= 0.77746 time= 0.07400
Epoch: 0017 train_loss= 0.32066 train_acc= 0.88746 val_loss= 0.49442 val_acc= 0.78028 time= 0.07200
Epoch: 0018 train_loss= 0.29851 train_acc= 0.89372 val_loss= 0.49530 val_acc= 0.78028 time= 0.08700
Epoch: 0019 train_loss= 0.28232 train_acc= 0.89372 val_loss= 0.49848 val_acc= 0.77606 time= 0.07358
Epoch: 0020 train_loss= 0.26094 train_acc= 0.90106 val_loss= 0.50405 val_acc= 0.77606 time= 0.07195
Epoch: 0021 train_loss= 0.24868 train_acc= 0.91060 val_loss= 0.51165 val_acc= 0.78310 time= 0.08208
Early stopping...
Optimization Finished!
Test set results: cost= 0.51673 accuracy= 0.76252 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7587    0.7698    0.7642      1777
           1     0.7664    0.7552    0.7608      1777

    accuracy                         0.7625      3554
   macro avg     0.7626    0.7625    0.7625      3554
weighted avg     0.7626    0.7625    0.7625      3554

Macro average Test Precision, Recall and F1-Score...
(0.7625773149833088, 0.7625211029825549, 0.7625083925562777, None)
Micro average Test Precision, Recall and F1-Score...
(0.7625211029825548, 0.7625211029825548, 0.7625211029825548, None)
embeddings:
18764 7108 3554
[[-1.10642407e-02  7.49311671e-02  7.90449679e-02 ...  9.65293348e-02
   5.98228425e-02  6.11124374e-02]
 [ 1.02883570e-01  2.49294285e-02  3.05722170e-02 ...  2.28600949e-02
   1.23341847e-03  1.20627079e-02]
 [-5.67535423e-02  1.36586055e-01  1.54851705e-01 ...  1.70032784e-01
   1.63518548e-01  1.86378509e-01]
 ...
 [-1.09113231e-02 -1.04319053e-02 -9.18205362e-03 ...  1.30480155e-01
   1.20737828e-01 -7.31151886e-05]
 [ 1.07222684e-01  3.25824767e-02  1.09971603e-02 ...  3.54129560e-02
   4.76457477e-02  2.04595719e-02]
 [ 1.67852968e-01  9.92050990e-02  1.21199623e-01 ...  8.62591416e-02
   1.23809993e-01  8.85027051e-02]]

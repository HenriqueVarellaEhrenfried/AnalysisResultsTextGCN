(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69315 train_acc= 0.50203 val_loss= 0.69183 val_acc= 0.75352 time= 0.29223
Epoch: 0002 train_loss= 0.69083 train_acc= 0.85683 val_loss= 0.68871 val_acc= 0.75211 time= 0.07086
Epoch: 0003 train_loss= 0.68568 train_acc= 0.85027 val_loss= 0.68471 val_acc= 0.75634 time= 0.05383
Epoch: 0004 train_loss= 0.67919 train_acc= 0.85542 val_loss= 0.67967 val_acc= 0.75352 time= 0.05397
Epoch: 0005 train_loss= 0.67062 train_acc= 0.85792 val_loss= 0.67350 val_acc= 0.74789 time= 0.05400
Epoch: 0006 train_loss= 0.65994 train_acc= 0.85652 val_loss= 0.66622 val_acc= 0.74789 time= 0.06703
Epoch: 0007 train_loss= 0.64797 train_acc= 0.86168 val_loss= 0.65786 val_acc= 0.74789 time= 0.05301
Epoch: 0008 train_loss= 0.63395 train_acc= 0.86574 val_loss= 0.64853 val_acc= 0.74789 time= 0.05399
Epoch: 0009 train_loss= 0.61829 train_acc= 0.86324 val_loss= 0.63831 val_acc= 0.74930 time= 0.06300
Epoch: 0010 train_loss= 0.60040 train_acc= 0.86636 val_loss= 0.62729 val_acc= 0.74789 time= 0.05300
Epoch: 0011 train_loss= 0.58178 train_acc= 0.87293 val_loss= 0.61563 val_acc= 0.75493 time= 0.05300
Epoch: 0012 train_loss= 0.56092 train_acc= 0.87699 val_loss= 0.60349 val_acc= 0.75915 time= 0.05500
Epoch: 0013 train_loss= 0.54025 train_acc= 0.88371 val_loss= 0.59109 val_acc= 0.76479 time= 0.06500
Epoch: 0014 train_loss= 0.51799 train_acc= 0.88637 val_loss= 0.57864 val_acc= 0.76479 time= 0.05401
Epoch: 0015 train_loss= 0.49474 train_acc= 0.88762 val_loss= 0.56640 val_acc= 0.77042 time= 0.05400
Epoch: 0016 train_loss= 0.47160 train_acc= 0.89356 val_loss= 0.55469 val_acc= 0.77324 time= 0.05301
Epoch: 0017 train_loss= 0.44812 train_acc= 0.89372 val_loss= 0.54377 val_acc= 0.77324 time= 0.06499
Epoch: 0018 train_loss= 0.42620 train_acc= 0.89700 val_loss= 0.53388 val_acc= 0.77887 time= 0.05301
Epoch: 0019 train_loss= 0.40278 train_acc= 0.89997 val_loss= 0.52523 val_acc= 0.77887 time= 0.05400
Epoch: 0020 train_loss= 0.38080 train_acc= 0.90309 val_loss= 0.51796 val_acc= 0.77887 time= 0.05400
Epoch: 0021 train_loss= 0.36078 train_acc= 0.90481 val_loss= 0.51214 val_acc= 0.78028 time= 0.06599
Epoch: 0022 train_loss= 0.34018 train_acc= 0.91122 val_loss= 0.50783 val_acc= 0.78169 time= 0.05300
Epoch: 0023 train_loss= 0.32458 train_acc= 0.91044 val_loss= 0.50505 val_acc= 0.77887 time= 0.05500
Epoch: 0024 train_loss= 0.30717 train_acc= 0.91013 val_loss= 0.50380 val_acc= 0.78028 time= 0.05400
Epoch: 0025 train_loss= 0.29141 train_acc= 0.91419 val_loss= 0.50405 val_acc= 0.77887 time= 0.06499
Epoch: 0026 train_loss= 0.27524 train_acc= 0.91904 val_loss= 0.50567 val_acc= 0.77887 time= 0.05301
Epoch: 0027 train_loss= 0.26260 train_acc= 0.91779 val_loss= 0.50849 val_acc= 0.78028 time= 0.05800
Epoch: 0028 train_loss= 0.24937 train_acc= 0.91997 val_loss= 0.51260 val_acc= 0.77746 time= 0.05299
Early stopping...
Optimization Finished!
Test set results: cost= 0.51270 accuracy= 0.76083 time= 0.02501
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7485    0.7856    0.7666      1777
           1     0.7744    0.7361    0.7548      1777

    accuracy                         0.7608      3554
   macro avg     0.7615    0.7608    0.7607      3554
weighted avg     0.7615    0.7608    0.7607      3554

Macro average Test Precision, Recall and F1-Score...
(0.7614741022576298, 0.7608328643781654, 0.7606861413280275, None)
Micro average Test Precision, Recall and F1-Score...
(0.7608328643781654, 0.7608328643781654, 0.7608328643781654, None)
embeddings:
18764 7108 3554
[[-0.00571555 -0.00955364 -0.00719241 ...  0.19309539 -0.00516514
   0.19313994]
 [ 0.24279924  0.21117994  0.26962328 ...  0.01993198  0.26302606
  -0.00310332]
 [-0.06739829 -0.0635078  -0.09371681 ...  0.30034918 -0.08253534
   0.30940104]
 ...
 [-0.10565573 -0.00799482 -0.06242872 ... -0.00904878 -0.06485451
   0.25230083]
 [ 0.24214981  0.192441    0.2858419  ...  0.03379137  0.29134345
   0.05522556]
 [ 0.44182193  0.3544062   0.4970263  ...  0.20538169  0.47719532
   0.18927406]]

(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.49734 val_loss= 0.69070 val_acc= 0.74366 time= 0.37581
Epoch: 0002 train_loss= 0.68857 train_acc= 0.84573 val_loss= 0.68405 val_acc= 0.69014 time= 0.08116
Epoch: 0003 train_loss= 0.67748 train_acc= 0.79853 val_loss= 0.67447 val_acc= 0.70000 time= 0.07600
Epoch: 0004 train_loss= 0.66098 train_acc= 0.80572 val_loss= 0.66159 val_acc= 0.71972 time= 0.07505
Epoch: 0005 train_loss= 0.63873 train_acc= 0.82729 val_loss= 0.64541 val_acc= 0.73944 time= 0.07600
Epoch: 0006 train_loss= 0.61101 train_acc= 0.85183 val_loss= 0.62641 val_acc= 0.74789 time= 0.07200
Epoch: 0007 train_loss= 0.57823 train_acc= 0.86293 val_loss= 0.60538 val_acc= 0.75915 time= 0.07300
Epoch: 0008 train_loss= 0.54159 train_acc= 0.87574 val_loss= 0.58338 val_acc= 0.76338 time= 0.07400
Epoch: 0009 train_loss= 0.50288 train_acc= 0.88746 val_loss= 0.56178 val_acc= 0.77324 time= 0.07600
Epoch: 0010 train_loss= 0.46174 train_acc= 0.89465 val_loss= 0.54198 val_acc= 0.77324 time= 0.07299
Epoch: 0011 train_loss= 0.42193 train_acc= 0.89653 val_loss= 0.52522 val_acc= 0.77324 time= 0.07400
Epoch: 0012 train_loss= 0.38370 train_acc= 0.89981 val_loss= 0.51254 val_acc= 0.77465 time= 0.07600
Epoch: 0013 train_loss= 0.34821 train_acc= 0.90278 val_loss= 0.50449 val_acc= 0.77887 time= 0.07299
Epoch: 0014 train_loss= 0.31593 train_acc= 0.90731 val_loss= 0.50131 val_acc= 0.77746 time= 0.07903
Epoch: 0015 train_loss= 0.28718 train_acc= 0.91060 val_loss= 0.50284 val_acc= 0.77887 time= 0.07900
Epoch: 0016 train_loss= 0.26097 train_acc= 0.91638 val_loss= 0.50902 val_acc= 0.78028 time= 0.07601
Epoch: 0017 train_loss= 0.23844 train_acc= 0.92248 val_loss= 0.51932 val_acc= 0.77606 time= 0.07299
Epoch: 0018 train_loss= 0.21882 train_acc= 0.92810 val_loss= 0.53271 val_acc= 0.77465 time= 0.07300
Epoch: 0019 train_loss= 0.20058 train_acc= 0.93138 val_loss= 0.54877 val_acc= 0.78028 time= 0.07575
Epoch: 0020 train_loss= 0.18429 train_acc= 0.93857 val_loss= 0.56752 val_acc= 0.78169 time= 0.07306
Epoch: 0021 train_loss= 0.17030 train_acc= 0.94420 val_loss= 0.58822 val_acc= 0.77746 time= 0.07403
Epoch: 0022 train_loss= 0.15694 train_acc= 0.94764 val_loss= 0.61086 val_acc= 0.77465 time= 0.07397
Epoch: 0023 train_loss= 0.14447 train_acc= 0.95045 val_loss= 0.63494 val_acc= 0.77183 time= 0.07303
Epoch: 0024 train_loss= 0.13292 train_acc= 0.95733 val_loss= 0.65905 val_acc= 0.76761 time= 0.07307
Epoch: 0025 train_loss= 0.12282 train_acc= 0.96108 val_loss= 0.68380 val_acc= 0.75634 time= 0.07503
Epoch: 0026 train_loss= 0.11333 train_acc= 0.96264 val_loss= 0.70943 val_acc= 0.74648 time= 0.07303
Epoch: 0027 train_loss= 0.10540 train_acc= 0.96718 val_loss= 0.73619 val_acc= 0.74648 time= 0.07405
Epoch: 0028 train_loss= 0.09761 train_acc= 0.97108 val_loss= 0.76375 val_acc= 0.74507 time= 0.08000
Epoch: 0029 train_loss= 0.08894 train_acc= 0.97515 val_loss= 0.79210 val_acc= 0.73803 time= 0.07754
Epoch: 0030 train_loss= 0.08352 train_acc= 0.97781 val_loss= 0.81822 val_acc= 0.73944 time= 0.07203
Epoch: 0031 train_loss= 0.07768 train_acc= 0.98062 val_loss= 0.84344 val_acc= 0.73521 time= 0.07538
Epoch: 0032 train_loss= 0.07127 train_acc= 0.98328 val_loss= 0.86957 val_acc= 0.73521 time= 0.07586
Epoch: 0033 train_loss= 0.06654 train_acc= 0.98468 val_loss= 0.89602 val_acc= 0.73239 time= 0.07403
Epoch: 0034 train_loss= 0.06181 train_acc= 0.98593 val_loss= 0.92170 val_acc= 0.72817 time= 0.07320
Epoch: 0035 train_loss= 0.05748 train_acc= 0.98812 val_loss= 0.94649 val_acc= 0.72676 time= 0.07429
Epoch: 0036 train_loss= 0.05305 train_acc= 0.99015 val_loss= 0.97030 val_acc= 0.72394 time= 0.07600
Epoch: 0037 train_loss= 0.05015 train_acc= 0.98968 val_loss= 0.99311 val_acc= 0.72394 time= 0.07300
Epoch: 0038 train_loss= 0.04626 train_acc= 0.99297 val_loss= 1.01485 val_acc= 0.72394 time= 0.07401
Epoch: 0039 train_loss= 0.04348 train_acc= 0.99265 val_loss= 1.03646 val_acc= 0.72535 time= 0.07500
Epoch: 0040 train_loss= 0.04081 train_acc= 0.99312 val_loss= 1.05798 val_acc= 0.72535 time= 0.07399
Epoch: 0041 train_loss= 0.03858 train_acc= 0.99406 val_loss= 1.07925 val_acc= 0.73099 time= 0.07701
Epoch: 0042 train_loss= 0.03504 train_acc= 0.99562 val_loss= 1.10075 val_acc= 0.73521 time= 0.07297
Epoch: 0043 train_loss= 0.03312 train_acc= 0.99547 val_loss= 1.12267 val_acc= 0.73944 time= 0.07603
Epoch: 0044 train_loss= 0.03247 train_acc= 0.99515 val_loss= 1.13922 val_acc= 0.74366 time= 0.07401
Epoch: 0045 train_loss= 0.02926 train_acc= 0.99625 val_loss= 1.15524 val_acc= 0.73521 time= 0.07507
Epoch: 0046 train_loss= 0.02794 train_acc= 0.99672 val_loss= 1.17308 val_acc= 0.73239 time= 0.07597
Epoch: 0047 train_loss= 0.02655 train_acc= 0.99719 val_loss= 1.19151 val_acc= 0.73662 time= 0.07400
Epoch: 0048 train_loss= 0.02472 train_acc= 0.99781 val_loss= 1.21075 val_acc= 0.74225 time= 0.07403
Epoch: 0049 train_loss= 0.02359 train_acc= 0.99828 val_loss= 1.23072 val_acc= 0.74648 time= 0.07297
Epoch: 0050 train_loss= 0.02214 train_acc= 0.99828 val_loss= 1.24788 val_acc= 0.74225 time= 0.07428
Epoch: 0051 train_loss= 0.02084 train_acc= 0.99812 val_loss= 1.26229 val_acc= 0.74085 time= 0.07699
Epoch: 0052 train_loss= 0.01951 train_acc= 0.99875 val_loss= 1.27641 val_acc= 0.73521 time= 0.07502
Epoch: 0053 train_loss= 0.01914 train_acc= 0.99859 val_loss= 1.29204 val_acc= 0.73380 time= 0.07298
Epoch: 0054 train_loss= 0.01869 train_acc= 0.99812 val_loss= 1.30906 val_acc= 0.74225 time= 0.07600
Epoch: 0055 train_loss= 0.01680 train_acc= 0.99906 val_loss= 1.32660 val_acc= 0.74225 time= 0.07800
Epoch: 0056 train_loss= 0.01653 train_acc= 0.99906 val_loss= 1.34296 val_acc= 0.74085 time= 0.07700
Epoch: 0057 train_loss= 0.01585 train_acc= 0.99906 val_loss= 1.35729 val_acc= 0.74085 time= 0.07500
Epoch: 0058 train_loss= 0.01479 train_acc= 0.99906 val_loss= 1.36978 val_acc= 0.74085 time= 0.07308
Epoch: 0059 train_loss= 0.01481 train_acc= 0.99906 val_loss= 1.38216 val_acc= 0.74225 time= 0.07403
Epoch: 0060 train_loss= 0.01367 train_acc= 0.99969 val_loss= 1.39538 val_acc= 0.74085 time= 0.07501
Epoch: 0061 train_loss= 0.01337 train_acc= 0.99953 val_loss= 1.40990 val_acc= 0.74507 time= 0.07301
Epoch: 0062 train_loss= 0.01279 train_acc= 0.99953 val_loss= 1.42336 val_acc= 0.74225 time= 0.07300
Epoch: 0063 train_loss= 0.01222 train_acc= 0.99984 val_loss= 1.43635 val_acc= 0.74225 time= 0.07200
Epoch: 0064 train_loss= 0.01186 train_acc= 0.99984 val_loss= 1.44853 val_acc= 0.74225 time= 0.07496
Epoch: 0065 train_loss= 0.01158 train_acc= 0.99969 val_loss= 1.45913 val_acc= 0.74225 time= 0.07300
Epoch: 0066 train_loss= 0.01056 train_acc= 0.99969 val_loss= 1.46946 val_acc= 0.74225 time= 0.07403
Epoch: 0067 train_loss= 0.01061 train_acc= 1.00000 val_loss= 1.47994 val_acc= 0.74085 time= 0.07497
Epoch: 0068 train_loss= 0.00999 train_acc= 1.00000 val_loss= 1.49071 val_acc= 0.74085 time= 0.07803
Epoch: 0069 train_loss= 0.00964 train_acc= 0.99984 val_loss= 1.50191 val_acc= 0.74085 time= 0.07196
Epoch: 0070 train_loss= 0.00942 train_acc= 0.99984 val_loss= 1.51340 val_acc= 0.74085 time= 0.07604
Epoch: 0071 train_loss= 0.00914 train_acc= 1.00000 val_loss= 1.52466 val_acc= 0.74085 time= 0.07504
Epoch: 0072 train_loss= 0.00888 train_acc= 0.99984 val_loss= 1.53419 val_acc= 0.74085 time= 0.07400
Epoch: 0073 train_loss= 0.00863 train_acc= 0.99984 val_loss= 1.54318 val_acc= 0.73944 time= 0.07500
Epoch: 0074 train_loss= 0.00862 train_acc= 0.99984 val_loss= 1.55131 val_acc= 0.73944 time= 0.07300
Epoch: 0075 train_loss= 0.00814 train_acc= 0.99984 val_loss= 1.55975 val_acc= 0.73944 time= 0.07412
Epoch: 0076 train_loss= 0.00789 train_acc= 1.00000 val_loss= 1.56834 val_acc= 0.73803 time= 0.07504
Epoch: 0077 train_loss= 0.00769 train_acc= 1.00000 val_loss= 1.57749 val_acc= 0.73803 time= 0.07197
Epoch: 0078 train_loss= 0.00793 train_acc= 0.99984 val_loss= 1.58817 val_acc= 0.73944 time= 0.07403
Epoch: 0079 train_loss= 0.00720 train_acc= 0.99984 val_loss= 1.59954 val_acc= 0.73521 time= 0.07597
Epoch: 0080 train_loss= 0.00712 train_acc= 1.00000 val_loss= 1.60897 val_acc= 0.73662 time= 0.07310
Epoch: 0081 train_loss= 0.00692 train_acc= 1.00000 val_loss= 1.61697 val_acc= 0.73521 time= 0.07527
Epoch: 0082 train_loss= 0.00662 train_acc= 1.00000 val_loss= 1.62401 val_acc= 0.73662 time= 0.07707
Epoch: 0083 train_loss= 0.00648 train_acc= 0.99984 val_loss= 1.63088 val_acc= 0.73944 time= 0.07403
Epoch: 0084 train_loss= 0.00643 train_acc= 1.00000 val_loss= 1.63823 val_acc= 0.74085 time= 0.07400
Epoch: 0085 train_loss= 0.00625 train_acc= 1.00000 val_loss= 1.64582 val_acc= 0.73944 time= 0.07297
Epoch: 0086 train_loss= 0.00592 train_acc= 1.00000 val_loss= 1.65321 val_acc= 0.73944 time= 0.08004
Epoch: 0087 train_loss= 0.00611 train_acc= 0.99984 val_loss= 1.66114 val_acc= 0.73944 time= 0.07400
Epoch: 0088 train_loss= 0.00593 train_acc= 1.00000 val_loss= 1.66983 val_acc= 0.73803 time= 0.07400
Epoch: 0089 train_loss= 0.00585 train_acc= 1.00000 val_loss= 1.67822 val_acc= 0.73521 time= 0.07599
Epoch: 0090 train_loss= 0.00560 train_acc= 1.00000 val_loss= 1.68516 val_acc= 0.73662 time= 0.07297
Epoch: 0091 train_loss= 0.00534 train_acc= 1.00000 val_loss= 1.69169 val_acc= 0.73944 time= 0.07503
Epoch: 0092 train_loss= 0.00537 train_acc= 1.00000 val_loss= 1.69817 val_acc= 0.74085 time= 0.07300
Epoch: 0093 train_loss= 0.00521 train_acc= 1.00000 val_loss= 1.70436 val_acc= 0.74085 time= 0.07300
Epoch: 0094 train_loss= 0.00518 train_acc= 1.00000 val_loss= 1.71023 val_acc= 0.73944 time= 0.07600
Epoch: 0095 train_loss= 0.00503 train_acc= 0.99984 val_loss= 1.71658 val_acc= 0.73944 time= 0.07505
Epoch: 0096 train_loss= 0.00486 train_acc= 1.00000 val_loss= 1.72335 val_acc= 0.74085 time= 0.07437
Epoch: 0097 train_loss= 0.00475 train_acc= 1.00000 val_loss= 1.72973 val_acc= 0.73944 time= 0.07600
Epoch: 0098 train_loss= 0.00479 train_acc= 1.00000 val_loss= 1.73625 val_acc= 0.73944 time= 0.07199
Epoch: 0099 train_loss= 0.00458 train_acc= 1.00000 val_loss= 1.74256 val_acc= 0.73944 time= 0.07700
Epoch: 0100 train_loss= 0.00460 train_acc= 1.00000 val_loss= 1.74958 val_acc= 0.73944 time= 0.07600
Epoch: 0101 train_loss= 0.00445 train_acc= 1.00000 val_loss= 1.75581 val_acc= 0.73944 time= 0.07601
Epoch: 0102 train_loss= 0.00424 train_acc= 1.00000 val_loss= 1.76199 val_acc= 0.73944 time= 0.07508
Early stopping...
Optimization Finished!
Test set results: cost= 1.88340 accuracy= 0.72510 time= 0.03100
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7165    0.7451    0.7305      1777
           1     0.7345    0.7051    0.7195      1777

    accuracy                         0.7251      3554
   macro avg     0.7255    0.7251    0.7250      3554
weighted avg     0.7255    0.7251    0.7250      3554

Macro average Test Precision, Recall and F1-Score...
(0.7254584024806767, 0.7250984805852561, 0.724988723553813, None)
Micro average Test Precision, Recall and F1-Score...
(0.7250984805852561, 0.7250984805852561, 0.725098480585256, None)
embeddings:
18764 7108 3554
[[ 0.06133831  0.19443487  0.02938223 ...  0.1839352   0.15817298
   0.03485762]
 [ 0.19659331  0.09573534  0.15580395 ...  0.07353847  0.08085921
   0.23297592]
 [-0.06303219  0.32581177 -0.05510179 ...  0.34228373  0.32390594
  -0.06942342]
 ...
 [-0.07380334 -0.0057319  -0.05729482 ...  0.18911856  0.26891425
  -0.09666669]
 [ 0.28502187  0.03944734  0.29173356 ...  0.05561421  0.08861507
   0.32908806]
 [ 0.22756155  0.48558527  0.30684766 ...  0.43176052  0.4057068
   0.27225113]]

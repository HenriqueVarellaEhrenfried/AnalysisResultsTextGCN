(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69316 train_acc= 0.49093 val_loss= 0.68151 val_acc= 0.52676 time= 0.38604
Epoch: 0002 train_loss= 0.67140 train_acc= 0.55345 val_loss= 0.67307 val_acc= 0.51268 time= 0.07604
Epoch: 0003 train_loss= 0.61811 train_acc= 0.55173 val_loss= 0.60006 val_acc= 0.70282 time= 0.07600
Epoch: 0004 train_loss= 0.52658 train_acc= 0.82745 val_loss= 0.56292 val_acc= 0.69437 time= 0.07500
Epoch: 0005 train_loss= 0.43938 train_acc= 0.82088 val_loss= 0.51248 val_acc= 0.76901 time= 0.07600
Epoch: 0006 train_loss= 0.33744 train_acc= 0.87606 val_loss= 0.51134 val_acc= 0.76761 time= 0.07300
Epoch: 0007 train_loss= 0.27716 train_acc= 0.88684 val_loss= 0.55939 val_acc= 0.74507 time= 0.07505
Epoch: 0008 train_loss= 0.24676 train_acc= 0.89450 val_loss= 0.56356 val_acc= 0.78451 time= 0.07501
Epoch: 0009 train_loss= 0.19212 train_acc= 0.91826 val_loss= 0.63480 val_acc= 0.76901 time= 0.07299
Epoch: 0010 train_loss= 0.17890 train_acc= 0.92357 val_loss= 0.67307 val_acc= 0.78028 time= 0.07401
Epoch: 0011 train_loss= 0.14102 train_acc= 0.94436 val_loss= 0.75394 val_acc= 0.75352 time= 0.07600
Epoch: 0012 train_loss= 0.12551 train_acc= 0.95202 val_loss= 0.79889 val_acc= 0.76479 time= 0.07299
Early stopping...
Optimization Finished!
Test set results: cost= 0.78566 accuracy= 0.75521 time= 0.03301
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7466    0.7727    0.7594      1777
           1     0.7644    0.7378    0.7509      1777

    accuracy                         0.7552      3554
   macro avg     0.7555    0.7552    0.7551      3554
weighted avg     0.7555    0.7552    0.7551      3554

Macro average Test Precision, Recall and F1-Score...
(0.7555164503461604, 0.7552054023635341, 0.7551308806982331, None)
Micro average Test Precision, Recall and F1-Score...
(0.7552054023635341, 0.7552054023635341, 0.755205402363534, None)
embeddings:
18764 7108 3554
[[ 0.1725232   0.0084061  -0.00852532 ...  0.08892407  0.1664499
   0.05720093]
 [-0.00909954  0.01164494  0.18870336 ... -0.00584753 -0.01168734
  -0.04337269]
 [ 0.35222697  0.31768394 -0.07587    ...  0.26306668  0.39831445
   0.33418855]
 ...
 [-0.00419391  0.43715656 -0.00189386 ...  0.57946855  0.5055781
   0.5058832 ]
 [ 0.059497   -0.00422863  0.1114141  ... -0.01271396  0.06180789
   0.06700876]
 [ 0.27702108 -0.02965478  0.22270705 ... -0.06053821  0.25458264
   0.18513933]]

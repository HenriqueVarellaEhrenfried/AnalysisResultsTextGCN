(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)
29426
  (0, 10623)	6.18339884402439
  (0, 13919)	0.5465010749888104
  (0, 14559)	1.1115554606675069
  (0, 15700)	3.7569884009179986
  (0, 18226)	1.5195310253612764
  (0, 18467)	4.04333268052812
  (0, 19338)	7.328531148327393
  (0, 22286)	6.789534647594706
  (0, 23328)	1.9258537664551136
  (1, 7795)	5.147306912337615
  (1, 8008)	5.36241829195456
  (1, 9081)	2.030213781779356
  (1, 9548)	1.8885902192574975
  (1, 9842)	5.487127339313642
  (1, 10712)	7.077216720046487
  (1, 11167)	4.763581790865857
  (1, 11240)	1.9780280286087861
  (1, 12074)	8.581294116822761
  (1, 12204)	3.001693660045025
  (1, 13383)	8.175829008714597
  (1, 13384)	1.1367531124051018
  (1, 13919)	2.732505374944052
  (1, 14559)	0.5557777303337534
  (1, 15052)	2.8176716418105427
  (1, 15552)	3.1786167349504817
  :	:
  (29423, 20674)	1.2821726541119611
  (29423, 21548)	2.0777547265483562
  (29423, 21594)	4.71009310591487
  (29423, 22686)	1.3489223175957803
  (29423, 23714)	0.7656835847875705
  (29423, 24605)	6.18339884402439
  (29424, 9508)	7.665003384948606
  (29424, 12204)	1.5008468300225124
  (29424, 12719)	8.175829008714597
  (29424, 13792)	7.19499975570287
  (29424, 14440)	2.1419437457226627
  (29424, 14647)	4.23101618046346
  (29424, 14666)	6.384069539486542
  (29424, 14849)	2.730529451817914
  (29424, 15098)	3.941722504117337
  (29424, 15191)	8.175829008714597
  (29424, 16855)	7.665003384948606
  (29424, 18226)	1.5195310253612764
  (29424, 19164)	4.983981856234315
  (29424, 22983)	4.630050398241334
  (29424, 24152)	4.168495823482126
  (29425, 8194)	6.278709023828715
  (29425, 13921)	5.249089606647557
  (29425, 19699)	0.6764057181223682
  (29425, 22407)	4.855600689586108
(29426, 29426)
(29426, 29426)
29426
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 2), dtype=float32)
Epoch: 0001 train_loss= 0.69314 train_acc= 0.50719 val_loss= 0.69186 val_acc= 0.72817 time= 0.37304
Epoch: 0002 train_loss= 0.69067 train_acc= 0.84417 val_loss= 0.68901 val_acc= 0.71408 time= 0.07963
Epoch: 0003 train_loss= 0.68593 train_acc= 0.82260 val_loss= 0.68512 val_acc= 0.71972 time= 0.07699
Epoch: 0004 train_loss= 0.67930 train_acc= 0.82495 val_loss= 0.68025 val_acc= 0.72535 time= 0.07500
Epoch: 0005 train_loss= 0.67103 train_acc= 0.82870 val_loss= 0.67431 val_acc= 0.72113 time= 0.07600
Epoch: 0006 train_loss= 0.66105 train_acc= 0.83948 val_loss= 0.66728 val_acc= 0.73380 time= 0.07305
Epoch: 0007 train_loss= 0.64907 train_acc= 0.84573 val_loss= 0.65921 val_acc= 0.73944 time= 0.07401
Epoch: 0008 train_loss= 0.63546 train_acc= 0.85605 val_loss= 0.65013 val_acc= 0.74507 time= 0.07200
Epoch: 0009 train_loss= 0.62008 train_acc= 0.85871 val_loss= 0.64016 val_acc= 0.74789 time= 0.07600
Epoch: 0010 train_loss= 0.60313 train_acc= 0.86605 val_loss= 0.62940 val_acc= 0.75493 time= 0.07700
Epoch: 0011 train_loss= 0.58443 train_acc= 0.87121 val_loss= 0.61797 val_acc= 0.75915 time= 0.07300
Epoch: 0012 train_loss= 0.56411 train_acc= 0.87715 val_loss= 0.60605 val_acc= 0.76620 time= 0.07501
Epoch: 0013 train_loss= 0.54333 train_acc= 0.88309 val_loss= 0.59383 val_acc= 0.76901 time= 0.07700
Epoch: 0014 train_loss= 0.52141 train_acc= 0.88793 val_loss= 0.58155 val_acc= 0.76901 time= 0.07300
Epoch: 0015 train_loss= 0.49914 train_acc= 0.89012 val_loss= 0.56944 val_acc= 0.77324 time= 0.07407
Epoch: 0016 train_loss= 0.47654 train_acc= 0.89669 val_loss= 0.55777 val_acc= 0.77465 time= 0.07497
Epoch: 0017 train_loss= 0.45415 train_acc= 0.89778 val_loss= 0.54677 val_acc= 0.77183 time= 0.07803
Epoch: 0018 train_loss= 0.43148 train_acc= 0.90309 val_loss= 0.53665 val_acc= 0.77465 time= 0.07600
Epoch: 0019 train_loss= 0.40936 train_acc= 0.89997 val_loss= 0.52763 val_acc= 0.78028 time= 0.07297
Epoch: 0020 train_loss= 0.38737 train_acc= 0.90481 val_loss= 0.51983 val_acc= 0.77887 time= 0.07510
Epoch: 0021 train_loss= 0.36734 train_acc= 0.90356 val_loss= 0.51338 val_acc= 0.77887 time= 0.07499
Epoch: 0022 train_loss= 0.34831 train_acc= 0.90903 val_loss= 0.50836 val_acc= 0.78028 time= 0.07300
Epoch: 0023 train_loss= 0.32916 train_acc= 0.91091 val_loss= 0.50477 val_acc= 0.78028 time= 0.07497
Epoch: 0024 train_loss= 0.31156 train_acc= 0.91357 val_loss= 0.50262 val_acc= 0.78169 time= 0.07803
Epoch: 0025 train_loss= 0.29526 train_acc= 0.91482 val_loss= 0.50188 val_acc= 0.78310 time= 0.07503
Epoch: 0026 train_loss= 0.28073 train_acc= 0.91716 val_loss= 0.50251 val_acc= 0.78310 time= 0.07298
Epoch: 0027 train_loss= 0.26530 train_acc= 0.92216 val_loss= 0.50445 val_acc= 0.78592 time= 0.07401
Epoch: 0028 train_loss= 0.25225 train_acc= 0.92482 val_loss= 0.50753 val_acc= 0.78451 time= 0.07500
Epoch: 0029 train_loss= 0.24001 train_acc= 0.92920 val_loss= 0.51169 val_acc= 0.78451 time= 0.07300
Early stopping...
Optimization Finished!
Test set results: cost= 0.51191 accuracy= 0.75886 time= 0.03399
29426
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7481    0.7805    0.7640      1777
           1     0.7706    0.7372    0.7535      1777

    accuracy                         0.7589      3554
   macro avg     0.7594    0.7589    0.7588      3554
weighted avg     0.7594    0.7589    0.7588      3554

Macro average Test Precision, Recall and F1-Score...
(0.7593502125769402, 0.7588632526730444, 0.7587500090099104, None)
Micro average Test Precision, Recall and F1-Score...
(0.7588632526730444, 0.7588632526730444, 0.7588632526730444, None)
embeddings:
18764 7108 3554
[[ 0.09262485  0.00294835  0.09661734 ...  0.09067468  0.08750403
   0.07897812]
 [ 0.00554365  0.12859872  0.00205072 ...  0.00574549  0.00259688
   0.01748875]
 [ 0.15477747 -0.0447164   0.15282664 ...  0.13389212  0.14330085
   0.13616416]
 ...
 [ 0.15506749 -0.01022896  0.15855691 ...  0.16473784  0.16424482
   0.15273902]
 [ 0.02700821  0.13623041  0.02994153 ...  0.01420049  0.03170205
   0.02609393]
 [ 0.10060697  0.24222255  0.10399604 ...  0.10394399  0.08544686
   0.08844052]]

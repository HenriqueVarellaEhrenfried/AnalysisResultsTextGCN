(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95112 train_acc= 0.01174 val_loss= 3.89112 val_acc= 0.67228 time= 0.45089
Epoch: 0002 train_loss= 3.89107 train_acc= 0.64994 val_loss= 3.77948 val_acc= 0.66922 time= 0.16801
Epoch: 0003 train_loss= 3.77920 train_acc= 0.64603 val_loss= 3.60822 val_acc= 0.66922 time= 0.17196
Epoch: 0004 train_loss= 3.60872 train_acc= 0.64416 val_loss= 3.37934 val_acc= 0.67075 time= 0.20003
Epoch: 0005 train_loss= 3.37826 train_acc= 0.64569 val_loss= 3.10744 val_acc= 0.66922 time= 0.16900
Epoch: 0006 train_loss= 3.10884 train_acc= 0.64722 val_loss= 2.82102 val_acc= 0.67075 time= 0.17200
Epoch: 0007 train_loss= 2.81740 train_acc= 0.65164 val_loss= 2.55663 val_acc= 0.67075 time= 0.16800
Epoch: 0008 train_loss= 2.54937 train_acc= 0.65113 val_loss= 2.35538 val_acc= 0.65084 time= 0.18800
Epoch: 0009 train_loss= 2.34697 train_acc= 0.64093 val_loss= 2.23674 val_acc= 0.60337 time= 0.16997
Epoch: 0010 train_loss= 2.23070 train_acc= 0.59551 val_loss= 2.17655 val_acc= 0.51149 time= 0.18203
Epoch: 0011 train_loss= 2.17564 train_acc= 0.49974 val_loss= 2.13464 val_acc= 0.47933 time= 0.16800
Epoch: 0012 train_loss= 2.14080 train_acc= 0.45093 val_loss= 2.08328 val_acc= 0.46708 time= 0.17099
Epoch: 0013 train_loss= 2.09561 train_acc= 0.43885 val_loss= 2.01118 val_acc= 0.47014 time= 0.16980
Epoch: 0014 train_loss= 2.02883 train_acc= 0.43953 val_loss= 1.92160 val_acc= 0.48392 time= 0.20100
Epoch: 0015 train_loss= 1.94281 train_acc= 0.45926 val_loss= 1.82661 val_acc= 0.52374 time= 0.17600
Epoch: 0016 train_loss= 1.85574 train_acc= 0.51352 val_loss= 1.74032 val_acc= 0.59571 time= 0.17000
Epoch: 0017 train_loss= 1.76607 train_acc= 0.59245 val_loss= 1.67057 val_acc= 0.65084 time= 0.16905
Epoch: 0018 train_loss= 1.69980 train_acc= 0.64178 val_loss= 1.61474 val_acc= 0.67228 time= 0.17100
Epoch: 0019 train_loss= 1.64497 train_acc= 0.65130 val_loss= 1.56465 val_acc= 0.67841 time= 0.17154
Epoch: 0020 train_loss= 1.59229 train_acc= 0.65725 val_loss= 1.51433 val_acc= 0.68147 time= 0.20183
Epoch: 0021 train_loss= 1.54116 train_acc= 0.66015 val_loss= 1.46285 val_acc= 0.67841 time= 0.17500
Epoch: 0022 train_loss= 1.48982 train_acc= 0.66270 val_loss= 1.41220 val_acc= 0.68147 time= 0.17000
Epoch: 0023 train_loss= 1.43759 train_acc= 0.66678 val_loss= 1.36476 val_acc= 0.68606 time= 0.16713
Epoch: 0024 train_loss= 1.38919 train_acc= 0.67239 val_loss= 1.32177 val_acc= 0.69066 time= 0.16896
Epoch: 0025 train_loss= 1.34496 train_acc= 0.67920 val_loss= 1.28337 val_acc= 0.70291 time= 0.20300
Epoch: 0026 train_loss= 1.30658 train_acc= 0.68617 val_loss= 1.24893 val_acc= 0.71210 time= 0.17000
Epoch: 0027 train_loss= 1.27171 train_acc= 0.69553 val_loss= 1.21760 val_acc= 0.71822 time= 0.17400
Epoch: 0028 train_loss= 1.23918 train_acc= 0.70794 val_loss= 1.18850 val_acc= 0.72282 time= 0.17100
Epoch: 0029 train_loss= 1.20721 train_acc= 0.71951 val_loss= 1.16079 val_acc= 0.72588 time= 0.16900
Epoch: 0030 train_loss= 1.17732 train_acc= 0.72870 val_loss= 1.13374 val_acc= 0.72741 time= 0.16904
Epoch: 0031 train_loss= 1.14826 train_acc= 0.74043 val_loss= 1.10685 val_acc= 0.73507 time= 0.19999
Epoch: 0032 train_loss= 1.12067 train_acc= 0.74826 val_loss= 1.07996 val_acc= 0.74426 time= 0.17495
Epoch: 0033 train_loss= 1.09104 train_acc= 0.75438 val_loss= 1.05308 val_acc= 0.75038 time= 0.16605
Epoch: 0034 train_loss= 1.06193 train_acc= 0.76237 val_loss= 1.02647 val_acc= 0.76570 time= 0.17095
Epoch: 0035 train_loss= 1.03628 train_acc= 0.76867 val_loss= 1.00050 val_acc= 0.77182 time= 0.17000
Epoch: 0036 train_loss= 1.00866 train_acc= 0.77445 val_loss= 0.97541 val_acc= 0.77489 time= 0.17205
Epoch: 0037 train_loss= 0.98304 train_acc= 0.78075 val_loss= 0.95130 val_acc= 0.77948 time= 0.19797
Epoch: 0038 train_loss= 0.95787 train_acc= 0.78857 val_loss= 0.92805 val_acc= 0.79326 time= 0.16704
Epoch: 0039 train_loss= 0.93377 train_acc= 0.79775 val_loss= 0.90547 val_acc= 0.80245 time= 0.16798
Epoch: 0040 train_loss= 0.90913 train_acc= 0.80490 val_loss= 0.88328 val_acc= 0.80704 time= 0.16605
Epoch: 0041 train_loss= 0.88913 train_acc= 0.81425 val_loss= 0.86134 val_acc= 0.81011 time= 0.16899
Epoch: 0042 train_loss= 0.86460 train_acc= 0.82089 val_loss= 0.83949 val_acc= 0.82083 time= 0.19800
Epoch: 0043 train_loss= 0.84163 train_acc= 0.82531 val_loss= 0.81783 val_acc= 0.82695 time= 0.17058
Epoch: 0044 train_loss= 0.81936 train_acc= 0.83024 val_loss= 0.79636 val_acc= 0.82848 time= 0.17900
Epoch: 0045 train_loss= 0.79502 train_acc= 0.83279 val_loss= 0.77517 val_acc= 0.83308 time= 0.16900
Epoch: 0046 train_loss= 0.77492 train_acc= 0.83569 val_loss= 0.75452 val_acc= 0.83461 time= 0.16700
Epoch: 0047 train_loss= 0.74709 train_acc= 0.84062 val_loss= 0.73419 val_acc= 0.83920 time= 0.16797
Epoch: 0048 train_loss= 0.72752 train_acc= 0.84538 val_loss= 0.71427 val_acc= 0.84533 time= 0.17203
Epoch: 0049 train_loss= 0.70424 train_acc= 0.85065 val_loss= 0.69466 val_acc= 0.84686 time= 0.17302
Epoch: 0050 train_loss= 0.68420 train_acc= 0.85372 val_loss= 0.67538 val_acc= 0.85299 time= 0.17521
Epoch: 0051 train_loss= 0.66467 train_acc= 0.85848 val_loss= 0.65649 val_acc= 0.85911 time= 0.17101
Epoch: 0052 train_loss= 0.64272 train_acc= 0.86222 val_loss= 0.63799 val_acc= 0.86217 time= 0.16900
Epoch: 0053 train_loss= 0.62565 train_acc= 0.86528 val_loss= 0.62006 val_acc= 0.87136 time= 0.17001
Epoch: 0054 train_loss= 0.60490 train_acc= 0.86971 val_loss= 0.60282 val_acc= 0.87289 time= 0.20396
Epoch: 0055 train_loss= 0.58611 train_acc= 0.87498 val_loss= 0.58643 val_acc= 0.87749 time= 0.17500
Epoch: 0056 train_loss= 0.56721 train_acc= 0.87532 val_loss= 0.57076 val_acc= 0.87902 time= 0.17000
Epoch: 0057 train_loss= 0.55024 train_acc= 0.87889 val_loss= 0.55574 val_acc= 0.87902 time= 0.17003
Epoch: 0058 train_loss= 0.53011 train_acc= 0.88331 val_loss= 0.54125 val_acc= 0.88055 time= 0.17316
Epoch: 0059 train_loss= 0.51642 train_acc= 0.88621 val_loss= 0.52730 val_acc= 0.87902 time= 0.17063
Epoch: 0060 train_loss= 0.49976 train_acc= 0.89012 val_loss= 0.51373 val_acc= 0.87749 time= 0.19800
Epoch: 0061 train_loss= 0.48398 train_acc= 0.89233 val_loss= 0.50049 val_acc= 0.87749 time= 0.17800
Epoch: 0062 train_loss= 0.46804 train_acc= 0.89573 val_loss= 0.48746 val_acc= 0.88361 time= 0.16900
Epoch: 0063 train_loss= 0.45577 train_acc= 0.89709 val_loss= 0.47449 val_acc= 0.88515 time= 0.16801
Epoch: 0064 train_loss= 0.43901 train_acc= 0.90492 val_loss= 0.46176 val_acc= 0.88668 time= 0.16999
Epoch: 0065 train_loss= 0.42855 train_acc= 0.90883 val_loss= 0.44964 val_acc= 0.89127 time= 0.20865
Epoch: 0066 train_loss= 0.41077 train_acc= 0.91359 val_loss= 0.43807 val_acc= 0.89740 time= 0.16697
Epoch: 0067 train_loss= 0.39859 train_acc= 0.91648 val_loss= 0.42712 val_acc= 0.89740 time= 0.16803
Epoch: 0068 train_loss= 0.38566 train_acc= 0.92073 val_loss= 0.41712 val_acc= 0.89740 time= 0.16800
Epoch: 0069 train_loss= 0.37359 train_acc= 0.92261 val_loss= 0.40781 val_acc= 0.89740 time= 0.16605
Epoch: 0070 train_loss= 0.36057 train_acc= 0.92465 val_loss= 0.39913 val_acc= 0.90046 time= 0.17000
Epoch: 0071 train_loss= 0.35146 train_acc= 0.92737 val_loss= 0.39092 val_acc= 0.90352 time= 0.20097
Epoch: 0072 train_loss= 0.34013 train_acc= 0.92958 val_loss= 0.38302 val_acc= 0.90505 time= 0.18626
Epoch: 0073 train_loss= 0.32941 train_acc= 0.93264 val_loss= 0.37511 val_acc= 0.90352 time= 0.17005
Epoch: 0074 train_loss= 0.31664 train_acc= 0.93706 val_loss= 0.36726 val_acc= 0.90658 time= 0.17000
Epoch: 0075 train_loss= 0.30817 train_acc= 0.93808 val_loss= 0.35936 val_acc= 0.90812 time= 0.17000
Epoch: 0076 train_loss= 0.29870 train_acc= 0.93825 val_loss= 0.35175 val_acc= 0.90965 time= 0.17300
Epoch: 0077 train_loss= 0.28813 train_acc= 0.94404 val_loss= 0.34470 val_acc= 0.91118 time= 0.19700
Epoch: 0078 train_loss= 0.27964 train_acc= 0.94625 val_loss= 0.33831 val_acc= 0.91424 time= 0.17900
Epoch: 0079 train_loss= 0.26977 train_acc= 0.94693 val_loss= 0.33248 val_acc= 0.91424 time= 0.17200
Epoch: 0080 train_loss= 0.26021 train_acc= 0.94846 val_loss= 0.32705 val_acc= 0.91577 time= 0.17005
Epoch: 0081 train_loss= 0.25213 train_acc= 0.95101 val_loss= 0.32225 val_acc= 0.91730 time= 0.16800
Epoch: 0082 train_loss= 0.24409 train_acc= 0.95339 val_loss= 0.31794 val_acc= 0.91730 time= 0.20195
Epoch: 0083 train_loss= 0.23595 train_acc= 0.95526 val_loss= 0.31377 val_acc= 0.91884 time= 0.17012
Epoch: 0084 train_loss= 0.22552 train_acc= 0.95680 val_loss= 0.30964 val_acc= 0.92190 time= 0.16900
Epoch: 0085 train_loss= 0.22010 train_acc= 0.95884 val_loss= 0.30530 val_acc= 0.92190 time= 0.16800
Epoch: 0086 train_loss= 0.21477 train_acc= 0.95952 val_loss= 0.30074 val_acc= 0.92496 time= 0.17251
Epoch: 0087 train_loss= 0.20431 train_acc= 0.96309 val_loss= 0.29616 val_acc= 0.92649 time= 0.17401
Epoch: 0088 train_loss= 0.20020 train_acc= 0.96258 val_loss= 0.29151 val_acc= 0.92802 time= 0.19799
Epoch: 0089 train_loss= 0.19397 train_acc= 0.96377 val_loss= 0.28746 val_acc= 0.92802 time= 0.17900
Epoch: 0090 train_loss= 0.18579 train_acc= 0.96649 val_loss= 0.28390 val_acc= 0.92956 time= 0.17055
Epoch: 0091 train_loss= 0.17997 train_acc= 0.96887 val_loss= 0.28084 val_acc= 0.93109 time= 0.16970
Epoch: 0092 train_loss= 0.17338 train_acc= 0.96921 val_loss= 0.27802 val_acc= 0.93109 time= 0.16695
Epoch: 0093 train_loss= 0.16883 train_acc= 0.96904 val_loss= 0.27563 val_acc= 0.93109 time= 0.17400
Epoch: 0094 train_loss= 0.16358 train_acc= 0.97142 val_loss= 0.27317 val_acc= 0.93109 time= 0.18800
Epoch: 0095 train_loss= 0.15857 train_acc= 0.97210 val_loss= 0.27045 val_acc= 0.93109 time= 0.17805
Epoch: 0096 train_loss= 0.15265 train_acc= 0.97415 val_loss= 0.26795 val_acc= 0.93262 time= 0.16796
Epoch: 0097 train_loss= 0.14833 train_acc= 0.97653 val_loss= 0.26575 val_acc= 0.93262 time= 0.17005
Epoch: 0098 train_loss= 0.14261 train_acc= 0.97755 val_loss= 0.26301 val_acc= 0.93415 time= 0.16818
Epoch: 0099 train_loss= 0.13884 train_acc= 0.97687 val_loss= 0.26060 val_acc= 0.93415 time= 0.19900
Epoch: 0100 train_loss= 0.13459 train_acc= 0.97840 val_loss= 0.25856 val_acc= 0.93415 time= 0.17295
Epoch: 0101 train_loss= 0.12869 train_acc= 0.97942 val_loss= 0.25654 val_acc= 0.93568 time= 0.17804
Epoch: 0102 train_loss= 0.12504 train_acc= 0.97959 val_loss= 0.25456 val_acc= 0.93415 time= 0.17100
Epoch: 0103 train_loss= 0.12190 train_acc= 0.97993 val_loss= 0.25234 val_acc= 0.93721 time= 0.17005
Epoch: 0104 train_loss= 0.11676 train_acc= 0.98248 val_loss= 0.25038 val_acc= 0.93721 time= 0.16895
Epoch: 0105 train_loss= 0.11385 train_acc= 0.98367 val_loss= 0.24890 val_acc= 0.93721 time= 0.20108
Epoch: 0106 train_loss= 0.11146 train_acc= 0.98248 val_loss= 0.24771 val_acc= 0.93721 time= 0.17601
Epoch: 0107 train_loss= 0.10658 train_acc= 0.98282 val_loss= 0.24695 val_acc= 0.93721 time= 0.16795
Epoch: 0108 train_loss= 0.10488 train_acc= 0.98401 val_loss= 0.24631 val_acc= 0.93874 time= 0.17395
Epoch: 0109 train_loss= 0.10018 train_acc= 0.98605 val_loss= 0.24570 val_acc= 0.93874 time= 0.17103
Epoch: 0110 train_loss= 0.09790 train_acc= 0.98571 val_loss= 0.24507 val_acc= 0.94028 time= 0.17096
Epoch: 0111 train_loss= 0.09503 train_acc= 0.98639 val_loss= 0.24440 val_acc= 0.94028 time= 0.19704
Epoch: 0112 train_loss= 0.09287 train_acc= 0.98673 val_loss= 0.24250 val_acc= 0.94028 time= 0.17696
Epoch: 0113 train_loss= 0.08944 train_acc= 0.98622 val_loss= 0.24058 val_acc= 0.94181 time= 0.17104
Epoch: 0114 train_loss= 0.08622 train_acc= 0.98843 val_loss= 0.23903 val_acc= 0.94181 time= 0.16899
Epoch: 0115 train_loss= 0.08442 train_acc= 0.98707 val_loss= 0.23781 val_acc= 0.94028 time= 0.17035
Epoch: 0116 train_loss= 0.08087 train_acc= 0.98860 val_loss= 0.23716 val_acc= 0.93874 time= 0.20604
Epoch: 0117 train_loss= 0.07828 train_acc= 0.98860 val_loss= 0.23715 val_acc= 0.93721 time= 0.16701
Epoch: 0118 train_loss= 0.07742 train_acc= 0.98877 val_loss= 0.23759 val_acc= 0.93874 time= 0.16800
Epoch: 0119 train_loss= 0.07471 train_acc= 0.98945 val_loss= 0.23789 val_acc= 0.93874 time= 0.17001
Epoch: 0120 train_loss= 0.07173 train_acc= 0.99064 val_loss= 0.23801 val_acc= 0.94028 time= 0.16899
Epoch: 0121 train_loss= 0.06973 train_acc= 0.98996 val_loss= 0.23703 val_acc= 0.94334 time= 0.17397
Epoch: 0122 train_loss= 0.06768 train_acc= 0.99064 val_loss= 0.23584 val_acc= 0.94334 time= 0.18300
Epoch: 0123 train_loss= 0.06698 train_acc= 0.99115 val_loss= 0.23475 val_acc= 0.94181 time= 0.18201
Epoch: 0124 train_loss= 0.06510 train_acc= 0.99133 val_loss= 0.23413 val_acc= 0.94181 time= 0.16878
Epoch: 0125 train_loss= 0.06161 train_acc= 0.99218 val_loss= 0.23419 val_acc= 0.94028 time= 0.16900
Epoch: 0126 train_loss= 0.06004 train_acc= 0.99167 val_loss= 0.23450 val_acc= 0.93874 time= 0.16600
Epoch: 0127 train_loss= 0.05927 train_acc= 0.99235 val_loss= 0.23476 val_acc= 0.93721 time= 0.17396
Epoch: 0128 train_loss= 0.05800 train_acc= 0.99167 val_loss= 0.23422 val_acc= 0.93721 time= 0.19805
Epoch: 0129 train_loss= 0.05639 train_acc= 0.99252 val_loss= 0.23286 val_acc= 0.93874 time= 0.16799
Epoch: 0130 train_loss= 0.05428 train_acc= 0.99337 val_loss= 0.23201 val_acc= 0.94334 time= 0.17049
Epoch: 0131 train_loss= 0.05411 train_acc= 0.99201 val_loss= 0.23167 val_acc= 0.94181 time= 0.17104
Epoch: 0132 train_loss= 0.05223 train_acc= 0.99405 val_loss= 0.23148 val_acc= 0.94334 time= 0.16802
Epoch: 0133 train_loss= 0.05013 train_acc= 0.99354 val_loss= 0.23141 val_acc= 0.94334 time= 0.20399
Epoch: 0134 train_loss= 0.04876 train_acc= 0.99439 val_loss= 0.23134 val_acc= 0.93874 time= 0.16800
Epoch: 0135 train_loss= 0.04739 train_acc= 0.99337 val_loss= 0.23125 val_acc= 0.93721 time= 0.18401
Epoch: 0136 train_loss= 0.04599 train_acc= 0.99456 val_loss= 0.23115 val_acc= 0.93874 time= 0.16799
Epoch: 0137 train_loss= 0.04598 train_acc= 0.99388 val_loss= 0.23117 val_acc= 0.93874 time= 0.17096
Epoch: 0138 train_loss= 0.04434 train_acc= 0.99541 val_loss= 0.23175 val_acc= 0.93874 time= 0.17255
Epoch: 0139 train_loss= 0.04368 train_acc= 0.99388 val_loss= 0.23257 val_acc= 0.93721 time= 0.19400
Early stopping...
Optimization Finished!
Test set results: cost= 0.25339 accuracy= 0.93341 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.5000    0.1667    0.2500         6
           2     0.5000    1.0000    0.6667         1
           3     0.7931    0.9200    0.8519        75
           4     1.0000    1.0000    1.0000         9
           5     0.8495    0.9080    0.8778        87
           6     0.9200    0.9200    0.9200        25
           7     0.6875    0.8462    0.7586        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.5556    0.7143         9
          10     0.8148    0.6111    0.6984        36
          11     1.0000    0.9167    0.9565        12
          12     0.8511    0.9917    0.9160       121
          13     1.0000    0.6842    0.8125        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.8636    0.9500    0.9048        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.8235    0.8235    0.8235        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9667    0.9598    0.9632       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8072    0.8272    0.8171        81
          36     1.0000    0.3333    0.5000        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9763    0.9908    0.9835      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.8182    0.7500    0.7826        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8750    0.9333    0.9032        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9334      2568
   macro avg     0.7330    0.6590    0.6734      2568
weighted avg     0.9309    0.9334    0.9286      2568

Macro average Test Precision, Recall and F1-Score...
(0.7329944547498133, 0.6589566267545434, 0.673372350984658, None)
Micro average Test Precision, Recall and F1-Score...
(0.9334112149532711, 0.9334112149532711, 0.9334112149532711, None)
embeddings:
8892 6532 2568
[[ 1.65218532e+00  3.34860384e-01  8.69880095e-02 ... -1.22465625e-01
   4.07883115e-02  1.71667993e-01]
 [ 5.13371706e-01  6.68341890e-02  3.82029325e-01 ...  2.15781387e-03
   5.55465147e-02 -9.15542431e-03]
 [ 1.02011180e+00  3.25770676e-01  4.12520438e-01 ... -2.44668759e-02
  -2.42504179e-02  3.90020274e-02]
 ...
 [ 3.90326977e-01  5.24357140e-01  2.51748890e-01 ... -7.65066594e-04
   3.71381752e-02 -1.64278932e-02]
 [ 4.63714421e-01  4.68041569e-01  2.33890742e-01 ...  3.93227413e-02
   5.05108237e-02  4.80963588e-02]
 [ 2.97287136e-01  4.68052834e-01  5.08836746e-01 ...  2.48093575e-01
   2.43435353e-01  2.27146462e-01]]

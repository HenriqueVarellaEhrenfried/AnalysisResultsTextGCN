(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.00068 val_loss= 3.89882 val_acc= 0.56508 time= 0.44617
Epoch: 0002 train_loss= 3.89886 train_acc= 0.56217 val_loss= 3.80086 val_acc= 0.62175 time= 0.18500
Epoch: 0003 train_loss= 3.80229 train_acc= 0.61677 val_loss= 3.64981 val_acc= 0.63706 time= 0.17200
Epoch: 0004 train_loss= 3.65163 train_acc= 0.62783 val_loss= 3.44551 val_acc= 0.64012 time= 0.16807
Epoch: 0005 train_loss= 3.44596 train_acc= 0.62613 val_loss= 3.19678 val_acc= 0.63859 time= 0.17097
Epoch: 0006 train_loss= 3.20612 train_acc= 0.62664 val_loss= 2.92586 val_acc= 0.63859 time= 0.17203
Epoch: 0007 train_loss= 2.93597 train_acc= 0.62715 val_loss= 2.66465 val_acc= 0.63859 time= 0.19706
Epoch: 0008 train_loss= 2.66880 train_acc= 0.62324 val_loss= 2.44669 val_acc= 0.63706 time= 0.17700
Epoch: 0009 train_loss= 2.44488 train_acc= 0.62681 val_loss= 2.29912 val_acc= 0.61868 time= 0.16703
Epoch: 0010 train_loss= 2.30519 train_acc= 0.60435 val_loss= 2.21780 val_acc= 0.57887 time= 0.17097
Epoch: 0011 train_loss= 2.22425 train_acc= 0.56200 val_loss= 2.16939 val_acc= 0.51455 time= 0.17250
Epoch: 0012 train_loss= 2.17571 train_acc= 0.50332 val_loss= 2.12219 val_acc= 0.48086 time= 0.16899
Epoch: 0013 train_loss= 2.13313 train_acc= 0.45603 val_loss= 2.06035 val_acc= 0.47473 time= 0.17097
Epoch: 0014 train_loss= 2.08164 train_acc= 0.45127 val_loss= 1.98111 val_acc= 0.47933 time= 0.16808
Epoch: 0015 train_loss= 2.00370 train_acc= 0.45399 val_loss= 1.89061 val_acc= 0.49311 time= 0.17000
Epoch: 0016 train_loss= 1.91316 train_acc= 0.47848 val_loss= 1.80029 val_acc= 0.52833 time= 0.16700
Epoch: 0017 train_loss= 1.82596 train_acc= 0.52407 val_loss= 1.72102 val_acc= 0.60031 time= 0.17097
Epoch: 0018 train_loss= 1.75035 train_acc= 0.60640 val_loss= 1.65634 val_acc= 0.65084 time= 0.20629
Epoch: 0019 train_loss= 1.68421 train_acc= 0.64076 val_loss= 1.60119 val_acc= 0.67534 time= 0.17700
Epoch: 0020 train_loss= 1.62985 train_acc= 0.65811 val_loss= 1.54847 val_acc= 0.67994 time= 0.16797
Epoch: 0021 train_loss= 1.57962 train_acc= 0.66508 val_loss= 1.49449 val_acc= 0.69066 time= 0.16803
Epoch: 0022 train_loss= 1.51841 train_acc= 0.66865 val_loss= 1.44044 val_acc= 0.68913 time= 0.16700
Epoch: 0023 train_loss= 1.46540 train_acc= 0.67307 val_loss= 1.38868 val_acc= 0.69066 time= 0.18900
Epoch: 0024 train_loss= 1.41065 train_acc= 0.67733 val_loss= 1.34120 val_acc= 0.69525 time= 0.16900
Epoch: 0025 train_loss= 1.36862 train_acc= 0.67988 val_loss= 1.29843 val_acc= 0.70597 time= 0.17157
Epoch: 0026 train_loss= 1.32104 train_acc= 0.69025 val_loss= 1.26005 val_acc= 0.71669 time= 0.17100
Epoch: 0027 train_loss= 1.28203 train_acc= 0.69893 val_loss= 1.22526 val_acc= 0.71975 time= 0.16738
Epoch: 0028 train_loss= 1.24535 train_acc= 0.71135 val_loss= 1.19319 val_acc= 0.72129 time= 0.16898
Epoch: 0029 train_loss= 1.21661 train_acc= 0.72138 val_loss= 1.16286 val_acc= 0.72741 time= 0.19998
Epoch: 0030 train_loss= 1.18309 train_acc= 0.73057 val_loss= 1.13328 val_acc= 0.73201 time= 0.17203
Epoch: 0031 train_loss= 1.14918 train_acc= 0.73941 val_loss= 1.10379 val_acc= 0.73813 time= 0.18097
Epoch: 0032 train_loss= 1.11986 train_acc= 0.74928 val_loss= 1.07407 val_acc= 0.74273 time= 0.17000
Epoch: 0033 train_loss= 1.08866 train_acc= 0.75421 val_loss= 1.04419 val_acc= 0.75345 time= 0.17200
Epoch: 0034 train_loss= 1.05813 train_acc= 0.76254 val_loss= 1.01487 val_acc= 0.75804 time= 0.16900
Epoch: 0035 train_loss= 1.02579 train_acc= 0.77088 val_loss= 0.98661 val_acc= 0.77029 time= 0.20105
Epoch: 0036 train_loss= 1.00056 train_acc= 0.78040 val_loss= 0.95973 val_acc= 0.78254 time= 0.17895
Epoch: 0037 train_loss= 0.96937 train_acc= 0.79027 val_loss= 0.93415 val_acc= 0.79786 time= 0.16704
Epoch: 0038 train_loss= 0.94681 train_acc= 0.80031 val_loss= 0.90956 val_acc= 0.79939 time= 0.16900
Epoch: 0039 train_loss= 0.91802 train_acc= 0.80779 val_loss= 0.88559 val_acc= 0.80858 time= 0.17096
Epoch: 0040 train_loss= 0.89123 train_acc= 0.81391 val_loss= 0.86196 val_acc= 0.81470 time= 0.17400
Epoch: 0041 train_loss= 0.86983 train_acc= 0.81902 val_loss= 0.83870 val_acc= 0.81930 time= 0.19900
Epoch: 0042 train_loss= 0.84178 train_acc= 0.82548 val_loss= 0.81571 val_acc= 0.83002 time= 0.16960
Epoch: 0043 train_loss= 0.81451 train_acc= 0.83211 val_loss= 0.79286 val_acc= 0.83461 time= 0.16601
Epoch: 0044 train_loss= 0.79464 train_acc= 0.83535 val_loss= 0.77031 val_acc= 0.84227 time= 0.16800
Epoch: 0045 train_loss= 0.76579 train_acc= 0.84198 val_loss= 0.74796 val_acc= 0.84227 time= 0.16947
Epoch: 0046 train_loss= 0.74686 train_acc= 0.84453 val_loss= 0.72584 val_acc= 0.84533 time= 0.20071
Epoch: 0047 train_loss= 0.72409 train_acc= 0.84929 val_loss= 0.70406 val_acc= 0.84686 time= 0.17100
Epoch: 0048 train_loss= 0.70170 train_acc= 0.85185 val_loss= 0.68274 val_acc= 0.85299 time= 0.18500
Epoch: 0049 train_loss= 0.67893 train_acc= 0.85712 val_loss= 0.66196 val_acc= 0.85299 time= 0.16835
Epoch: 0050 train_loss= 0.65755 train_acc= 0.86086 val_loss= 0.64178 val_acc= 0.86371 time= 0.16688
Epoch: 0051 train_loss= 0.63456 train_acc= 0.86358 val_loss= 0.62227 val_acc= 0.86830 time= 0.16797
Epoch: 0052 train_loss= 0.61520 train_acc= 0.86715 val_loss= 0.60368 val_acc= 0.87289 time= 0.16903
Epoch: 0053 train_loss= 0.59283 train_acc= 0.87192 val_loss= 0.58595 val_acc= 0.87443 time= 0.16999
Epoch: 0054 train_loss= 0.57585 train_acc= 0.87379 val_loss= 0.56897 val_acc= 0.87443 time= 0.17000
Epoch: 0055 train_loss= 0.56087 train_acc= 0.87906 val_loss= 0.55278 val_acc= 0.87443 time= 0.16900
Epoch: 0056 train_loss= 0.53547 train_acc= 0.88297 val_loss= 0.53719 val_acc= 0.87596 time= 0.16800
Epoch: 0057 train_loss= 0.51477 train_acc= 0.88655 val_loss= 0.52215 val_acc= 0.87749 time= 0.17000
Epoch: 0058 train_loss= 0.50265 train_acc= 0.88910 val_loss= 0.50741 val_acc= 0.88055 time= 0.20097
Epoch: 0059 train_loss= 0.48291 train_acc= 0.89046 val_loss= 0.49320 val_acc= 0.88055 time= 0.17603
Epoch: 0060 train_loss= 0.46769 train_acc= 0.89386 val_loss= 0.47921 val_acc= 0.88055 time= 0.17000
Epoch: 0061 train_loss= 0.44746 train_acc= 0.89930 val_loss= 0.46546 val_acc= 0.88361 time= 0.17200
Epoch: 0062 train_loss= 0.43819 train_acc= 0.90253 val_loss= 0.45207 val_acc= 0.88361 time= 0.17100
Epoch: 0063 train_loss= 0.41969 train_acc= 0.90917 val_loss= 0.43911 val_acc= 0.88668 time= 0.17100
Epoch: 0064 train_loss= 0.40819 train_acc= 0.90985 val_loss= 0.42668 val_acc= 0.88974 time= 0.19700
Epoch: 0065 train_loss= 0.39345 train_acc= 0.91427 val_loss= 0.41515 val_acc= 0.89433 time= 0.17805
Epoch: 0066 train_loss= 0.38251 train_acc= 0.91988 val_loss= 0.40450 val_acc= 0.89740 time= 0.16999
Epoch: 0067 train_loss= 0.36652 train_acc= 0.92465 val_loss= 0.39444 val_acc= 0.89740 time= 0.16767
Epoch: 0068 train_loss= 0.35558 train_acc= 0.92788 val_loss= 0.38512 val_acc= 0.89893 time= 0.17100
Epoch: 0069 train_loss= 0.34174 train_acc= 0.92686 val_loss= 0.37684 val_acc= 0.90046 time= 0.20600
Epoch: 0070 train_loss= 0.33137 train_acc= 0.93077 val_loss= 0.36928 val_acc= 0.90046 time= 0.16804
Epoch: 0071 train_loss= 0.32149 train_acc= 0.93298 val_loss= 0.36207 val_acc= 0.90199 time= 0.16700
Epoch: 0072 train_loss= 0.31178 train_acc= 0.93672 val_loss= 0.35522 val_acc= 0.90352 time= 0.16830
Epoch: 0073 train_loss= 0.30104 train_acc= 0.93672 val_loss= 0.34843 val_acc= 0.90199 time= 0.16797
Epoch: 0074 train_loss= 0.29056 train_acc= 0.94132 val_loss= 0.34144 val_acc= 0.90352 time= 0.17103
Epoch: 0075 train_loss= 0.28008 train_acc= 0.94183 val_loss= 0.33466 val_acc= 0.90812 time= 0.19797
Epoch: 0076 train_loss= 0.27116 train_acc= 0.94523 val_loss= 0.32788 val_acc= 0.90965 time= 0.18500
Epoch: 0077 train_loss= 0.26179 train_acc= 0.94795 val_loss= 0.32100 val_acc= 0.91424 time= 0.16903
Epoch: 0078 train_loss= 0.25071 train_acc= 0.95067 val_loss= 0.31479 val_acc= 0.91884 time= 0.16997
Epoch: 0079 train_loss= 0.24338 train_acc= 0.95135 val_loss= 0.30940 val_acc= 0.91884 time= 0.16900
Epoch: 0080 train_loss= 0.23707 train_acc= 0.95135 val_loss= 0.30502 val_acc= 0.92037 time= 0.17103
Epoch: 0081 train_loss= 0.22657 train_acc= 0.95356 val_loss= 0.30146 val_acc= 0.92343 time= 0.19905
Epoch: 0082 train_loss= 0.22268 train_acc= 0.95441 val_loss= 0.29797 val_acc= 0.92190 time= 0.17601
Epoch: 0083 train_loss= 0.21464 train_acc= 0.95714 val_loss= 0.29474 val_acc= 0.92037 time= 0.16999
Epoch: 0084 train_loss= 0.20745 train_acc= 0.95833 val_loss= 0.29146 val_acc= 0.91884 time= 0.17104
Epoch: 0085 train_loss= 0.20217 train_acc= 0.96139 val_loss= 0.28750 val_acc= 0.92190 time= 0.16900
Epoch: 0086 train_loss= 0.19413 train_acc= 0.96258 val_loss= 0.28331 val_acc= 0.92343 time= 0.20000
Epoch: 0087 train_loss= 0.18623 train_acc= 0.96309 val_loss= 0.27946 val_acc= 0.92496 time= 0.17000
Epoch: 0088 train_loss= 0.18123 train_acc= 0.96513 val_loss= 0.27572 val_acc= 0.92649 time= 0.16800
Epoch: 0089 train_loss= 0.17271 train_acc= 0.96649 val_loss= 0.27253 val_acc= 0.92649 time= 0.17006
Epoch: 0090 train_loss= 0.16919 train_acc= 0.96649 val_loss= 0.26954 val_acc= 0.92956 time= 0.17311
Epoch: 0091 train_loss= 0.16371 train_acc= 0.96632 val_loss= 0.26723 val_acc= 0.92802 time= 0.17156
Epoch: 0092 train_loss= 0.15648 train_acc= 0.97040 val_loss= 0.26528 val_acc= 0.92802 time= 0.18300
Epoch: 0093 train_loss= 0.15299 train_acc= 0.97057 val_loss= 0.26367 val_acc= 0.92802 time= 0.17900
Epoch: 0094 train_loss= 0.14708 train_acc= 0.97125 val_loss= 0.26187 val_acc= 0.92956 time= 0.16800
Epoch: 0095 train_loss= 0.14417 train_acc= 0.97381 val_loss= 0.25962 val_acc= 0.92956 time= 0.16796
Epoch: 0096 train_loss= 0.13925 train_acc= 0.97398 val_loss= 0.25603 val_acc= 0.93109 time= 0.17103
Epoch: 0097 train_loss= 0.13512 train_acc= 0.97585 val_loss= 0.25344 val_acc= 0.93109 time= 0.17497
Epoch: 0098 train_loss= 0.13115 train_acc= 0.97636 val_loss= 0.25115 val_acc= 0.93109 time= 0.20303
Epoch: 0099 train_loss= 0.12797 train_acc= 0.97653 val_loss= 0.24929 val_acc= 0.93109 time= 0.16859
Epoch: 0100 train_loss= 0.12196 train_acc= 0.97840 val_loss= 0.24766 val_acc= 0.93109 time= 0.16747
Epoch: 0101 train_loss= 0.11929 train_acc= 0.97721 val_loss= 0.24649 val_acc= 0.93109 time= 0.17100
Epoch: 0102 train_loss= 0.11561 train_acc= 0.98095 val_loss= 0.24620 val_acc= 0.93109 time= 0.16800
Epoch: 0103 train_loss= 0.11112 train_acc= 0.98129 val_loss= 0.24631 val_acc= 0.92802 time= 0.19504
Epoch: 0104 train_loss= 0.10885 train_acc= 0.98282 val_loss= 0.24625 val_acc= 0.92802 time= 0.16601
Epoch: 0105 train_loss= 0.10508 train_acc= 0.98384 val_loss= 0.24487 val_acc= 0.92956 time= 0.18395
Epoch: 0106 train_loss= 0.10053 train_acc= 0.98418 val_loss= 0.24313 val_acc= 0.93109 time= 0.17200
Epoch: 0107 train_loss= 0.09725 train_acc= 0.98503 val_loss= 0.24190 val_acc= 0.93262 time= 0.16600
Epoch: 0108 train_loss= 0.09657 train_acc= 0.98231 val_loss= 0.24006 val_acc= 0.93262 time= 0.16805
Epoch: 0109 train_loss= 0.08852 train_acc= 0.98758 val_loss= 0.23857 val_acc= 0.93415 time= 0.17900
Epoch: 0110 train_loss= 0.09019 train_acc= 0.98469 val_loss= 0.23716 val_acc= 0.93415 time= 0.16800
Epoch: 0111 train_loss= 0.08637 train_acc= 0.98673 val_loss= 0.23625 val_acc= 0.93262 time= 0.16796
Epoch: 0112 train_loss= 0.08377 train_acc= 0.98520 val_loss= 0.23706 val_acc= 0.93262 time= 0.17100
Epoch: 0113 train_loss= 0.08236 train_acc= 0.98758 val_loss= 0.23732 val_acc= 0.93262 time= 0.17128
Epoch: 0114 train_loss= 0.08069 train_acc= 0.98775 val_loss= 0.23692 val_acc= 0.93262 time= 0.17103
Epoch: 0115 train_loss= 0.07860 train_acc= 0.98877 val_loss= 0.23578 val_acc= 0.93262 time= 0.19901
Epoch: 0116 train_loss= 0.07435 train_acc= 0.98860 val_loss= 0.23502 val_acc= 0.92956 time= 0.18100
Epoch: 0117 train_loss= 0.07319 train_acc= 0.98962 val_loss= 0.23515 val_acc= 0.92956 time= 0.16896
Epoch: 0118 train_loss= 0.07146 train_acc= 0.98911 val_loss= 0.23439 val_acc= 0.93415 time= 0.16841
Epoch: 0119 train_loss= 0.06879 train_acc= 0.98979 val_loss= 0.23415 val_acc= 0.93262 time= 0.16896
Epoch: 0120 train_loss= 0.06728 train_acc= 0.99064 val_loss= 0.23398 val_acc= 0.93721 time= 0.17603
Epoch: 0121 train_loss= 0.06369 train_acc= 0.98996 val_loss= 0.23407 val_acc= 0.93874 time= 0.20160
Epoch: 0122 train_loss= 0.06259 train_acc= 0.99201 val_loss= 0.23332 val_acc= 0.93721 time= 0.17504
Epoch: 0123 train_loss= 0.06149 train_acc= 0.99116 val_loss= 0.23135 val_acc= 0.93874 time= 0.16700
Epoch: 0124 train_loss= 0.06129 train_acc= 0.99047 val_loss= 0.23039 val_acc= 0.93568 time= 0.16800
Epoch: 0125 train_loss= 0.05849 train_acc= 0.99167 val_loss= 0.22973 val_acc= 0.93262 time= 0.16707
Epoch: 0126 train_loss= 0.05579 train_acc= 0.99201 val_loss= 0.23007 val_acc= 0.93415 time= 0.18000
Epoch: 0127 train_loss= 0.05456 train_acc= 0.99235 val_loss= 0.23069 val_acc= 0.93568 time= 0.17232
Epoch: 0128 train_loss= 0.05365 train_acc= 0.99201 val_loss= 0.23153 val_acc= 0.93721 time= 0.18305
Epoch: 0129 train_loss= 0.05197 train_acc= 0.99337 val_loss= 0.23292 val_acc= 0.93415 time= 0.16699
Early stopping...
Optimization Finished!
Test set results: cost= 0.25118 accuracy= 0.93653 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.8118    0.9200    0.8625        75
           4     1.0000    1.0000    1.0000         9
           5     0.8333    0.9195    0.8743        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.5556    0.7143         9
          10     0.8889    0.6667    0.7619        36
          11     1.0000    0.9167    0.9565        12
          12     0.8333    0.9917    0.9057       121
          13     0.9375    0.7895    0.8571        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8667    0.7647    0.8125        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9668    0.9626    0.9647       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8250    0.8148    0.8199        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9755    0.9908    0.9831      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9365      2568
   macro avg     0.7610    0.6872    0.7002      2568
weighted avg     0.9349    0.9365    0.9319      2568

Macro average Test Precision, Recall and F1-Score...
(0.7609749774483938, 0.6872390111350204, 0.700198544392064, None)
Micro average Test Precision, Recall and F1-Score...
(0.9365264797507789, 0.9365264797507789, 0.9365264797507789, None)
embeddings:
8892 6532 2568
[[-3.9166179e-02 -2.2099774e-02  9.7440995e-02 ...  2.5160229e-03
  -1.5078145e-01  1.4050587e-01]
 [-3.5681464e-02  5.8580214e-01  7.7538319e-02 ... -1.9677635e-04
   1.4022727e-01 -1.3073804e-03]
 [ 4.1665137e-02  2.3422338e-01  3.0743511e-02 ...  1.6318455e-02
   6.4274490e-02  6.2945122e-01]
 ...
 [ 3.8509458e-02  4.7328469e-01 -1.6315565e-02 ...  8.1387505e-02
   2.6762854e-02  2.1544541e-01]
 [ 5.7763066e-02  7.7856742e-02  5.9399981e-02 ...  4.2464729e-02
   5.5351652e-02  3.4269136e-01]
 [ 2.6546738e-01  3.1001386e-01  2.8191268e-01 ...  2.3809808e-01
   2.6798403e-01  5.8707786e-01]]

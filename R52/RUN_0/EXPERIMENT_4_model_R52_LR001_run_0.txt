(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.01021 val_loss= 3.92974 val_acc= 0.67534 time= 0.44363
Epoch: 0002 train_loss= 3.92999 train_acc= 0.65504 val_loss= 3.89373 val_acc= 0.67381 time= 0.17100
Epoch: 0003 train_loss= 3.89431 train_acc= 0.64977 val_loss= 3.84259 val_acc= 0.67075 time= 0.16604
Epoch: 0004 train_loss= 3.84267 train_acc= 0.64603 val_loss= 3.77527 val_acc= 0.66769 time= 0.21796
Epoch: 0005 train_loss= 3.77752 train_acc= 0.64161 val_loss= 3.69085 val_acc= 0.66309 time= 0.17200
Epoch: 0006 train_loss= 3.69520 train_acc= 0.63769 val_loss= 3.58917 val_acc= 0.66156 time= 0.16900
Epoch: 0007 train_loss= 3.59214 train_acc= 0.63667 val_loss= 3.47097 val_acc= 0.65544 time= 0.16709
Epoch: 0008 train_loss= 3.46620 train_acc= 0.63293 val_loss= 3.33821 val_acc= 0.65391 time= 0.16700
Epoch: 0009 train_loss= 3.34392 train_acc= 0.62902 val_loss= 3.19440 val_acc= 0.64472 time= 0.16800
Epoch: 0010 train_loss= 3.20433 train_acc= 0.62409 val_loss= 3.04437 val_acc= 0.64165 time= 0.18600
Epoch: 0011 train_loss= 3.05322 train_acc= 0.61388 val_loss= 2.89354 val_acc= 0.63093 time= 0.16900
Epoch: 0012 train_loss= 2.89655 train_acc= 0.61813 val_loss= 2.74722 val_acc= 0.62481 time= 0.17065
Epoch: 0013 train_loss= 2.73516 train_acc= 0.61116 val_loss= 2.61161 val_acc= 0.61562 time= 0.17100
Epoch: 0014 train_loss= 2.59969 train_acc= 0.60759 val_loss= 2.49301 val_acc= 0.61562 time= 0.16818
Epoch: 0015 train_loss= 2.48961 train_acc= 0.60810 val_loss= 2.39614 val_acc= 0.62021 time= 0.16612
Epoch: 0016 train_loss= 2.39674 train_acc= 0.59806 val_loss= 2.32163 val_acc= 0.64165 time= 0.21351
Epoch: 0017 train_loss= 2.32520 train_acc= 0.61592 val_loss= 2.26547 val_acc= 0.66616 time= 0.16800
Epoch: 0018 train_loss= 2.27369 train_acc= 0.63752 val_loss= 2.22125 val_acc= 0.65544 time= 0.16700
Epoch: 0019 train_loss= 2.22624 train_acc= 0.63514 val_loss= 2.18264 val_acc= 0.54058 time= 0.17078
Epoch: 0020 train_loss= 2.18639 train_acc= 0.53274 val_loss= 2.14535 val_acc= 0.47933 time= 0.17100
Epoch: 0021 train_loss= 2.15312 train_acc= 0.46011 val_loss= 2.10672 val_acc= 0.46248 time= 0.20203
Epoch: 0022 train_loss= 2.10868 train_acc= 0.43936 val_loss= 2.06514 val_acc= 0.45636 time= 0.17241
Epoch: 0023 train_loss= 2.08732 train_acc= 0.43630 val_loss= 2.02005 val_acc= 0.45636 time= 0.16696
Epoch: 0024 train_loss= 2.03628 train_acc= 0.43290 val_loss= 1.97195 val_acc= 0.45636 time= 0.17003
Epoch: 0025 train_loss= 1.98659 train_acc= 0.43562 val_loss= 1.92188 val_acc= 0.46248 time= 0.16797
Epoch: 0026 train_loss= 1.95119 train_acc= 0.43766 val_loss= 1.87169 val_acc= 0.46554 time= 0.16900
Epoch: 0027 train_loss= 1.90111 train_acc= 0.44378 val_loss= 1.82312 val_acc= 0.48545 time= 0.20632
Epoch: 0028 train_loss= 1.84561 train_acc= 0.46419 val_loss= 1.77752 val_acc= 0.51608 time= 0.17100
Epoch: 0029 train_loss= 1.80092 train_acc= 0.51131 val_loss= 1.73553 val_acc= 0.57427 time= 0.16800
Epoch: 0030 train_loss= 1.75843 train_acc= 0.55605 val_loss= 1.69708 val_acc= 0.62328 time= 0.16900
Epoch: 0031 train_loss= 1.71966 train_acc= 0.61847 val_loss= 1.66138 val_acc= 0.65237 time= 0.16804
Epoch: 0032 train_loss= 1.68620 train_acc= 0.63463 val_loss= 1.62742 val_acc= 0.66769 time= 0.16796
Epoch: 0033 train_loss= 1.64832 train_acc= 0.64722 val_loss= 1.59432 val_acc= 0.67228 time= 0.20503
Epoch: 0034 train_loss= 1.61978 train_acc= 0.64841 val_loss= 1.56165 val_acc= 0.67381 time= 0.17384
Epoch: 0035 train_loss= 1.59250 train_acc= 0.64875 val_loss= 1.52934 val_acc= 0.67534 time= 0.18206
Epoch: 0036 train_loss= 1.54737 train_acc= 0.64841 val_loss= 1.49775 val_acc= 0.68147 time= 0.17000
Epoch: 0037 train_loss= 1.52781 train_acc= 0.64960 val_loss= 1.46702 val_acc= 0.68147 time= 0.16900
Epoch: 0038 train_loss= 1.48917 train_acc= 0.65130 val_loss= 1.43759 val_acc= 0.68300 time= 0.20407
Epoch: 0039 train_loss= 1.45911 train_acc= 0.65828 val_loss= 1.40941 val_acc= 0.68300 time= 0.16903
Epoch: 0040 train_loss= 1.43395 train_acc= 0.66117 val_loss= 1.38259 val_acc= 0.68147 time= 0.17100
Epoch: 0041 train_loss= 1.41418 train_acc= 0.66270 val_loss= 1.35706 val_acc= 0.68606 time= 0.17100
Epoch: 0042 train_loss= 1.37960 train_acc= 0.67035 val_loss= 1.33266 val_acc= 0.68606 time= 0.17261
Epoch: 0043 train_loss= 1.35552 train_acc= 0.67648 val_loss= 1.30925 val_acc= 0.69219 time= 0.17000
Epoch: 0044 train_loss= 1.33459 train_acc= 0.67801 val_loss= 1.28673 val_acc= 0.69678 time= 0.19401
Epoch: 0045 train_loss= 1.31443 train_acc= 0.69059 val_loss= 1.26490 val_acc= 0.70138 time= 0.16700
Epoch: 0046 train_loss= 1.29556 train_acc= 0.69059 val_loss= 1.24368 val_acc= 0.70750 time= 0.17105
Epoch: 0047 train_loss= 1.26468 train_acc= 0.70726 val_loss= 1.22294 val_acc= 0.71363 time= 0.17004
Epoch: 0048 train_loss= 1.24846 train_acc= 0.71611 val_loss= 1.20261 val_acc= 0.72588 time= 0.16696
Epoch: 0049 train_loss= 1.22633 train_acc= 0.71747 val_loss= 1.18273 val_acc= 0.72741 time= 0.17193
Epoch: 0050 train_loss= 1.20877 train_acc= 0.72750 val_loss= 1.16334 val_acc= 0.73047 time= 0.21357
Epoch: 0051 train_loss= 1.18868 train_acc= 0.73499 val_loss= 1.14446 val_acc= 0.73201 time= 0.16657
Epoch: 0052 train_loss= 1.16617 train_acc= 0.73856 val_loss= 1.12610 val_acc= 0.73354 time= 0.17103
Epoch: 0053 train_loss= 1.14957 train_acc= 0.74843 val_loss= 1.10833 val_acc= 0.74119 time= 0.17001
Epoch: 0054 train_loss= 1.12958 train_acc= 0.74979 val_loss= 1.09110 val_acc= 0.74885 time= 0.16719
Epoch: 0055 train_loss= 1.10905 train_acc= 0.75965 val_loss= 1.07435 val_acc= 0.75498 time= 0.19600
Epoch: 0056 train_loss= 1.09483 train_acc= 0.76220 val_loss= 1.05799 val_acc= 0.76570 time= 0.17987
Epoch: 0057 train_loss= 1.07707 train_acc= 0.76595 val_loss= 1.04197 val_acc= 0.77182 time= 0.17200
Epoch: 0058 train_loss= 1.05986 train_acc= 0.77224 val_loss= 1.02618 val_acc= 0.77948 time= 0.16800
Epoch: 0059 train_loss= 1.04115 train_acc= 0.77751 val_loss= 1.01060 val_acc= 0.78254 time= 0.17104
Epoch: 0060 train_loss= 1.02570 train_acc= 0.77887 val_loss= 0.99516 val_acc= 0.79020 time= 0.16806
Epoch: 0061 train_loss= 1.01774 train_acc= 0.78415 val_loss= 0.97983 val_acc= 0.79632 time= 0.20100
Epoch: 0062 train_loss= 0.99089 train_acc= 0.79452 val_loss= 0.96463 val_acc= 0.79939 time= 0.16607
Epoch: 0063 train_loss= 0.97970 train_acc= 0.79367 val_loss= 0.94953 val_acc= 0.80245 time= 0.16900
Epoch: 0064 train_loss= 0.96706 train_acc= 0.80320 val_loss= 0.93459 val_acc= 0.80245 time= 0.17264
Epoch: 0065 train_loss= 0.94571 train_acc= 0.80677 val_loss= 0.91982 val_acc= 0.81011 time= 0.17206
Epoch: 0066 train_loss= 0.93466 train_acc= 0.80966 val_loss= 0.90527 val_acc= 0.81011 time= 0.16700
Epoch: 0067 train_loss= 0.92194 train_acc= 0.81493 val_loss= 0.89092 val_acc= 0.81164 time= 0.19300
Epoch: 0068 train_loss= 0.90321 train_acc= 0.81425 val_loss= 0.87674 val_acc= 0.81470 time= 0.16700
Epoch: 0069 train_loss= 0.89132 train_acc= 0.81698 val_loss= 0.86264 val_acc= 0.81776 time= 0.16800
Epoch: 0070 train_loss= 0.88327 train_acc= 0.81715 val_loss= 0.84873 val_acc= 0.82389 time= 0.16707
Epoch: 0071 train_loss= 0.85402 train_acc= 0.82497 val_loss= 0.83498 val_acc= 0.82389 time= 0.17400
Epoch: 0072 train_loss= 0.84185 train_acc= 0.82854 val_loss= 0.82134 val_acc= 0.82542 time= 0.17303
Epoch: 0073 train_loss= 0.82944 train_acc= 0.82939 val_loss= 0.80793 val_acc= 0.83002 time= 0.20700
Epoch: 0074 train_loss= 0.81910 train_acc= 0.83331 val_loss= 0.79475 val_acc= 0.83002 time= 0.16700
Epoch: 0075 train_loss= 0.80602 train_acc= 0.83092 val_loss= 0.78184 val_acc= 0.83308 time= 0.16700
Epoch: 0076 train_loss= 0.78749 train_acc= 0.83790 val_loss= 0.76920 val_acc= 0.83614 time= 0.16800
Epoch: 0077 train_loss= 0.77222 train_acc= 0.84368 val_loss= 0.75688 val_acc= 0.84074 time= 0.17100
Epoch: 0078 train_loss= 0.76206 train_acc= 0.84147 val_loss= 0.74488 val_acc= 0.84380 time= 0.21607
Epoch: 0079 train_loss= 0.74609 train_acc= 0.85014 val_loss= 0.73310 val_acc= 0.84686 time= 0.17100
Epoch: 0080 train_loss= 0.74361 train_acc= 0.84878 val_loss= 0.72156 val_acc= 0.84839 time= 0.16904
Epoch: 0081 train_loss= 0.72413 train_acc= 0.85117 val_loss= 0.71020 val_acc= 0.85299 time= 0.16799
Epoch: 0082 train_loss= 0.70791 train_acc= 0.85491 val_loss= 0.69896 val_acc= 0.85605 time= 0.16800
Epoch: 0083 train_loss= 0.70095 train_acc= 0.85661 val_loss= 0.68792 val_acc= 0.85605 time= 0.17101
Epoch: 0084 train_loss= 0.68903 train_acc= 0.85865 val_loss= 0.67720 val_acc= 0.85911 time= 0.18200
Epoch: 0085 train_loss= 0.67950 train_acc= 0.85712 val_loss= 0.66660 val_acc= 0.86064 time= 0.16696
Epoch: 0086 train_loss= 0.65672 train_acc= 0.86307 val_loss= 0.65613 val_acc= 0.86064 time= 0.17052
Epoch: 0087 train_loss= 0.64751 train_acc= 0.86086 val_loss= 0.64564 val_acc= 0.86217 time= 0.16903
Epoch: 0088 train_loss= 0.63687 train_acc= 0.86443 val_loss= 0.63544 val_acc= 0.86371 time= 0.16700
Epoch: 0089 train_loss= 0.63205 train_acc= 0.86920 val_loss= 0.62534 val_acc= 0.86677 time= 0.17100
Epoch: 0090 train_loss= 0.61566 train_acc= 0.87056 val_loss= 0.61537 val_acc= 0.86677 time= 0.21001
Epoch: 0091 train_loss= 0.60968 train_acc= 0.87056 val_loss= 0.60576 val_acc= 0.86830 time= 0.16800
Epoch: 0092 train_loss= 0.60053 train_acc= 0.87209 val_loss= 0.59644 val_acc= 0.86830 time= 0.16904
Epoch: 0093 train_loss= 0.58760 train_acc= 0.87719 val_loss= 0.58742 val_acc= 0.86830 time= 0.17006
Epoch: 0094 train_loss= 0.57460 train_acc= 0.88212 val_loss= 0.57869 val_acc= 0.86830 time= 0.17100
Epoch: 0095 train_loss= 0.56888 train_acc= 0.88212 val_loss= 0.57009 val_acc= 0.87136 time= 0.19903
Epoch: 0096 train_loss= 0.55698 train_acc= 0.87923 val_loss= 0.56157 val_acc= 0.87443 time= 0.16900
Epoch: 0097 train_loss= 0.54983 train_acc= 0.88842 val_loss= 0.55322 val_acc= 0.87749 time= 0.16997
Epoch: 0098 train_loss= 0.53757 train_acc= 0.88893 val_loss= 0.54506 val_acc= 0.87749 time= 0.16857
Epoch: 0099 train_loss= 0.53375 train_acc= 0.88757 val_loss= 0.53713 val_acc= 0.87902 time= 0.16699
Epoch: 0100 train_loss= 0.52066 train_acc= 0.88808 val_loss= 0.52929 val_acc= 0.88208 time= 0.17099
Epoch: 0101 train_loss= 0.51632 train_acc= 0.89063 val_loss= 0.52143 val_acc= 0.88361 time= 0.21579
Epoch: 0102 train_loss= 0.50329 train_acc= 0.89607 val_loss= 0.51368 val_acc= 0.88361 time= 0.17100
Epoch: 0103 train_loss= 0.49918 train_acc= 0.89743 val_loss= 0.50616 val_acc= 0.88668 time= 0.16800
Epoch: 0104 train_loss= 0.48702 train_acc= 0.89301 val_loss= 0.49870 val_acc= 0.88821 time= 0.16814
Epoch: 0105 train_loss= 0.48191 train_acc= 0.89539 val_loss= 0.49155 val_acc= 0.88974 time= 0.16701
Epoch: 0106 train_loss= 0.47052 train_acc= 0.90151 val_loss= 0.48477 val_acc= 0.89127 time= 0.16900
Epoch: 0107 train_loss= 0.47237 train_acc= 0.90270 val_loss= 0.47835 val_acc= 0.89433 time= 0.20900
Epoch: 0108 train_loss= 0.45719 train_acc= 0.90594 val_loss= 0.47224 val_acc= 0.89433 time= 0.17096
Epoch: 0109 train_loss= 0.44542 train_acc= 0.90594 val_loss= 0.46621 val_acc= 0.89433 time= 0.17397
Epoch: 0110 train_loss= 0.44620 train_acc= 0.90866 val_loss= 0.46043 val_acc= 0.89433 time= 0.16904
Epoch: 0111 train_loss= 0.43164 train_acc= 0.90917 val_loss= 0.45509 val_acc= 0.89433 time= 0.16700
Epoch: 0112 train_loss= 0.42645 train_acc= 0.90951 val_loss= 0.44938 val_acc= 0.89587 time= 0.17800
Epoch: 0113 train_loss= 0.41855 train_acc= 0.91393 val_loss= 0.44369 val_acc= 0.89587 time= 0.19000
Epoch: 0114 train_loss= 0.41226 train_acc= 0.91223 val_loss= 0.43777 val_acc= 0.90046 time= 0.16800
Epoch: 0115 train_loss= 0.41175 train_acc= 0.91563 val_loss= 0.43175 val_acc= 0.90046 time= 0.17013
Epoch: 0116 train_loss= 0.39730 train_acc= 0.91767 val_loss= 0.42574 val_acc= 0.90352 time= 0.17121
Epoch: 0117 train_loss= 0.40315 train_acc= 0.91206 val_loss= 0.41994 val_acc= 0.90199 time= 0.17000
Epoch: 0118 train_loss= 0.39309 train_acc= 0.91716 val_loss= 0.41440 val_acc= 0.90199 time= 0.20800
Epoch: 0119 train_loss= 0.38785 train_acc= 0.91920 val_loss= 0.40959 val_acc= 0.90505 time= 0.16954
Epoch: 0120 train_loss= 0.37707 train_acc= 0.92448 val_loss= 0.40521 val_acc= 0.90505 time= 0.16900
Epoch: 0121 train_loss= 0.37986 train_acc= 0.92482 val_loss= 0.40116 val_acc= 0.90352 time= 0.16800
Epoch: 0122 train_loss= 0.35997 train_acc= 0.92686 val_loss= 0.39729 val_acc= 0.90199 time= 0.16804
Epoch: 0123 train_loss= 0.36560 train_acc= 0.92805 val_loss= 0.39328 val_acc= 0.90199 time= 0.17139
Epoch: 0124 train_loss= 0.35294 train_acc= 0.92737 val_loss= 0.38899 val_acc= 0.90352 time= 0.19658
Epoch: 0125 train_loss= 0.34746 train_acc= 0.93077 val_loss= 0.38443 val_acc= 0.90505 time= 0.16700
Epoch: 0126 train_loss= 0.34124 train_acc= 0.93213 val_loss= 0.37987 val_acc= 0.90505 time= 0.16899
Epoch: 0127 train_loss= 0.34152 train_acc= 0.92890 val_loss= 0.37547 val_acc= 0.90658 time= 0.16800
Epoch: 0128 train_loss= 0.32820 train_acc= 0.93264 val_loss= 0.37105 val_acc= 0.90505 time= 0.16801
Epoch: 0129 train_loss= 0.32414 train_acc= 0.93570 val_loss= 0.36695 val_acc= 0.90505 time= 0.16899
Epoch: 0130 train_loss= 0.32505 train_acc= 0.93264 val_loss= 0.36317 val_acc= 0.90658 time= 0.21097
Epoch: 0131 train_loss= 0.31785 train_acc= 0.93621 val_loss= 0.35925 val_acc= 0.90505 time= 0.17061
Epoch: 0132 train_loss= 0.31407 train_acc= 0.93383 val_loss= 0.35557 val_acc= 0.90965 time= 0.17204
Epoch: 0133 train_loss= 0.31155 train_acc= 0.93655 val_loss= 0.35241 val_acc= 0.90965 time= 0.16700
Epoch: 0134 train_loss= 0.30090 train_acc= 0.93706 val_loss= 0.34954 val_acc= 0.90812 time= 0.16800
Epoch: 0135 train_loss= 0.29605 train_acc= 0.94200 val_loss= 0.34705 val_acc= 0.91118 time= 0.21099
Epoch: 0136 train_loss= 0.29899 train_acc= 0.94064 val_loss= 0.34466 val_acc= 0.91424 time= 0.17000
Epoch: 0137 train_loss= 0.29066 train_acc= 0.94353 val_loss= 0.34253 val_acc= 0.91577 time= 0.16997
Epoch: 0138 train_loss= 0.29101 train_acc= 0.94047 val_loss= 0.33972 val_acc= 0.91577 time= 0.17500
Epoch: 0139 train_loss= 0.28246 train_acc= 0.94421 val_loss= 0.33635 val_acc= 0.91730 time= 0.17004
Epoch: 0140 train_loss= 0.27248 train_acc= 0.94625 val_loss= 0.33294 val_acc= 0.91884 time= 0.16799
Epoch: 0141 train_loss= 0.27436 train_acc= 0.94608 val_loss= 0.32950 val_acc= 0.92037 time= 0.18400
Epoch: 0142 train_loss= 0.26689 train_acc= 0.94676 val_loss= 0.32601 val_acc= 0.92037 time= 0.16806
Epoch: 0143 train_loss= 0.26286 train_acc= 0.94829 val_loss= 0.32262 val_acc= 0.92037 time= 0.16700
Epoch: 0144 train_loss= 0.26527 train_acc= 0.94336 val_loss= 0.31950 val_acc= 0.92190 time= 0.17097
Epoch: 0145 train_loss= 0.25260 train_acc= 0.94863 val_loss= 0.31677 val_acc= 0.91884 time= 0.17300
Epoch: 0146 train_loss= 0.25258 train_acc= 0.94982 val_loss= 0.31452 val_acc= 0.92037 time= 0.17100
Epoch: 0147 train_loss= 0.24929 train_acc= 0.94982 val_loss= 0.31256 val_acc= 0.92037 time= 0.21500
Epoch: 0148 train_loss= 0.24789 train_acc= 0.95067 val_loss= 0.31130 val_acc= 0.91884 time= 0.16800
Epoch: 0149 train_loss= 0.24204 train_acc= 0.95084 val_loss= 0.30968 val_acc= 0.92037 time= 0.16658
Epoch: 0150 train_loss= 0.24657 train_acc= 0.95101 val_loss= 0.30825 val_acc= 0.92343 time= 0.17104
Epoch: 0151 train_loss= 0.24061 train_acc= 0.95254 val_loss= 0.30713 val_acc= 0.92190 time= 0.16608
Epoch: 0152 train_loss= 0.23266 train_acc= 0.95407 val_loss= 0.30527 val_acc= 0.92190 time= 0.21400
Epoch: 0153 train_loss= 0.23117 train_acc= 0.95322 val_loss= 0.30271 val_acc= 0.92190 time= 0.17101
Epoch: 0154 train_loss= 0.22539 train_acc= 0.95867 val_loss= 0.30019 val_acc= 0.92190 time= 0.16800
Epoch: 0155 train_loss= 0.22114 train_acc= 0.95833 val_loss= 0.29713 val_acc= 0.92190 time= 0.16800
Epoch: 0156 train_loss= 0.21470 train_acc= 0.95765 val_loss= 0.29408 val_acc= 0.92343 time= 0.17001
Epoch: 0157 train_loss= 0.21533 train_acc= 0.95884 val_loss= 0.29156 val_acc= 0.92343 time= 0.16800
Epoch: 0158 train_loss= 0.21548 train_acc= 0.95339 val_loss= 0.28923 val_acc= 0.92649 time= 0.19000
Epoch: 0159 train_loss= 0.21663 train_acc= 0.95816 val_loss= 0.28739 val_acc= 0.92802 time= 0.16700
Epoch: 0160 train_loss= 0.20286 train_acc= 0.96037 val_loss= 0.28556 val_acc= 0.92649 time= 0.17085
Epoch: 0161 train_loss= 0.20845 train_acc= 0.95748 val_loss= 0.28384 val_acc= 0.92649 time= 0.17000
Epoch: 0162 train_loss= 0.20225 train_acc= 0.96105 val_loss= 0.28273 val_acc= 0.92649 time= 0.17100
Epoch: 0163 train_loss= 0.19520 train_acc= 0.96326 val_loss= 0.28214 val_acc= 0.92496 time= 0.16800
Epoch: 0164 train_loss= 0.19872 train_acc= 0.96173 val_loss= 0.28180 val_acc= 0.92343 time= 0.20600
Epoch: 0165 train_loss= 0.19610 train_acc= 0.96343 val_loss= 0.28123 val_acc= 0.92496 time= 0.16800
Epoch: 0166 train_loss= 0.19019 train_acc= 0.96275 val_loss= 0.28044 val_acc= 0.92496 time= 0.16706
Epoch: 0167 train_loss= 0.18880 train_acc= 0.96020 val_loss= 0.27873 val_acc= 0.92190 time= 0.17297
Epoch: 0168 train_loss= 0.18803 train_acc= 0.96343 val_loss= 0.27697 val_acc= 0.92496 time= 0.17484
Epoch: 0169 train_loss= 0.18433 train_acc= 0.96479 val_loss= 0.27472 val_acc= 0.92649 time= 0.18203
Epoch: 0170 train_loss= 0.18669 train_acc= 0.96394 val_loss= 0.27262 val_acc= 0.92802 time= 0.18201
Epoch: 0171 train_loss= 0.18033 train_acc= 0.96547 val_loss= 0.27054 val_acc= 0.92802 time= 0.16899
Epoch: 0172 train_loss= 0.18363 train_acc= 0.96275 val_loss= 0.26876 val_acc= 0.92802 time= 0.16799
Epoch: 0173 train_loss= 0.17414 train_acc= 0.96836 val_loss= 0.26758 val_acc= 0.92956 time= 0.16699
Epoch: 0174 train_loss= 0.17350 train_acc= 0.96870 val_loss= 0.26665 val_acc= 0.92802 time= 0.17297
Epoch: 0175 train_loss= 0.16993 train_acc= 0.96938 val_loss= 0.26594 val_acc= 0.92802 time= 0.21300
Epoch: 0176 train_loss= 0.17030 train_acc= 0.96615 val_loss= 0.26551 val_acc= 0.92802 time= 0.17007
Epoch: 0177 train_loss= 0.16554 train_acc= 0.96870 val_loss= 0.26484 val_acc= 0.92649 time= 0.16800
Epoch: 0178 train_loss= 0.16516 train_acc= 0.96785 val_loss= 0.26400 val_acc= 0.92649 time= 0.16700
Epoch: 0179 train_loss= 0.16078 train_acc= 0.96972 val_loss= 0.26327 val_acc= 0.92956 time= 0.16701
Epoch: 0180 train_loss= 0.15953 train_acc= 0.96938 val_loss= 0.26171 val_acc= 0.92802 time= 0.16999
Epoch: 0181 train_loss= 0.15877 train_acc= 0.96904 val_loss= 0.25996 val_acc= 0.93109 time= 0.19200
Epoch: 0182 train_loss= 0.15460 train_acc= 0.96870 val_loss= 0.25858 val_acc= 0.93109 time= 0.17097
Epoch: 0183 train_loss= 0.15615 train_acc= 0.97210 val_loss= 0.25769 val_acc= 0.93109 time= 0.17003
Epoch: 0184 train_loss= 0.15240 train_acc= 0.97176 val_loss= 0.25687 val_acc= 0.93109 time= 0.16700
Epoch: 0185 train_loss= 0.15162 train_acc= 0.97398 val_loss= 0.25582 val_acc= 0.93109 time= 0.16801
Epoch: 0186 train_loss= 0.14748 train_acc= 0.97329 val_loss= 0.25486 val_acc= 0.92956 time= 0.17306
Epoch: 0187 train_loss= 0.14605 train_acc= 0.97398 val_loss= 0.25393 val_acc= 0.93109 time= 0.21400
Epoch: 0188 train_loss= 0.14661 train_acc= 0.97295 val_loss= 0.25335 val_acc= 0.92956 time= 0.16609
Epoch: 0189 train_loss= 0.14635 train_acc= 0.97091 val_loss= 0.25326 val_acc= 0.93109 time= 0.17100
Epoch: 0190 train_loss= 0.14221 train_acc= 0.97329 val_loss= 0.25283 val_acc= 0.92956 time= 0.17100
Epoch: 0191 train_loss= 0.13873 train_acc= 0.97398 val_loss= 0.25168 val_acc= 0.93262 time= 0.16800
Epoch: 0192 train_loss= 0.13998 train_acc= 0.97466 val_loss= 0.25001 val_acc= 0.93415 time= 0.21400
Epoch: 0193 train_loss= 0.13911 train_acc= 0.97517 val_loss= 0.24879 val_acc= 0.93415 time= 0.17000
Epoch: 0194 train_loss= 0.13413 train_acc= 0.97534 val_loss= 0.24772 val_acc= 0.93568 time= 0.16900
Epoch: 0195 train_loss= 0.13240 train_acc= 0.97363 val_loss= 0.24697 val_acc= 0.93568 time= 0.16801
Epoch: 0196 train_loss= 0.13288 train_acc= 0.97261 val_loss= 0.24635 val_acc= 0.93568 time= 0.16999
Epoch: 0197 train_loss= 0.12679 train_acc= 0.97619 val_loss= 0.24576 val_acc= 0.93568 time= 0.17300
Epoch: 0198 train_loss= 0.12958 train_acc= 0.97568 val_loss= 0.24545 val_acc= 0.93415 time= 0.18897
Epoch: 0199 train_loss= 0.12753 train_acc= 0.97619 val_loss= 0.24495 val_acc= 0.93415 time= 0.17108
Epoch: 0200 train_loss= 0.12821 train_acc= 0.97755 val_loss= 0.24442 val_acc= 0.93415 time= 0.16793
Epoch: 0201 train_loss= 0.12241 train_acc= 0.97789 val_loss= 0.24455 val_acc= 0.93415 time= 0.16995
Epoch: 0202 train_loss= 0.12258 train_acc= 0.97959 val_loss= 0.24446 val_acc= 0.93109 time= 0.16807
Epoch: 0203 train_loss= 0.11962 train_acc= 0.98044 val_loss= 0.24473 val_acc= 0.92956 time= 0.17000
Epoch: 0204 train_loss= 0.11872 train_acc= 0.97908 val_loss= 0.24455 val_acc= 0.92956 time= 0.21700
Epoch: 0205 train_loss= 0.11986 train_acc= 0.97789 val_loss= 0.24375 val_acc= 0.92956 time= 0.17430
Epoch: 0206 train_loss= 0.11730 train_acc= 0.97908 val_loss= 0.24244 val_acc= 0.92956 time= 0.16716
Epoch: 0207 train_loss= 0.11725 train_acc= 0.97942 val_loss= 0.24200 val_acc= 0.93109 time= 0.16800
Epoch: 0208 train_loss= 0.12071 train_acc= 0.97687 val_loss= 0.24214 val_acc= 0.92956 time= 0.16700
Epoch: 0209 train_loss= 0.11663 train_acc= 0.97806 val_loss= 0.24198 val_acc= 0.92956 time= 0.20900
Epoch: 0210 train_loss= 0.11497 train_acc= 0.97687 val_loss= 0.24165 val_acc= 0.93415 time= 0.16697
Epoch: 0211 train_loss= 0.11302 train_acc= 0.98231 val_loss= 0.24111 val_acc= 0.93568 time= 0.17360
Epoch: 0212 train_loss= 0.11000 train_acc= 0.97976 val_loss= 0.24035 val_acc= 0.93721 time= 0.17200
Epoch: 0213 train_loss= 0.10844 train_acc= 0.98010 val_loss= 0.24022 val_acc= 0.93721 time= 0.16900
Epoch: 0214 train_loss= 0.11340 train_acc= 0.98027 val_loss= 0.24003 val_acc= 0.93568 time= 0.16815
Epoch: 0215 train_loss= 0.11303 train_acc= 0.97874 val_loss= 0.24048 val_acc= 0.93415 time= 0.18263
Epoch: 0216 train_loss= 0.10691 train_acc= 0.98231 val_loss= 0.24067 val_acc= 0.93568 time= 0.16800
Epoch: 0217 train_loss= 0.10742 train_acc= 0.98163 val_loss= 0.24048 val_acc= 0.93415 time= 0.16909
Epoch: 0218 train_loss= 0.10424 train_acc= 0.98129 val_loss= 0.24028 val_acc= 0.93568 time= 0.16697
Epoch: 0219 train_loss= 0.10145 train_acc= 0.98299 val_loss= 0.24019 val_acc= 0.93568 time= 0.17400
Epoch: 0220 train_loss= 0.10399 train_acc= 0.98265 val_loss= 0.23929 val_acc= 0.93415 time= 0.17000
Epoch: 0221 train_loss= 0.10409 train_acc= 0.98265 val_loss= 0.23807 val_acc= 0.93568 time= 0.20904
Epoch: 0222 train_loss= 0.10058 train_acc= 0.98214 val_loss= 0.23696 val_acc= 0.93568 time= 0.16896
Epoch: 0223 train_loss= 0.10105 train_acc= 0.98129 val_loss= 0.23594 val_acc= 0.93721 time= 0.17200
Epoch: 0224 train_loss= 0.09964 train_acc= 0.98010 val_loss= 0.23521 val_acc= 0.93874 time= 0.16704
Epoch: 0225 train_loss= 0.09709 train_acc= 0.98316 val_loss= 0.23437 val_acc= 0.93874 time= 0.16901
Epoch: 0226 train_loss= 0.09568 train_acc= 0.98333 val_loss= 0.23337 val_acc= 0.93721 time= 0.18996
Epoch: 0227 train_loss= 0.09881 train_acc= 0.98214 val_loss= 0.23303 val_acc= 0.93568 time= 0.18351
Epoch: 0228 train_loss= 0.09305 train_acc= 0.98537 val_loss= 0.23279 val_acc= 0.93568 time= 0.16715
Epoch: 0229 train_loss= 0.09326 train_acc= 0.98333 val_loss= 0.23313 val_acc= 0.93568 time= 0.17201
Epoch: 0230 train_loss= 0.09353 train_acc= 0.98350 val_loss= 0.23400 val_acc= 0.93568 time= 0.16698
Epoch: 0231 train_loss= 0.08977 train_acc= 0.98503 val_loss= 0.23469 val_acc= 0.93415 time= 0.16704
Early stopping...
Optimization Finished!
Test set results: cost= 0.26328 accuracy= 0.93692 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7826    0.9600    0.8623        75
           4     1.0000    1.0000    1.0000         9
           5     0.8000    0.9195    0.8556        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.7333    1.0000    0.8462        11
           9     1.0000    0.4444    0.6154         9
          10     0.9630    0.7222    0.8254        36
          11     1.0000    0.9167    0.9565        12
          12     0.8613    0.9752    0.9147       121
          13     0.8750    0.7368    0.8000        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    0.5000    0.6667         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8000    0.7059    0.7500        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9642    0.9670    0.9656       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8701    0.8272    0.8481        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9917    0.9844      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8889    0.6667    0.7619        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7617    0.6702    0.6860      2568
weighted avg     0.9360    0.9369    0.9320      2568

Macro average Test Precision, Recall and F1-Score...
(0.7617068636263229, 0.6702479349381707, 0.6860418810742532, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[-0.0606638  -0.06290649  0.06183452 ...  0.14375679 -0.01465669
  -0.1098437 ]
 [ 0.7904681   0.26832676 -0.04306603 ... -0.14299262  0.25148976
   0.16097301]
 [ 0.31502858  0.14088048  0.6972881  ...  0.7455528   0.19041981
   0.10051571]
 ...
 [ 0.2907357   0.2356554   0.35898122 ...  0.17886442  0.01821229
   0.33545735]
 [ 0.2011537   0.06109333  0.4276455  ...  0.4635216   0.07243019
   0.12897268]
 [ 0.37329936  0.22390331  0.27719933 ...  0.16970539  0.22835478
   0.2736399 ]]

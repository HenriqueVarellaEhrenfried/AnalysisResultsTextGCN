(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95121 train_acc= 0.04099 val_loss= 3.90700 val_acc= 0.67688 time= 0.44890
Epoch: 0002 train_loss= 3.90877 train_acc= 0.64518 val_loss= 3.81417 val_acc= 0.66616 time= 0.17219
Epoch: 0003 train_loss= 3.81285 train_acc= 0.64212 val_loss= 3.67058 val_acc= 0.64931 time= 0.19706
Epoch: 0004 train_loss= 3.68883 train_acc= 0.62119 val_loss= 3.47736 val_acc= 0.61868 time= 0.16800
Epoch: 0005 train_loss= 3.46880 train_acc= 0.59347 val_loss= 3.24303 val_acc= 0.58499 time= 0.16800
Epoch: 0006 train_loss= 3.25039 train_acc= 0.56574 val_loss= 2.98754 val_acc= 0.55130 time= 0.16597
Epoch: 0007 train_loss= 2.96073 train_acc= 0.55622 val_loss= 2.73841 val_acc= 0.50842 time= 0.17190
Epoch: 0008 train_loss= 2.71488 train_acc= 0.51335 val_loss= 2.52459 val_acc= 0.49923 time= 0.19403
Epoch: 0009 train_loss= 2.51476 train_acc= 0.51522 val_loss= 2.37313 val_acc= 0.48086 time= 0.16697
Epoch: 0010 train_loss= 2.35617 train_acc= 0.50349 val_loss= 2.28898 val_acc= 0.47320 time= 0.18203
Epoch: 0011 train_loss= 2.27228 train_acc= 0.45501 val_loss= 2.24781 val_acc= 0.46708 time= 0.16901
Epoch: 0012 train_loss= 2.26091 train_acc= 0.47219 val_loss= 2.21547 val_acc= 0.46401 time= 0.16708
Epoch: 0013 train_loss= 2.23380 train_acc= 0.46402 val_loss= 2.16976 val_acc= 0.46248 time= 0.17025
Epoch: 0014 train_loss= 2.20694 train_acc= 0.45603 val_loss= 2.10560 val_acc= 0.46095 time= 0.20415
Epoch: 0015 train_loss= 2.13998 train_acc= 0.45841 val_loss= 2.02522 val_acc= 0.46401 time= 0.18000
Epoch: 0016 train_loss= 2.04974 train_acc= 0.46028 val_loss= 1.93724 val_acc= 0.47320 time= 0.16905
Epoch: 0017 train_loss= 1.95466 train_acc= 0.45909 val_loss= 1.85269 val_acc= 0.49923 time= 0.16809
Epoch: 0018 train_loss= 1.89884 train_acc= 0.48903 val_loss= 1.78047 val_acc= 0.54518 time= 0.16670
Epoch: 0019 train_loss= 1.81988 train_acc= 0.53495 val_loss= 1.72284 val_acc= 0.61409 time= 0.17099
Epoch: 0020 train_loss= 1.78448 train_acc= 0.58649 val_loss= 1.67466 val_acc= 0.65084 time= 0.19096
Epoch: 0021 train_loss= 1.74132 train_acc= 0.62136 val_loss= 1.62889 val_acc= 0.66922 time= 0.18000
Epoch: 0022 train_loss= 1.68514 train_acc= 0.63786 val_loss= 1.58102 val_acc= 0.67841 time= 0.17400
Epoch: 0023 train_loss= 1.60929 train_acc= 0.63684 val_loss= 1.53274 val_acc= 0.68147 time= 0.17000
Epoch: 0024 train_loss= 1.57989 train_acc= 0.64994 val_loss= 1.48507 val_acc= 0.68453 time= 0.16804
Epoch: 0025 train_loss= 1.55390 train_acc= 0.65555 val_loss= 1.43985 val_acc= 0.68760 time= 0.17096
Epoch: 0026 train_loss= 1.46226 train_acc= 0.66848 val_loss= 1.39888 val_acc= 0.69066 time= 0.17900
Epoch: 0027 train_loss= 1.45997 train_acc= 0.65725 val_loss= 1.36183 val_acc= 0.69219 time= 0.16600
Epoch: 0028 train_loss= 1.41349 train_acc= 0.66678 val_loss= 1.32823 val_acc= 0.69678 time= 0.16900
Epoch: 0029 train_loss= 1.35893 train_acc= 0.66797 val_loss= 1.29755 val_acc= 0.69985 time= 0.17304
Epoch: 0030 train_loss= 1.33936 train_acc= 0.67477 val_loss= 1.26875 val_acc= 0.70138 time= 0.17005
Epoch: 0031 train_loss= 1.32823 train_acc= 0.67767 val_loss= 1.24093 val_acc= 0.71210 time= 0.18695
Epoch: 0032 train_loss= 1.26968 train_acc= 0.69689 val_loss= 1.21387 val_acc= 0.72129 time= 0.16800
Epoch: 0033 train_loss= 1.25408 train_acc= 0.70233 val_loss= 1.18718 val_acc= 0.72435 time= 0.17805
Epoch: 0034 train_loss= 1.22507 train_acc= 0.72087 val_loss= 1.16104 val_acc= 0.73201 time= 0.16800
Epoch: 0035 train_loss= 1.19532 train_acc= 0.72308 val_loss= 1.13519 val_acc= 0.73660 time= 0.16795
Epoch: 0036 train_loss= 1.17138 train_acc= 0.72631 val_loss= 1.10996 val_acc= 0.73660 time= 0.16999
Epoch: 0037 train_loss= 1.13555 train_acc= 0.72597 val_loss= 1.08565 val_acc= 0.74273 time= 0.20754
Epoch: 0038 train_loss= 1.12309 train_acc= 0.73159 val_loss= 1.06217 val_acc= 0.74579 time= 0.16900
Epoch: 0039 train_loss= 1.08489 train_acc= 0.75047 val_loss= 1.03899 val_acc= 0.74885 time= 0.16800
Epoch: 0040 train_loss= 1.08079 train_acc= 0.74707 val_loss= 1.01679 val_acc= 0.75498 time= 0.17000
Epoch: 0041 train_loss= 1.04647 train_acc= 0.75727 val_loss= 0.99522 val_acc= 0.76723 time= 0.16903
Epoch: 0042 train_loss= 1.02899 train_acc= 0.76033 val_loss= 0.97412 val_acc= 0.77795 time= 0.16997
Epoch: 0043 train_loss= 1.01560 train_acc= 0.76323 val_loss= 0.95385 val_acc= 0.78101 time= 0.19159
Epoch: 0044 train_loss= 0.98473 train_acc= 0.77292 val_loss= 0.93414 val_acc= 0.78407 time= 0.18388
Epoch: 0045 train_loss= 0.98481 train_acc= 0.77513 val_loss= 0.91499 val_acc= 0.79939 time= 0.16883
Epoch: 0046 train_loss= 0.96316 train_acc= 0.78092 val_loss= 0.89620 val_acc= 0.80551 time= 0.16997
Epoch: 0047 train_loss= 0.93385 train_acc= 0.79180 val_loss= 0.87773 val_acc= 0.81011 time= 0.16600
Epoch: 0048 train_loss= 0.92066 train_acc= 0.80116 val_loss= 0.85988 val_acc= 0.81930 time= 0.16903
Epoch: 0049 train_loss= 0.90938 train_acc= 0.80337 val_loss= 0.84221 val_acc= 0.82389 time= 0.19501
Epoch: 0050 train_loss= 0.88328 train_acc= 0.80592 val_loss= 0.82478 val_acc= 0.82542 time= 0.17096
Epoch: 0051 train_loss= 0.86575 train_acc= 0.80660 val_loss= 0.80749 val_acc= 0.82542 time= 0.17365
Epoch: 0052 train_loss= 0.85878 train_acc= 0.81255 val_loss= 0.79059 val_acc= 0.83155 time= 0.17004
Epoch: 0053 train_loss= 0.82787 train_acc= 0.80541 val_loss= 0.77354 val_acc= 0.83155 time= 0.16762
Epoch: 0054 train_loss= 0.81232 train_acc= 0.81613 val_loss= 0.75737 val_acc= 0.83920 time= 0.20101
Epoch: 0055 train_loss= 0.80240 train_acc= 0.81885 val_loss= 0.74129 val_acc= 0.83767 time= 0.17097
Epoch: 0056 train_loss= 0.78117 train_acc= 0.82463 val_loss= 0.72531 val_acc= 0.84380 time= 0.17904
Epoch: 0057 train_loss= 0.76056 train_acc= 0.82293 val_loss= 0.70859 val_acc= 0.84686 time= 0.16901
Epoch: 0058 train_loss= 0.75730 train_acc= 0.82888 val_loss= 0.69181 val_acc= 0.84686 time= 0.17487
Epoch: 0059 train_loss= 0.73278 train_acc= 0.83705 val_loss= 0.67583 val_acc= 0.84992 time= 0.17004
Epoch: 0060 train_loss= 0.72984 train_acc= 0.83858 val_loss= 0.66051 val_acc= 0.85605 time= 0.16899
Epoch: 0061 train_loss= 0.69867 train_acc= 0.83450 val_loss= 0.64573 val_acc= 0.85911 time= 0.18100
Epoch: 0062 train_loss= 0.68093 train_acc= 0.84317 val_loss= 0.63199 val_acc= 0.86217 time= 0.16716
Epoch: 0063 train_loss= 0.66773 train_acc= 0.84317 val_loss= 0.61854 val_acc= 0.86524 time= 0.16865
Epoch: 0064 train_loss= 0.68751 train_acc= 0.83603 val_loss= 0.60589 val_acc= 0.86677 time= 0.16795
Epoch: 0065 train_loss= 0.65938 train_acc= 0.84011 val_loss= 0.59381 val_acc= 0.86983 time= 0.17500
Epoch: 0066 train_loss= 0.62825 train_acc= 0.85542 val_loss= 0.58263 val_acc= 0.87136 time= 0.19251
Epoch: 0067 train_loss= 0.63095 train_acc= 0.84980 val_loss= 0.57208 val_acc= 0.86983 time= 0.16925
Epoch: 0068 train_loss= 0.59524 train_acc= 0.86137 val_loss= 0.56214 val_acc= 0.86983 time= 0.17203
Epoch: 0069 train_loss= 0.58780 train_acc= 0.85440 val_loss= 0.55225 val_acc= 0.87136 time= 0.16700
Epoch: 0070 train_loss= 0.59317 train_acc= 0.85712 val_loss= 0.54269 val_acc= 0.87596 time= 0.16800
Epoch: 0071 train_loss= 0.58754 train_acc= 0.86477 val_loss= 0.53220 val_acc= 0.87749 time= 0.20503
Epoch: 0072 train_loss= 0.54005 train_acc= 0.87090 val_loss= 0.52098 val_acc= 0.88361 time= 0.17100
Epoch: 0073 train_loss= 0.55179 train_acc= 0.86137 val_loss= 0.50877 val_acc= 0.88668 time= 0.18805
Epoch: 0074 train_loss= 0.53771 train_acc= 0.87362 val_loss= 0.49732 val_acc= 0.89127 time= 0.17099
Epoch: 0075 train_loss= 0.51057 train_acc= 0.88042 val_loss= 0.48745 val_acc= 0.89127 time= 0.16706
Epoch: 0076 train_loss= 0.52485 train_acc= 0.87328 val_loss= 0.47827 val_acc= 0.89280 time= 0.16800
Epoch: 0077 train_loss= 0.50677 train_acc= 0.87260 val_loss= 0.46986 val_acc= 0.88974 time= 0.17352
Epoch: 0078 train_loss= 0.50171 train_acc= 0.87515 val_loss= 0.46187 val_acc= 0.88974 time= 0.17200
Epoch: 0079 train_loss= 0.49728 train_acc= 0.87804 val_loss= 0.45490 val_acc= 0.88974 time= 0.16800
Epoch: 0080 train_loss= 0.47841 train_acc= 0.88586 val_loss= 0.44924 val_acc= 0.88974 time= 0.17100
Epoch: 0081 train_loss= 0.48369 train_acc= 0.88859 val_loss= 0.44371 val_acc= 0.88974 time= 0.17121
Epoch: 0082 train_loss= 0.46851 train_acc= 0.88501 val_loss= 0.43785 val_acc= 0.89127 time= 0.16800
Epoch: 0083 train_loss= 0.48582 train_acc= 0.88093 val_loss= 0.43106 val_acc= 0.89587 time= 0.19797
Epoch: 0084 train_loss= 0.46807 train_acc= 0.88501 val_loss= 0.42445 val_acc= 0.89587 time= 0.17603
Epoch: 0085 train_loss= 0.44276 train_acc= 0.89114 val_loss= 0.41802 val_acc= 0.90046 time= 0.17101
Epoch: 0086 train_loss= 0.44473 train_acc= 0.89930 val_loss= 0.41153 val_acc= 0.90046 time= 0.17000
Epoch: 0087 train_loss= 0.43630 train_acc= 0.88910 val_loss= 0.40519 val_acc= 0.90046 time= 0.17500
Epoch: 0088 train_loss= 0.42768 train_acc= 0.89454 val_loss= 0.39801 val_acc= 0.90812 time= 0.19300
Epoch: 0089 train_loss= 0.42097 train_acc= 0.89522 val_loss= 0.39113 val_acc= 0.90965 time= 0.17801
Epoch: 0090 train_loss= 0.42722 train_acc= 0.89267 val_loss= 0.38450 val_acc= 0.90505 time= 0.17699
Epoch: 0091 train_loss= 0.40852 train_acc= 0.90049 val_loss= 0.37909 val_acc= 0.90505 time= 0.16900
Epoch: 0092 train_loss= 0.40612 train_acc= 0.90645 val_loss= 0.37458 val_acc= 0.90352 time= 0.16800
Epoch: 0093 train_loss= 0.38736 train_acc= 0.90287 val_loss= 0.37114 val_acc= 0.90352 time= 0.17100
Epoch: 0094 train_loss= 0.41377 train_acc= 0.89641 val_loss= 0.36883 val_acc= 0.90505 time= 0.19500
Epoch: 0095 train_loss= 0.37430 train_acc= 0.91325 val_loss= 0.36734 val_acc= 0.90658 time= 0.17731
Epoch: 0096 train_loss= 0.38067 train_acc= 0.90407 val_loss= 0.36490 val_acc= 0.90812 time= 0.17703
Epoch: 0097 train_loss= 0.37747 train_acc= 0.90968 val_loss= 0.36190 val_acc= 0.90658 time= 0.16601
Epoch: 0098 train_loss= 0.38392 train_acc= 0.90662 val_loss= 0.35797 val_acc= 0.90658 time= 0.16696
Epoch: 0099 train_loss= 0.38059 train_acc= 0.90577 val_loss= 0.35306 val_acc= 0.90812 time= 0.16603
Epoch: 0100 train_loss= 0.36486 train_acc= 0.90815 val_loss= 0.34808 val_acc= 0.90812 time= 0.17599
Epoch: 0101 train_loss= 0.36584 train_acc= 0.91359 val_loss= 0.34406 val_acc= 0.90812 time= 0.18097
Epoch: 0102 train_loss= 0.35409 train_acc= 0.91733 val_loss= 0.34060 val_acc= 0.91118 time= 0.17103
Epoch: 0103 train_loss= 0.35759 train_acc= 0.91682 val_loss= 0.33763 val_acc= 0.91118 time= 0.17136
Epoch: 0104 train_loss= 0.34000 train_acc= 0.91478 val_loss= 0.33464 val_acc= 0.90965 time= 0.16791
Epoch: 0105 train_loss= 0.34185 train_acc= 0.91257 val_loss= 0.33134 val_acc= 0.91271 time= 0.17097
Epoch: 0106 train_loss= 0.32778 train_acc= 0.92278 val_loss= 0.32829 val_acc= 0.91271 time= 0.19704
Epoch: 0107 train_loss= 0.34181 train_acc= 0.91410 val_loss= 0.32585 val_acc= 0.91118 time= 0.16696
Epoch: 0108 train_loss= 0.31736 train_acc= 0.91971 val_loss= 0.32275 val_acc= 0.91424 time= 0.17303
Epoch: 0109 train_loss= 0.31834 train_acc= 0.92107 val_loss= 0.31962 val_acc= 0.91118 time= 0.16997
Epoch: 0110 train_loss= 0.30094 train_acc= 0.92448 val_loss= 0.31673 val_acc= 0.90965 time= 0.17200
Epoch: 0111 train_loss= 0.30880 train_acc= 0.92125 val_loss= 0.31442 val_acc= 0.91118 time= 0.20004
Epoch: 0112 train_loss= 0.31090 train_acc= 0.92022 val_loss= 0.31297 val_acc= 0.91118 time= 0.16901
Epoch: 0113 train_loss= 0.30450 train_acc= 0.92465 val_loss= 0.31147 val_acc= 0.91424 time= 0.18095
Epoch: 0114 train_loss= 0.30115 train_acc= 0.92465 val_loss= 0.30920 val_acc= 0.91271 time= 0.16904
Epoch: 0115 train_loss= 0.30057 train_acc= 0.92771 val_loss= 0.30699 val_acc= 0.91424 time= 0.16900
Epoch: 0116 train_loss= 0.29462 train_acc= 0.92754 val_loss= 0.30454 val_acc= 0.91424 time= 0.16697
Epoch: 0117 train_loss= 0.27290 train_acc= 0.93349 val_loss= 0.30220 val_acc= 0.91884 time= 0.20466
Epoch: 0118 train_loss= 0.28068 train_acc= 0.92958 val_loss= 0.30001 val_acc= 0.91884 time= 0.17804
Epoch: 0119 train_loss= 0.28312 train_acc= 0.93485 val_loss= 0.29785 val_acc= 0.92037 time= 0.16700
Epoch: 0120 train_loss= 0.28585 train_acc= 0.92567 val_loss= 0.29604 val_acc= 0.92190 time= 0.17101
Epoch: 0121 train_loss= 0.27683 train_acc= 0.92822 val_loss= 0.29431 val_acc= 0.92190 time= 0.16700
Epoch: 0122 train_loss= 0.27651 train_acc= 0.92754 val_loss= 0.29205 val_acc= 0.92343 time= 0.17096
Epoch: 0123 train_loss= 0.26537 train_acc= 0.93570 val_loss= 0.28898 val_acc= 0.91884 time= 0.19803
Epoch: 0124 train_loss= 0.26497 train_acc= 0.93281 val_loss= 0.28622 val_acc= 0.92037 time= 0.17263
Epoch: 0125 train_loss= 0.24376 train_acc= 0.93570 val_loss= 0.28355 val_acc= 0.92190 time= 0.17000
Epoch: 0126 train_loss= 0.26042 train_acc= 0.93128 val_loss= 0.28160 val_acc= 0.92190 time= 0.17004
Epoch: 0127 train_loss= 0.26440 train_acc= 0.93536 val_loss= 0.27986 val_acc= 0.92037 time= 0.16901
Epoch: 0128 train_loss= 0.25878 train_acc= 0.93196 val_loss= 0.27838 val_acc= 0.92343 time= 0.20099
Epoch: 0129 train_loss= 0.24268 train_acc= 0.93706 val_loss= 0.27771 val_acc= 0.92190 time= 0.16901
Epoch: 0130 train_loss= 0.25695 train_acc= 0.93672 val_loss= 0.27593 val_acc= 0.92037 time= 0.17700
Epoch: 0131 train_loss= 0.25223 train_acc= 0.93247 val_loss= 0.27436 val_acc= 0.92037 time= 0.16870
Epoch: 0132 train_loss= 0.23625 train_acc= 0.93996 val_loss= 0.27241 val_acc= 0.92037 time= 0.17452
Epoch: 0133 train_loss= 0.24307 train_acc= 0.93349 val_loss= 0.27019 val_acc= 0.92037 time= 0.17152
Epoch: 0134 train_loss= 0.23898 train_acc= 0.93621 val_loss= 0.26871 val_acc= 0.92343 time= 0.19799
Epoch: 0135 train_loss= 0.23706 train_acc= 0.94183 val_loss= 0.26852 val_acc= 0.92343 time= 0.16901
Epoch: 0136 train_loss= 0.22336 train_acc= 0.94421 val_loss= 0.26903 val_acc= 0.92190 time= 0.16600
Epoch: 0137 train_loss= 0.23440 train_acc= 0.93587 val_loss= 0.26860 val_acc= 0.92037 time= 0.16900
Epoch: 0138 train_loss= 0.24468 train_acc= 0.93774 val_loss= 0.26777 val_acc= 0.92037 time= 0.17100
Epoch: 0139 train_loss= 0.23701 train_acc= 0.94115 val_loss= 0.26732 val_acc= 0.92190 time= 0.17504
Epoch: 0140 train_loss= 0.23546 train_acc= 0.94098 val_loss= 0.26827 val_acc= 0.92037 time= 0.18700
Epoch: 0141 train_loss= 0.21643 train_acc= 0.94268 val_loss= 0.26877 val_acc= 0.92037 time= 0.17900
Epoch: 0142 train_loss= 0.22168 train_acc= 0.94183 val_loss= 0.26857 val_acc= 0.92496 time= 0.16600
Epoch: 0143 train_loss= 0.20536 train_acc= 0.94506 val_loss= 0.26864 val_acc= 0.92343 time= 0.16800
Early stopping...
Optimization Finished!
Test set results: cost= 0.30245 accuracy= 0.92484 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.6250    0.7692         8
           1     1.0000    0.1667    0.2857         6
           2     0.0000    0.0000    0.0000         1
           3     0.8193    0.9067    0.8608        75
           4     1.0000    1.0000    1.0000         9
           5     0.8247    0.9195    0.8696        87
           6     0.8846    0.9200    0.9020        25
           7     0.5789    0.8462    0.6875        13
           8     1.0000    0.5455    0.7059        11
           9     0.0000    0.0000    0.0000         9
          10     0.9000    0.7500    0.8182        36
          11     1.0000    0.9167    0.9565        12
          12     0.8731    0.9669    0.9176       121
          13     0.8125    0.6842    0.7429        19
          14     0.7353    0.8929    0.8065        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.3750    0.6000    0.4615         5
          23     0.0000    0.0000    0.0000         1
          24     0.4444    0.7059    0.5455        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.5833    0.7368        12
          28     1.0000    0.7273    0.8421        11
          29     0.9601    0.9684    0.9642       696
          30     0.9167    1.0000    0.9565        22
          31     0.0000    0.0000    0.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8256    0.8765    0.8503        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9782    0.9926    0.9853      1083
          40     0.0000    0.0000    0.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.6667    0.6667    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.6667    0.9333    0.7778        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.5000    0.7500    0.6000         4

    accuracy                         0.9248      2568
   macro avg     0.5990    0.5289    0.5369      2568
weighted avg     0.9153    0.9248    0.9156      2568

Macro average Test Precision, Recall and F1-Score...
(0.5989603716341156, 0.5289176497826671, 0.5369442455695015, None)
Micro average Test Precision, Recall and F1-Score...
(0.9248442367601246, 0.9248442367601246, 0.9248442367601246, None)
embeddings:
8892 6532 2568
[[-0.04031499 -0.01290858  0.19992603 ...  0.16986081 -0.16246426
   0.36924103]
 [ 0.03354077  0.10488993  0.04374648 ...  0.09029669  0.14423665
   0.08491968]
 [ 0.02628789  0.17010449  0.01660296 ...  0.10189926  0.18864462
   0.16505796]
 ...
 [ 0.18722823  0.04576913  0.08739808 ...  0.04016831  0.10741609
   0.00534896]
 [ 0.050475    0.06749302  0.06320362 ...  0.07921593  0.08166113
   0.0885603 ]
 [ 0.24060224  0.15595116  0.24383399 ...  0.19644442  0.21782506
   0.19189204]]

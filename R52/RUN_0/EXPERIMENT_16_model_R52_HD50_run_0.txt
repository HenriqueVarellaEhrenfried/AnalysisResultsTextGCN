(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95132 train_acc= 0.00272 val_loss= 3.93513 val_acc= 0.58193 time= 0.37205
Epoch: 0002 train_loss= 3.93478 train_acc= 0.56251 val_loss= 3.90753 val_acc= 0.57121 time= 0.13100
Epoch: 0003 train_loss= 3.90982 train_acc= 0.56370 val_loss= 3.86640 val_acc= 0.55590 time= 0.13013
Epoch: 0004 train_loss= 3.86678 train_acc= 0.55350 val_loss= 3.81013 val_acc= 0.54824 time= 0.16299
Epoch: 0005 train_loss= 3.81626 train_acc= 0.53734 val_loss= 3.73733 val_acc= 0.52680 time= 0.13400
Epoch: 0006 train_loss= 3.74319 train_acc= 0.51012 val_loss= 3.64751 val_acc= 0.50536 time= 0.14100
Epoch: 0007 train_loss= 3.65143 train_acc= 0.48784 val_loss= 3.54067 val_acc= 0.49464 time= 0.13286
Epoch: 0008 train_loss= 3.55327 train_acc= 0.48410 val_loss= 3.41741 val_acc= 0.48698 time= 0.13304
Epoch: 0009 train_loss= 3.43098 train_acc= 0.47508 val_loss= 3.28007 val_acc= 0.47779 time= 0.13000
Epoch: 0010 train_loss= 3.28813 train_acc= 0.46947 val_loss= 3.13219 val_acc= 0.47014 time= 0.16401
Epoch: 0011 train_loss= 3.13476 train_acc= 0.45722 val_loss= 2.97885 val_acc= 0.46708 time= 0.13009
Epoch: 0012 train_loss= 2.99970 train_acc= 0.45127 val_loss= 2.82605 val_acc= 0.46248 time= 0.13200
Epoch: 0013 train_loss= 2.83178 train_acc= 0.45280 val_loss= 2.67971 val_acc= 0.46095 time= 0.15401
Epoch: 0014 train_loss= 2.71706 train_acc= 0.44412 val_loss= 2.54749 val_acc= 0.46095 time= 0.13199
Epoch: 0015 train_loss= 2.58520 train_acc= 0.44531 val_loss= 2.43620 val_acc= 0.46095 time= 0.13200
Epoch: 0016 train_loss= 2.42741 train_acc= 0.44770 val_loss= 2.35009 val_acc= 0.46095 time= 0.13397
Epoch: 0017 train_loss= 2.35032 train_acc= 0.44038 val_loss= 2.28894 val_acc= 0.45942 time= 0.16769
Epoch: 0018 train_loss= 2.29705 train_acc= 0.44616 val_loss= 2.24743 val_acc= 0.45789 time= 0.13100
Epoch: 0019 train_loss= 2.23163 train_acc= 0.43902 val_loss= 2.21734 val_acc= 0.45636 time= 0.13204
Epoch: 0020 train_loss= 2.21927 train_acc= 0.43919 val_loss= 2.19043 val_acc= 0.45636 time= 0.14500
Epoch: 0021 train_loss= 2.21436 train_acc= 0.43358 val_loss= 2.16109 val_acc= 0.45636 time= 0.13201
Epoch: 0022 train_loss= 2.17999 train_acc= 0.43715 val_loss= 2.12596 val_acc= 0.45636 time= 0.13199
Epoch: 0023 train_loss= 2.12480 train_acc= 0.43477 val_loss= 2.08421 val_acc= 0.45636 time= 0.13099
Epoch: 0024 train_loss= 2.10924 train_acc= 0.43545 val_loss= 2.03731 val_acc= 0.45636 time= 0.13600
Epoch: 0025 train_loss= 2.03987 train_acc= 0.43596 val_loss= 1.98701 val_acc= 0.45636 time= 0.15897
Epoch: 0026 train_loss= 2.02210 train_acc= 0.43936 val_loss= 1.93597 val_acc= 0.46248 time= 0.13300
Epoch: 0027 train_loss= 1.94703 train_acc= 0.44514 val_loss= 1.88627 val_acc= 0.46861 time= 0.15204
Epoch: 0028 train_loss= 1.89610 train_acc= 0.45025 val_loss= 1.83976 val_acc= 0.48392 time= 0.13199
Epoch: 0029 train_loss= 1.86659 train_acc= 0.47015 val_loss= 1.79744 val_acc= 0.51302 time= 0.13100
Epoch: 0030 train_loss= 1.81780 train_acc= 0.51505 val_loss= 1.75920 val_acc= 0.54518 time= 0.13204
Epoch: 0031 train_loss= 1.76718 train_acc= 0.55145 val_loss= 1.72424 val_acc= 0.60184 time= 0.16300
Epoch: 0032 train_loss= 1.75225 train_acc= 0.59636 val_loss= 1.69121 val_acc= 0.62787 time= 0.13003
Epoch: 0033 train_loss= 1.70167 train_acc= 0.61235 val_loss= 1.65945 val_acc= 0.64625 time= 0.13097
Epoch: 0034 train_loss= 1.69670 train_acc= 0.63021 val_loss= 1.62797 val_acc= 0.65697 time= 0.15300
Epoch: 0035 train_loss= 1.67093 train_acc= 0.62817 val_loss= 1.59672 val_acc= 0.66922 time= 0.13635
Epoch: 0036 train_loss= 1.63708 train_acc= 0.63854 val_loss= 1.56598 val_acc= 0.67228 time= 0.13403
Epoch: 0037 train_loss= 1.58168 train_acc= 0.63956 val_loss= 1.53623 val_acc= 0.67381 time= 0.13097
Epoch: 0038 train_loss= 1.55382 train_acc= 0.64467 val_loss= 1.50786 val_acc= 0.67228 time= 0.13100
Epoch: 0039 train_loss= 1.52985 train_acc= 0.64348 val_loss= 1.48094 val_acc= 0.67534 time= 0.16303
Epoch: 0040 train_loss= 1.50111 train_acc= 0.64858 val_loss= 1.45546 val_acc= 0.67534 time= 0.13100
Epoch: 0041 train_loss= 1.48032 train_acc= 0.64858 val_loss= 1.43128 val_acc= 0.67534 time= 0.12897
Epoch: 0042 train_loss= 1.45996 train_acc= 0.65232 val_loss= 1.40810 val_acc= 0.67534 time= 0.14896
Epoch: 0043 train_loss= 1.42513 train_acc= 0.65283 val_loss= 1.38566 val_acc= 0.67534 time= 0.13300
Epoch: 0044 train_loss= 1.41035 train_acc= 0.65879 val_loss= 1.36388 val_acc= 0.67994 time= 0.13357
Epoch: 0045 train_loss= 1.38821 train_acc= 0.65811 val_loss= 1.34256 val_acc= 0.68606 time= 0.13465
Epoch: 0046 train_loss= 1.36502 train_acc= 0.66933 val_loss= 1.32161 val_acc= 0.69219 time= 0.16297
Epoch: 0047 train_loss= 1.33604 train_acc= 0.67648 val_loss= 1.30095 val_acc= 0.69678 time= 0.13200
Epoch: 0048 train_loss= 1.31396 train_acc= 0.68141 val_loss= 1.28057 val_acc= 0.70291 time= 0.13103
Epoch: 0049 train_loss= 1.30112 train_acc= 0.69263 val_loss= 1.26050 val_acc= 0.70750 time= 0.14697
Epoch: 0050 train_loss= 1.28681 train_acc= 0.69910 val_loss= 1.24078 val_acc= 0.71210 time= 0.13103
Epoch: 0051 train_loss= 1.25572 train_acc= 0.71015 val_loss= 1.22139 val_acc= 0.71822 time= 0.13397
Epoch: 0052 train_loss= 1.25040 train_acc= 0.71067 val_loss= 1.20239 val_acc= 0.71822 time= 0.13103
Epoch: 0053 train_loss= 1.21491 train_acc= 0.72223 val_loss= 1.18371 val_acc= 0.72435 time= 0.16432
Epoch: 0054 train_loss= 1.20502 train_acc= 0.72308 val_loss= 1.16539 val_acc= 0.73047 time= 0.13300
Epoch: 0055 train_loss= 1.17124 train_acc= 0.73550 val_loss= 1.14746 val_acc= 0.73507 time= 0.13104
Epoch: 0056 train_loss= 1.17215 train_acc= 0.72767 val_loss= 1.12991 val_acc= 0.73813 time= 0.13896
Epoch: 0057 train_loss= 1.15084 train_acc= 0.73635 val_loss= 1.11269 val_acc= 0.74273 time= 0.13000
Epoch: 0058 train_loss= 1.12208 train_acc= 0.74213 val_loss= 1.09582 val_acc= 0.74579 time= 0.13103
Epoch: 0059 train_loss= 1.10548 train_acc= 0.74281 val_loss= 1.07925 val_acc= 0.75651 time= 0.13117
Epoch: 0060 train_loss= 1.09032 train_acc= 0.74622 val_loss= 1.06291 val_acc= 0.75804 time= 0.13300
Epoch: 0061 train_loss= 1.07412 train_acc= 0.75608 val_loss= 1.04670 val_acc= 0.76110 time= 0.16601
Epoch: 0062 train_loss= 1.05463 train_acc= 0.76459 val_loss= 1.03070 val_acc= 0.76570 time= 0.13296
Epoch: 0063 train_loss= 1.04304 train_acc= 0.76408 val_loss= 1.01493 val_acc= 0.76876 time= 0.15452
Epoch: 0064 train_loss= 1.03563 train_acc= 0.77071 val_loss= 0.99951 val_acc= 0.77948 time= 0.13251
Epoch: 0065 train_loss= 1.00934 train_acc= 0.77139 val_loss= 0.98436 val_acc= 0.78101 time= 0.13203
Epoch: 0066 train_loss= 0.99541 train_acc= 0.77853 val_loss= 0.96939 val_acc= 0.78560 time= 0.13200
Epoch: 0067 train_loss= 0.97669 train_acc= 0.78568 val_loss= 0.95453 val_acc= 0.79173 time= 0.13500
Epoch: 0068 train_loss= 0.95485 train_acc= 0.78585 val_loss= 0.93988 val_acc= 0.79632 time= 0.16100
Epoch: 0069 train_loss= 0.96420 train_acc= 0.78636 val_loss= 0.92531 val_acc= 0.79786 time= 0.13001
Epoch: 0070 train_loss= 0.94974 train_acc= 0.79180 val_loss= 0.91097 val_acc= 0.79786 time= 0.14699
Epoch: 0071 train_loss= 0.92391 train_acc= 0.80201 val_loss= 0.89668 val_acc= 0.79939 time= 0.13097
Epoch: 0072 train_loss= 0.91325 train_acc= 0.80609 val_loss= 0.88262 val_acc= 0.80092 time= 0.13401
Epoch: 0073 train_loss= 0.90171 train_acc= 0.80932 val_loss= 0.86897 val_acc= 0.80398 time= 0.13251
Epoch: 0074 train_loss= 0.87935 train_acc= 0.81221 val_loss= 0.85552 val_acc= 0.80551 time= 0.16203
Epoch: 0075 train_loss= 0.86015 train_acc= 0.81544 val_loss= 0.84235 val_acc= 0.80551 time= 0.13300
Epoch: 0076 train_loss= 0.85397 train_acc= 0.81800 val_loss= 0.82958 val_acc= 0.81164 time= 0.13201
Epoch: 0077 train_loss= 0.82855 train_acc= 0.82650 val_loss= 0.81692 val_acc= 0.81930 time= 0.13199
Epoch: 0078 train_loss= 0.83588 train_acc= 0.81578 val_loss= 0.80444 val_acc= 0.82389 time= 0.15701
Epoch: 0079 train_loss= 0.81755 train_acc= 0.82004 val_loss= 0.79210 val_acc= 0.82542 time= 0.13101
Epoch: 0080 train_loss= 0.80732 train_acc= 0.82378 val_loss= 0.77959 val_acc= 0.82848 time= 0.13199
Epoch: 0081 train_loss= 0.80102 train_acc= 0.82514 val_loss= 0.76708 val_acc= 0.82695 time= 0.13598
Epoch: 0082 train_loss= 0.77547 train_acc= 0.83365 val_loss= 0.75456 val_acc= 0.83002 time= 0.16800
Epoch: 0083 train_loss= 0.76411 train_acc= 0.83671 val_loss= 0.74248 val_acc= 0.83461 time= 0.13800
Epoch: 0084 train_loss= 0.74199 train_acc= 0.83773 val_loss= 0.73051 val_acc= 0.84227 time= 0.13320
Epoch: 0085 train_loss= 0.72651 train_acc= 0.84334 val_loss= 0.71870 val_acc= 0.84533 time= 0.15400
Epoch: 0086 train_loss= 0.72317 train_acc= 0.84130 val_loss= 0.70696 val_acc= 0.84686 time= 0.13200
Epoch: 0087 train_loss= 0.72137 train_acc= 0.83807 val_loss= 0.69525 val_acc= 0.84686 time= 0.13000
Epoch: 0088 train_loss= 0.69032 train_acc= 0.84674 val_loss= 0.68377 val_acc= 0.84839 time= 0.13001
Epoch: 0089 train_loss= 0.68910 train_acc= 0.84640 val_loss= 0.67273 val_acc= 0.84839 time= 0.15299
Epoch: 0090 train_loss= 0.68018 train_acc= 0.84572 val_loss= 0.66207 val_acc= 0.85299 time= 0.13697
Epoch: 0091 train_loss= 0.66665 train_acc= 0.85729 val_loss= 0.65181 val_acc= 0.85299 time= 0.13600
Epoch: 0092 train_loss= 0.65275 train_acc= 0.85933 val_loss= 0.64224 val_acc= 0.85299 time= 0.15204
Epoch: 0093 train_loss= 0.63914 train_acc= 0.85797 val_loss= 0.63302 val_acc= 0.85452 time= 0.13119
Epoch: 0094 train_loss= 0.65082 train_acc= 0.84980 val_loss= 0.62402 val_acc= 0.85452 time= 0.13104
Epoch: 0095 train_loss= 0.62599 train_acc= 0.86460 val_loss= 0.61477 val_acc= 0.85605 time= 0.13400
Epoch: 0096 train_loss= 0.62852 train_acc= 0.86035 val_loss= 0.60550 val_acc= 0.85605 time= 0.15501
Epoch: 0097 train_loss= 0.60964 train_acc= 0.86307 val_loss= 0.59600 val_acc= 0.85911 time= 0.13295
Epoch: 0098 train_loss= 0.59007 train_acc= 0.86562 val_loss= 0.58626 val_acc= 0.86677 time= 0.13037
Epoch: 0099 train_loss= 0.58747 train_acc= 0.86732 val_loss= 0.57700 val_acc= 0.86830 time= 0.15700
Epoch: 0100 train_loss= 0.58348 train_acc= 0.86971 val_loss= 0.56799 val_acc= 0.86983 time= 0.13262
Epoch: 0101 train_loss= 0.57995 train_acc= 0.86903 val_loss= 0.55929 val_acc= 0.86983 time= 0.13200
Epoch: 0102 train_loss= 0.55031 train_acc= 0.87379 val_loss= 0.55085 val_acc= 0.87289 time= 0.13000
Epoch: 0103 train_loss= 0.55539 train_acc= 0.87566 val_loss= 0.54290 val_acc= 0.87902 time= 0.13897
Epoch: 0104 train_loss= 0.54211 train_acc= 0.87787 val_loss= 0.53500 val_acc= 0.88361 time= 0.14703
Epoch: 0105 train_loss= 0.53577 train_acc= 0.88552 val_loss= 0.52727 val_acc= 0.88668 time= 0.13500
Epoch: 0106 train_loss= 0.52606 train_acc= 0.87821 val_loss= 0.52014 val_acc= 0.88974 time= 0.15001
Epoch: 0107 train_loss= 0.52984 train_acc= 0.87736 val_loss= 0.51317 val_acc= 0.89127 time= 0.13000
Epoch: 0108 train_loss= 0.50322 train_acc= 0.88825 val_loss= 0.50630 val_acc= 0.89280 time= 0.13243
Epoch: 0109 train_loss= 0.50496 train_acc= 0.88842 val_loss= 0.49938 val_acc= 0.89127 time= 0.13454
Epoch: 0110 train_loss= 0.49185 train_acc= 0.88978 val_loss= 0.49250 val_acc= 0.89127 time= 0.16604
Epoch: 0111 train_loss= 0.49274 train_acc= 0.89522 val_loss= 0.48551 val_acc= 0.89127 time= 0.13001
Epoch: 0112 train_loss= 0.48628 train_acc= 0.89488 val_loss= 0.47874 val_acc= 0.89127 time= 0.13200
Epoch: 0113 train_loss= 0.48158 train_acc= 0.89182 val_loss= 0.47201 val_acc= 0.89127 time= 0.13500
Epoch: 0114 train_loss= 0.47348 train_acc= 0.89760 val_loss= 0.46564 val_acc= 0.89280 time= 0.14800
Epoch: 0115 train_loss= 0.45994 train_acc= 0.89760 val_loss= 0.45968 val_acc= 0.89433 time= 0.13100
Epoch: 0116 train_loss= 0.46269 train_acc= 0.90015 val_loss= 0.45371 val_acc= 0.89587 time= 0.13000
Epoch: 0117 train_loss= 0.44294 train_acc= 0.90049 val_loss= 0.44793 val_acc= 0.89893 time= 0.12898
Epoch: 0118 train_loss= 0.45408 train_acc= 0.89845 val_loss= 0.44206 val_acc= 0.89740 time= 0.17200
Epoch: 0119 train_loss= 0.43654 train_acc= 0.89981 val_loss= 0.43593 val_acc= 0.89587 time= 0.13401
Epoch: 0120 train_loss= 0.42579 train_acc= 0.90662 val_loss= 0.43015 val_acc= 0.89740 time= 0.13300
Epoch: 0121 train_loss= 0.42393 train_acc= 0.90611 val_loss= 0.42510 val_acc= 0.90046 time= 0.15204
Epoch: 0122 train_loss= 0.43287 train_acc= 0.90475 val_loss= 0.42053 val_acc= 0.90199 time= 0.12996
Epoch: 0123 train_loss= 0.42200 train_acc= 0.90543 val_loss= 0.41645 val_acc= 0.90199 time= 0.13018
Epoch: 0124 train_loss= 0.41341 train_acc= 0.90764 val_loss= 0.41227 val_acc= 0.90352 time= 0.13100
Epoch: 0125 train_loss= 0.40653 train_acc= 0.90628 val_loss= 0.40812 val_acc= 0.90352 time= 0.16301
Epoch: 0126 train_loss= 0.39627 train_acc= 0.91546 val_loss= 0.40407 val_acc= 0.90352 time= 0.13000
Epoch: 0127 train_loss= 0.40712 train_acc= 0.91070 val_loss= 0.39960 val_acc= 0.90352 time= 0.13300
Epoch: 0128 train_loss= 0.38580 train_acc= 0.92108 val_loss= 0.39546 val_acc= 0.90505 time= 0.15751
Epoch: 0129 train_loss= 0.38474 train_acc= 0.91138 val_loss= 0.39172 val_acc= 0.90505 time= 0.13100
Epoch: 0130 train_loss= 0.37182 train_acc= 0.91257 val_loss= 0.38844 val_acc= 0.90658 time= 0.13100
Epoch: 0131 train_loss= 0.38286 train_acc= 0.91308 val_loss= 0.38528 val_acc= 0.90505 time= 0.13200
Epoch: 0132 train_loss= 0.37700 train_acc= 0.91580 val_loss= 0.38221 val_acc= 0.90505 time= 0.16100
Epoch: 0133 train_loss= 0.36514 train_acc= 0.91376 val_loss= 0.37872 val_acc= 0.90505 time= 0.13100
Epoch: 0134 train_loss= 0.35699 train_acc= 0.92210 val_loss= 0.37487 val_acc= 0.90505 time= 0.13100
Epoch: 0135 train_loss= 0.36700 train_acc= 0.91614 val_loss= 0.37072 val_acc= 0.90965 time= 0.15101
Epoch: 0136 train_loss= 0.35683 train_acc= 0.92022 val_loss= 0.36674 val_acc= 0.90812 time= 0.13399
Epoch: 0137 train_loss= 0.35219 train_acc= 0.91665 val_loss= 0.36259 val_acc= 0.90812 time= 0.13500
Epoch: 0138 train_loss= 0.35123 train_acc= 0.91869 val_loss= 0.35848 val_acc= 0.90658 time= 0.13303
Epoch: 0139 train_loss= 0.34138 train_acc= 0.92771 val_loss= 0.35463 val_acc= 0.90965 time= 0.13331
Epoch: 0140 train_loss= 0.33881 train_acc= 0.92073 val_loss= 0.35132 val_acc= 0.91118 time= 0.16099
Epoch: 0141 train_loss= 0.33008 train_acc= 0.92584 val_loss= 0.34805 val_acc= 0.91271 time= 0.13100
Epoch: 0142 train_loss= 0.33744 train_acc= 0.92482 val_loss= 0.34461 val_acc= 0.91424 time= 0.14800
Epoch: 0143 train_loss= 0.34274 train_acc= 0.92312 val_loss= 0.34144 val_acc= 0.91118 time= 0.13300
Epoch: 0144 train_loss= 0.32391 train_acc= 0.93043 val_loss= 0.33853 val_acc= 0.91118 time= 0.13100
Epoch: 0145 train_loss= 0.33044 train_acc= 0.92703 val_loss= 0.33627 val_acc= 0.91271 time= 0.12996
Epoch: 0146 train_loss= 0.31298 train_acc= 0.93332 val_loss= 0.33453 val_acc= 0.91271 time= 0.13777
Epoch: 0147 train_loss= 0.31951 train_acc= 0.93230 val_loss= 0.33312 val_acc= 0.90965 time= 0.16160
Epoch: 0148 train_loss= 0.31002 train_acc= 0.92771 val_loss= 0.33174 val_acc= 0.90812 time= 0.13003
Epoch: 0149 train_loss= 0.30354 train_acc= 0.93349 val_loss= 0.33028 val_acc= 0.90812 time= 0.14600
Epoch: 0150 train_loss= 0.30126 train_acc= 0.93247 val_loss= 0.32888 val_acc= 0.90965 time= 0.13201
Epoch: 0151 train_loss= 0.30387 train_acc= 0.92975 val_loss= 0.32627 val_acc= 0.91118 time= 0.13301
Epoch: 0152 train_loss= 0.30322 train_acc= 0.92788 val_loss= 0.32328 val_acc= 0.91271 time= 0.13113
Epoch: 0153 train_loss= 0.29130 train_acc= 0.93621 val_loss= 0.32025 val_acc= 0.91424 time= 0.12900
Epoch: 0154 train_loss= 0.30113 train_acc= 0.93638 val_loss= 0.31715 val_acc= 0.91730 time= 0.16500
Epoch: 0155 train_loss= 0.29321 train_acc= 0.93026 val_loss= 0.31410 val_acc= 0.91577 time= 0.13205
Epoch: 0156 train_loss= 0.28587 train_acc= 0.93536 val_loss= 0.31199 val_acc= 0.91577 time= 0.13404
Epoch: 0157 train_loss= 0.27249 train_acc= 0.93860 val_loss= 0.31028 val_acc= 0.91424 time= 0.15201
Epoch: 0158 train_loss= 0.27740 train_acc= 0.93706 val_loss= 0.30922 val_acc= 0.91424 time= 0.13076
Epoch: 0159 train_loss= 0.27167 train_acc= 0.93928 val_loss= 0.30867 val_acc= 0.91424 time= 0.13495
Epoch: 0160 train_loss= 0.28585 train_acc= 0.93417 val_loss= 0.30801 val_acc= 0.91424 time= 0.13200
Epoch: 0161 train_loss= 0.27098 train_acc= 0.93894 val_loss= 0.30686 val_acc= 0.91577 time= 0.16373
Epoch: 0162 train_loss= 0.26497 train_acc= 0.94081 val_loss= 0.30557 val_acc= 0.91730 time= 0.13299
Epoch: 0163 train_loss= 0.27593 train_acc= 0.93060 val_loss= 0.30322 val_acc= 0.91730 time= 0.13100
Epoch: 0164 train_loss= 0.26814 train_acc= 0.93638 val_loss= 0.30052 val_acc= 0.91577 time= 0.15312
Epoch: 0165 train_loss= 0.25518 train_acc= 0.93621 val_loss= 0.29743 val_acc= 0.91884 time= 0.13300
Epoch: 0166 train_loss= 0.27296 train_acc= 0.93553 val_loss= 0.29496 val_acc= 0.91884 time= 0.13307
Epoch: 0167 train_loss= 0.26844 train_acc= 0.93587 val_loss= 0.29305 val_acc= 0.92190 time= 0.13100
Epoch: 0168 train_loss= 0.25656 train_acc= 0.93996 val_loss= 0.29133 val_acc= 0.92190 time= 0.16897
Epoch: 0169 train_loss= 0.25635 train_acc= 0.94132 val_loss= 0.29023 val_acc= 0.92037 time= 0.13003
Epoch: 0170 train_loss= 0.25936 train_acc= 0.94353 val_loss= 0.28969 val_acc= 0.92190 time= 0.13100
Epoch: 0171 train_loss= 0.25470 train_acc= 0.94115 val_loss= 0.28972 val_acc= 0.92190 time= 0.14100
Epoch: 0172 train_loss= 0.25128 train_acc= 0.94115 val_loss= 0.29027 val_acc= 0.92037 time= 0.12901
Epoch: 0173 train_loss= 0.24638 train_acc= 0.94404 val_loss= 0.28991 val_acc= 0.92037 time= 0.13196
Epoch: 0174 train_loss= 0.24223 train_acc= 0.94302 val_loss= 0.28911 val_acc= 0.92037 time= 0.13600
Epoch: 0175 train_loss= 0.24417 train_acc= 0.94319 val_loss= 0.28816 val_acc= 0.92037 time= 0.13600
Epoch: 0176 train_loss= 0.23551 train_acc= 0.94523 val_loss= 0.28694 val_acc= 0.92343 time= 0.15822
Epoch: 0177 train_loss= 0.24057 train_acc= 0.94455 val_loss= 0.28543 val_acc= 0.92037 time= 0.13200
Epoch: 0178 train_loss= 0.23780 train_acc= 0.94778 val_loss= 0.28371 val_acc= 0.91730 time= 0.14799
Epoch: 0179 train_loss= 0.23874 train_acc= 0.94557 val_loss= 0.28202 val_acc= 0.92037 time= 0.12900
Epoch: 0180 train_loss= 0.23257 train_acc= 0.94489 val_loss= 0.28029 val_acc= 0.91884 time= 0.13101
Epoch: 0181 train_loss= 0.23597 train_acc= 0.94200 val_loss= 0.27833 val_acc= 0.91884 time= 0.12901
Epoch: 0182 train_loss= 0.23528 train_acc= 0.94744 val_loss= 0.27720 val_acc= 0.91884 time= 0.16500
Epoch: 0183 train_loss= 0.23282 train_acc= 0.94574 val_loss= 0.27579 val_acc= 0.92343 time= 0.13400
Epoch: 0184 train_loss= 0.22354 train_acc= 0.94676 val_loss= 0.27456 val_acc= 0.92190 time= 0.13603
Epoch: 0185 train_loss= 0.22497 train_acc= 0.94676 val_loss= 0.27264 val_acc= 0.92343 time= 0.14197
Epoch: 0186 train_loss= 0.22003 train_acc= 0.95016 val_loss= 0.27093 val_acc= 0.92190 time= 0.13903
Epoch: 0187 train_loss= 0.22877 train_acc= 0.94302 val_loss= 0.26978 val_acc= 0.92190 time= 0.13101
Epoch: 0188 train_loss= 0.22490 train_acc= 0.94744 val_loss= 0.26871 val_acc= 0.92190 time= 0.13039
Epoch: 0189 train_loss= 0.21641 train_acc= 0.95033 val_loss= 0.26825 val_acc= 0.92343 time= 0.13100
Epoch: 0190 train_loss= 0.22395 train_acc= 0.94693 val_loss= 0.26783 val_acc= 0.92037 time= 0.16401
Epoch: 0191 train_loss= 0.21233 train_acc= 0.94999 val_loss= 0.26755 val_acc= 0.92037 time= 0.13254
Epoch: 0192 train_loss= 0.21765 train_acc= 0.94948 val_loss= 0.26734 val_acc= 0.92037 time= 0.13810
Epoch: 0193 train_loss= 0.20388 train_acc= 0.95016 val_loss= 0.26667 val_acc= 0.92190 time= 0.15997
Epoch: 0194 train_loss= 0.21652 train_acc= 0.95135 val_loss= 0.26583 val_acc= 0.92190 time= 0.13000
Epoch: 0195 train_loss= 0.20599 train_acc= 0.95067 val_loss= 0.26477 val_acc= 0.92343 time= 0.13000
Epoch: 0196 train_loss= 0.19939 train_acc= 0.95288 val_loss= 0.26332 val_acc= 0.92649 time= 0.13100
Epoch: 0197 train_loss= 0.21174 train_acc= 0.95288 val_loss= 0.26179 val_acc= 0.92496 time= 0.16651
Epoch: 0198 train_loss= 0.19252 train_acc= 0.95458 val_loss= 0.26016 val_acc= 0.92343 time= 0.13200
Epoch: 0199 train_loss= 0.20513 train_acc= 0.95186 val_loss= 0.25883 val_acc= 0.92343 time= 0.13100
Epoch: 0200 train_loss= 0.19604 train_acc= 0.95237 val_loss= 0.25796 val_acc= 0.92190 time= 0.14905
Epoch: 0201 train_loss= 0.18976 train_acc= 0.95697 val_loss= 0.25749 val_acc= 0.92037 time= 0.13395
Epoch: 0202 train_loss= 0.18243 train_acc= 0.95969 val_loss= 0.25739 val_acc= 0.92037 time= 0.13300
Epoch: 0203 train_loss= 0.20016 train_acc= 0.95237 val_loss= 0.25730 val_acc= 0.92037 time= 0.13200
Epoch: 0204 train_loss= 0.19035 train_acc= 0.95833 val_loss= 0.25740 val_acc= 0.92190 time= 0.15000
Epoch: 0205 train_loss= 0.18777 train_acc= 0.95714 val_loss= 0.25714 val_acc= 0.92037 time= 0.13600
Epoch: 0206 train_loss= 0.19706 train_acc= 0.95663 val_loss= 0.25681 val_acc= 0.92037 time= 0.13200
Epoch: 0207 train_loss= 0.18316 train_acc= 0.95509 val_loss= 0.25651 val_acc= 0.92190 time= 0.14400
Epoch: 0208 train_loss= 0.19013 train_acc= 0.95543 val_loss= 0.25597 val_acc= 0.92190 time= 0.13200
Epoch: 0209 train_loss= 0.19907 train_acc= 0.95016 val_loss= 0.25507 val_acc= 0.92190 time= 0.13500
Epoch: 0210 train_loss= 0.18723 train_acc= 0.95577 val_loss= 0.25363 val_acc= 0.92649 time= 0.13400
Epoch: 0211 train_loss= 0.19505 train_acc= 0.95203 val_loss= 0.25154 val_acc= 0.92956 time= 0.13700
Epoch: 0212 train_loss= 0.17692 train_acc= 0.95935 val_loss= 0.25001 val_acc= 0.92956 time= 0.16704
Epoch: 0213 train_loss= 0.17539 train_acc= 0.95748 val_loss= 0.24857 val_acc= 0.92956 time= 0.13200
Epoch: 0214 train_loss= 0.18544 train_acc= 0.95612 val_loss= 0.24779 val_acc= 0.93109 time= 0.15100
Epoch: 0215 train_loss= 0.18596 train_acc= 0.95663 val_loss= 0.24758 val_acc= 0.92802 time= 0.13241
Epoch: 0216 train_loss= 0.17696 train_acc= 0.95850 val_loss= 0.24756 val_acc= 0.92802 time= 0.13201
Epoch: 0217 train_loss= 0.17796 train_acc= 0.95714 val_loss= 0.24722 val_acc= 0.92956 time= 0.13099
Epoch: 0218 train_loss= 0.18150 train_acc= 0.95646 val_loss= 0.24694 val_acc= 0.92956 time= 0.14900
Epoch: 0219 train_loss= 0.17235 train_acc= 0.95969 val_loss= 0.24721 val_acc= 0.92956 time= 0.14211
Epoch: 0220 train_loss= 0.17218 train_acc= 0.95731 val_loss= 0.24675 val_acc= 0.92956 time= 0.13800
Epoch: 0221 train_loss= 0.17951 train_acc= 0.95646 val_loss= 0.24605 val_acc= 0.92649 time= 0.15404
Epoch: 0222 train_loss= 0.17685 train_acc= 0.95748 val_loss= 0.24475 val_acc= 0.92649 time= 0.12901
Epoch: 0223 train_loss= 0.16499 train_acc= 0.95952 val_loss= 0.24375 val_acc= 0.92802 time= 0.13396
Epoch: 0224 train_loss= 0.17314 train_acc= 0.95918 val_loss= 0.24276 val_acc= 0.92956 time= 0.12951
Epoch: 0225 train_loss= 0.16702 train_acc= 0.95986 val_loss= 0.24219 val_acc= 0.92802 time= 0.13303
Epoch: 0226 train_loss= 0.17023 train_acc= 0.95646 val_loss= 0.24166 val_acc= 0.92496 time= 0.15901
Epoch: 0227 train_loss= 0.17646 train_acc= 0.95663 val_loss= 0.24138 val_acc= 0.92496 time= 0.13195
Epoch: 0228 train_loss= 0.16507 train_acc= 0.96207 val_loss= 0.24085 val_acc= 0.92496 time= 0.13500
Epoch: 0229 train_loss= 0.15606 train_acc= 0.96411 val_loss= 0.24089 val_acc= 0.92649 time= 0.15100
Epoch: 0230 train_loss= 0.15830 train_acc= 0.96309 val_loss= 0.24118 val_acc= 0.92802 time= 0.13400
Epoch: 0231 train_loss= 0.16283 train_acc= 0.96258 val_loss= 0.24080 val_acc= 0.92802 time= 0.13105
Epoch: 0232 train_loss= 0.16303 train_acc= 0.96377 val_loss= 0.24008 val_acc= 0.92343 time= 0.13206
Epoch: 0233 train_loss= 0.15725 train_acc= 0.96207 val_loss= 0.23911 val_acc= 0.92649 time= 0.16400
Epoch: 0234 train_loss= 0.15739 train_acc= 0.96428 val_loss= 0.23814 val_acc= 0.93109 time= 0.13200
Epoch: 0235 train_loss= 0.15550 train_acc= 0.96258 val_loss= 0.23751 val_acc= 0.93262 time= 0.12997
Epoch: 0236 train_loss= 0.15763 train_acc= 0.96190 val_loss= 0.23752 val_acc= 0.93415 time= 0.15203
Epoch: 0237 train_loss= 0.15385 train_acc= 0.96173 val_loss= 0.23792 val_acc= 0.93109 time= 0.13108
Epoch: 0238 train_loss= 0.16604 train_acc= 0.96054 val_loss= 0.23838 val_acc= 0.93109 time= 0.13296
Epoch: 0239 train_loss= 0.15744 train_acc= 0.96241 val_loss= 0.23827 val_acc= 0.93415 time= 0.13400
Epoch: 0240 train_loss= 0.15493 train_acc= 0.95935 val_loss= 0.23794 val_acc= 0.93262 time= 0.16400
Epoch: 0241 train_loss= 0.15072 train_acc= 0.96411 val_loss= 0.23763 val_acc= 0.92956 time= 0.13104
Epoch: 0242 train_loss= 0.14640 train_acc= 0.96700 val_loss= 0.23773 val_acc= 0.92956 time= 0.13099
Epoch: 0243 train_loss= 0.14220 train_acc= 0.96632 val_loss= 0.23810 val_acc= 0.92802 time= 0.14601
Early stopping...
Optimization Finished!
Test set results: cost= 0.29462 accuracy= 0.92796 time= 0.06096
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     1.0000    0.3333    0.5000         6
           2     0.3333    1.0000    0.5000         1
           3     0.7684    0.9733    0.8588        75
           4     1.0000    1.0000    1.0000         9
           5     0.8144    0.9080    0.8587        87
           6     0.8214    0.9200    0.8679        25
           7     0.5000    0.8462    0.6286        13
           8     0.7143    0.9091    0.8000        11
           9     1.0000    0.4444    0.6154         9
          10     0.8846    0.6389    0.7419        36
          11     1.0000    0.9167    0.9565        12
          12     0.8182    0.9669    0.8864       121
          13     0.7647    0.6842    0.7222        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.2500    0.4000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.2222    0.3636         9
          21     0.8261    0.9500    0.8837        20
          22     0.4000    0.4000    0.4000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8125    0.7647    0.7879        17
          25     1.0000    0.7333    0.8462        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.7273    0.8421        11
          29     0.9656    0.9684    0.9670       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.5000    0.8000    0.6154        10
          33     1.0000    0.3333    0.5000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8590    0.8272    0.8428        81
          36     0.6667    0.3333    0.4444        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9799    0.9908    0.9853      1083
          40     0.0000    0.0000    0.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7895    1.0000    0.8824        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9280      2568
   macro avg     0.6847    0.5886    0.6006      2568
weighted avg     0.9248    0.9280    0.9207      2568

Macro average Test Precision, Recall and F1-Score...
(0.6846717432970372, 0.5885905376732598, 0.6005664605765324, None)
Micro average Test Precision, Recall and F1-Score...
(0.9279595015576324, 0.9279595015576324, 0.9279595015576324, None)
embeddings:
8892 6532 2568
[[-1.7843595e-01  2.8871291e+00 -2.3054920e-02 ...  2.7819848e+00
  -2.1955626e-01  1.0202388e+00]
 [ 8.0496073e-04  4.3367732e-01 -5.2412931e-02 ...  1.0325793e+00
  -2.2042935e-01  1.3747597e+00]
 [ 2.4958332e-01  1.3929818e+00  1.9926551e-01 ...  1.5069524e+00
   1.1415300e+00  9.4769460e-01]
 ...
 [ 1.7934147e-01  6.3390809e-01  3.8046354e-01 ...  5.5508977e-01
   3.4571153e-01  2.7141642e-02]
 [ 1.9453277e-01  6.0047543e-01  2.1598235e-01 ...  8.0352783e-01
   5.1303655e-01 -3.5068329e-02]
 [ 4.0986526e-01  4.2600390e-01  4.3952969e-01 ...  4.6557280e-01
   5.1476574e-01  5.6632847e-01]]

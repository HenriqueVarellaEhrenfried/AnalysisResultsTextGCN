(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95146 train_acc= 0.00289 val_loss= 3.45317 val_acc= 0.66462 time= 0.46759
Epoch: 0002 train_loss= 3.45330 train_acc= 0.65555 val_loss= 2.46874 val_acc= 0.62634 time= 0.17854
Epoch: 0003 train_loss= 2.46791 train_acc= 0.62324 val_loss= 2.24670 val_acc= 0.51608 time= 0.17373
Epoch: 0004 train_loss= 2.24555 train_acc= 0.50995 val_loss= 2.07673 val_acc= 0.54364 time= 0.16800
Epoch: 0005 train_loss= 2.08907 train_acc= 0.53615 val_loss= 1.69044 val_acc= 0.66769 time= 0.19400
Epoch: 0006 train_loss= 1.72024 train_acc= 0.65181 val_loss= 1.47710 val_acc= 0.68147 time= 0.17000
Epoch: 0007 train_loss= 1.51823 train_acc= 0.65640 val_loss= 1.37947 val_acc= 0.71516 time= 0.16811
Epoch: 0008 train_loss= 1.41707 train_acc= 0.70913 val_loss= 1.26034 val_acc= 0.72129 time= 0.16699
Epoch: 0009 train_loss= 1.29156 train_acc= 0.72172 val_loss= 1.15675 val_acc= 0.72282 time= 0.19504
Epoch: 0010 train_loss= 1.17238 train_acc= 0.72802 val_loss= 1.07572 val_acc= 0.73966 time= 0.16900
Epoch: 0011 train_loss= 1.08769 train_acc= 0.74077 val_loss= 1.01464 val_acc= 0.75498 time= 0.18000
Epoch: 0012 train_loss= 1.01192 train_acc= 0.76203 val_loss= 0.96141 val_acc= 0.76110 time= 0.17100
Epoch: 0013 train_loss= 0.95728 train_acc= 0.77207 val_loss= 0.89959 val_acc= 0.77335 time= 0.16903
Epoch: 0014 train_loss= 0.89654 train_acc= 0.78449 val_loss= 0.83532 val_acc= 0.79173 time= 0.16900
Epoch: 0015 train_loss= 0.83242 train_acc= 0.79605 val_loss= 0.77650 val_acc= 0.81623 time= 0.17197
Epoch: 0016 train_loss= 0.77263 train_acc= 0.81970 val_loss= 0.72541 val_acc= 0.84074 time= 0.18300
Epoch: 0017 train_loss= 0.72815 train_acc= 0.83773 val_loss= 0.68412 val_acc= 0.85299 time= 0.18000
Epoch: 0018 train_loss= 0.68657 train_acc= 0.84742 val_loss= 0.64944 val_acc= 0.84992 time= 0.17159
Epoch: 0019 train_loss= 0.64419 train_acc= 0.85389 val_loss= 0.61919 val_acc= 0.85145 time= 0.16797
Epoch: 0020 train_loss= 0.59361 train_acc= 0.86358 val_loss= 0.59105 val_acc= 0.84992 time= 0.16804
Epoch: 0021 train_loss= 0.55637 train_acc= 0.86817 val_loss= 0.56087 val_acc= 0.85605 time= 0.20010
Epoch: 0022 train_loss= 0.51289 train_acc= 0.88042 val_loss= 0.52590 val_acc= 0.86217 time= 0.17298
Epoch: 0023 train_loss= 0.47474 train_acc= 0.88961 val_loss= 0.49264 val_acc= 0.86371 time= 0.16698
Epoch: 0024 train_loss= 0.44204 train_acc= 0.89216 val_loss= 0.46455 val_acc= 0.86677 time= 0.17066
Epoch: 0025 train_loss= 0.41730 train_acc= 0.89471 val_loss= 0.44167 val_acc= 0.87443 time= 0.17004
Epoch: 0026 train_loss= 0.38098 train_acc= 0.90151 val_loss= 0.42373 val_acc= 0.87443 time= 0.17200
Epoch: 0027 train_loss= 0.36022 train_acc= 0.91019 val_loss= 0.40871 val_acc= 0.88208 time= 0.20401
Epoch: 0028 train_loss= 0.33081 train_acc= 0.91750 val_loss= 0.39335 val_acc= 0.89127 time= 0.18599
Epoch: 0029 train_loss= 0.30359 train_acc= 0.92346 val_loss= 0.37512 val_acc= 0.90046 time= 0.16800
Epoch: 0030 train_loss= 0.29261 train_acc= 0.92975 val_loss= 0.35076 val_acc= 0.90812 time= 0.17400
Epoch: 0031 train_loss= 0.26731 train_acc= 0.93621 val_loss= 0.33513 val_acc= 0.91118 time= 0.17051
Epoch: 0032 train_loss= 0.24411 train_acc= 0.94268 val_loss= 0.32831 val_acc= 0.91118 time= 0.17304
Epoch: 0033 train_loss= 0.23138 train_acc= 0.94064 val_loss= 0.31830 val_acc= 0.91118 time= 0.19968
Epoch: 0034 train_loss= 0.21225 train_acc= 0.94353 val_loss= 0.31417 val_acc= 0.91730 time= 0.17500
Epoch: 0035 train_loss= 0.19070 train_acc= 0.95135 val_loss= 0.31413 val_acc= 0.91730 time= 0.16800
Epoch: 0036 train_loss= 0.17756 train_acc= 0.95867 val_loss= 0.31530 val_acc= 0.91424 time= 0.17000
Epoch: 0037 train_loss= 0.16815 train_acc= 0.95799 val_loss= 0.30942 val_acc= 0.91730 time= 0.16997
Epoch: 0038 train_loss= 0.16040 train_acc= 0.95901 val_loss= 0.29724 val_acc= 0.91884 time= 0.20600
Epoch: 0039 train_loss= 0.14053 train_acc= 0.96394 val_loss= 0.28954 val_acc= 0.92343 time= 0.18403
Epoch: 0040 train_loss= 0.13598 train_acc= 0.96411 val_loss= 0.28690 val_acc= 0.92190 time= 0.17000
Epoch: 0041 train_loss= 0.12163 train_acc= 0.96921 val_loss= 0.28561 val_acc= 0.92343 time= 0.17000
Epoch: 0042 train_loss= 0.11433 train_acc= 0.97040 val_loss= 0.28462 val_acc= 0.92649 time= 0.17404
Epoch: 0043 train_loss= 0.10194 train_acc= 0.97449 val_loss= 0.27932 val_acc= 0.92496 time= 0.17200
Epoch: 0044 train_loss= 0.09667 train_acc= 0.97721 val_loss= 0.27378 val_acc= 0.93109 time= 0.19400
Epoch: 0045 train_loss= 0.09239 train_acc= 0.97602 val_loss= 0.27305 val_acc= 0.93109 time= 0.16800
Epoch: 0046 train_loss= 0.08273 train_acc= 0.97840 val_loss= 0.27628 val_acc= 0.93415 time= 0.17157
Epoch: 0047 train_loss= 0.07915 train_acc= 0.98112 val_loss= 0.28013 val_acc= 0.93262 time= 0.17403
Early stopping...
Optimization Finished!
Test set results: cost= 0.30069 accuracy= 0.93341 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.6667    0.3333    0.4444         6
           2     0.0000    0.0000    0.0000         1
           3     0.8140    0.9333    0.8696        75
           4     1.0000    1.0000    1.0000         9
           5     0.8000    0.9195    0.8556        87
           6     1.0000    0.9200    0.9583        25
           7     0.7500    0.9231    0.8276        13
           8     0.8333    0.9091    0.8696        11
           9     1.0000    0.2222    0.3636         9
          10     0.8800    0.6111    0.7213        36
          11     1.0000    0.9167    0.9565        12
          12     0.8276    0.9917    0.9023       121
          13     1.0000    0.7368    0.8485        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.6667    0.4444    0.5333         9
          21     0.9048    0.9500    0.9268        20
          22     0.3750    0.6000    0.4615         5
          23     0.0000    0.0000    0.0000         1
          24     0.6667    0.8235    0.7368        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9167    0.9167    0.9167        12
          28     0.9000    0.8182    0.8571        11
          29     0.9655    0.9655    0.9655       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.6667    0.8000         3
          32     0.6154    0.8000    0.6957        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8442    0.8025    0.8228        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9782    0.9926    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.6923    0.7500    0.7200        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9334      2568
   macro avg     0.7404    0.6267    0.6537      2568
weighted avg     0.9316    0.9334    0.9275      2568

Macro average Test Precision, Recall and F1-Score...
(0.7404304714276285, 0.6266677578363097, 0.6536995942398982, None)
Micro average Test Precision, Recall and F1-Score...
(0.9334112149532711, 0.9334112149532711, 0.9334112149532711, None)
embeddings:
8892 6532 2568
[[-0.54738826 -0.7988089   1.952446   ... -0.40100268 -0.14935628
   0.8381239 ]
 [-0.09179515 -0.47265726  0.837661   ... -0.31677502 -0.15900348
   0.29349566]
 [-0.35916322 -0.55904186  0.7887261  ...  0.23572315  0.05733477
  -0.25624114]
 ...
 [-0.16095299 -0.23837015  0.03001662 ... -0.13238063  0.516744
  -0.06872952]
 [-0.13177781 -0.2561305   0.4679051  ...  0.06579751 -0.03762287
  -0.02926085]
 [ 0.1520961  -0.01442289  0.08025149 ...  0.0356987  -0.16895075
   0.17386624]]

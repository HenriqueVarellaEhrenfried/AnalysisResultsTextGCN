(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.00357 val_loss= 3.40116 val_acc= 0.64625 time= 0.46800
Epoch: 0002 train_loss= 3.41338 train_acc= 0.63259 val_loss= 2.41033 val_acc= 0.64319 time= 0.17300
Epoch: 0003 train_loss= 2.41320 train_acc= 0.63106 val_loss= 2.23755 val_acc= 0.61256 time= 0.16601
Epoch: 0004 train_loss= 2.24236 train_acc= 0.60912 val_loss= 2.03359 val_acc= 0.51914 time= 0.16699
Epoch: 0005 train_loss= 2.05563 train_acc= 0.51641 val_loss= 1.64333 val_acc= 0.66616 time= 0.16900
Epoch: 0006 train_loss= 1.68622 train_acc= 0.65079 val_loss= 1.47556 val_acc= 0.67688 time= 0.20333
Epoch: 0007 train_loss= 1.52726 train_acc= 0.65215 val_loss= 1.37846 val_acc= 0.70444 time= 0.17403
Epoch: 0008 train_loss= 1.42263 train_acc= 0.69093 val_loss= 1.26647 val_acc= 0.71210 time= 0.16700
Epoch: 0009 train_loss= 1.28960 train_acc= 0.71084 val_loss= 1.17294 val_acc= 0.71822 time= 0.16701
Epoch: 0010 train_loss= 1.19466 train_acc= 0.71577 val_loss= 1.09635 val_acc= 0.72894 time= 0.16799
Epoch: 0011 train_loss= 1.11629 train_acc= 0.73006 val_loss= 1.03715 val_acc= 0.74579 time= 0.16600
Epoch: 0012 train_loss= 1.04716 train_acc= 0.74979 val_loss= 0.98652 val_acc= 0.75038 time= 0.20597
Epoch: 0013 train_loss= 1.00264 train_acc= 0.76169 val_loss= 0.93029 val_acc= 0.75651 time= 0.17407
Epoch: 0014 train_loss= 0.93500 train_acc= 0.76816 val_loss= 0.87025 val_acc= 0.78714 time= 0.17012
Epoch: 0015 train_loss= 0.88034 train_acc= 0.78942 val_loss= 0.81282 val_acc= 0.81317 time= 0.16700
Epoch: 0016 train_loss= 0.81415 train_acc= 0.81664 val_loss= 0.76309 val_acc= 0.82695 time= 0.16707
Epoch: 0017 train_loss= 0.77040 train_acc= 0.82667 val_loss= 0.71885 val_acc= 0.84686 time= 0.16798
Epoch: 0018 train_loss= 0.73054 train_acc= 0.84113 val_loss= 0.67874 val_acc= 0.85299 time= 0.19500
Epoch: 0019 train_loss= 0.67990 train_acc= 0.85219 val_loss= 0.64297 val_acc= 0.85452 time= 0.17108
Epoch: 0020 train_loss= 0.63404 train_acc= 0.85831 val_loss= 0.61144 val_acc= 0.85758 time= 0.16800
Epoch: 0021 train_loss= 0.58527 train_acc= 0.86869 val_loss= 0.58200 val_acc= 0.85911 time= 0.17189
Epoch: 0022 train_loss= 0.55289 train_acc= 0.86971 val_loss= 0.55242 val_acc= 0.86217 time= 0.16904
Epoch: 0023 train_loss= 0.51866 train_acc= 0.87736 val_loss= 0.52043 val_acc= 0.86064 time= 0.17000
Epoch: 0024 train_loss= 0.48659 train_acc= 0.88399 val_loss= 0.49013 val_acc= 0.87136 time= 0.20400
Epoch: 0025 train_loss= 0.44952 train_acc= 0.89233 val_loss= 0.46435 val_acc= 0.87443 time= 0.17003
Epoch: 0026 train_loss= 0.41570 train_acc= 0.89403 val_loss= 0.44154 val_acc= 0.88055 time= 0.16700
Epoch: 0027 train_loss= 0.39206 train_acc= 0.90151 val_loss= 0.42221 val_acc= 0.88515 time= 0.16800
Epoch: 0028 train_loss= 0.36272 train_acc= 0.90968 val_loss= 0.40625 val_acc= 0.89433 time= 0.17100
Epoch: 0029 train_loss= 0.34357 train_acc= 0.91376 val_loss= 0.39195 val_acc= 0.89587 time= 0.21301
Epoch: 0030 train_loss= 0.31590 train_acc= 0.91869 val_loss= 0.37808 val_acc= 0.90199 time= 0.16700
Epoch: 0031 train_loss= 0.28760 train_acc= 0.93026 val_loss= 0.35851 val_acc= 0.90812 time= 0.17200
Epoch: 0032 train_loss= 0.26200 train_acc= 0.93689 val_loss= 0.34252 val_acc= 0.90965 time= 0.16804
Epoch: 0033 train_loss= 0.24836 train_acc= 0.93962 val_loss= 0.33082 val_acc= 0.91118 time= 0.16700
Epoch: 0034 train_loss= 0.23349 train_acc= 0.94149 val_loss= 0.32301 val_acc= 0.91730 time= 0.16696
Epoch: 0035 train_loss= 0.20940 train_acc= 0.94880 val_loss= 0.31721 val_acc= 0.91884 time= 0.18900
Epoch: 0036 train_loss= 0.20274 train_acc= 0.94931 val_loss= 0.31384 val_acc= 0.91884 time= 0.17100
Epoch: 0037 train_loss= 0.17407 train_acc= 0.95594 val_loss= 0.30356 val_acc= 0.92649 time= 0.17200
Epoch: 0038 train_loss= 0.16345 train_acc= 0.95884 val_loss= 0.29706 val_acc= 0.92649 time= 0.16800
Epoch: 0039 train_loss= 0.15343 train_acc= 0.96258 val_loss= 0.29269 val_acc= 0.92496 time= 0.16700
Epoch: 0040 train_loss= 0.13630 train_acc= 0.96904 val_loss= 0.28833 val_acc= 0.92802 time= 0.16700
Epoch: 0041 train_loss= 0.13552 train_acc= 0.96377 val_loss= 0.28896 val_acc= 0.93109 time= 0.20800
Epoch: 0042 train_loss= 0.12410 train_acc= 0.96819 val_loss= 0.28808 val_acc= 0.92956 time= 0.16709
Epoch: 0043 train_loss= 0.11126 train_acc= 0.96870 val_loss= 0.28713 val_acc= 0.92956 time= 0.17300
Epoch: 0044 train_loss= 0.10656 train_acc= 0.97227 val_loss= 0.28754 val_acc= 0.92649 time= 0.17100
Epoch: 0045 train_loss= 0.09602 train_acc= 0.97500 val_loss= 0.27986 val_acc= 0.93262 time= 0.16703
Epoch: 0046 train_loss= 0.09644 train_acc= 0.97704 val_loss= 0.27244 val_acc= 0.93721 time= 0.19299
Epoch: 0047 train_loss= 0.08898 train_acc= 0.97891 val_loss= 0.26900 val_acc= 0.93415 time= 0.17200
Epoch: 0048 train_loss= 0.07572 train_acc= 0.98010 val_loss= 0.27207 val_acc= 0.93109 time= 0.16800
Epoch: 0049 train_loss= 0.07286 train_acc= 0.98010 val_loss= 0.28773 val_acc= 0.93109 time= 0.16900
Early stopping...
Optimization Finished!
Test set results: cost= 0.30703 accuracy= 0.92640 time= 0.07497
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.2500    0.4000         8
           1     0.7500    0.5000    0.6000         6
           2     0.0000    0.0000    0.0000         1
           3     0.7931    0.9200    0.8519        75
           4     1.0000    1.0000    1.0000         9
           5     0.7273    0.9195    0.8122        87
           6     0.9583    0.9200    0.9388        25
           7     0.7857    0.8462    0.8148        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.4444    0.6154         9
          10     0.9500    0.5278    0.6786        36
          11     1.0000    0.9167    0.9565        12
          12     0.7947    0.9917    0.8824       121
          13     0.9333    0.7368    0.8235        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.5714    0.4444    0.5000         9
          21     0.9048    0.9500    0.9268        20
          22     0.4000    0.4000    0.4000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7059    0.7059    0.7059        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.8571    1.0000    0.9231        12
          28     1.0000    0.7273    0.8421        11
          29     0.9639    0.9598    0.9618       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    0.6667    0.8000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     1.0000    1.0000    1.0000         1
          35     0.8788    0.7160    0.7891        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9746    0.9926    0.9835      1083
          40     0.7143    1.0000    0.8333         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.6923    0.7500    0.7200        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9167    0.7333    0.8148        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9264      2568
   macro avg     0.7490    0.6378    0.6681      2568
weighted avg     0.9270    0.9264    0.9210      2568

Macro average Test Precision, Recall and F1-Score...
(0.7490050251818821, 0.6378385661507182, 0.668119074890069, None)
Micro average Test Precision, Recall and F1-Score...
(0.9264018691588785, 0.9264018691588785, 0.9264018691588785, None)
embeddings:
8892 6532 2568
[[-0.7389236  -0.21160911 -0.47349805 ... -0.5492036  -0.2896667
  -0.47419623]
 [-0.19936372 -0.26272038 -0.2966777  ... -0.13047542 -0.17026012
  -0.26187727]
 [-0.3347336  -0.345304   -0.4807118  ... -0.4748094   0.07026533
  -0.4428754 ]
 ...
 [-0.16742538 -0.00787585 -0.14765383 ... -0.1421528   0.08358262
  -0.23330916]
 [-0.11627731 -0.15631492 -0.1858144  ... -0.14187002  0.11629777
  -0.25168887]
 [ 0.17990504  0.14728907  0.15988989 ...  0.2078357   0.37713012
  -0.39553118]]

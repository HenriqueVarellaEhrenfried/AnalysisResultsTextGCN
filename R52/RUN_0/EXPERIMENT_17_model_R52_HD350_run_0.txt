(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95121 train_acc= 0.01089 val_loss= 3.87371 val_acc= 0.67075 time= 0.52905
Epoch: 0002 train_loss= 3.87454 train_acc= 0.65555 val_loss= 3.71034 val_acc= 0.67381 time= 0.22600
Epoch: 0003 train_loss= 3.71112 train_acc= 0.65283 val_loss= 3.45373 val_acc= 0.67228 time= 0.20996
Epoch: 0004 train_loss= 3.46150 train_acc= 0.65130 val_loss= 3.11935 val_acc= 0.67075 time= 0.21604
Epoch: 0005 train_loss= 3.12666 train_acc= 0.65147 val_loss= 2.75961 val_acc= 0.66462 time= 0.21196
Epoch: 0006 train_loss= 2.78193 train_acc= 0.65062 val_loss= 2.44895 val_acc= 0.65237 time= 0.21500
Epoch: 0007 train_loss= 2.44139 train_acc= 0.64195 val_loss= 2.25858 val_acc= 0.61868 time= 0.24004
Epoch: 0008 train_loss= 2.26113 train_acc= 0.60555 val_loss= 2.18082 val_acc= 0.52986 time= 0.20900
Epoch: 0009 train_loss= 2.18172 train_acc= 0.54278 val_loss= 2.14137 val_acc= 0.48392 time= 0.21301
Epoch: 0010 train_loss= 2.13889 train_acc= 0.46249 val_loss= 2.08465 val_acc= 0.47933 time= 0.21195
Epoch: 0011 train_loss= 2.09525 train_acc= 0.45484 val_loss= 1.99355 val_acc= 0.49464 time= 0.23614
Epoch: 0012 train_loss= 2.00906 train_acc= 0.47083 val_loss= 1.87983 val_acc= 0.53446 time= 0.21104
Epoch: 0013 train_loss= 1.89965 train_acc= 0.53274 val_loss= 1.76852 val_acc= 0.62021 time= 0.20783
Epoch: 0014 train_loss= 1.79298 train_acc= 0.60878 val_loss= 1.68002 val_acc= 0.65850 time= 0.21400
Epoch: 0015 train_loss= 1.71271 train_acc= 0.64722 val_loss= 1.61376 val_acc= 0.67381 time= 0.21600
Epoch: 0016 train_loss= 1.64321 train_acc= 0.65232 val_loss= 1.55454 val_acc= 0.67994 time= 0.21200
Epoch: 0017 train_loss= 1.57950 train_acc= 0.65964 val_loss= 1.49322 val_acc= 0.67994 time= 0.21300
Epoch: 0018 train_loss= 1.52073 train_acc= 0.66117 val_loss= 1.43036 val_acc= 0.68606 time= 0.21306
Epoch: 0019 train_loss= 1.45777 train_acc= 0.66355 val_loss= 1.37051 val_acc= 0.69066 time= 0.21400
Epoch: 0020 train_loss= 1.39581 train_acc= 0.67886 val_loss= 1.31692 val_acc= 0.69678 time= 0.24199
Epoch: 0021 train_loss= 1.34516 train_acc= 0.68039 val_loss= 1.27023 val_acc= 0.70444 time= 0.21000
Epoch: 0022 train_loss= 1.29918 train_acc= 0.68379 val_loss= 1.22942 val_acc= 0.71669 time= 0.21101
Epoch: 0023 train_loss= 1.25680 train_acc= 0.69842 val_loss= 1.19287 val_acc= 0.71975 time= 0.21596
Epoch: 0024 train_loss= 1.21726 train_acc= 0.71203 val_loss= 1.15931 val_acc= 0.72741 time= 0.24004
Epoch: 0025 train_loss= 1.18051 train_acc= 0.72597 val_loss= 1.12749 val_acc= 0.73047 time= 0.21201
Epoch: 0026 train_loss= 1.15016 train_acc= 0.73295 val_loss= 1.09653 val_acc= 0.73813 time= 0.21100
Epoch: 0027 train_loss= 1.11021 train_acc= 0.74519 val_loss= 1.06577 val_acc= 0.74426 time= 0.21100
Epoch: 0028 train_loss= 1.07913 train_acc= 0.74911 val_loss= 1.03472 val_acc= 0.75038 time= 0.21096
Epoch: 0029 train_loss= 1.04688 train_acc= 0.75472 val_loss= 1.00354 val_acc= 0.75957 time= 0.24703
Epoch: 0030 train_loss= 1.01250 train_acc= 0.76374 val_loss= 0.97273 val_acc= 0.76263 time= 0.22300
Epoch: 0031 train_loss= 0.99005 train_acc= 0.76816 val_loss= 0.94297 val_acc= 0.77335 time= 0.21603
Epoch: 0032 train_loss= 0.94962 train_acc= 0.77819 val_loss= 0.91447 val_acc= 0.78254 time= 0.21301
Epoch: 0033 train_loss= 0.92488 train_acc= 0.78925 val_loss= 0.88709 val_acc= 0.79786 time= 0.21396
Epoch: 0034 train_loss= 0.90027 train_acc= 0.80201 val_loss= 0.86063 val_acc= 0.81470 time= 0.24203
Epoch: 0035 train_loss= 0.86638 train_acc= 0.81561 val_loss= 0.83476 val_acc= 0.82542 time= 0.21301
Epoch: 0036 train_loss= 0.84668 train_acc= 0.82242 val_loss= 0.80951 val_acc= 0.84074 time= 0.20900
Epoch: 0037 train_loss= 0.81875 train_acc= 0.82837 val_loss= 0.78481 val_acc= 0.84227 time= 0.20899
Epoch: 0038 train_loss= 0.79378 train_acc= 0.83501 val_loss= 0.76081 val_acc= 0.84380 time= 0.22700
Epoch: 0039 train_loss= 0.76587 train_acc= 0.83790 val_loss= 0.73760 val_acc= 0.84533 time= 0.21697
Epoch: 0040 train_loss= 0.73713 train_acc= 0.84385 val_loss= 0.71498 val_acc= 0.84839 time= 0.21900
Epoch: 0041 train_loss= 0.70938 train_acc= 0.84708 val_loss= 0.69279 val_acc= 0.85605 time= 0.21501
Epoch: 0042 train_loss= 0.69047 train_acc= 0.85202 val_loss= 0.67099 val_acc= 0.85758 time= 0.21100
Epoch: 0043 train_loss= 0.66634 train_acc= 0.85661 val_loss= 0.64962 val_acc= 0.85758 time= 0.24500
Epoch: 0044 train_loss= 0.64185 train_acc= 0.86086 val_loss= 0.62876 val_acc= 0.86371 time= 0.21097
Epoch: 0045 train_loss= 0.61868 train_acc= 0.86613 val_loss= 0.60845 val_acc= 0.86677 time= 0.21203
Epoch: 0046 train_loss= 0.59299 train_acc= 0.87209 val_loss= 0.58861 val_acc= 0.86830 time= 0.21359
Epoch: 0047 train_loss= 0.57952 train_acc= 0.87277 val_loss= 0.56946 val_acc= 0.87136 time= 0.21755
Epoch: 0048 train_loss= 0.56342 train_acc= 0.87719 val_loss= 0.55134 val_acc= 0.87596 time= 0.24189
Epoch: 0049 train_loss= 0.53663 train_acc= 0.87906 val_loss= 0.53435 val_acc= 0.87902 time= 0.21098
Epoch: 0050 train_loss= 0.52293 train_acc= 0.88450 val_loss= 0.51853 val_acc= 0.87902 time= 0.21401
Epoch: 0051 train_loss= 0.50193 train_acc= 0.88484 val_loss= 0.50343 val_acc= 0.87902 time= 0.20800
Epoch: 0052 train_loss= 0.48329 train_acc= 0.88893 val_loss= 0.48940 val_acc= 0.88208 time= 0.24897
Epoch: 0053 train_loss= 0.46411 train_acc= 0.89437 val_loss= 0.47584 val_acc= 0.88208 time= 0.21255
Epoch: 0054 train_loss= 0.44431 train_acc= 0.89896 val_loss= 0.46240 val_acc= 0.88055 time= 0.21001
Epoch: 0055 train_loss= 0.43692 train_acc= 0.89998 val_loss= 0.44898 val_acc= 0.88208 time= 0.20999
Epoch: 0056 train_loss= 0.41821 train_acc= 0.90441 val_loss= 0.43540 val_acc= 0.88821 time= 0.21300
Epoch: 0057 train_loss= 0.40297 train_acc= 0.90679 val_loss= 0.42186 val_acc= 0.89127 time= 0.23700
Epoch: 0058 train_loss= 0.38771 train_acc= 0.91614 val_loss= 0.40898 val_acc= 0.90199 time= 0.21790
Epoch: 0059 train_loss= 0.37195 train_acc= 0.91801 val_loss= 0.39724 val_acc= 0.90352 time= 0.21220
Epoch: 0060 train_loss= 0.35518 train_acc= 0.92142 val_loss= 0.38683 val_acc= 0.90199 time= 0.21096
Epoch: 0061 train_loss= 0.35060 train_acc= 0.92482 val_loss= 0.37778 val_acc= 0.90199 time= 0.24404
Epoch: 0062 train_loss= 0.33118 train_acc= 0.92941 val_loss= 0.36989 val_acc= 0.90505 time= 0.20899
Epoch: 0063 train_loss= 0.32633 train_acc= 0.93213 val_loss= 0.36254 val_acc= 0.90505 time= 0.21300
Epoch: 0064 train_loss= 0.31252 train_acc= 0.93587 val_loss= 0.35578 val_acc= 0.90505 time= 0.21497
Epoch: 0065 train_loss= 0.29944 train_acc= 0.93638 val_loss= 0.34921 val_acc= 0.90505 time= 0.21303
Epoch: 0066 train_loss= 0.29021 train_acc= 0.94200 val_loss= 0.34219 val_acc= 0.90505 time= 0.24201
Epoch: 0067 train_loss= 0.27641 train_acc= 0.94472 val_loss= 0.33532 val_acc= 0.90658 time= 0.21096
Epoch: 0068 train_loss= 0.26727 train_acc= 0.94523 val_loss= 0.32835 val_acc= 0.90812 time= 0.21404
Epoch: 0069 train_loss= 0.25994 train_acc= 0.94727 val_loss= 0.32168 val_acc= 0.91271 time= 0.21096
Epoch: 0070 train_loss= 0.24729 train_acc= 0.94914 val_loss= 0.31526 val_acc= 0.91730 time= 0.24924
Epoch: 0071 train_loss= 0.24058 train_acc= 0.95067 val_loss= 0.30961 val_acc= 0.91730 time= 0.21601
Epoch: 0072 train_loss= 0.23354 train_acc= 0.95254 val_loss= 0.30400 val_acc= 0.91730 time= 0.21400
Epoch: 0073 train_loss= 0.22601 train_acc= 0.95680 val_loss= 0.29901 val_acc= 0.91577 time= 0.21300
Epoch: 0074 train_loss= 0.21531 train_acc= 0.95373 val_loss= 0.29387 val_acc= 0.91424 time= 0.20700
Epoch: 0075 train_loss= 0.20750 train_acc= 0.95867 val_loss= 0.28887 val_acc= 0.91884 time= 0.24545
Epoch: 0076 train_loss= 0.19966 train_acc= 0.95782 val_loss= 0.28515 val_acc= 0.92190 time= 0.21507
Epoch: 0077 train_loss= 0.19614 train_acc= 0.96037 val_loss= 0.28210 val_acc= 0.92190 time= 0.21201
Epoch: 0078 train_loss= 0.18648 train_acc= 0.96241 val_loss= 0.27945 val_acc= 0.92343 time= 0.21099
Epoch: 0079 train_loss= 0.18541 train_acc= 0.96139 val_loss= 0.27677 val_acc= 0.92343 time= 0.23901
Epoch: 0080 train_loss= 0.17534 train_acc= 0.96377 val_loss= 0.27420 val_acc= 0.92343 time= 0.21799
Epoch: 0081 train_loss= 0.17083 train_acc= 0.96564 val_loss= 0.27090 val_acc= 0.92496 time= 0.21209
Epoch: 0082 train_loss= 0.16268 train_acc= 0.96751 val_loss= 0.26649 val_acc= 0.92496 time= 0.21501
Epoch: 0083 train_loss= 0.15560 train_acc= 0.96751 val_loss= 0.26190 val_acc= 0.92956 time= 0.21100
Epoch: 0084 train_loss= 0.15193 train_acc= 0.96870 val_loss= 0.25797 val_acc= 0.92956 time= 0.24300
Epoch: 0085 train_loss= 0.14658 train_acc= 0.97023 val_loss= 0.25502 val_acc= 0.92802 time= 0.21000
Epoch: 0086 train_loss= 0.14453 train_acc= 0.97363 val_loss= 0.25283 val_acc= 0.92956 time= 0.21001
Epoch: 0087 train_loss= 0.13704 train_acc= 0.97381 val_loss= 0.25097 val_acc= 0.92956 time= 0.21198
Epoch: 0088 train_loss= 0.13352 train_acc= 0.97619 val_loss= 0.24941 val_acc= 0.92956 time= 0.21400
Epoch: 0089 train_loss= 0.13121 train_acc= 0.97636 val_loss= 0.24758 val_acc= 0.92956 time= 0.24105
Epoch: 0090 train_loss= 0.12487 train_acc= 0.97466 val_loss= 0.24566 val_acc= 0.93109 time= 0.20998
Epoch: 0091 train_loss= 0.12060 train_acc= 0.97534 val_loss= 0.24372 val_acc= 0.93109 time= 0.20901
Epoch: 0092 train_loss= 0.11537 train_acc= 0.97908 val_loss= 0.24260 val_acc= 0.93109 time= 0.21300
Epoch: 0093 train_loss= 0.11334 train_acc= 0.97857 val_loss= 0.24075 val_acc= 0.93109 time= 0.24697
Epoch: 0094 train_loss= 0.10764 train_acc= 0.98078 val_loss= 0.23855 val_acc= 0.93262 time= 0.21134
Epoch: 0095 train_loss= 0.10621 train_acc= 0.98078 val_loss= 0.23669 val_acc= 0.93262 time= 0.21197
Epoch: 0096 train_loss= 0.10021 train_acc= 0.98231 val_loss= 0.23591 val_acc= 0.93262 time= 0.21103
Epoch: 0097 train_loss= 0.09957 train_acc= 0.98299 val_loss= 0.23503 val_acc= 0.93109 time= 0.21606
Epoch: 0098 train_loss= 0.09447 train_acc= 0.98248 val_loss= 0.23426 val_acc= 0.93262 time= 0.23900
Epoch: 0099 train_loss= 0.09353 train_acc= 0.98469 val_loss= 0.23336 val_acc= 0.93262 time= 0.21554
Epoch: 0100 train_loss= 0.08904 train_acc= 0.98401 val_loss= 0.23222 val_acc= 0.93262 time= 0.21000
Epoch: 0101 train_loss= 0.09016 train_acc= 0.98418 val_loss= 0.23160 val_acc= 0.93262 time= 0.20900
Epoch: 0102 train_loss= 0.08542 train_acc= 0.98571 val_loss= 0.23093 val_acc= 0.93721 time= 0.24697
Epoch: 0103 train_loss= 0.08253 train_acc= 0.98707 val_loss= 0.23075 val_acc= 0.93721 time= 0.22103
Epoch: 0104 train_loss= 0.08087 train_acc= 0.98724 val_loss= 0.23151 val_acc= 0.93568 time= 0.21501
Epoch: 0105 train_loss= 0.07641 train_acc= 0.98741 val_loss= 0.23199 val_acc= 0.93415 time= 0.21500
Epoch: 0106 train_loss= 0.07458 train_acc= 0.98775 val_loss= 0.23178 val_acc= 0.93415 time= 0.21200
Epoch: 0107 train_loss= 0.07270 train_acc= 0.98945 val_loss= 0.23071 val_acc= 0.93568 time= 0.24297
Epoch: 0108 train_loss= 0.07328 train_acc= 0.98758 val_loss= 0.22894 val_acc= 0.93415 time= 0.21104
Epoch: 0109 train_loss= 0.06902 train_acc= 0.98826 val_loss= 0.22771 val_acc= 0.93721 time= 0.20899
Epoch: 0110 train_loss= 0.06780 train_acc= 0.98792 val_loss= 0.22548 val_acc= 0.93568 time= 0.21300
Epoch: 0111 train_loss= 0.06390 train_acc= 0.99047 val_loss= 0.22410 val_acc= 0.93415 time= 0.24997
Epoch: 0112 train_loss= 0.06355 train_acc= 0.98996 val_loss= 0.22358 val_acc= 0.93568 time= 0.21826
Epoch: 0113 train_loss= 0.06128 train_acc= 0.98996 val_loss= 0.22366 val_acc= 0.93721 time= 0.21100
Epoch: 0114 train_loss= 0.06244 train_acc= 0.99047 val_loss= 0.22350 val_acc= 0.93721 time= 0.21300
Epoch: 0115 train_loss= 0.05840 train_acc= 0.99064 val_loss= 0.22385 val_acc= 0.93874 time= 0.21500
Epoch: 0116 train_loss= 0.05573 train_acc= 0.99201 val_loss= 0.22362 val_acc= 0.93568 time= 0.24900
Epoch: 0117 train_loss= 0.05906 train_acc= 0.99064 val_loss= 0.22242 val_acc= 0.93415 time= 0.21605
Epoch: 0118 train_loss= 0.05409 train_acc= 0.99184 val_loss= 0.22051 val_acc= 0.93721 time= 0.21200
Epoch: 0119 train_loss= 0.05229 train_acc= 0.99218 val_loss= 0.21768 val_acc= 0.93721 time= 0.21199
Epoch: 0120 train_loss= 0.05059 train_acc= 0.99286 val_loss= 0.21507 val_acc= 0.93874 time= 0.21400
Epoch: 0121 train_loss= 0.05251 train_acc= 0.99133 val_loss= 0.21478 val_acc= 0.93721 time= 0.24100
Epoch: 0122 train_loss= 0.04775 train_acc= 0.99320 val_loss= 0.21497 val_acc= 0.93874 time= 0.21400
Epoch: 0123 train_loss= 0.04884 train_acc= 0.99201 val_loss= 0.21602 val_acc= 0.93721 time= 0.21101
Epoch: 0124 train_loss= 0.04631 train_acc= 0.99405 val_loss= 0.21798 val_acc= 0.94028 time= 0.20900
Epoch: 0125 train_loss= 0.04618 train_acc= 0.99252 val_loss= 0.22025 val_acc= 0.93721 time= 0.24400
Early stopping...
Optimization Finished!
Test set results: cost= 0.24745 accuracy= 0.93731 time= 0.08899
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.3333    1.0000    0.5000         1
           3     0.7684    0.9733    0.8588        75
           4     1.0000    1.0000    1.0000         9
           5     0.7900    0.9080    0.8449        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.5556    0.7143         9
          10     0.8621    0.6944    0.7692        36
          11     1.0000    1.0000    1.0000        12
          12     0.8815    0.9835    0.9297       121
          13     0.9375    0.7895    0.8571        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8750    0.8235    0.8485        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9000    0.7500    0.8182        12
          28     1.0000    0.7273    0.8421        11
          29     0.9697    0.9655    0.9676       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8354    0.8148    0.8250        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9908    0.9844      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9091    0.8333    0.8696        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7383    0.6633    0.6817      2568
weighted avg     0.9352    0.9373    0.9330      2568

Macro average Test Precision, Recall and F1-Score...
(0.7383420359093201, 0.6632844461229227, 0.6817179944733263, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[ 0.43350816  0.6177465   0.8805771  ...  0.13651757 -0.16532524
   0.5548682 ]
 [ 0.3551652   0.28025272  0.5707107  ... -0.01472795 -0.07422388
   0.46926722]
 [ 0.6271094   0.07584855  0.39908415 ...  0.45035398  0.05868711
   0.5662742 ]
 ...
 [ 0.43799934  0.2662295   0.00394101 ...  0.09637505  0.015335
   0.12131558]
 [ 0.31631076  0.07053503  0.19871937 ...  0.2705151   0.05376664
   0.17592815]
 [ 0.24612962  0.17288837  0.1292242  ...  0.39354533  0.17904383
   0.25871426]]

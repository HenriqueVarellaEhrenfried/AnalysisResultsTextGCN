(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95138 train_acc= 0.01446 val_loss= 3.86720 val_acc= 0.66922 time= 2.98208
Epoch: 0002 train_loss= 3.86845 train_acc= 0.65725 val_loss= 3.67627 val_acc= 0.67075 time= 2.88300
Epoch: 0003 train_loss= 3.67983 train_acc= 0.65504 val_loss= 3.37083 val_acc= 0.66922 time= 2.85867
Epoch: 0004 train_loss= 3.37671 train_acc= 0.65436 val_loss= 2.98396 val_acc= 0.66462 time= 2.84556
Epoch: 0005 train_loss= 2.99192 train_acc= 0.65232 val_loss= 2.59823 val_acc= 0.65850 time= 2.89400
Epoch: 0006 train_loss= 2.59528 train_acc= 0.64943 val_loss= 2.32000 val_acc= 0.64931 time= 2.88899
Epoch: 0007 train_loss= 2.32986 train_acc= 0.63837 val_loss= 2.20032 val_acc= 0.59571 time= 2.87101
Epoch: 0008 train_loss= 2.20789 train_acc= 0.59024 val_loss= 2.15698 val_acc= 0.50689 time= 2.85300
Epoch: 0009 train_loss= 2.16111 train_acc= 0.49039 val_loss= 2.10107 val_acc= 0.48086 time= 2.86300
Epoch: 0010 train_loss= 2.11977 train_acc= 0.45824 val_loss= 2.00180 val_acc= 0.49005 time= 2.85801
Epoch: 0011 train_loss= 2.02254 train_acc= 0.46334 val_loss= 1.87075 val_acc= 0.53292 time= 2.87098
Epoch: 0012 train_loss= 1.89808 train_acc= 0.53445 val_loss= 1.74310 val_acc= 0.63093 time= 2.89002
Epoch: 0013 train_loss= 1.76652 train_acc= 0.62051 val_loss= 1.64731 val_acc= 0.66462 time= 2.86899
Epoch: 0014 train_loss= 1.67220 train_acc= 0.64926 val_loss= 1.57986 val_acc= 0.67688 time= 2.88524
Epoch: 0015 train_loss= 1.60237 train_acc= 0.65657 val_loss= 1.51813 val_acc= 0.67994 time= 2.86199
Epoch: 0016 train_loss= 1.54197 train_acc= 0.66270 val_loss= 1.45064 val_acc= 0.68913 time= 2.88401
Epoch: 0017 train_loss= 1.47720 train_acc= 0.67494 val_loss= 1.38093 val_acc= 0.68760 time= 2.85700
Epoch: 0018 train_loss= 1.40774 train_acc= 0.67614 val_loss= 1.31605 val_acc= 0.69372 time= 2.85558
Epoch: 0019 train_loss= 1.33685 train_acc= 0.68260 val_loss= 1.25915 val_acc= 0.70444 time= 2.88134
Epoch: 0020 train_loss= 1.28352 train_acc= 0.69280 val_loss= 1.20959 val_acc= 0.71516 time= 2.87343
Epoch: 0021 train_loss= 1.23552 train_acc= 0.70658 val_loss= 1.16563 val_acc= 0.72588 time= 2.87100
Epoch: 0022 train_loss= 1.18783 train_acc= 0.72104 val_loss= 1.12565 val_acc= 0.73201 time= 2.85600
Epoch: 0023 train_loss= 1.14324 train_acc= 0.73822 val_loss= 1.08813 val_acc= 0.73813 time= 2.87701
Epoch: 0024 train_loss= 1.10266 train_acc= 0.75166 val_loss= 1.05151 val_acc= 0.75498 time= 2.87120
Epoch: 0025 train_loss= 1.06297 train_acc= 0.76186 val_loss= 1.01500 val_acc= 0.76417 time= 2.87300
Epoch: 0026 train_loss= 1.02736 train_acc= 0.76867 val_loss= 0.97841 val_acc= 0.77029 time= 2.86369
Epoch: 0027 train_loss= 0.98624 train_acc= 0.77921 val_loss= 0.94230 val_acc= 0.77335 time= 2.82901
Epoch: 0028 train_loss= 0.95101 train_acc= 0.78415 val_loss= 0.90760 val_acc= 0.78714 time= 2.88499
Epoch: 0029 train_loss= 0.90741 train_acc= 0.79639 val_loss= 0.87469 val_acc= 0.80551 time= 2.88300
Epoch: 0030 train_loss= 0.88330 train_acc= 0.80388 val_loss= 0.84350 val_acc= 0.80858 time= 2.89401
Epoch: 0031 train_loss= 0.84803 train_acc= 0.81680 val_loss= 0.81382 val_acc= 0.81776 time= 2.99100
Epoch: 0032 train_loss= 0.82150 train_acc= 0.82293 val_loss= 0.78557 val_acc= 0.82542 time= 2.90404
Epoch: 0033 train_loss= 0.79122 train_acc= 0.83194 val_loss= 0.75840 val_acc= 0.83614 time= 2.90400
Epoch: 0034 train_loss= 0.76814 train_acc= 0.83807 val_loss= 0.73202 val_acc= 0.84686 time= 2.90599
Epoch: 0035 train_loss= 0.73720 train_acc= 0.84385 val_loss= 0.70650 val_acc= 0.85146 time= 2.88291
Epoch: 0036 train_loss= 0.70925 train_acc= 0.85082 val_loss= 0.68195 val_acc= 0.85299 time= 2.87501
Epoch: 0037 train_loss= 0.68179 train_acc= 0.85355 val_loss= 0.65844 val_acc= 0.85911 time= 2.87100
Epoch: 0038 train_loss= 0.65066 train_acc= 0.85695 val_loss= 0.63608 val_acc= 0.85758 time= 2.88700
Epoch: 0039 train_loss= 0.62271 train_acc= 0.86205 val_loss= 0.61444 val_acc= 0.86064 time= 2.91401
Epoch: 0040 train_loss= 0.60316 train_acc= 0.86630 val_loss= 0.59333 val_acc= 0.86524 time= 2.91001
Epoch: 0041 train_loss= 0.57907 train_acc= 0.87226 val_loss= 0.57232 val_acc= 0.87136 time= 2.91590
Epoch: 0042 train_loss= 0.55835 train_acc= 0.87702 val_loss= 0.55149 val_acc= 0.87596 time= 2.88401
Epoch: 0043 train_loss= 0.53463 train_acc= 0.88569 val_loss= 0.53107 val_acc= 0.88055 time= 2.88900
Epoch: 0044 train_loss= 0.51128 train_acc= 0.88842 val_loss= 0.51176 val_acc= 0.88055 time= 2.88700
Epoch: 0045 train_loss= 0.48543 train_acc= 0.88893 val_loss= 0.49374 val_acc= 0.88515 time= 2.88397
Epoch: 0046 train_loss= 0.46697 train_acc= 0.89369 val_loss= 0.47729 val_acc= 0.88515 time= 2.89202
Epoch: 0047 train_loss= 0.44771 train_acc= 0.89709 val_loss= 0.46234 val_acc= 0.88361 time= 2.88001
Epoch: 0048 train_loss= 0.43079 train_acc= 0.90202 val_loss= 0.44837 val_acc= 0.88361 time= 2.89400
Epoch: 0049 train_loss= 0.41431 train_acc= 0.90798 val_loss= 0.43500 val_acc= 0.88361 time= 2.88400
Epoch: 0050 train_loss= 0.39555 train_acc= 0.91291 val_loss= 0.42114 val_acc= 0.89127 time= 2.91100
Epoch: 0051 train_loss= 0.38077 train_acc= 0.91342 val_loss= 0.40718 val_acc= 0.89893 time= 2.92610
Epoch: 0052 train_loss= 0.36334 train_acc= 0.91852 val_loss= 0.39340 val_acc= 0.90199 time= 2.87400
Epoch: 0053 train_loss= 0.34819 train_acc= 0.92175 val_loss= 0.38022 val_acc= 0.90659 time= 2.88318
Epoch: 0054 train_loss= 0.33360 train_acc= 0.92924 val_loss= 0.36864 val_acc= 0.90659 time= 2.87961
Epoch: 0055 train_loss= 0.31691 train_acc= 0.93315 val_loss= 0.35887 val_acc= 0.90659 time= 2.88603
Epoch: 0056 train_loss= 0.30410 train_acc= 0.93621 val_loss= 0.35056 val_acc= 0.90965 time= 2.88632
Epoch: 0057 train_loss= 0.29275 train_acc= 0.93655 val_loss= 0.34322 val_acc= 0.90965 time= 2.88400
Epoch: 0058 train_loss= 0.27882 train_acc= 0.94166 val_loss= 0.33604 val_acc= 0.90812 time= 2.88965
Epoch: 0059 train_loss= 0.26605 train_acc= 0.94540 val_loss= 0.32904 val_acc= 0.90812 time= 2.88910
Epoch: 0060 train_loss= 0.25712 train_acc= 0.94710 val_loss= 0.32201 val_acc= 0.90812 time= 2.87984
Epoch: 0061 train_loss= 0.24384 train_acc= 0.94982 val_loss= 0.31548 val_acc= 0.91118 time= 2.88198
Epoch: 0062 train_loss= 0.23366 train_acc= 0.95084 val_loss= 0.30927 val_acc= 0.91424 time= 2.86802
Epoch: 0063 train_loss= 0.22370 train_acc= 0.95526 val_loss= 0.30364 val_acc= 0.91577 time= 2.89401
Epoch: 0064 train_loss= 0.21721 train_acc= 0.95628 val_loss= 0.29851 val_acc= 0.91577 time= 2.90200
Epoch: 0065 train_loss= 0.20236 train_acc= 0.95832 val_loss= 0.29377 val_acc= 0.91731 time= 2.89100
Epoch: 0066 train_loss= 0.19788 train_acc= 0.95832 val_loss= 0.28818 val_acc= 0.92190 time= 2.89300
Epoch: 0067 train_loss= 0.18749 train_acc= 0.96020 val_loss= 0.28192 val_acc= 0.92343 time= 2.88300
Epoch: 0068 train_loss= 0.18086 train_acc= 0.96462 val_loss= 0.27635 val_acc= 0.92190 time= 2.90421
Epoch: 0069 train_loss= 0.17172 train_acc= 0.96683 val_loss= 0.27218 val_acc= 0.92496 time= 2.88336
Epoch: 0070 train_loss= 0.16787 train_acc= 0.96632 val_loss= 0.26925 val_acc= 0.92649 time= 2.86927
Epoch: 0071 train_loss= 0.15758 train_acc= 0.96649 val_loss= 0.26654 val_acc= 0.92802 time= 2.86918
Epoch: 0072 train_loss= 0.15018 train_acc= 0.96751 val_loss= 0.26425 val_acc= 0.92956 time= 2.88272
Epoch: 0073 train_loss= 0.14666 train_acc= 0.97006 val_loss= 0.26180 val_acc= 0.92956 time= 2.88400
Epoch: 0074 train_loss= 0.13785 train_acc= 0.97091 val_loss= 0.25946 val_acc= 0.92802 time= 2.88900
Epoch: 0075 train_loss= 0.13548 train_acc= 0.97380 val_loss= 0.25626 val_acc= 0.92956 time= 2.90893
Epoch: 0076 train_loss= 0.12791 train_acc= 0.97755 val_loss= 0.25285 val_acc= 0.92802 time= 2.89901
Epoch: 0077 train_loss= 0.12320 train_acc= 0.97738 val_loss= 0.25028 val_acc= 0.92802 time= 2.89343
Epoch: 0078 train_loss= 0.11814 train_acc= 0.97806 val_loss= 0.24828 val_acc= 0.93262 time= 2.89500
Epoch: 0079 train_loss= 0.11342 train_acc= 0.97704 val_loss= 0.24586 val_acc= 0.93262 time= 2.90500
Epoch: 0080 train_loss= 0.10815 train_acc= 0.98027 val_loss= 0.24394 val_acc= 0.93262 time= 2.88000
Epoch: 0081 train_loss= 0.10227 train_acc= 0.98163 val_loss= 0.24238 val_acc= 0.93109 time= 2.85500
Epoch: 0082 train_loss= 0.10077 train_acc= 0.98163 val_loss= 0.24011 val_acc= 0.93109 time= 2.88700
Epoch: 0083 train_loss= 0.09634 train_acc= 0.98180 val_loss= 0.23945 val_acc= 0.93415 time= 2.89399
Epoch: 0084 train_loss= 0.09097 train_acc= 0.98418 val_loss= 0.23824 val_acc= 0.93415 time= 2.89300
Epoch: 0085 train_loss= 0.08607 train_acc= 0.98520 val_loss= 0.23664 val_acc= 0.93568 time= 2.89801
Epoch: 0086 train_loss= 0.08532 train_acc= 0.98486 val_loss= 0.23348 val_acc= 0.93415 time= 2.87598
Epoch: 0087 train_loss= 0.07860 train_acc= 0.98758 val_loss= 0.23237 val_acc= 0.93415 time= 2.88636
Epoch: 0088 train_loss= 0.07865 train_acc= 0.98605 val_loss= 0.23130 val_acc= 0.93415 time= 2.88098
Epoch: 0089 train_loss= 0.07420 train_acc= 0.98707 val_loss= 0.23189 val_acc= 0.93262 time= 2.89582
Epoch: 0090 train_loss= 0.07269 train_acc= 0.98826 val_loss= 0.23189 val_acc= 0.93262 time= 2.88000
Epoch: 0091 train_loss= 0.06811 train_acc= 0.98826 val_loss= 0.23221 val_acc= 0.93415 time= 2.86581
Epoch: 0092 train_loss= 0.06507 train_acc= 0.98962 val_loss= 0.23390 val_acc= 0.93568 time= 2.90900
Epoch: 0093 train_loss= 0.06279 train_acc= 0.98996 val_loss= 0.23711 val_acc= 0.93568 time= 2.89409
Early stopping...
Optimization Finished!
Test set results: cost= 0.25064 accuracy= 0.93730 time= 0.98701
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.7714    0.9310    0.8438        87
           6     0.9200    0.9200    0.9200        25
           7     0.7059    0.9231    0.8000        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.5556    0.7143         9
          10     0.9231    0.6667    0.7742        36
          11     1.0000    0.9167    0.9565        12
          12     0.8633    0.9917    0.9231       121
          13     1.0000    0.7895    0.8824        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7647    0.7647    0.7647        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9711    0.9641    0.9676       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8493    0.7654    0.8052        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9926    0.9849      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8182    0.7500    0.7826        12
          45     0.5000    0.1667    0.2500         6
          46     1.0000    0.2857    0.4444         7
          47     0.7647    0.8667    0.8125        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7456    0.6638    0.6832      2568
weighted avg     0.9357    0.9373    0.9326      2568

Macro average Test Precision, Recall and F1-Score...
(0.7455534954202955, 0.6638251043577803, 0.6832091824635455, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[ 0.16015294  0.17027284  0.05533615 ...  0.01167255 -0.0859295
  -0.09567957]
 [ 0.20919381  0.02956733 -0.02012396 ...  0.03090909  0.04235056
  -0.07563194]
 [ 0.09558801 -0.03953231  0.26303238 ... -0.02225566  0.02493901
  -0.0891076 ]
 ...
 [-0.01028267 -0.01661953  0.06247067 ...  0.04353546 -0.01146663
  -0.04521626]
 [ 0.0870093   0.02785398  0.19200513 ...  0.01825401  0.01609301
  -0.04772316]
 [ 0.16998145  0.1761571   0.30542624 ...  0.16377816  0.140787
  -0.07805955]]

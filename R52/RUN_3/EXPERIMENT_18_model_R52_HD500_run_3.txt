(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95136 train_acc= 0.00272 val_loss= 3.85822 val_acc= 0.66156 time= 2.95836
Epoch: 0002 train_loss= 3.86017 train_acc= 0.65164 val_loss= 3.65572 val_acc= 0.66309 time= 2.84315
Epoch: 0003 train_loss= 3.65814 train_acc= 0.65113 val_loss= 3.33771 val_acc= 0.65850 time= 2.84600
Epoch: 0004 train_loss= 3.33766 train_acc= 0.64688 val_loss= 2.94211 val_acc= 0.65391 time= 2.85616
Epoch: 0005 train_loss= 2.94508 train_acc= 0.64263 val_loss= 2.56071 val_acc= 0.64778 time= 2.85200
Epoch: 0006 train_loss= 2.56700 train_acc= 0.63769 val_loss= 2.30532 val_acc= 0.64625 time= 2.83812
Epoch: 0007 train_loss= 2.29580 train_acc= 0.63021 val_loss= 2.20414 val_acc= 0.55896 time= 2.83700
Epoch: 0008 train_loss= 2.21000 train_acc= 0.56591 val_loss= 2.16178 val_acc= 0.47933 time= 2.84052
Epoch: 0009 train_loss= 2.17419 train_acc= 0.45535 val_loss= 2.10149 val_acc= 0.46554 time= 2.87700
Epoch: 0010 train_loss= 2.11918 train_acc= 0.43851 val_loss= 1.99755 val_acc= 0.47320 time= 2.87200
Epoch: 0011 train_loss= 2.01799 train_acc= 0.44667 val_loss= 1.86568 val_acc= 0.51761 time= 2.81800
Epoch: 0012 train_loss= 1.88882 train_acc= 0.50621 val_loss= 1.74300 val_acc= 0.61715 time= 2.83471
Epoch: 0013 train_loss= 1.76652 train_acc= 0.60640 val_loss= 1.65471 val_acc= 0.66769 time= 2.83938
Epoch: 0014 train_loss= 1.68761 train_acc= 0.64790 val_loss= 1.58878 val_acc= 0.67381 time= 2.82464
Epoch: 0015 train_loss= 1.62187 train_acc= 0.64875 val_loss= 1.52321 val_acc= 0.67534 time= 2.83100
Epoch: 0016 train_loss= 1.55472 train_acc= 0.65640 val_loss= 1.45260 val_acc= 0.67994 time= 2.84500
Epoch: 0017 train_loss= 1.48576 train_acc= 0.66304 val_loss= 1.38278 val_acc= 0.68300 time= 2.86227
Epoch: 0018 train_loss= 1.41365 train_acc= 0.67069 val_loss= 1.31963 val_acc= 0.69678 time= 2.81800
Epoch: 0019 train_loss= 1.34604 train_acc= 0.68056 val_loss= 1.26539 val_acc= 0.70750 time= 2.83400
Epoch: 0020 train_loss= 1.30004 train_acc= 0.69161 val_loss= 1.21885 val_acc= 0.71976 time= 2.86643
Epoch: 0021 train_loss= 1.24899 train_acc= 0.70709 val_loss= 1.17839 val_acc= 0.72588 time= 2.83900
Epoch: 0022 train_loss= 1.20795 train_acc= 0.72223 val_loss= 1.14204 val_acc= 0.73201 time= 2.83700
Epoch: 0023 train_loss= 1.16313 train_acc= 0.74009 val_loss= 1.10777 val_acc= 0.74119 time= 2.85241
Epoch: 0024 train_loss= 1.12507 train_acc= 0.74775 val_loss= 1.07405 val_acc= 0.74885 time= 3.10800
Epoch: 0025 train_loss= 1.09095 train_acc= 0.75506 val_loss= 1.04021 val_acc= 0.75498 time= 2.91500
Epoch: 0026 train_loss= 1.05160 train_acc= 0.76135 val_loss= 1.00649 val_acc= 0.76110 time= 2.91266
Epoch: 0027 train_loss= 1.01424 train_acc= 0.76714 val_loss= 0.97347 val_acc= 0.76570 time= 2.90700
Epoch: 0028 train_loss= 0.98327 train_acc= 0.77445 val_loss= 0.94170 val_acc= 0.77795 time= 2.94476
Epoch: 0029 train_loss= 0.95148 train_acc= 0.78125 val_loss= 0.91104 val_acc= 0.78714 time= 2.84900
Epoch: 0030 train_loss= 0.91786 train_acc= 0.78874 val_loss= 0.88123 val_acc= 0.79939 time= 2.86888
Epoch: 0031 train_loss= 0.89171 train_acc= 0.79946 val_loss= 0.85206 val_acc= 0.80704 time= 2.87419
Epoch: 0032 train_loss= 0.85774 train_acc= 0.81663 val_loss= 0.82357 val_acc= 0.81623 time= 2.86100
Epoch: 0033 train_loss= 0.82600 train_acc= 0.82565 val_loss= 0.79598 val_acc= 0.83155 time= 2.88599
Epoch: 0034 train_loss= 0.80164 train_acc= 0.83245 val_loss= 0.76925 val_acc= 0.83461 time= 2.87500
Epoch: 0035 train_loss= 0.77127 train_acc= 0.83858 val_loss= 0.74341 val_acc= 0.84380 time= 2.87701
Epoch: 0036 train_loss= 0.75149 train_acc= 0.84181 val_loss= 0.71830 val_acc= 0.85299 time= 2.86828
Epoch: 0037 train_loss= 0.71986 train_acc= 0.85082 val_loss= 0.69362 val_acc= 0.85146 time= 2.88410
Epoch: 0038 train_loss= 0.69380 train_acc= 0.85525 val_loss= 0.66952 val_acc= 0.84992 time= 2.88502
Epoch: 0039 train_loss= 0.66546 train_acc= 0.85814 val_loss= 0.64612 val_acc= 0.85452 time= 2.85600
Epoch: 0040 train_loss= 0.63979 train_acc= 0.86052 val_loss= 0.62363 val_acc= 0.86217 time= 2.86699
Epoch: 0041 train_loss= 0.60909 train_acc= 0.86256 val_loss= 0.60201 val_acc= 0.86217 time= 2.86299
Epoch: 0042 train_loss= 0.58882 train_acc= 0.86664 val_loss= 0.58132 val_acc= 0.86983 time= 2.86601
Epoch: 0043 train_loss= 0.56730 train_acc= 0.87566 val_loss= 0.56086 val_acc= 0.87289 time= 2.86400
Epoch: 0044 train_loss= 0.55061 train_acc= 0.87583 val_loss= 0.54109 val_acc= 0.87443 time= 2.84500
Epoch: 0045 train_loss= 0.52703 train_acc= 0.88110 val_loss= 0.52239 val_acc= 0.87749 time= 2.88300
Epoch: 0046 train_loss= 0.50316 train_acc= 0.88671 val_loss= 0.50457 val_acc= 0.88055 time= 2.87033
Epoch: 0047 train_loss= 0.47934 train_acc= 0.89182 val_loss= 0.48809 val_acc= 0.88208 time= 2.87602
Epoch: 0048 train_loss= 0.46392 train_acc= 0.89488 val_loss= 0.47247 val_acc= 0.88821 time= 2.84300
Epoch: 0049 train_loss= 0.44215 train_acc= 0.89760 val_loss= 0.45767 val_acc= 0.88821 time= 2.87395
Epoch: 0050 train_loss= 0.42605 train_acc= 0.90066 val_loss= 0.44326 val_acc= 0.88821 time= 2.88558
Epoch: 0051 train_loss= 0.40591 train_acc= 0.90525 val_loss= 0.42867 val_acc= 0.89280 time= 2.86901
Epoch: 0052 train_loss= 0.39173 train_acc= 0.90934 val_loss= 0.41364 val_acc= 0.89587 time= 2.85501
Epoch: 0053 train_loss= 0.37837 train_acc= 0.91359 val_loss= 0.39930 val_acc= 0.89893 time= 2.86331
Epoch: 0054 train_loss= 0.35928 train_acc= 0.91801 val_loss= 0.38666 val_acc= 0.90046 time= 2.86043
Epoch: 0055 train_loss= 0.34008 train_acc= 0.92567 val_loss= 0.37549 val_acc= 0.90505 time= 2.85715
Epoch: 0056 train_loss= 0.33098 train_acc= 0.92907 val_loss= 0.36569 val_acc= 0.90199 time= 2.85300
Epoch: 0057 train_loss= 0.31570 train_acc= 0.93298 val_loss= 0.35731 val_acc= 0.90659 time= 2.87667
Epoch: 0058 train_loss= 0.30416 train_acc= 0.93808 val_loss= 0.34992 val_acc= 0.90505 time= 2.86785
Epoch: 0059 train_loss= 0.29061 train_acc= 0.93689 val_loss= 0.34228 val_acc= 0.90659 time= 2.90601
Epoch: 0060 train_loss= 0.27821 train_acc= 0.93893 val_loss= 0.33438 val_acc= 0.90659 time= 2.86342
Epoch: 0061 train_loss= 0.26412 train_acc= 0.94268 val_loss= 0.32621 val_acc= 0.90659 time= 2.86041
Epoch: 0062 train_loss= 0.25734 train_acc= 0.94574 val_loss= 0.31818 val_acc= 0.91118 time= 2.86518
Epoch: 0063 train_loss= 0.24252 train_acc= 0.94812 val_loss= 0.31130 val_acc= 0.91118 time= 2.86401
Epoch: 0064 train_loss= 0.23324 train_acc= 0.94914 val_loss= 0.30480 val_acc= 0.91731 time= 2.85899
Epoch: 0065 train_loss= 0.22405 train_acc= 0.95305 val_loss= 0.29928 val_acc= 0.91424 time= 2.86601
Epoch: 0066 train_loss= 0.21515 train_acc= 0.95356 val_loss= 0.29534 val_acc= 0.91424 time= 2.89887
Epoch: 0067 train_loss= 0.20675 train_acc= 0.95645 val_loss= 0.29116 val_acc= 0.91577 time= 2.87502
Epoch: 0068 train_loss= 0.19685 train_acc= 0.95952 val_loss= 0.28616 val_acc= 0.91577 time= 2.84101
Epoch: 0069 train_loss= 0.18911 train_acc= 0.95935 val_loss= 0.28120 val_acc= 0.91884 time= 2.85296
Epoch: 0070 train_loss= 0.18223 train_acc= 0.96173 val_loss= 0.27670 val_acc= 0.92037 time= 2.86604
Epoch: 0071 train_loss= 0.17149 train_acc= 0.96292 val_loss= 0.27227 val_acc= 0.92343 time= 2.86100
Epoch: 0072 train_loss= 0.16840 train_acc= 0.96598 val_loss= 0.26874 val_acc= 0.92496 time= 2.84198
Epoch: 0073 train_loss= 0.16036 train_acc= 0.97091 val_loss= 0.26559 val_acc= 0.92802 time= 2.88602
Epoch: 0074 train_loss= 0.15339 train_acc= 0.96972 val_loss= 0.26255 val_acc= 0.92802 time= 2.88899
Epoch: 0075 train_loss= 0.14402 train_acc= 0.97329 val_loss= 0.25941 val_acc= 0.92649 time= 2.88032
Epoch: 0076 train_loss= 0.13725 train_acc= 0.97244 val_loss= 0.25600 val_acc= 0.92802 time= 2.85498
Epoch: 0077 train_loss= 0.13316 train_acc= 0.97227 val_loss= 0.25287 val_acc= 0.92649 time= 2.87011
Epoch: 0078 train_loss= 0.12585 train_acc= 0.97670 val_loss= 0.24961 val_acc= 0.92956 time= 2.87558
Epoch: 0079 train_loss= 0.12176 train_acc= 0.97482 val_loss= 0.24735 val_acc= 0.93109 time= 2.86701
Epoch: 0080 train_loss= 0.11723 train_acc= 0.97840 val_loss= 0.24518 val_acc= 0.93109 time= 2.88358
Epoch: 0081 train_loss= 0.11102 train_acc= 0.97806 val_loss= 0.24441 val_acc= 0.92649 time= 2.86935
Epoch: 0082 train_loss= 0.10814 train_acc= 0.98044 val_loss= 0.24356 val_acc= 0.92956 time= 2.87503
Epoch: 0083 train_loss= 0.10371 train_acc= 0.98112 val_loss= 0.24118 val_acc= 0.92956 time= 2.86900
Epoch: 0084 train_loss= 0.09738 train_acc= 0.98214 val_loss= 0.23955 val_acc= 0.93262 time= 2.84399
Epoch: 0085 train_loss= 0.09496 train_acc= 0.98384 val_loss= 0.23818 val_acc= 0.93262 time= 2.85600
Epoch: 0086 train_loss= 0.09282 train_acc= 0.98299 val_loss= 0.23610 val_acc= 0.93262 time= 2.86999
Epoch: 0087 train_loss= 0.08620 train_acc= 0.98486 val_loss= 0.23544 val_acc= 0.93262 time= 2.88401
Epoch: 0088 train_loss= 0.08255 train_acc= 0.98520 val_loss= 0.23571 val_acc= 0.93262 time= 2.85001
Epoch: 0089 train_loss= 0.08157 train_acc= 0.98588 val_loss= 0.23662 val_acc= 0.93262 time= 2.86342
Epoch: 0090 train_loss= 0.07613 train_acc= 0.98724 val_loss= 0.23578 val_acc= 0.93415 time= 2.86385
Epoch: 0091 train_loss= 0.07416 train_acc= 0.98673 val_loss= 0.23475 val_acc= 0.93415 time= 2.85501
Epoch: 0092 train_loss= 0.07129 train_acc= 0.98741 val_loss= 0.23305 val_acc= 0.93568 time= 2.84805
Epoch: 0093 train_loss= 0.06787 train_acc= 0.98792 val_loss= 0.23070 val_acc= 0.93721 time= 2.87602
Epoch: 0094 train_loss= 0.06531 train_acc= 0.99115 val_loss= 0.22864 val_acc= 0.94028 time= 2.88000
Epoch: 0095 train_loss= 0.06324 train_acc= 0.98945 val_loss= 0.22762 val_acc= 0.93721 time= 2.88099
Epoch: 0096 train_loss= 0.06239 train_acc= 0.98928 val_loss= 0.22744 val_acc= 0.93874 time= 2.83919
Epoch: 0097 train_loss= 0.05918 train_acc= 0.99047 val_loss= 0.22724 val_acc= 0.93721 time= 2.86052
Epoch: 0098 train_loss= 0.05610 train_acc= 0.98962 val_loss= 0.22728 val_acc= 0.93721 time= 2.87826
Epoch: 0099 train_loss= 0.05371 train_acc= 0.99183 val_loss= 0.22816 val_acc= 0.93721 time= 2.86614
Epoch: 0100 train_loss= 0.05101 train_acc= 0.99217 val_loss= 0.22803 val_acc= 0.93721 time= 2.87099
Epoch: 0101 train_loss= 0.04835 train_acc= 0.99251 val_loss= 0.22836 val_acc= 0.93874 time= 2.88201
Epoch: 0102 train_loss= 0.04742 train_acc= 0.99234 val_loss= 0.22785 val_acc= 0.93874 time= 2.86800
Epoch: 0103 train_loss= 0.04684 train_acc= 0.99183 val_loss= 0.22712 val_acc= 0.94028 time= 2.87300
Epoch: 0104 train_loss= 0.04609 train_acc= 0.99302 val_loss= 0.22716 val_acc= 0.94028 time= 2.84601
Epoch: 0105 train_loss= 0.04315 train_acc= 0.99319 val_loss= 0.22708 val_acc= 0.94334 time= 2.84601
Epoch: 0106 train_loss= 0.04076 train_acc= 0.99404 val_loss= 0.22742 val_acc= 0.94334 time= 2.87098
Epoch: 0107 train_loss= 0.04016 train_acc= 0.99507 val_loss= 0.22603 val_acc= 0.94334 time= 2.86214
Epoch: 0108 train_loss= 0.03900 train_acc= 0.99439 val_loss= 0.22497 val_acc= 0.94487 time= 2.85900
Epoch: 0109 train_loss= 0.03803 train_acc= 0.99507 val_loss= 0.22409 val_acc= 0.94487 time= 2.85136
Epoch: 0110 train_loss= 0.03621 train_acc= 0.99439 val_loss= 0.22397 val_acc= 0.94487 time= 2.88300
Epoch: 0111 train_loss= 0.03667 train_acc= 0.99490 val_loss= 0.22577 val_acc= 0.94028 time= 2.87112
Epoch: 0112 train_loss= 0.03308 train_acc= 0.99558 val_loss= 0.22821 val_acc= 0.94028 time= 2.84299
Early stopping...
Optimization Finished!
Test set results: cost= 0.24765 accuracy= 0.93886 time= 0.98154
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7865    0.9333    0.8537        75
           4     1.0000    1.0000    1.0000         9
           5     0.8100    0.9310    0.8663        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.6667    0.8000         9
          10     0.8800    0.6111    0.7213        36
          11     1.0000    0.9167    0.9565        12
          12     0.8633    0.9917    0.9231       121
          13     0.9375    0.7895    0.8571        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.7500    0.8571         4
          16     0.3333    0.2500    0.2857         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     1.0000    1.0000    1.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9167    0.9167    0.9167        12
          28     1.0000    0.8182    0.9000        11
          29     0.9668    0.9626    0.9647       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8205    0.7901    0.8050        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    1.0000    1.0000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9782    0.9926    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9389      2568
   macro avg     0.7997    0.7057    0.7325      2568
weighted avg     0.9375    0.9389    0.9348      2568

Macro average Test Precision, Recall and F1-Score...
(0.7996711844574443, 0.7057160588208339, 0.7324594396501691, None)
Micro average Test Precision, Recall and F1-Score...
(0.9388629283489096, 0.9388629283489096, 0.9388629283489096, None)
embeddings:
8892 6532 2568
[[ 0.06407053 -0.07158408  1.1760684  ...  0.08373556  0.02212914
   0.98432815]
 [ 0.43807134  0.0390949   0.0897596  ...  0.14174029  0.25109518
   0.48451975]
 [ 0.41835165 -0.04550777  0.50807077 ...  0.04357206  0.25117162
   0.16764228]
 ...
 [ 0.13965786 -0.02537888  0.33277127 ...  0.1267403   0.08352491
   0.3120832 ]
 [ 0.17028798  0.00760236  0.33984765 ...  0.0913044   0.0494576
   0.14265275]
 [ 0.10173564  0.14788958  0.21395999 ...  0.24953784  0.11009225
   0.26055738]]

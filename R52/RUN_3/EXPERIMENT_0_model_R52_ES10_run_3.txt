(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95126 train_acc= 0.01990 val_loss= 3.90078 val_acc= 0.65544 time= 0.44225
Epoch: 0002 train_loss= 3.90187 train_acc= 0.63293 val_loss= 3.80231 val_acc= 0.65544 time= 0.17800
Epoch: 0003 train_loss= 3.80329 train_acc= 0.63412 val_loss= 3.64904 val_acc= 0.65544 time= 0.17200
Epoch: 0004 train_loss= 3.65564 train_acc= 0.63191 val_loss= 3.44066 val_acc= 0.66003 time= 0.16700
Epoch: 0005 train_loss= 3.44380 train_acc= 0.63293 val_loss= 3.18785 val_acc= 0.66156 time= 0.19500
Epoch: 0006 train_loss= 3.19644 train_acc= 0.63718 val_loss= 2.91380 val_acc= 0.66922 time= 0.16800
Epoch: 0007 train_loss= 2.91933 train_acc= 0.63582 val_loss= 2.65007 val_acc= 0.66922 time= 0.16822
Epoch: 0008 train_loss= 2.65132 train_acc= 0.64297 val_loss= 2.43280 val_acc= 0.66769 time= 0.18600
Epoch: 0009 train_loss= 2.43972 train_acc= 0.64620 val_loss= 2.29101 val_acc= 0.66922 time= 0.17100
Epoch: 0010 train_loss= 2.28298 train_acc= 0.64535 val_loss= 2.21705 val_acc= 0.60643 time= 0.17104
Epoch: 0011 train_loss= 2.22206 train_acc= 0.58820 val_loss= 2.17424 val_acc= 0.49617 time= 0.17105
Epoch: 0012 train_loss= 2.18605 train_acc= 0.48359 val_loss= 2.13222 val_acc= 0.46708 time= 0.19199
Epoch: 0013 train_loss= 2.14548 train_acc= 0.44463 val_loss= 2.07484 val_acc= 0.46401 time= 0.17797
Epoch: 0014 train_loss= 2.09938 train_acc= 0.43732 val_loss= 1.99736 val_acc= 0.46708 time= 0.16703
Epoch: 0015 train_loss= 2.01981 train_acc= 0.44055 val_loss= 1.90597 val_acc= 0.48239 time= 0.16599
Epoch: 0016 train_loss= 1.92969 train_acc= 0.46096 val_loss= 1.81357 val_acc= 0.52680 time= 0.16797
Epoch: 0017 train_loss= 1.83262 train_acc= 0.52662 val_loss= 1.73184 val_acc= 0.62328 time= 0.20167
Epoch: 0018 train_loss= 1.76037 train_acc= 0.60963 val_loss= 1.66545 val_acc= 0.65697 time= 0.17114
Epoch: 0019 train_loss= 1.68801 train_acc= 0.64620 val_loss= 1.61070 val_acc= 0.67228 time= 0.17402
Epoch: 0020 train_loss= 1.63279 train_acc= 0.65300 val_loss= 1.56084 val_acc= 0.67994 time= 0.17101
Epoch: 0021 train_loss= 1.58473 train_acc= 0.65555 val_loss= 1.51112 val_acc= 0.67228 time= 0.16800
Epoch: 0022 train_loss= 1.52903 train_acc= 0.65776 val_loss= 1.46037 val_acc= 0.67688 time= 0.16900
Epoch: 0023 train_loss= 1.48782 train_acc= 0.66185 val_loss= 1.41000 val_acc= 0.68453 time= 0.19200
Epoch: 0024 train_loss= 1.43783 train_acc= 0.66814 val_loss= 1.36195 val_acc= 0.68760 time= 0.17013
Epoch: 0025 train_loss= 1.37789 train_acc= 0.67205 val_loss= 1.31774 val_acc= 0.69985 time= 0.18500
Epoch: 0026 train_loss= 1.34025 train_acc= 0.67937 val_loss= 1.27757 val_acc= 0.70750 time= 0.16930
Epoch: 0027 train_loss= 1.29887 train_acc= 0.69195 val_loss= 1.24087 val_acc= 0.71822 time= 0.16800
Epoch: 0028 train_loss= 1.25586 train_acc= 0.70233 val_loss= 1.20677 val_acc= 0.72282 time= 0.19774
Epoch: 0029 train_loss= 1.22454 train_acc= 0.71696 val_loss= 1.17446 val_acc= 0.73201 time= 0.16903
Epoch: 0030 train_loss= 1.19116 train_acc= 0.72887 val_loss= 1.14331 val_acc= 0.73813 time= 0.18297
Epoch: 0031 train_loss= 1.15785 train_acc= 0.73839 val_loss= 1.11293 val_acc= 0.74273 time= 0.16703
Epoch: 0032 train_loss= 1.12591 train_acc= 0.74962 val_loss= 1.08304 val_acc= 0.74732 time= 0.16899
Epoch: 0033 train_loss= 1.09789 train_acc= 0.75948 val_loss= 1.05361 val_acc= 0.75498 time= 0.17031
Epoch: 0034 train_loss= 1.06339 train_acc= 0.76425 val_loss= 1.02469 val_acc= 0.76570 time= 0.20204
Epoch: 0035 train_loss= 1.03780 train_acc= 0.77513 val_loss= 0.99630 val_acc= 0.77335 time= 0.16699
Epoch: 0036 train_loss= 1.00675 train_acc= 0.77904 val_loss= 0.96872 val_acc= 0.78254 time= 0.16997
Epoch: 0037 train_loss= 0.97238 train_acc= 0.79180 val_loss= 0.94201 val_acc= 0.79020 time= 0.16707
Epoch: 0038 train_loss= 0.95487 train_acc= 0.79282 val_loss= 0.91642 val_acc= 0.79632 time= 0.16799
Epoch: 0039 train_loss= 0.92477 train_acc= 0.79622 val_loss= 0.89190 val_acc= 0.80092 time= 0.17100
Epoch: 0040 train_loss= 0.89790 train_acc= 0.80898 val_loss= 0.86812 val_acc= 0.80858 time= 0.17800
Epoch: 0041 train_loss= 0.87458 train_acc= 0.81323 val_loss= 0.84501 val_acc= 0.81317 time= 0.17026
Epoch: 0042 train_loss= 0.86109 train_acc= 0.82004 val_loss= 0.82246 val_acc= 0.81776 time= 0.16600
Epoch: 0043 train_loss= 0.83537 train_acc= 0.82531 val_loss= 0.80044 val_acc= 0.82695 time= 0.16797
Epoch: 0044 train_loss= 0.80432 train_acc= 0.83348 val_loss= 0.77895 val_acc= 0.83002 time= 0.16803
Epoch: 0045 train_loss= 0.78377 train_acc= 0.83228 val_loss= 0.75818 val_acc= 0.83920 time= 0.17099
Epoch: 0046 train_loss= 0.76209 train_acc= 0.83722 val_loss= 0.73807 val_acc= 0.84074 time= 0.19397
Epoch: 0047 train_loss= 0.73695 train_acc= 0.84096 val_loss= 0.71860 val_acc= 0.84533 time= 0.16800
Epoch: 0048 train_loss= 0.72177 train_acc= 0.84759 val_loss= 0.69966 val_acc= 0.84992 time= 0.18761
Epoch: 0049 train_loss= 0.69559 train_acc= 0.85236 val_loss= 0.68135 val_acc= 0.85605 time= 0.16801
Epoch: 0050 train_loss= 0.68409 train_acc= 0.85372 val_loss= 0.66344 val_acc= 0.86064 time= 0.16700
Epoch: 0051 train_loss= 0.65624 train_acc= 0.86409 val_loss= 0.64579 val_acc= 0.86064 time= 0.17000
Epoch: 0052 train_loss= 0.63935 train_acc= 0.86460 val_loss= 0.62870 val_acc= 0.86524 time= 0.19200
Epoch: 0053 train_loss= 0.61625 train_acc= 0.86732 val_loss= 0.61182 val_acc= 0.86677 time= 0.17897
Epoch: 0054 train_loss= 0.60061 train_acc= 0.86920 val_loss= 0.59536 val_acc= 0.86830 time= 0.16700
Epoch: 0055 train_loss= 0.59420 train_acc= 0.87141 val_loss= 0.57955 val_acc= 0.87136 time= 0.16903
Epoch: 0056 train_loss= 0.56415 train_acc= 0.87583 val_loss= 0.56432 val_acc= 0.87136 time= 0.17000
Epoch: 0057 train_loss= 0.54689 train_acc= 0.87804 val_loss= 0.54981 val_acc= 0.87749 time= 0.19801
Epoch: 0058 train_loss= 0.53203 train_acc= 0.88433 val_loss= 0.53616 val_acc= 0.87749 time= 0.16903
Epoch: 0059 train_loss= 0.51843 train_acc= 0.88569 val_loss= 0.52297 val_acc= 0.88208 time= 0.17101
Epoch: 0060 train_loss= 0.50563 train_acc= 0.88706 val_loss= 0.51038 val_acc= 0.88055 time= 0.16799
Epoch: 0061 train_loss= 0.49248 train_acc= 0.89403 val_loss= 0.49850 val_acc= 0.88208 time= 0.16800
Epoch: 0062 train_loss= 0.47543 train_acc= 0.89811 val_loss= 0.48667 val_acc= 0.88515 time= 0.19500
Epoch: 0063 train_loss= 0.46622 train_acc= 0.89352 val_loss= 0.47516 val_acc= 0.88668 time= 0.16919
Epoch: 0064 train_loss= 0.45122 train_acc= 0.89862 val_loss= 0.46352 val_acc= 0.88974 time= 0.17005
Epoch: 0065 train_loss= 0.43875 train_acc= 0.90781 val_loss= 0.45180 val_acc= 0.89127 time= 0.18799
Epoch: 0066 train_loss= 0.42510 train_acc= 0.90934 val_loss= 0.44059 val_acc= 0.89280 time= 0.16697
Epoch: 0067 train_loss= 0.41200 train_acc= 0.90985 val_loss= 0.43008 val_acc= 0.89433 time= 0.16703
Epoch: 0068 train_loss= 0.40523 train_acc= 0.91325 val_loss= 0.42019 val_acc= 0.89433 time= 0.17000
Epoch: 0069 train_loss= 0.39208 train_acc= 0.91597 val_loss= 0.41090 val_acc= 0.89740 time= 0.19308
Epoch: 0070 train_loss= 0.37575 train_acc= 0.91529 val_loss= 0.40225 val_acc= 0.90199 time= 0.16855
Epoch: 0071 train_loss= 0.36910 train_acc= 0.91818 val_loss= 0.39391 val_acc= 0.90352 time= 0.17000
Epoch: 0072 train_loss= 0.35906 train_acc= 0.91852 val_loss= 0.38605 val_acc= 0.90505 time= 0.17003
Epoch: 0073 train_loss= 0.34879 train_acc= 0.92397 val_loss= 0.37890 val_acc= 0.90505 time= 0.16700
Epoch: 0074 train_loss= 0.35048 train_acc= 0.92465 val_loss= 0.37243 val_acc= 0.90505 time= 0.19900
Epoch: 0075 train_loss= 0.33159 train_acc= 0.92873 val_loss= 0.36609 val_acc= 0.90352 time= 0.16700
Epoch: 0076 train_loss= 0.32434 train_acc= 0.93179 val_loss= 0.36013 val_acc= 0.90199 time= 0.18800
Epoch: 0077 train_loss= 0.31242 train_acc= 0.93553 val_loss= 0.35390 val_acc= 0.90199 time= 0.16722
Epoch: 0078 train_loss= 0.31024 train_acc= 0.93298 val_loss= 0.34819 val_acc= 0.90352 time= 0.16900
Epoch: 0079 train_loss= 0.30184 train_acc= 0.93740 val_loss= 0.34240 val_acc= 0.90658 time= 0.17000
Epoch: 0080 train_loss= 0.29019 train_acc= 0.94132 val_loss= 0.33658 val_acc= 0.90658 time= 0.16800
Epoch: 0081 train_loss= 0.28399 train_acc= 0.94098 val_loss= 0.33097 val_acc= 0.91118 time= 0.16600
Epoch: 0082 train_loss= 0.27778 train_acc= 0.94438 val_loss= 0.32543 val_acc= 0.91424 time= 0.17200
Epoch: 0083 train_loss= 0.26619 train_acc= 0.94438 val_loss= 0.32057 val_acc= 0.91424 time= 0.16700
Epoch: 0084 train_loss= 0.26347 train_acc= 0.94591 val_loss= 0.31611 val_acc= 0.91271 time= 0.16835
Epoch: 0085 train_loss= 0.25317 train_acc= 0.94608 val_loss= 0.31227 val_acc= 0.91271 time= 0.16697
Epoch: 0086 train_loss= 0.24900 train_acc= 0.94812 val_loss= 0.30909 val_acc= 0.91884 time= 0.19704
Epoch: 0087 train_loss= 0.24031 train_acc= 0.95101 val_loss= 0.30678 val_acc= 0.91577 time= 0.17100
Epoch: 0088 train_loss= 0.23170 train_acc= 0.94812 val_loss= 0.30400 val_acc= 0.91577 time= 0.18231
Epoch: 0089 train_loss= 0.22960 train_acc= 0.95288 val_loss= 0.30152 val_acc= 0.91730 time= 0.16799
Epoch: 0090 train_loss= 0.22510 train_acc= 0.95441 val_loss= 0.29815 val_acc= 0.91730 time= 0.16700
Epoch: 0091 train_loss= 0.21612 train_acc= 0.95373 val_loss= 0.29370 val_acc= 0.91884 time= 0.19696
Epoch: 0092 train_loss= 0.21321 train_acc= 0.95543 val_loss= 0.28827 val_acc= 0.92037 time= 0.16705
Epoch: 0093 train_loss= 0.20777 train_acc= 0.95543 val_loss= 0.28337 val_acc= 0.92037 time= 0.17132
Epoch: 0094 train_loss= 0.19753 train_acc= 0.96105 val_loss= 0.27956 val_acc= 0.92190 time= 0.17501
Epoch: 0095 train_loss= 0.19258 train_acc= 0.96224 val_loss= 0.27624 val_acc= 0.92496 time= 0.16905
Epoch: 0096 train_loss= 0.18881 train_acc= 0.96122 val_loss= 0.27354 val_acc= 0.92496 time= 0.16661
Epoch: 0097 train_loss= 0.18987 train_acc= 0.95867 val_loss= 0.27208 val_acc= 0.92496 time= 0.16999
Epoch: 0098 train_loss= 0.18249 train_acc= 0.96224 val_loss= 0.27109 val_acc= 0.92343 time= 0.17597
Epoch: 0099 train_loss= 0.17742 train_acc= 0.96377 val_loss= 0.27038 val_acc= 0.92496 time= 0.17900
Epoch: 0100 train_loss= 0.17857 train_acc= 0.96071 val_loss= 0.26907 val_acc= 0.92496 time= 0.16903
Epoch: 0101 train_loss= 0.16694 train_acc= 0.96734 val_loss= 0.26644 val_acc= 0.92649 time= 0.17099
Epoch: 0102 train_loss= 0.16101 train_acc= 0.97091 val_loss= 0.26334 val_acc= 0.92190 time= 0.17157
Epoch: 0103 train_loss= 0.15828 train_acc= 0.97023 val_loss= 0.26078 val_acc= 0.92649 time= 0.18899
Epoch: 0104 train_loss= 0.16035 train_acc= 0.97074 val_loss= 0.25932 val_acc= 0.92802 time= 0.16601
Epoch: 0105 train_loss= 0.14923 train_acc= 0.97278 val_loss= 0.25898 val_acc= 0.92496 time= 0.16800
Epoch: 0106 train_loss= 0.14995 train_acc= 0.97125 val_loss= 0.25837 val_acc= 0.92649 time= 0.16800
Epoch: 0107 train_loss= 0.14058 train_acc= 0.97346 val_loss= 0.25629 val_acc= 0.92956 time= 0.17101
Epoch: 0108 train_loss= 0.14106 train_acc= 0.97449 val_loss= 0.25441 val_acc= 0.93109 time= 0.19912
Epoch: 0109 train_loss= 0.14004 train_acc= 0.97227 val_loss= 0.25212 val_acc= 0.92956 time= 0.17372
Epoch: 0110 train_loss= 0.13963 train_acc= 0.97125 val_loss= 0.24949 val_acc= 0.93109 time= 0.17756
Epoch: 0111 train_loss= 0.13222 train_acc= 0.97346 val_loss= 0.24707 val_acc= 0.93415 time= 0.19003
Epoch: 0112 train_loss= 0.12424 train_acc= 0.97568 val_loss= 0.24483 val_acc= 0.93721 time= 0.16900
Epoch: 0113 train_loss= 0.13185 train_acc= 0.97381 val_loss= 0.24233 val_acc= 0.93721 time= 0.16700
Epoch: 0114 train_loss= 0.12642 train_acc= 0.97755 val_loss= 0.23979 val_acc= 0.93568 time= 0.19600
Epoch: 0115 train_loss= 0.11995 train_acc= 0.97857 val_loss= 0.23755 val_acc= 0.93109 time= 0.16803
Epoch: 0116 train_loss= 0.12001 train_acc= 0.97891 val_loss= 0.23735 val_acc= 0.93109 time= 0.18651
Epoch: 0117 train_loss= 0.11607 train_acc= 0.97721 val_loss= 0.23659 val_acc= 0.93109 time= 0.17001
Epoch: 0118 train_loss= 0.11168 train_acc= 0.97976 val_loss= 0.23687 val_acc= 0.93262 time= 0.16897
Epoch: 0119 train_loss= 0.10716 train_acc= 0.98010 val_loss= 0.23672 val_acc= 0.93109 time= 0.16703
Epoch: 0120 train_loss= 0.11056 train_acc= 0.97857 val_loss= 0.23566 val_acc= 0.93568 time= 0.19513
Epoch: 0121 train_loss= 0.10371 train_acc= 0.98231 val_loss= 0.23515 val_acc= 0.93721 time= 0.17100
Epoch: 0122 train_loss= 0.10334 train_acc= 0.98078 val_loss= 0.23507 val_acc= 0.93721 time= 0.17200
Epoch: 0123 train_loss= 0.10083 train_acc= 0.97976 val_loss= 0.23477 val_acc= 0.93721 time= 0.16800
Epoch: 0124 train_loss= 0.10110 train_acc= 0.98180 val_loss= 0.23378 val_acc= 0.93721 time= 0.16900
Epoch: 0125 train_loss= 0.10053 train_acc= 0.98163 val_loss= 0.23338 val_acc= 0.93109 time= 0.17204
Epoch: 0126 train_loss= 0.09653 train_acc= 0.98197 val_loss= 0.23350 val_acc= 0.93262 time= 0.19700
Epoch: 0127 train_loss= 0.09081 train_acc= 0.98231 val_loss= 0.23402 val_acc= 0.93262 time= 0.16700
Epoch: 0128 train_loss= 0.09651 train_acc= 0.98197 val_loss= 0.23377 val_acc= 0.93415 time= 0.17998
Epoch: 0129 train_loss= 0.09028 train_acc= 0.98452 val_loss= 0.23205 val_acc= 0.93415 time= 0.16805
Epoch: 0130 train_loss= 0.09029 train_acc= 0.98078 val_loss= 0.23080 val_acc= 0.93415 time= 0.16700
Epoch: 0131 train_loss= 0.08841 train_acc= 0.98367 val_loss= 0.22981 val_acc= 0.93415 time= 0.19500
Epoch: 0132 train_loss= 0.08147 train_acc= 0.98452 val_loss= 0.22909 val_acc= 0.93874 time= 0.17046
Epoch: 0133 train_loss= 0.08555 train_acc= 0.98537 val_loss= 0.22926 val_acc= 0.93874 time= 0.17900
Epoch: 0134 train_loss= 0.07802 train_acc= 0.98622 val_loss= 0.22982 val_acc= 0.93874 time= 0.16746
Epoch: 0135 train_loss= 0.07829 train_acc= 0.98605 val_loss= 0.22933 val_acc= 0.93874 time= 0.16811
Epoch: 0136 train_loss= 0.07577 train_acc= 0.98571 val_loss= 0.22800 val_acc= 0.93874 time= 0.16601
Epoch: 0137 train_loss= 0.07428 train_acc= 0.98707 val_loss= 0.22683 val_acc= 0.93874 time= 0.19299
Epoch: 0138 train_loss= 0.06989 train_acc= 0.98758 val_loss= 0.22696 val_acc= 0.93721 time= 0.16800
Epoch: 0139 train_loss= 0.07306 train_acc= 0.98690 val_loss= 0.22713 val_acc= 0.93415 time= 0.18897
Epoch: 0140 train_loss= 0.07598 train_acc= 0.98571 val_loss= 0.22724 val_acc= 0.93262 time= 0.17360
Epoch: 0141 train_loss= 0.07280 train_acc= 0.98741 val_loss= 0.22672 val_acc= 0.93262 time= 0.16901
Epoch: 0142 train_loss= 0.06802 train_acc= 0.98809 val_loss= 0.22607 val_acc= 0.93568 time= 0.16800
Epoch: 0143 train_loss= 0.07040 train_acc= 0.98758 val_loss= 0.22535 val_acc= 0.94028 time= 0.19400
Epoch: 0144 train_loss= 0.06719 train_acc= 0.98928 val_loss= 0.22581 val_acc= 0.93874 time= 0.16800
Epoch: 0145 train_loss= 0.06516 train_acc= 0.98945 val_loss= 0.22628 val_acc= 0.93721 time= 0.18759
Epoch: 0146 train_loss= 0.06442 train_acc= 0.98996 val_loss= 0.22605 val_acc= 0.93874 time= 0.16799
Epoch: 0147 train_loss= 0.06410 train_acc= 0.98996 val_loss= 0.22521 val_acc= 0.94028 time= 0.17086
Epoch: 0148 train_loss= 0.06137 train_acc= 0.98945 val_loss= 0.22444 val_acc= 0.94028 time= 0.17251
Epoch: 0149 train_loss= 0.05830 train_acc= 0.99235 val_loss= 0.22332 val_acc= 0.93721 time= 0.19565
Epoch: 0150 train_loss= 0.06237 train_acc= 0.99013 val_loss= 0.22167 val_acc= 0.93874 time= 0.16695
Epoch: 0151 train_loss= 0.05736 train_acc= 0.98962 val_loss= 0.22006 val_acc= 0.94028 time= 0.16804
Epoch: 0152 train_loss= 0.05777 train_acc= 0.99047 val_loss= 0.21887 val_acc= 0.93874 time= 0.16700
Epoch: 0153 train_loss= 0.06102 train_acc= 0.98962 val_loss= 0.21738 val_acc= 0.93874 time= 0.16801
Epoch: 0154 train_loss= 0.06170 train_acc= 0.98877 val_loss= 0.21744 val_acc= 0.93721 time= 0.19595
Epoch: 0155 train_loss= 0.05339 train_acc= 0.99116 val_loss= 0.21964 val_acc= 0.94028 time= 0.17000
Epoch: 0156 train_loss= 0.05495 train_acc= 0.99081 val_loss= 0.22367 val_acc= 0.93721 time= 0.18904
Early stopping...
Optimization Finished!
Test set results: cost= 0.24951 accuracy= 0.93769 time= 0.07399
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7826    0.9600    0.8623        75
           4     1.0000    1.0000    1.0000         9
           5     0.7788    0.9310    0.8482        87
           6     0.9200    0.9200    0.9200        25
           7     0.7500    0.9231    0.8276        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.4444    0.6154         9
          10     0.8846    0.6389    0.7419        36
          11     1.0000    0.9167    0.9565        12
          12     0.8264    0.9835    0.8981       121
          13     1.0000    0.7368    0.8485        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9711    0.9670    0.9690       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8611    0.7654    0.8105        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9917    0.9862      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8182    0.7500    0.7826        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.9231    0.8000    0.8571        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9377      2568
   macro avg     0.8014    0.6966    0.7190      2568
weighted avg     0.9391    0.9377    0.9335      2568

Macro average Test Precision, Recall and F1-Score...
(0.8013513052001221, 0.6966188450353866, 0.7190450380386336, None)
Micro average Test Precision, Recall and F1-Score...
(0.9376947040498442, 0.9376947040498442, 0.9376947040498442, None)
embeddings:
8892 6532 2568
[[-4.6790056e-02  4.1863823e-01 -1.3176742e-01 ...  7.6501004e-02
  -1.3127491e-01 -1.2719442e-01]
 [ 1.2708658e-01  2.9892248e-01 -1.4869392e-02 ... -1.3706276e-01
   2.1847363e-01  1.8951695e-01]
 [ 1.1027668e-01  8.1625074e-02 -7.1111098e-03 ...  5.6027341e-01
  -1.5909983e-02  4.9058512e-02]
 ...
 [ 3.9242249e-02  3.7938915e-04  8.9870304e-02 ...  5.5243015e-01
   3.4077843e-03  2.7357191e-02]
 [ 7.0470072e-02  7.4239515e-02  5.2142367e-02 ...  2.9763213e-01
   1.9402059e-02  5.4929636e-02]
 [ 3.1258568e-01  2.5418830e-01  2.6267934e-01 ...  4.5736837e-01
   2.8962138e-01  2.3475175e-01]]

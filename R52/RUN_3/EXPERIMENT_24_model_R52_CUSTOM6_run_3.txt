(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95116 train_acc= 0.03283 val_loss= 3.89241 val_acc= 0.60490 time= 0.46144
Epoch: 0002 train_loss= 3.89315 train_acc= 0.60708 val_loss= 3.78183 val_acc= 0.56202 time= 0.20368
Epoch: 0003 train_loss= 3.78357 train_acc= 0.55792 val_loss= 3.61138 val_acc= 0.51914 time= 0.17455
Epoch: 0004 train_loss= 3.61408 train_acc= 0.50451 val_loss= 3.38252 val_acc= 0.49005 time= 0.17714
Epoch: 0005 train_loss= 3.39031 train_acc= 0.46709 val_loss= 3.10895 val_acc= 0.47167 time= 0.19644
Epoch: 0006 train_loss= 3.11569 train_acc= 0.44838 val_loss= 2.82015 val_acc= 0.46248 time= 0.17182
Epoch: 0007 train_loss= 2.82282 train_acc= 0.43936 val_loss= 2.55345 val_acc= 0.45942 time= 0.17105
Epoch: 0008 train_loss= 2.55243 train_acc= 0.43511 val_loss= 2.35141 val_acc= 0.45942 time= 0.18304
Epoch: 0009 train_loss= 2.35437 train_acc= 0.43341 val_loss= 2.23518 val_acc= 0.45942 time= 0.17158
Epoch: 0010 train_loss= 2.23693 train_acc= 0.43358 val_loss= 2.18264 val_acc= 0.45942 time= 0.17404
Epoch: 0011 train_loss= 2.18297 train_acc= 0.43324 val_loss= 2.14881 val_acc= 0.46248 time= 0.19097
Epoch: 0012 train_loss= 2.16087 train_acc= 0.43460 val_loss= 2.10089 val_acc= 0.46401 time= 0.18000
Epoch: 0013 train_loss= 2.11449 train_acc= 0.43732 val_loss= 2.02915 val_acc= 0.47473 time= 0.18303
Epoch: 0014 train_loss= 2.04794 train_acc= 0.44412 val_loss= 1.93928 val_acc= 0.49770 time= 0.17100
Epoch: 0015 train_loss= 1.95993 train_acc= 0.47695 val_loss= 1.84450 val_acc= 0.54058 time= 0.17700
Epoch: 0016 train_loss= 1.86710 train_acc= 0.53649 val_loss= 1.75881 val_acc= 0.60490 time= 0.19000
Epoch: 0017 train_loss= 1.78589 train_acc= 0.59483 val_loss= 1.68972 val_acc= 0.64165 time= 0.17205
Epoch: 0018 train_loss= 1.71669 train_acc= 0.63242 val_loss= 1.63439 val_acc= 0.66309 time= 0.18907
Epoch: 0019 train_loss= 1.65963 train_acc= 0.64824 val_loss= 1.58413 val_acc= 0.67994 time= 0.17209
Epoch: 0020 train_loss= 1.60957 train_acc= 0.66032 val_loss= 1.53262 val_acc= 0.68606 time= 0.17713
Epoch: 0021 train_loss= 1.55824 train_acc= 0.66882 val_loss= 1.47917 val_acc= 0.69525 time= 0.17914
Epoch: 0022 train_loss= 1.50533 train_acc= 0.67409 val_loss= 1.42654 val_acc= 0.69832 time= 0.18954
Epoch: 0023 train_loss= 1.44963 train_acc= 0.67597 val_loss= 1.37777 val_acc= 0.69985 time= 0.17192
Epoch: 0024 train_loss= 1.40329 train_acc= 0.67767 val_loss= 1.33439 val_acc= 0.70291 time= 0.17430
Epoch: 0025 train_loss= 1.35767 train_acc= 0.67988 val_loss= 1.29639 val_acc= 0.70904 time= 0.19205
Epoch: 0026 train_loss= 1.31944 train_acc= 0.68702 val_loss= 1.26265 val_acc= 0.71363 time= 0.17199
Epoch: 0027 train_loss= 1.28484 train_acc= 0.69451 val_loss= 1.23178 val_acc= 0.71516 time= 0.19573
Epoch: 0028 train_loss= 1.24989 train_acc= 0.70437 val_loss= 1.20259 val_acc= 0.72282 time= 0.17500
Epoch: 0029 train_loss= 1.22010 train_acc= 0.71611 val_loss= 1.17423 val_acc= 0.72741 time= 0.17636
Epoch: 0030 train_loss= 1.19043 train_acc= 0.72597 val_loss= 1.14619 val_acc= 0.73354 time= 0.17658
Epoch: 0031 train_loss= 1.15869 train_acc= 0.73669 val_loss= 1.11817 val_acc= 0.73660 time= 0.17304
Epoch: 0032 train_loss= 1.12954 train_acc= 0.74502 val_loss= 1.09014 val_acc= 0.74119 time= 0.17104
Epoch: 0033 train_loss= 1.09992 train_acc= 0.75013 val_loss= 1.06222 val_acc= 0.74885 time= 0.19271
Epoch: 0034 train_loss= 1.07178 train_acc= 0.75625 val_loss= 1.03472 val_acc= 0.75957 time= 0.17144
Epoch: 0035 train_loss= 1.04309 train_acc= 0.76493 val_loss= 1.00799 val_acc= 0.77029 time= 0.17707
Epoch: 0036 train_loss= 1.01504 train_acc= 0.77054 val_loss= 0.98226 val_acc= 0.77182 time= 0.19652
Epoch: 0037 train_loss= 0.99086 train_acc= 0.77802 val_loss= 0.95753 val_acc= 0.77948 time= 0.17400
Epoch: 0038 train_loss= 0.96563 train_acc= 0.78551 val_loss= 0.93372 val_acc= 0.79326 time= 0.17400
Epoch: 0039 train_loss= 0.94250 train_acc= 0.79350 val_loss= 0.91068 val_acc= 0.80245 time= 0.18904
Epoch: 0040 train_loss= 0.91814 train_acc= 0.80745 val_loss= 0.88822 val_acc= 0.80704 time= 0.17507
Epoch: 0041 train_loss= 0.89587 train_acc= 0.81527 val_loss= 0.86609 val_acc= 0.81164 time= 0.19123
Epoch: 0042 train_loss= 0.87248 train_acc= 0.82191 val_loss= 0.84424 val_acc= 0.82083 time= 0.17757
Epoch: 0043 train_loss= 0.84954 train_acc= 0.82565 val_loss= 0.82267 val_acc= 0.82695 time= 0.17649
Epoch: 0044 train_loss= 0.82476 train_acc= 0.83041 val_loss= 0.80135 val_acc= 0.82695 time= 0.17900
Epoch: 0045 train_loss= 0.80155 train_acc= 0.83484 val_loss= 0.78032 val_acc= 0.83155 time= 0.17204
Epoch: 0046 train_loss= 0.77785 train_acc= 0.83756 val_loss= 0.75961 val_acc= 0.83461 time= 0.17369
Epoch: 0047 train_loss= 0.75602 train_acc= 0.84096 val_loss= 0.73916 val_acc= 0.83614 time= 0.19120
Epoch: 0048 train_loss= 0.73672 train_acc= 0.84147 val_loss= 0.71888 val_acc= 0.83920 time= 0.17597
Epoch: 0049 train_loss= 0.71190 train_acc= 0.84674 val_loss= 0.69873 val_acc= 0.84380 time= 0.17605
Epoch: 0050 train_loss= 0.69341 train_acc= 0.85134 val_loss= 0.67884 val_acc= 0.84686 time= 0.19267
Epoch: 0051 train_loss= 0.67268 train_acc= 0.85559 val_loss= 0.65930 val_acc= 0.85452 time= 0.17607
Epoch: 0052 train_loss= 0.65057 train_acc= 0.86137 val_loss= 0.64040 val_acc= 0.86064 time= 0.19412
Epoch: 0053 train_loss= 0.63098 train_acc= 0.86698 val_loss= 0.62224 val_acc= 0.86371 time= 0.17305
Epoch: 0054 train_loss= 0.61198 train_acc= 0.86800 val_loss= 0.60494 val_acc= 0.87136 time= 0.17202
Epoch: 0055 train_loss= 0.59207 train_acc= 0.87090 val_loss= 0.58860 val_acc= 0.87443 time= 0.19209
Epoch: 0056 train_loss= 0.57302 train_acc= 0.87362 val_loss= 0.57328 val_acc= 0.87749 time= 0.17111
Epoch: 0057 train_loss= 0.55662 train_acc= 0.87838 val_loss= 0.55884 val_acc= 0.87749 time= 0.17378
Epoch: 0058 train_loss= 0.53873 train_acc= 0.87804 val_loss= 0.54502 val_acc= 0.87902 time= 0.19702
Epoch: 0059 train_loss= 0.52263 train_acc= 0.88433 val_loss= 0.53157 val_acc= 0.88361 time= 0.17778
Epoch: 0060 train_loss= 0.50628 train_acc= 0.88604 val_loss= 0.51818 val_acc= 0.88208 time= 0.17622
Epoch: 0061 train_loss= 0.48808 train_acc= 0.89216 val_loss= 0.50467 val_acc= 0.88361 time= 0.19117
Epoch: 0062 train_loss= 0.47446 train_acc= 0.89199 val_loss= 0.49092 val_acc= 0.88361 time= 0.17205
Epoch: 0063 train_loss= 0.45891 train_acc= 0.89743 val_loss= 0.47748 val_acc= 0.88515 time= 0.19037
Epoch: 0064 train_loss= 0.44447 train_acc= 0.90151 val_loss= 0.46422 val_acc= 0.89127 time= 0.17398
Epoch: 0065 train_loss= 0.42848 train_acc= 0.90458 val_loss= 0.45158 val_acc= 0.89127 time= 0.17393
Epoch: 0066 train_loss= 0.41684 train_acc= 0.90798 val_loss= 0.43986 val_acc= 0.89433 time= 0.19974
Epoch: 0067 train_loss= 0.40470 train_acc= 0.91155 val_loss= 0.42895 val_acc= 0.89587 time= 0.17472
Epoch: 0068 train_loss= 0.38968 train_acc= 0.91937 val_loss= 0.41874 val_acc= 0.89740 time= 0.17217
Epoch: 0069 train_loss= 0.37795 train_acc= 0.92108 val_loss= 0.40915 val_acc= 0.89893 time= 0.19061
Epoch: 0070 train_loss= 0.36672 train_acc= 0.92346 val_loss= 0.39999 val_acc= 0.90046 time= 0.17200
Epoch: 0071 train_loss= 0.35487 train_acc= 0.92873 val_loss= 0.39121 val_acc= 0.90199 time= 0.17640
Epoch: 0072 train_loss= 0.34348 train_acc= 0.93026 val_loss= 0.38294 val_acc= 0.90352 time= 0.19108
Epoch: 0073 train_loss= 0.33252 train_acc= 0.93264 val_loss= 0.37474 val_acc= 0.90658 time= 0.17549
Epoch: 0074 train_loss= 0.31963 train_acc= 0.93638 val_loss= 0.36672 val_acc= 0.90658 time= 0.18603
Epoch: 0075 train_loss= 0.31058 train_acc= 0.93723 val_loss= 0.35917 val_acc= 0.90505 time= 0.17375
Epoch: 0076 train_loss= 0.30272 train_acc= 0.93860 val_loss= 0.35187 val_acc= 0.90505 time= 0.17358
Epoch: 0077 train_loss= 0.28996 train_acc= 0.94200 val_loss= 0.34486 val_acc= 0.90658 time= 0.19200
Epoch: 0078 train_loss= 0.28202 train_acc= 0.94353 val_loss= 0.33823 val_acc= 0.90812 time= 0.17104
Epoch: 0079 train_loss= 0.27046 train_acc= 0.94523 val_loss= 0.33226 val_acc= 0.91271 time= 0.17372
Epoch: 0080 train_loss= 0.26403 train_acc= 0.94778 val_loss= 0.32660 val_acc= 0.91424 time= 0.19059
Epoch: 0081 train_loss= 0.25498 train_acc= 0.94948 val_loss= 0.32140 val_acc= 0.91577 time= 0.17540
Epoch: 0082 train_loss= 0.24793 train_acc= 0.95254 val_loss= 0.31626 val_acc= 0.91730 time= 0.19716
Epoch: 0083 train_loss= 0.23891 train_acc= 0.95475 val_loss= 0.31139 val_acc= 0.91577 time= 0.17502
Epoch: 0084 train_loss= 0.23080 train_acc= 0.95526 val_loss= 0.30664 val_acc= 0.92037 time= 0.17201
Epoch: 0085 train_loss= 0.22291 train_acc= 0.95867 val_loss= 0.30226 val_acc= 0.92190 time= 0.19646
Epoch: 0086 train_loss= 0.21427 train_acc= 0.95867 val_loss= 0.29795 val_acc= 0.92190 time= 0.17100
Epoch: 0087 train_loss= 0.21062 train_acc= 0.95986 val_loss= 0.29382 val_acc= 0.92343 time= 0.17460
Epoch: 0088 train_loss= 0.20216 train_acc= 0.96275 val_loss= 0.28979 val_acc= 0.92343 time= 0.19300
Epoch: 0089 train_loss= 0.19568 train_acc= 0.96343 val_loss= 0.28603 val_acc= 0.92343 time= 0.17504
Epoch: 0090 train_loss= 0.18938 train_acc= 0.96309 val_loss= 0.28227 val_acc= 0.92343 time= 0.17900
Epoch: 0091 train_loss= 0.18213 train_acc= 0.96496 val_loss= 0.27832 val_acc= 0.92343 time= 0.19295
Epoch: 0092 train_loss= 0.17635 train_acc= 0.96734 val_loss= 0.27479 val_acc= 0.92496 time= 0.17321
Epoch: 0093 train_loss= 0.16966 train_acc= 0.96887 val_loss= 0.27173 val_acc= 0.92649 time= 0.17701
Epoch: 0094 train_loss= 0.16333 train_acc= 0.96921 val_loss= 0.26874 val_acc= 0.92649 time= 0.19108
Epoch: 0095 train_loss= 0.15929 train_acc= 0.96955 val_loss= 0.26568 val_acc= 0.92649 time= 0.17107
Epoch: 0096 train_loss= 0.15126 train_acc= 0.97210 val_loss= 0.26297 val_acc= 0.92649 time= 0.17960
Epoch: 0097 train_loss= 0.15019 train_acc= 0.97312 val_loss= 0.26010 val_acc= 0.92956 time= 0.19068
Epoch: 0098 train_loss= 0.14318 train_acc= 0.97500 val_loss= 0.25770 val_acc= 0.93109 time= 0.17514
Epoch: 0099 train_loss= 0.13886 train_acc= 0.97687 val_loss= 0.25555 val_acc= 0.92956 time= 0.17266
Epoch: 0100 train_loss= 0.13483 train_acc= 0.97636 val_loss= 0.25390 val_acc= 0.93262 time= 0.19179
Epoch: 0101 train_loss= 0.13013 train_acc= 0.97755 val_loss= 0.25259 val_acc= 0.93262 time= 0.17179
Epoch: 0102 train_loss= 0.12484 train_acc= 0.98010 val_loss= 0.25163 val_acc= 0.93262 time= 0.18220
Epoch: 0103 train_loss= 0.12181 train_acc= 0.98027 val_loss= 0.25054 val_acc= 0.93568 time= 0.17296
Epoch: 0104 train_loss= 0.11676 train_acc= 0.98265 val_loss= 0.24885 val_acc= 0.93721 time= 0.17659
Epoch: 0105 train_loss= 0.11453 train_acc= 0.98112 val_loss= 0.24633 val_acc= 0.93721 time= 0.21498
Epoch: 0106 train_loss= 0.10949 train_acc= 0.98384 val_loss= 0.24398 val_acc= 0.93874 time= 0.18600
Epoch: 0107 train_loss= 0.10835 train_acc= 0.98333 val_loss= 0.24224 val_acc= 0.93721 time= 0.17503
Epoch: 0108 train_loss= 0.10257 train_acc= 0.98452 val_loss= 0.24097 val_acc= 0.93721 time= 0.17697
Epoch: 0109 train_loss= 0.09850 train_acc= 0.98503 val_loss= 0.24054 val_acc= 0.93568 time= 0.17203
Epoch: 0110 train_loss= 0.09613 train_acc= 0.98605 val_loss= 0.24002 val_acc= 0.93721 time= 0.19500
Epoch: 0111 train_loss= 0.09402 train_acc= 0.98554 val_loss= 0.23931 val_acc= 0.93874 time= 0.16997
Epoch: 0112 train_loss= 0.08921 train_acc= 0.98656 val_loss= 0.23831 val_acc= 0.94028 time= 0.17631
Epoch: 0113 train_loss= 0.08723 train_acc= 0.98809 val_loss= 0.23736 val_acc= 0.94028 time= 0.20372
Epoch: 0114 train_loss= 0.08333 train_acc= 0.98877 val_loss= 0.23631 val_acc= 0.93874 time= 0.17700
Epoch: 0115 train_loss= 0.08130 train_acc= 0.98979 val_loss= 0.23520 val_acc= 0.93874 time= 0.17479
Epoch: 0116 train_loss= 0.07920 train_acc= 0.98911 val_loss= 0.23426 val_acc= 0.93874 time= 0.19355
Epoch: 0117 train_loss= 0.07619 train_acc= 0.98996 val_loss= 0.23340 val_acc= 0.93721 time= 0.17228
Epoch: 0118 train_loss= 0.07506 train_acc= 0.99030 val_loss= 0.23247 val_acc= 0.93721 time= 0.18763
Epoch: 0119 train_loss= 0.07210 train_acc= 0.99013 val_loss= 0.23182 val_acc= 0.93721 time= 0.17330
Epoch: 0120 train_loss= 0.07086 train_acc= 0.98996 val_loss= 0.23122 val_acc= 0.93721 time= 0.17296
Epoch: 0121 train_loss= 0.06770 train_acc= 0.98996 val_loss= 0.23139 val_acc= 0.93874 time= 0.19574
Epoch: 0122 train_loss= 0.06612 train_acc= 0.99081 val_loss= 0.23106 val_acc= 0.93721 time= 0.17503
Epoch: 0123 train_loss= 0.06451 train_acc= 0.99047 val_loss= 0.23062 val_acc= 0.93874 time= 0.19000
Epoch: 0124 train_loss= 0.06288 train_acc= 0.99098 val_loss= 0.23047 val_acc= 0.94028 time= 0.17397
Epoch: 0125 train_loss= 0.06085 train_acc= 0.99235 val_loss= 0.22993 val_acc= 0.94181 time= 0.17326
Epoch: 0126 train_loss= 0.05807 train_acc= 0.99201 val_loss= 0.22958 val_acc= 0.94334 time= 0.17497
Epoch: 0127 train_loss= 0.05718 train_acc= 0.99235 val_loss= 0.22934 val_acc= 0.94334 time= 0.19004
Epoch: 0128 train_loss= 0.05458 train_acc= 0.99252 val_loss= 0.22925 val_acc= 0.94334 time= 0.17695
Epoch: 0129 train_loss= 0.05412 train_acc= 0.99201 val_loss= 0.22877 val_acc= 0.94334 time= 0.17955
Epoch: 0130 train_loss= 0.05282 train_acc= 0.99218 val_loss= 0.22776 val_acc= 0.94334 time= 0.18096
Epoch: 0131 train_loss= 0.05101 train_acc= 0.99337 val_loss= 0.22690 val_acc= 0.94334 time= 0.17265
Epoch: 0132 train_loss= 0.04953 train_acc= 0.99388 val_loss= 0.22678 val_acc= 0.94487 time= 0.17457
Epoch: 0133 train_loss= 0.04816 train_acc= 0.99337 val_loss= 0.22728 val_acc= 0.94487 time= 0.17880
Epoch: 0134 train_loss= 0.04703 train_acc= 0.99439 val_loss= 0.22749 val_acc= 0.94487 time= 0.17916
Epoch: 0135 train_loss= 0.04558 train_acc= 0.99405 val_loss= 0.22821 val_acc= 0.94487 time= 0.19097
Epoch: 0136 train_loss= 0.04439 train_acc= 0.99473 val_loss= 0.22873 val_acc= 0.94640 time= 0.17403
Early stopping...
Optimization Finished!
Test set results: cost= 0.25233 accuracy= 0.93536 time= 0.07800
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.5000    0.1667    0.2500         6
           2     1.0000    1.0000    1.0000         1
           3     0.7692    0.9333    0.8434        75
           4     1.0000    1.0000    1.0000         9
           5     0.8100    0.9310    0.8663        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.5556    0.7143         9
          10     0.9583    0.6389    0.7667        36
          11     1.0000    0.9167    0.9565        12
          12     0.8392    0.9917    0.9091       121
          13     1.0000    0.6842    0.8125        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.3333    0.2500    0.2857         4
          17     1.0000    0.3333    0.5000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9655    0.9641    0.9648       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8421    0.7901    0.8153        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    1.0000    1.0000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9908    0.9849      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9000    0.7500    0.8182        12
          45     0.5000    0.1667    0.2500         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9354      2568
   macro avg     0.7788    0.6691    0.6964      2568
weighted avg     0.9364    0.9354    0.9312      2568

Macro average Test Precision, Recall and F1-Score...
(0.7787630670876664, 0.669075436744043, 0.6963760069925639, None)
Micro average Test Precision, Recall and F1-Score...
(0.9353582554517134, 0.9353582554517134, 0.9353582554517134, None)
embeddings:
8892 6532 2568
[[ 0.20716137  0.05355078  0.9277144  ... -0.1849564   0.02328268
   0.09264545]
 [ 0.068679    0.13619597  0.39686334 ...  0.01073949  0.00726332
   0.25167733]
 [ 0.09559856  0.07357617  0.76191765 ... -0.02972964  0.09478699
   0.12995024]
 ...
 [ 0.1017715   0.02273241  0.26092726 ...  0.03131312  0.0306303
   0.1601303 ]
 [ 0.07410777  0.06085042  0.28625104 ...  0.05023866  0.0651511
   0.05562826]
 [ 0.25057647  0.30063695  0.19063778 ...  0.26557344  0.21363848
   0.21736397]]

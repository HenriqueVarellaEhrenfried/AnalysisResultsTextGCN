(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95122 train_acc= 0.01225 val_loss= 3.90741 val_acc= 0.67228 time= 0.47521
Epoch: 0002 train_loss= 3.90784 train_acc= 0.63429 val_loss= 3.81541 val_acc= 0.67228 time= 0.17476
Epoch: 0003 train_loss= 3.81043 train_acc= 0.63072 val_loss= 3.67172 val_acc= 0.66769 time= 0.17105
Epoch: 0004 train_loss= 3.68018 train_acc= 0.61473 val_loss= 3.47764 val_acc= 0.66003 time= 0.19303
Epoch: 0005 train_loss= 3.47999 train_acc= 0.60589 val_loss= 3.24294 val_acc= 0.65084 time= 0.17301
Epoch: 0006 train_loss= 3.24073 train_acc= 0.58598 val_loss= 2.98777 val_acc= 0.62940 time= 0.17892
Epoch: 0007 train_loss= 2.98126 train_acc= 0.56285 val_loss= 2.74102 val_acc= 0.61715 time= 0.19255
Epoch: 0008 train_loss= 2.72735 train_acc= 0.58649 val_loss= 2.53134 val_acc= 0.61103 time= 0.17472
Epoch: 0009 train_loss= 2.50892 train_acc= 0.53836 val_loss= 2.38328 val_acc= 0.59112 time= 0.17405
Epoch: 0010 train_loss= 2.35619 train_acc= 0.55503 val_loss= 2.29926 val_acc= 0.51914 time= 0.18200
Epoch: 0011 train_loss= 2.35320 train_acc= 0.51488 val_loss= 2.25626 val_acc= 0.46554 time= 0.17200
Epoch: 0012 train_loss= 2.30041 train_acc= 0.47627 val_loss= 2.22544 val_acc= 0.45636 time= 0.19010
Epoch: 0013 train_loss= 2.21927 train_acc= 0.44821 val_loss= 2.18363 val_acc= 0.45636 time= 0.17156
Epoch: 0014 train_loss= 2.23823 train_acc= 0.44021 val_loss= 2.12514 val_acc= 0.45636 time= 0.17508
Epoch: 0015 train_loss= 2.14569 train_acc= 0.43698 val_loss= 2.05103 val_acc= 0.45636 time= 0.19851
Epoch: 0016 train_loss= 2.09656 train_acc= 0.43732 val_loss= 1.96773 val_acc= 0.45636 time= 0.17308
Epoch: 0017 train_loss= 1.99327 train_acc= 0.44787 val_loss= 1.88528 val_acc= 0.47167 time= 0.17050
Epoch: 0018 train_loss= 1.94290 train_acc= 0.45977 val_loss= 1.81121 val_acc= 0.51149 time= 0.18974
Epoch: 0019 train_loss= 1.84857 train_acc= 0.50978 val_loss= 1.74847 val_acc= 0.59724 time= 0.17337
Epoch: 0020 train_loss= 1.79515 train_acc= 0.59806 val_loss= 1.69461 val_acc= 0.65237 time= 0.18802
Epoch: 0021 train_loss= 1.76860 train_acc= 0.61983 val_loss= 1.64437 val_acc= 0.67075 time= 0.17193
Epoch: 0022 train_loss= 1.72078 train_acc= 0.63123 val_loss= 1.59428 val_acc= 0.67075 time= 0.17603
Epoch: 0023 train_loss= 1.65199 train_acc= 0.63497 val_loss= 1.54446 val_acc= 0.67688 time= 0.18016
Epoch: 0024 train_loss= 1.59252 train_acc= 0.63548 val_loss= 1.49627 val_acc= 0.67841 time= 0.17303
Epoch: 0025 train_loss= 1.54697 train_acc= 0.64467 val_loss= 1.45096 val_acc= 0.68300 time= 0.17201
Epoch: 0026 train_loss= 1.52938 train_acc= 0.64569 val_loss= 1.40876 val_acc= 0.68760 time= 0.19278
Epoch: 0027 train_loss= 1.44396 train_acc= 0.65504 val_loss= 1.37067 val_acc= 0.69372 time= 0.17200
Epoch: 0028 train_loss= 1.40219 train_acc= 0.66151 val_loss= 1.33638 val_acc= 0.69219 time= 0.17797
Epoch: 0029 train_loss= 1.36353 train_acc= 0.67563 val_loss= 1.30527 val_acc= 0.69985 time= 0.17611
Epoch: 0030 train_loss= 1.35382 train_acc= 0.68209 val_loss= 1.27633 val_acc= 0.70444 time= 0.17254
Epoch: 0031 train_loss= 1.30658 train_acc= 0.68345 val_loss= 1.24886 val_acc= 0.71057 time= 0.18020
Epoch: 0032 train_loss= 1.28947 train_acc= 0.69246 val_loss= 1.22227 val_acc= 0.71669 time= 0.19304
Epoch: 0033 train_loss= 1.25826 train_acc= 0.70743 val_loss= 1.19605 val_acc= 0.72129 time= 0.17714
Epoch: 0034 train_loss= 1.22190 train_acc= 0.71747 val_loss= 1.17024 val_acc= 0.73047 time= 0.19825
Epoch: 0035 train_loss= 1.20970 train_acc= 0.71645 val_loss= 1.14505 val_acc= 0.73507 time= 0.17054
Epoch: 0036 train_loss= 1.18197 train_acc= 0.72716 val_loss= 1.12067 val_acc= 0.73813 time= 0.17000
Epoch: 0037 train_loss= 1.14536 train_acc= 0.73295 val_loss= 1.09702 val_acc= 0.73813 time= 0.19315
Epoch: 0038 train_loss= 1.13537 train_acc= 0.73074 val_loss= 1.07424 val_acc= 0.74273 time= 0.17611
Epoch: 0039 train_loss= 1.11204 train_acc= 0.73380 val_loss= 1.05231 val_acc= 0.74273 time= 0.17848
Epoch: 0040 train_loss= 1.07974 train_acc= 0.73686 val_loss= 1.03099 val_acc= 0.74579 time= 0.18810
Epoch: 0041 train_loss= 1.07602 train_acc= 0.74690 val_loss= 1.01040 val_acc= 0.75191 time= 0.17296
Epoch: 0042 train_loss= 1.05269 train_acc= 0.75166 val_loss= 0.99030 val_acc= 0.76110 time= 0.17504
Epoch: 0043 train_loss= 1.04302 train_acc= 0.75982 val_loss= 0.97057 val_acc= 0.76876 time= 0.18997
Epoch: 0044 train_loss= 1.01248 train_acc= 0.77547 val_loss= 0.95111 val_acc= 0.77948 time= 0.17212
Epoch: 0045 train_loss= 0.99604 train_acc= 0.77615 val_loss= 0.93202 val_acc= 0.78867 time= 0.17703
Epoch: 0046 train_loss= 0.97763 train_acc= 0.77989 val_loss= 0.91307 val_acc= 0.79939 time= 0.18623
Epoch: 0047 train_loss= 0.93647 train_acc= 0.79418 val_loss= 0.89445 val_acc= 0.80551 time= 0.17509
Epoch: 0048 train_loss= 0.93236 train_acc= 0.79537 val_loss= 0.87622 val_acc= 0.81011 time= 0.17513
Epoch: 0049 train_loss= 0.93152 train_acc= 0.79265 val_loss= 0.85871 val_acc= 0.81164 time= 0.18858
Epoch: 0050 train_loss= 0.89584 train_acc= 0.80303 val_loss= 0.84152 val_acc= 0.81164 time= 0.17469
Epoch: 0051 train_loss= 0.87421 train_acc= 0.80813 val_loss= 0.82458 val_acc= 0.81317 time= 0.18206
Epoch: 0052 train_loss= 0.88181 train_acc= 0.80235 val_loss= 0.80782 val_acc= 0.81470 time= 0.17351
Epoch: 0053 train_loss= 0.86005 train_acc= 0.80660 val_loss= 0.79111 val_acc= 0.81776 time= 0.17334
Epoch: 0054 train_loss= 0.81507 train_acc= 0.81664 val_loss= 0.77457 val_acc= 0.81930 time= 0.19861
Epoch: 0055 train_loss= 0.82356 train_acc= 0.80915 val_loss= 0.75791 val_acc= 0.82542 time= 0.17720
Epoch: 0056 train_loss= 0.79031 train_acc= 0.82463 val_loss= 0.74109 val_acc= 0.83308 time= 0.17507
Epoch: 0057 train_loss= 0.77192 train_acc= 0.82378 val_loss= 0.72459 val_acc= 0.83920 time= 0.19250
Epoch: 0058 train_loss= 0.76940 train_acc= 0.82089 val_loss= 0.70851 val_acc= 0.84074 time= 0.17204
Epoch: 0059 train_loss= 0.74126 train_acc= 0.82650 val_loss= 0.69295 val_acc= 0.84227 time= 0.18112
Epoch: 0060 train_loss= 0.74079 train_acc= 0.82786 val_loss= 0.67734 val_acc= 0.84686 time= 0.18903
Epoch: 0061 train_loss= 0.72793 train_acc= 0.82548 val_loss= 0.66257 val_acc= 0.84992 time= 0.17307
Epoch: 0062 train_loss= 0.69601 train_acc= 0.83960 val_loss= 0.64864 val_acc= 0.85452 time= 0.19355
Epoch: 0063 train_loss= 0.68746 train_acc= 0.83739 val_loss= 0.63600 val_acc= 0.85911 time= 0.17626
Epoch: 0064 train_loss= 0.66967 train_acc= 0.84351 val_loss= 0.62426 val_acc= 0.86217 time= 0.17420
Epoch: 0065 train_loss= 0.66084 train_acc= 0.84317 val_loss= 0.61316 val_acc= 0.86677 time= 0.19200
Epoch: 0066 train_loss= 0.66098 train_acc= 0.84028 val_loss= 0.60207 val_acc= 0.86677 time= 0.17059
Epoch: 0067 train_loss= 0.64096 train_acc= 0.84436 val_loss= 0.59185 val_acc= 0.86830 time= 0.17324
Epoch: 0068 train_loss= 0.62263 train_acc= 0.85014 val_loss= 0.58147 val_acc= 0.86677 time= 0.18961
Epoch: 0069 train_loss= 0.63271 train_acc= 0.84810 val_loss= 0.57072 val_acc= 0.87136 time= 0.17400
Epoch: 0070 train_loss= 0.61620 train_acc= 0.85406 val_loss= 0.56042 val_acc= 0.86983 time= 0.17711
Epoch: 0071 train_loss= 0.60482 train_acc= 0.85355 val_loss= 0.54974 val_acc= 0.87289 time= 0.19627
Epoch: 0072 train_loss= 0.58869 train_acc= 0.86001 val_loss= 0.53762 val_acc= 0.87443 time= 0.17104
Epoch: 0073 train_loss= 0.57206 train_acc= 0.86222 val_loss= 0.52502 val_acc= 0.88361 time= 0.19114
Epoch: 0074 train_loss= 0.55253 train_acc= 0.87481 val_loss= 0.51334 val_acc= 0.88515 time= 0.17384
Epoch: 0075 train_loss= 0.54395 train_acc= 0.87600 val_loss= 0.50204 val_acc= 0.88974 time= 0.17223
Epoch: 0076 train_loss= 0.55299 train_acc= 0.87294 val_loss= 0.49224 val_acc= 0.88974 time= 0.19199
Epoch: 0077 train_loss= 0.53096 train_acc= 0.87549 val_loss= 0.48384 val_acc= 0.88668 time= 0.17452
Epoch: 0078 train_loss= 0.54165 train_acc= 0.87090 val_loss= 0.47625 val_acc= 0.88668 time= 0.17819
Epoch: 0079 train_loss= 0.51590 train_acc= 0.87719 val_loss= 0.46848 val_acc= 0.88821 time= 0.19604
Epoch: 0080 train_loss= 0.51147 train_acc= 0.87923 val_loss= 0.46095 val_acc= 0.88974 time= 0.17239
Epoch: 0081 train_loss= 0.50109 train_acc= 0.87940 val_loss= 0.45289 val_acc= 0.89127 time= 0.17269
Epoch: 0082 train_loss= 0.48240 train_acc= 0.88110 val_loss= 0.44477 val_acc= 0.89433 time= 0.19004
Epoch: 0083 train_loss= 0.47460 train_acc= 0.87991 val_loss= 0.43751 val_acc= 0.89587 time= 0.17499
Epoch: 0084 train_loss= 0.48165 train_acc= 0.88382 val_loss= 0.43109 val_acc= 0.89433 time= 0.19004
Epoch: 0085 train_loss= 0.48439 train_acc= 0.88876 val_loss= 0.42489 val_acc= 0.89433 time= 0.17374
Epoch: 0086 train_loss= 0.43838 train_acc= 0.89284 val_loss= 0.41828 val_acc= 0.89433 time= 0.17558
Epoch: 0087 train_loss= 0.45048 train_acc= 0.88995 val_loss= 0.41258 val_acc= 0.89587 time= 0.17429
Epoch: 0088 train_loss= 0.45388 train_acc= 0.89029 val_loss= 0.40717 val_acc= 0.89587 time= 0.17212
Epoch: 0089 train_loss= 0.44383 train_acc= 0.88450 val_loss= 0.40262 val_acc= 0.89893 time= 0.18440
Epoch: 0090 train_loss= 0.41090 train_acc= 0.90253 val_loss= 0.39703 val_acc= 0.89893 time= 0.18006
Epoch: 0091 train_loss= 0.42357 train_acc= 0.89709 val_loss= 0.39143 val_acc= 0.89740 time= 0.17180
Epoch: 0092 train_loss= 0.40611 train_acc= 0.89862 val_loss= 0.38685 val_acc= 0.89587 time= 0.17659
Epoch: 0093 train_loss= 0.42525 train_acc= 0.89471 val_loss= 0.38105 val_acc= 0.89740 time= 0.19192
Epoch: 0094 train_loss= 0.39413 train_acc= 0.89930 val_loss= 0.37525 val_acc= 0.90199 time= 0.18000
Epoch: 0095 train_loss= 0.41989 train_acc= 0.89607 val_loss= 0.36948 val_acc= 0.90352 time= 0.18955
Epoch: 0096 train_loss= 0.37608 train_acc= 0.90951 val_loss= 0.36456 val_acc= 0.90505 time= 0.17208
Epoch: 0097 train_loss= 0.39185 train_acc= 0.90219 val_loss= 0.36119 val_acc= 0.90505 time= 0.17199
Epoch: 0098 train_loss= 0.36643 train_acc= 0.90543 val_loss= 0.35820 val_acc= 0.90505 time= 0.19153
Epoch: 0099 train_loss= 0.38129 train_acc= 0.90338 val_loss= 0.35524 val_acc= 0.90505 time= 0.17646
Epoch: 0100 train_loss= 0.38118 train_acc= 0.90798 val_loss= 0.35234 val_acc= 0.90658 time= 0.17411
Epoch: 0101 train_loss= 0.34768 train_acc= 0.91597 val_loss= 0.34942 val_acc= 0.91424 time= 0.19672
Epoch: 0102 train_loss= 0.35128 train_acc= 0.91308 val_loss= 0.34635 val_acc= 0.91424 time= 0.17625
Epoch: 0103 train_loss= 0.34421 train_acc= 0.91733 val_loss= 0.34304 val_acc= 0.91577 time= 0.17669
Epoch: 0104 train_loss= 0.35463 train_acc= 0.91240 val_loss= 0.33803 val_acc= 0.91118 time= 0.18972
Epoch: 0105 train_loss= 0.34729 train_acc= 0.91342 val_loss= 0.33340 val_acc= 0.91271 time= 0.17173
Epoch: 0106 train_loss= 0.34106 train_acc= 0.91733 val_loss= 0.32976 val_acc= 0.91424 time= 0.17570
Epoch: 0107 train_loss= 0.33907 train_acc= 0.91682 val_loss= 0.32613 val_acc= 0.91577 time= 0.18128
Epoch: 0108 train_loss= 0.31993 train_acc= 0.92584 val_loss= 0.32300 val_acc= 0.91577 time= 0.17376
Epoch: 0109 train_loss= 0.33388 train_acc= 0.91682 val_loss= 0.32000 val_acc= 0.91730 time= 0.17873
Epoch: 0110 train_loss= 0.33448 train_acc= 0.91478 val_loss= 0.31685 val_acc= 0.91577 time= 0.19429
Epoch: 0111 train_loss= 0.31167 train_acc= 0.92635 val_loss= 0.31349 val_acc= 0.91730 time= 0.17300
Epoch: 0112 train_loss= 0.31026 train_acc= 0.92414 val_loss= 0.31056 val_acc= 0.91884 time= 0.17700
Epoch: 0113 train_loss= 0.30422 train_acc= 0.92720 val_loss= 0.30764 val_acc= 0.91730 time= 0.17300
Epoch: 0114 train_loss= 0.30687 train_acc= 0.92754 val_loss= 0.30427 val_acc= 0.91884 time= 0.17700
Epoch: 0115 train_loss= 0.30481 train_acc= 0.92686 val_loss= 0.30121 val_acc= 0.91884 time= 0.19579
Epoch: 0116 train_loss= 0.30281 train_acc= 0.92346 val_loss= 0.29929 val_acc= 0.91577 time= 0.17597
Epoch: 0117 train_loss= 0.28719 train_acc= 0.92584 val_loss= 0.29881 val_acc= 0.91577 time= 0.17600
Epoch: 0118 train_loss= 0.28324 train_acc= 0.92567 val_loss= 0.29829 val_acc= 0.91884 time= 0.20403
Epoch: 0119 train_loss= 0.28871 train_acc= 0.92686 val_loss= 0.29778 val_acc= 0.92037 time= 0.17300
Epoch: 0120 train_loss= 0.28822 train_acc= 0.93264 val_loss= 0.29685 val_acc= 0.91884 time= 0.17405
Epoch: 0121 train_loss= 0.28195 train_acc= 0.92992 val_loss= 0.29560 val_acc= 0.91730 time= 0.18997
Epoch: 0122 train_loss= 0.27645 train_acc= 0.93111 val_loss= 0.29443 val_acc= 0.91884 time= 0.17400
Epoch: 0123 train_loss= 0.27700 train_acc= 0.93604 val_loss= 0.29373 val_acc= 0.91884 time= 0.18500
Epoch: 0124 train_loss= 0.29572 train_acc= 0.93043 val_loss= 0.29146 val_acc= 0.91884 time= 0.17521
Epoch: 0125 train_loss= 0.26257 train_acc= 0.93366 val_loss= 0.28772 val_acc= 0.91730 time= 0.17738
Epoch: 0126 train_loss= 0.26879 train_acc= 0.93111 val_loss= 0.28406 val_acc= 0.91730 time= 0.17416
Epoch: 0127 train_loss= 0.27382 train_acc= 0.92737 val_loss= 0.28041 val_acc= 0.92190 time= 0.17300
Epoch: 0128 train_loss= 0.25987 train_acc= 0.93196 val_loss= 0.27631 val_acc= 0.92496 time= 0.17618
Epoch: 0129 train_loss= 0.26495 train_acc= 0.93230 val_loss= 0.27278 val_acc= 0.92343 time= 0.19200
Epoch: 0130 train_loss= 0.24103 train_acc= 0.93519 val_loss= 0.26940 val_acc= 0.92496 time= 0.17301
Epoch: 0131 train_loss= 0.25195 train_acc= 0.93400 val_loss= 0.26638 val_acc= 0.92802 time= 0.17604
Epoch: 0132 train_loss= 0.26576 train_acc= 0.93230 val_loss= 0.26366 val_acc= 0.92956 time= 0.19502
Epoch: 0133 train_loss= 0.24465 train_acc= 0.93859 val_loss= 0.26195 val_acc= 0.92802 time= 0.17714
Epoch: 0134 train_loss= 0.23605 train_acc= 0.94387 val_loss= 0.26077 val_acc= 0.92802 time= 0.19495
Epoch: 0135 train_loss= 0.23891 train_acc= 0.94047 val_loss= 0.26019 val_acc= 0.92649 time= 0.17370
Epoch: 0136 train_loss= 0.25543 train_acc= 0.93349 val_loss= 0.26101 val_acc= 0.92649 time= 0.17299
Epoch: 0137 train_loss= 0.23499 train_acc= 0.94574 val_loss= 0.26204 val_acc= 0.92343 time= 0.19361
Epoch: 0138 train_loss= 0.24056 train_acc= 0.93689 val_loss= 0.26296 val_acc= 0.92496 time= 0.17156
Epoch: 0139 train_loss= 0.24539 train_acc= 0.93706 val_loss= 0.26463 val_acc= 0.92190 time= 0.19710
Early stopping...
Optimization Finished!
Test set results: cost= 0.31131 accuracy= 0.92368 time= 0.08109
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     1.0000    0.3333    0.5000         6
           2     0.0000    0.0000    0.0000         1
           3     0.7553    0.9467    0.8402        75
           4     1.0000    1.0000    1.0000         9
           5     0.7714    0.9310    0.8438        87
           6     0.9200    0.9200    0.9200        25
           7     0.6471    0.8462    0.7333        13
           8     1.0000    0.6364    0.7778        11
           9     0.0000    0.0000    0.0000         9
          10     0.9524    0.5556    0.7018        36
          11     1.0000    0.9167    0.9565        12
          12     0.7843    0.9917    0.8759       121
          13     1.0000    0.6842    0.8125        19
          14     0.7576    0.8929    0.8197        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.8889    0.8000    0.8421        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.2222    0.3636         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.4643    0.7647    0.5778        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.4167    0.5882        12
          28     1.0000    0.7273    0.8421        11
          29     0.9658    0.9727    0.9692       696
          30     0.9565    1.0000    0.9778        22
          31     0.0000    0.0000    0.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.9028    0.8025    0.8497        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9926    0.9849      1083
          40     0.5000    0.4000    0.4444         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.6154    0.6667    0.6400        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9237      2568
   macro avg     0.6233    0.5294    0.5468      2568
weighted avg     0.9178    0.9237    0.9140      2568

Macro average Test Precision, Recall and F1-Score...
(0.6232827546009214, 0.52935848053352, 0.5468149875469012, None)
Micro average Test Precision, Recall and F1-Score...
(0.9236760124610592, 0.9236760124610592, 0.9236760124610592, None)
embeddings:
8892 6532 2568
[[ 0.39701417 -0.1054573  -0.119229   ... -0.00774607 -0.14897208
   0.74140865]
 [-0.11553798 -0.05405371 -0.07613628 ...  0.0725957   0.15098794
   0.57583433]
 [ 0.36348125 -0.01214639 -0.09269169 ... -0.06615397 -0.00385353
   0.6175246 ]
 ...
 [-0.03284376 -0.00555527 -0.05225585 ...  0.02975282  0.12516801
   0.24744956]
 [ 0.26402253  0.02254258 -0.05488624 ...  0.02706527  0.02084965
   0.18120791]
 [ 0.32309014  0.22051193 -0.08012585 ...  0.26779652  0.2664069
   0.15954362]]

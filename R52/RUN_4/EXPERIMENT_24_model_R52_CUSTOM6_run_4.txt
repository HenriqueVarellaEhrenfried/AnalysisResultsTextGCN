(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95119 train_acc= 0.00850 val_loss= 3.89042 val_acc= 0.64625 time= 0.44865
Epoch: 0002 train_loss= 3.89153 train_acc= 0.63667 val_loss= 3.77916 val_acc= 0.64625 time= 0.19346
Epoch: 0003 train_loss= 3.78007 train_acc= 0.63599 val_loss= 3.60993 val_acc= 0.62940 time= 0.16900
Epoch: 0004 train_loss= 3.61377 train_acc= 0.62102 val_loss= 3.38357 val_acc= 0.59571 time= 0.16775
Epoch: 0005 train_loss= 3.39092 train_acc= 0.58973 val_loss= 3.11339 val_acc= 0.55590 time= 0.19100
Epoch: 0006 train_loss= 3.12182 train_acc= 0.55520 val_loss= 2.82802 val_acc= 0.52527 time= 0.16808
Epoch: 0007 train_loss= 2.83138 train_acc= 0.51556 val_loss= 2.56458 val_acc= 0.49923 time= 0.18503
Epoch: 0008 train_loss= 2.57022 train_acc= 0.48903 val_loss= 2.36328 val_acc= 0.48698 time= 0.16697
Epoch: 0009 train_loss= 2.36632 train_acc= 0.46385 val_loss= 2.24345 val_acc= 0.47626 time= 0.17100
Epoch: 0010 train_loss= 2.24482 train_acc= 0.44991 val_loss= 2.18432 val_acc= 0.46708 time= 0.17400
Epoch: 0011 train_loss= 2.18808 train_acc= 0.43936 val_loss= 2.14650 val_acc= 0.46554 time= 0.19303
Epoch: 0012 train_loss= 2.15481 train_acc= 0.43783 val_loss= 2.10015 val_acc= 0.46554 time= 0.16700
Epoch: 0013 train_loss= 2.11292 train_acc= 0.43613 val_loss= 2.03383 val_acc= 0.46554 time= 0.16701
Epoch: 0014 train_loss= 2.05210 train_acc= 0.43851 val_loss= 1.95024 val_acc= 0.48239 time= 0.16805
Epoch: 0015 train_loss= 1.97103 train_acc= 0.45246 val_loss= 1.86030 val_acc= 0.50842 time= 0.16899
Epoch: 0016 train_loss= 1.88259 train_acc= 0.49430 val_loss= 1.77732 val_acc= 0.55896 time= 0.19200
Epoch: 0017 train_loss= 1.80303 train_acc= 0.56166 val_loss= 1.70919 val_acc= 0.62481 time= 0.17152
Epoch: 0018 train_loss= 1.73571 train_acc= 0.61337 val_loss= 1.65396 val_acc= 0.65237 time= 0.17100
Epoch: 0019 train_loss= 1.68120 train_acc= 0.63786 val_loss= 1.60419 val_acc= 0.67075 time= 0.18900
Epoch: 0020 train_loss= 1.63167 train_acc= 0.64467 val_loss= 1.55414 val_acc= 0.67381 time= 0.16701
Epoch: 0021 train_loss= 1.58070 train_acc= 0.64722 val_loss= 1.50300 val_acc= 0.67228 time= 0.16799
Epoch: 0022 train_loss= 1.53293 train_acc= 0.65232 val_loss= 1.45253 val_acc= 0.67688 time= 0.19200
Epoch: 0023 train_loss= 1.48175 train_acc= 0.65113 val_loss= 1.40494 val_acc= 0.67534 time= 0.16700
Epoch: 0024 train_loss= 1.43553 train_acc= 0.65334 val_loss= 1.36150 val_acc= 0.67994 time= 0.18500
Epoch: 0025 train_loss= 1.39512 train_acc= 0.65589 val_loss= 1.32234 val_acc= 0.68606 time= 0.17000
Epoch: 0026 train_loss= 1.35424 train_acc= 0.66287 val_loss= 1.28689 val_acc= 0.69678 time= 0.17100
Epoch: 0027 train_loss= 1.31700 train_acc= 0.67205 val_loss= 1.25427 val_acc= 0.70597 time= 0.17200
Epoch: 0028 train_loss= 1.28312 train_acc= 0.68311 val_loss= 1.22368 val_acc= 0.70750 time= 0.18900
Epoch: 0029 train_loss= 1.25319 train_acc= 0.69570 val_loss= 1.19436 val_acc= 0.71669 time= 0.16604
Epoch: 0030 train_loss= 1.22096 train_acc= 0.71288 val_loss= 1.16573 val_acc= 0.72435 time= 0.17204
Epoch: 0031 train_loss= 1.19129 train_acc= 0.72631 val_loss= 1.13750 val_acc= 0.73507 time= 0.16596
Epoch: 0032 train_loss= 1.15994 train_acc= 0.74162 val_loss= 1.10952 val_acc= 0.73813 time= 0.16708
Epoch: 0033 train_loss= 1.13095 train_acc= 0.74979 val_loss= 1.08194 val_acc= 0.75038 time= 0.17610
Epoch: 0034 train_loss= 1.10131 train_acc= 0.75693 val_loss= 1.05502 val_acc= 0.75957 time= 0.19600
Epoch: 0035 train_loss= 1.07235 train_acc= 0.76510 val_loss= 1.02898 val_acc= 0.76570 time= 0.16900
Epoch: 0036 train_loss= 1.04059 train_acc= 0.77241 val_loss= 1.00389 val_acc= 0.77335 time= 0.18203
Epoch: 0037 train_loss= 1.01591 train_acc= 0.78126 val_loss= 0.97964 val_acc= 0.78101 time= 0.16800
Epoch: 0038 train_loss= 0.99153 train_acc= 0.78721 val_loss= 0.95597 val_acc= 0.79173 time= 0.17000
Epoch: 0039 train_loss= 0.96629 train_acc= 0.79299 val_loss= 0.93268 val_acc= 0.79632 time= 0.19200
Epoch: 0040 train_loss= 0.93988 train_acc= 0.79980 val_loss= 0.90955 val_acc= 0.80245 time= 0.16697
Epoch: 0041 train_loss= 0.91888 train_acc= 0.80541 val_loss= 0.88648 val_acc= 0.80245 time= 0.17365
Epoch: 0042 train_loss= 0.89349 train_acc= 0.81289 val_loss= 0.86351 val_acc= 0.81317 time= 0.17100
Epoch: 0043 train_loss= 0.86977 train_acc= 0.81970 val_loss= 0.84069 val_acc= 0.82083 time= 0.16803
Epoch: 0044 train_loss= 0.84372 train_acc= 0.82565 val_loss= 0.81800 val_acc= 0.81930 time= 0.16700
Epoch: 0045 train_loss= 0.81903 train_acc= 0.83024 val_loss= 0.79555 val_acc= 0.82695 time= 0.19100
Epoch: 0046 train_loss= 0.79461 train_acc= 0.83569 val_loss= 0.77351 val_acc= 0.83308 time= 0.17199
Epoch: 0047 train_loss= 0.77608 train_acc= 0.84198 val_loss= 0.75191 val_acc= 0.83767 time= 0.18598
Epoch: 0048 train_loss= 0.75126 train_acc= 0.84657 val_loss= 0.73072 val_acc= 0.84380 time= 0.16900
Epoch: 0049 train_loss= 0.72573 train_acc= 0.85031 val_loss= 0.71001 val_acc= 0.84992 time= 0.17207
Epoch: 0050 train_loss= 0.70471 train_acc= 0.85372 val_loss= 0.68984 val_acc= 0.84992 time= 0.19100
Epoch: 0051 train_loss= 0.68106 train_acc= 0.85916 val_loss= 0.67014 val_acc= 0.85758 time= 0.16900
Epoch: 0052 train_loss= 0.66163 train_acc= 0.86460 val_loss= 0.65093 val_acc= 0.86064 time= 0.16903
Epoch: 0053 train_loss= 0.63827 train_acc= 0.86886 val_loss= 0.63221 val_acc= 0.86677 time= 0.16800
Epoch: 0054 train_loss= 0.62048 train_acc= 0.87073 val_loss= 0.61394 val_acc= 0.86983 time= 0.16700
Epoch: 0055 train_loss= 0.60144 train_acc= 0.87481 val_loss= 0.59622 val_acc= 0.87289 time= 0.16797
Epoch: 0056 train_loss= 0.57951 train_acc= 0.87634 val_loss= 0.57896 val_acc= 0.87443 time= 0.16900
Epoch: 0057 train_loss= 0.55952 train_acc= 0.88059 val_loss= 0.56230 val_acc= 0.87443 time= 0.17136
Epoch: 0058 train_loss= 0.54037 train_acc= 0.88501 val_loss= 0.54623 val_acc= 0.88055 time= 0.17000
Epoch: 0059 train_loss= 0.52303 train_acc= 0.88757 val_loss= 0.53068 val_acc= 0.88208 time= 0.19500
Epoch: 0060 train_loss= 0.50672 train_acc= 0.89080 val_loss= 0.51556 val_acc= 0.88208 time= 0.16873
Epoch: 0061 train_loss= 0.48851 train_acc= 0.89607 val_loss= 0.50085 val_acc= 0.88208 time= 0.16700
Epoch: 0062 train_loss= 0.47250 train_acc= 0.89726 val_loss= 0.48664 val_acc= 0.88361 time= 0.19205
Epoch: 0063 train_loss= 0.45599 train_acc= 0.90253 val_loss= 0.47304 val_acc= 0.88668 time= 0.16596
Epoch: 0064 train_loss= 0.44046 train_acc= 0.90696 val_loss= 0.46002 val_acc= 0.89127 time= 0.16966
Epoch: 0065 train_loss= 0.42355 train_acc= 0.91274 val_loss= 0.44763 val_acc= 0.89127 time= 0.17200
Epoch: 0066 train_loss= 0.40876 train_acc= 0.91546 val_loss= 0.43603 val_acc= 0.89587 time= 0.17100
Epoch: 0067 train_loss= 0.39779 train_acc= 0.91835 val_loss= 0.42510 val_acc= 0.89587 time= 0.16897
Epoch: 0068 train_loss= 0.38557 train_acc= 0.92278 val_loss= 0.41474 val_acc= 0.89740 time= 0.19103
Epoch: 0069 train_loss= 0.37026 train_acc= 0.92635 val_loss= 0.40492 val_acc= 0.89893 time= 0.16800
Epoch: 0070 train_loss= 0.35977 train_acc= 0.92873 val_loss= 0.39535 val_acc= 0.90046 time= 0.18300
Epoch: 0071 train_loss= 0.34382 train_acc= 0.93145 val_loss= 0.38583 val_acc= 0.90505 time= 0.16800
Epoch: 0072 train_loss= 0.33358 train_acc= 0.93366 val_loss= 0.37684 val_acc= 0.90658 time= 0.16800
Epoch: 0073 train_loss= 0.32155 train_acc= 0.93604 val_loss= 0.36824 val_acc= 0.90505 time= 0.17270
Epoch: 0074 train_loss= 0.30858 train_acc= 0.93962 val_loss= 0.36004 val_acc= 0.90505 time= 0.19300
Epoch: 0075 train_loss= 0.30049 train_acc= 0.94149 val_loss= 0.35208 val_acc= 0.90965 time= 0.17000
Epoch: 0076 train_loss= 0.28981 train_acc= 0.94336 val_loss= 0.34460 val_acc= 0.90965 time= 0.16600
Epoch: 0077 train_loss= 0.27887 train_acc= 0.94540 val_loss= 0.33759 val_acc= 0.91271 time= 0.16800
Epoch: 0078 train_loss= 0.26821 train_acc= 0.94693 val_loss= 0.33128 val_acc= 0.91424 time= 0.16800
Epoch: 0079 train_loss= 0.25995 train_acc= 0.94863 val_loss= 0.32556 val_acc= 0.91884 time= 0.19147
Epoch: 0080 train_loss= 0.25149 train_acc= 0.95135 val_loss= 0.32022 val_acc= 0.91884 time= 0.16800
Epoch: 0081 train_loss= 0.24234 train_acc= 0.95203 val_loss= 0.31525 val_acc= 0.91884 time= 0.17100
Epoch: 0082 train_loss= 0.23329 train_acc= 0.95424 val_loss= 0.31055 val_acc= 0.91884 time= 0.19400
Epoch: 0083 train_loss= 0.22588 train_acc= 0.95731 val_loss= 0.30614 val_acc= 0.92190 time= 0.17201
Epoch: 0084 train_loss= 0.21591 train_acc= 0.95816 val_loss= 0.30207 val_acc= 0.92037 time= 0.16700
Epoch: 0085 train_loss= 0.20993 train_acc= 0.96037 val_loss= 0.29786 val_acc= 0.92037 time= 0.17300
Epoch: 0086 train_loss= 0.20034 train_acc= 0.96173 val_loss= 0.29355 val_acc= 0.92190 time= 0.16700
Epoch: 0087 train_loss= 0.19454 train_acc= 0.96445 val_loss= 0.28934 val_acc= 0.92343 time= 0.16800
Epoch: 0088 train_loss= 0.18676 train_acc= 0.96479 val_loss= 0.28537 val_acc= 0.92496 time= 0.16716
Epoch: 0089 train_loss= 0.18202 train_acc= 0.96632 val_loss= 0.28191 val_acc= 0.92496 time= 0.17000
Epoch: 0090 train_loss= 0.17438 train_acc= 0.96598 val_loss= 0.27859 val_acc= 0.92802 time= 0.19700
Epoch: 0091 train_loss= 0.16811 train_acc= 0.96921 val_loss= 0.27588 val_acc= 0.92802 time= 0.16985
Epoch: 0092 train_loss= 0.16412 train_acc= 0.96955 val_loss= 0.27330 val_acc= 0.92956 time= 0.16700
Epoch: 0093 train_loss= 0.15686 train_acc= 0.97006 val_loss= 0.27088 val_acc= 0.92956 time= 0.19103
Epoch: 0094 train_loss= 0.15064 train_acc= 0.97227 val_loss= 0.26874 val_acc= 0.92802 time= 0.16808
Epoch: 0095 train_loss= 0.14477 train_acc= 0.97278 val_loss= 0.26654 val_acc= 0.92956 time= 0.16800
Epoch: 0096 train_loss= 0.14071 train_acc= 0.97415 val_loss= 0.26442 val_acc= 0.92649 time= 0.16900
Epoch: 0097 train_loss= 0.13509 train_acc= 0.97619 val_loss= 0.26244 val_acc= 0.92956 time= 0.18940
Epoch: 0098 train_loss= 0.13168 train_acc= 0.97585 val_loss= 0.26031 val_acc= 0.92956 time= 0.17100
Epoch: 0099 train_loss= 0.12600 train_acc= 0.97891 val_loss= 0.25848 val_acc= 0.92956 time= 0.17400
Epoch: 0100 train_loss= 0.12129 train_acc= 0.97942 val_loss= 0.25656 val_acc= 0.93109 time= 0.16760
Epoch: 0101 train_loss= 0.11642 train_acc= 0.98129 val_loss= 0.25509 val_acc= 0.93109 time= 0.16801
Epoch: 0102 train_loss= 0.11344 train_acc= 0.98214 val_loss= 0.25411 val_acc= 0.93568 time= 0.19007
Epoch: 0103 train_loss= 0.11044 train_acc= 0.98129 val_loss= 0.25290 val_acc= 0.93415 time= 0.16700
Epoch: 0104 train_loss= 0.10626 train_acc= 0.98231 val_loss= 0.25166 val_acc= 0.93262 time= 0.16697
Epoch: 0105 train_loss= 0.10173 train_acc= 0.98316 val_loss= 0.24953 val_acc= 0.93568 time= 0.19350
Epoch: 0106 train_loss= 0.09700 train_acc= 0.98435 val_loss= 0.24749 val_acc= 0.93568 time= 0.17017
Epoch: 0107 train_loss= 0.09431 train_acc= 0.98537 val_loss= 0.24624 val_acc= 0.93568 time= 0.16803
Epoch: 0108 train_loss= 0.09227 train_acc= 0.98486 val_loss= 0.24597 val_acc= 0.93568 time= 0.19300
Epoch: 0109 train_loss= 0.08842 train_acc= 0.98639 val_loss= 0.24579 val_acc= 0.93568 time= 0.16900
Epoch: 0110 train_loss= 0.08463 train_acc= 0.98639 val_loss= 0.24624 val_acc= 0.93568 time= 0.18200
Epoch: 0111 train_loss= 0.08238 train_acc= 0.98741 val_loss= 0.24673 val_acc= 0.93568 time= 0.16700
Epoch: 0112 train_loss= 0.08023 train_acc= 0.98792 val_loss= 0.24656 val_acc= 0.93568 time= 0.16700
Epoch: 0113 train_loss= 0.07777 train_acc= 0.98877 val_loss= 0.24567 val_acc= 0.93721 time= 0.17397
Epoch: 0114 train_loss= 0.07420 train_acc= 0.98928 val_loss= 0.24400 val_acc= 0.93415 time= 0.18900
Epoch: 0115 train_loss= 0.07231 train_acc= 0.98928 val_loss= 0.24252 val_acc= 0.93568 time= 0.16943
Epoch: 0116 train_loss= 0.07010 train_acc= 0.98945 val_loss= 0.24127 val_acc= 0.93568 time= 0.16697
Epoch: 0117 train_loss= 0.06816 train_acc= 0.99030 val_loss= 0.24167 val_acc= 0.93568 time= 0.16804
Epoch: 0118 train_loss= 0.06498 train_acc= 0.99013 val_loss= 0.24259 val_acc= 0.93262 time= 0.16799
Epoch: 0119 train_loss= 0.06270 train_acc= 0.99081 val_loss= 0.24346 val_acc= 0.93415 time= 0.19200
Epoch: 0120 train_loss= 0.06082 train_acc= 0.99081 val_loss= 0.24357 val_acc= 0.93262 time= 0.16700
Epoch: 0121 train_loss= 0.05908 train_acc= 0.99201 val_loss= 0.24293 val_acc= 0.93415 time= 0.17029
Epoch: 0122 train_loss= 0.05797 train_acc= 0.99218 val_loss= 0.24169 val_acc= 0.94028 time= 0.19200
Epoch: 0123 train_loss= 0.05521 train_acc= 0.99252 val_loss= 0.24026 val_acc= 0.94028 time= 0.16803
Epoch: 0124 train_loss= 0.05359 train_acc= 0.99354 val_loss= 0.23925 val_acc= 0.94028 time= 0.16597
Epoch: 0125 train_loss= 0.05253 train_acc= 0.99303 val_loss= 0.23902 val_acc= 0.93874 time= 0.19200
Epoch: 0126 train_loss= 0.05119 train_acc= 0.99252 val_loss= 0.24012 val_acc= 0.93721 time= 0.16817
Epoch: 0127 train_loss= 0.04891 train_acc= 0.99354 val_loss= 0.24132 val_acc= 0.93721 time= 0.17125
Epoch: 0128 train_loss= 0.04689 train_acc= 0.99337 val_loss= 0.24226 val_acc= 0.93721 time= 0.17300
Early stopping...
Optimization Finished!
Test set results: cost= 0.25971 accuracy= 0.93458 time= 0.07497
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8571    0.7500    0.8000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7660    0.9600    0.8521        75
           4     1.0000    1.0000    1.0000         9
           5     0.7900    0.9080    0.8449        87
           6     0.8846    0.9200    0.9020        25
           7     0.8462    0.8462    0.8462        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.6667    0.8000         9
          10     0.8571    0.6667    0.7500        36
          11     1.0000    0.9167    0.9565        12
          12     0.8571    0.9917    0.9195       121
          13     0.9375    0.7895    0.8571        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7778    0.8235    0.8000        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.7273    0.8421        11
          29     0.9641    0.9641    0.9641       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8267    0.7654    0.7949        81
          36     1.0000    0.3333    0.5000        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9889    0.9835      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9231    0.8000    0.8571        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9346      2568
   macro avg     0.7619    0.6839    0.6999      2568
weighted avg     0.9329    0.9346    0.9302      2568

Macro average Test Precision, Recall and F1-Score...
(0.7619458350754379, 0.6838621901138353, 0.6998926768223508, None)
Micro average Test Precision, Recall and F1-Score...
(0.9345794392523364, 0.9345794392523364, 0.9345794392523366, None)
embeddings:
8892 6532 2568
[[ 0.6513675  -0.04180466  1.9869397  ...  1.3697598   0.58002764
   0.054075  ]
 [ 0.76931083  0.12305836  0.3443057  ...  0.8328681  -0.00672626
   0.03005031]
 [ 0.63507557 -0.01267721  0.6413377  ...  0.34757978  0.26379442
   0.02201233]
 ...
 [ 0.3465803  -0.00548699  0.530935   ...  0.14079988  0.59587055
   0.02620631]
 [ 0.27839926  0.03751344  0.41984275 ...  0.21914342  0.32413557
   0.05665446]
 [ 0.20647615  0.23181795  0.35400215 ...  0.30045158  0.41255423
   0.2614649 ]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95119 train_acc= 0.00544 val_loss= 3.89642 val_acc= 0.66309 time= 0.46427
Epoch: 0002 train_loss= 3.89702 train_acc= 0.64909 val_loss= 3.79178 val_acc= 0.66309 time= 0.17300
Epoch: 0003 train_loss= 3.79344 train_acc= 0.65096 val_loss= 3.63109 val_acc= 0.65084 time= 0.17003
Epoch: 0004 train_loss= 3.63669 train_acc= 0.63667 val_loss= 3.41554 val_acc= 0.64012 time= 0.18497
Epoch: 0005 train_loss= 3.41770 train_acc= 0.61031 val_loss= 3.15694 val_acc= 0.61409 time= 0.16912
Epoch: 0006 train_loss= 3.16391 train_acc= 0.59755 val_loss= 2.88053 val_acc= 0.58346 time= 0.17500
Epoch: 0007 train_loss= 2.89338 train_acc= 0.57799 val_loss= 2.61985 val_acc= 0.56202 time= 0.20800
Epoch: 0008 train_loss= 2.60570 train_acc= 0.54856 val_loss= 2.41076 val_acc= 0.54977 time= 0.18100
Epoch: 0009 train_loss= 2.40468 train_acc= 0.54142 val_loss= 2.27883 val_acc= 0.52986 time= 0.20400
Epoch: 0010 train_loss= 2.26949 train_acc= 0.52781 val_loss= 2.21174 val_acc= 0.50842 time= 0.18300
Epoch: 0011 train_loss= 2.20984 train_acc= 0.48954 val_loss= 2.17126 val_acc= 0.47933 time= 0.19400
Epoch: 0012 train_loss= 2.17099 train_acc= 0.45977 val_loss= 2.12590 val_acc= 0.47014 time= 0.18500
Epoch: 0013 train_loss= 2.13614 train_acc= 0.44582 val_loss= 2.06214 val_acc= 0.47014 time= 0.18000
Epoch: 0014 train_loss= 2.07740 train_acc= 0.44038 val_loss= 1.97977 val_acc= 0.47779 time= 0.17805
Epoch: 0015 train_loss= 1.99608 train_acc= 0.44940 val_loss= 1.88745 val_acc= 0.50383 time= 0.19914
Epoch: 0016 train_loss= 1.91763 train_acc= 0.48291 val_loss= 1.79853 val_acc= 0.56202 time= 0.17000
Epoch: 0017 train_loss= 1.84047 train_acc= 0.56761 val_loss= 1.72229 val_acc= 0.62634 time= 0.17366
Epoch: 0018 train_loss= 1.75076 train_acc= 0.61082 val_loss= 1.66009 val_acc= 0.66003 time= 0.18700
Epoch: 0019 train_loss= 1.68490 train_acc= 0.64297 val_loss= 1.60658 val_acc= 0.67381 time= 0.17204
Epoch: 0020 train_loss= 1.63194 train_acc= 0.65147 val_loss= 1.55469 val_acc= 0.68147 time= 0.19097
Epoch: 0021 train_loss= 1.58929 train_acc= 0.65385 val_loss= 1.50133 val_acc= 0.68453 time= 0.17200
Epoch: 0022 train_loss= 1.53009 train_acc= 0.66151 val_loss= 1.44810 val_acc= 0.67688 time= 0.17503
Epoch: 0023 train_loss= 1.48405 train_acc= 0.66304 val_loss= 1.39727 val_acc= 0.68300 time= 0.17553
Epoch: 0024 train_loss= 1.42838 train_acc= 0.66559 val_loss= 1.35101 val_acc= 0.69066 time= 0.17100
Epoch: 0025 train_loss= 1.37617 train_acc= 0.67460 val_loss= 1.30980 val_acc= 0.70291 time= 0.17052
Epoch: 0026 train_loss= 1.33309 train_acc= 0.68566 val_loss= 1.27297 val_acc= 0.71516 time= 0.19903
Epoch: 0027 train_loss= 1.30071 train_acc= 0.69144 val_loss= 1.23947 val_acc= 0.71822 time= 0.17300
Epoch: 0028 train_loss= 1.26274 train_acc= 0.70709 val_loss= 1.20841 val_acc= 0.72282 time= 0.18000
Epoch: 0029 train_loss= 1.23199 train_acc= 0.71594 val_loss= 1.17905 val_acc= 0.73201 time= 0.17598
Epoch: 0030 train_loss= 1.20065 train_acc= 0.72870 val_loss= 1.15071 val_acc= 0.73047 time= 0.17303
Epoch: 0031 train_loss= 1.16910 train_acc= 0.73567 val_loss= 1.12301 val_acc= 0.73660 time= 0.17367
Epoch: 0032 train_loss= 1.14024 train_acc= 0.74298 val_loss= 1.09566 val_acc= 0.73966 time= 0.17186
Epoch: 0033 train_loss= 1.11658 train_acc= 0.74826 val_loss= 1.06868 val_acc= 0.74732 time= 0.17271
Epoch: 0034 train_loss= 1.08726 train_acc= 0.75166 val_loss= 1.04225 val_acc= 0.75191 time= 0.17122
Epoch: 0035 train_loss= 1.05572 train_acc= 0.75591 val_loss= 1.01637 val_acc= 0.76263 time= 0.18374
Epoch: 0036 train_loss= 1.02520 train_acc= 0.76152 val_loss= 0.99115 val_acc= 0.77182 time= 0.16897
Epoch: 0037 train_loss= 0.99816 train_acc= 0.76782 val_loss= 0.96672 val_acc= 0.78254 time= 0.18678
Epoch: 0038 train_loss= 0.97791 train_acc= 0.77887 val_loss= 0.94304 val_acc= 0.79326 time= 0.17157
Epoch: 0039 train_loss= 0.94763 train_acc= 0.78670 val_loss= 0.91983 val_acc= 0.80245 time= 0.17058
Epoch: 0040 train_loss= 0.93482 train_acc= 0.79775 val_loss= 0.89700 val_acc= 0.81011 time= 0.17904
Epoch: 0041 train_loss= 0.90433 train_acc= 0.81272 val_loss= 0.87441 val_acc= 0.81930 time= 0.17167
Epoch: 0042 train_loss= 0.88440 train_acc= 0.81970 val_loss= 0.85211 val_acc= 0.82236 time= 0.17304
Epoch: 0043 train_loss= 0.85872 train_acc= 0.82497 val_loss= 0.83026 val_acc= 0.82695 time= 0.17101
Epoch: 0044 train_loss= 0.83936 train_acc= 0.83211 val_loss= 0.80899 val_acc= 0.83155 time= 0.16905
Epoch: 0045 train_loss= 0.81216 train_acc= 0.83671 val_loss= 0.78838 val_acc= 0.83461 time= 0.18792
Epoch: 0046 train_loss= 0.79802 train_acc= 0.83654 val_loss= 0.76807 val_acc= 0.83767 time= 0.17103
Epoch: 0047 train_loss= 0.76880 train_acc= 0.84215 val_loss= 0.74793 val_acc= 0.84227 time= 0.17001
Epoch: 0048 train_loss= 0.74997 train_acc= 0.84521 val_loss= 0.72802 val_acc= 0.84992 time= 0.17307
Epoch: 0049 train_loss= 0.72607 train_acc= 0.84912 val_loss= 0.70864 val_acc= 0.85145 time= 0.18704
Epoch: 0050 train_loss= 0.70977 train_acc= 0.85031 val_loss= 0.68973 val_acc= 0.84992 time= 0.17000
Epoch: 0051 train_loss= 0.68661 train_acc= 0.85474 val_loss= 0.67123 val_acc= 0.85299 time= 0.19190
Epoch: 0052 train_loss= 0.66898 train_acc= 0.85491 val_loss= 0.65316 val_acc= 0.86064 time= 0.17800
Epoch: 0053 train_loss= 0.64888 train_acc= 0.86477 val_loss= 0.63555 val_acc= 0.86371 time= 0.17711
Epoch: 0054 train_loss= 0.62322 train_acc= 0.86937 val_loss= 0.61847 val_acc= 0.86830 time= 0.18005
Epoch: 0055 train_loss= 0.61145 train_acc= 0.86817 val_loss= 0.60209 val_acc= 0.87136 time= 0.17004
Epoch: 0056 train_loss= 0.59157 train_acc= 0.87379 val_loss= 0.58622 val_acc= 0.87443 time= 0.17503
Epoch: 0057 train_loss= 0.56989 train_acc= 0.87991 val_loss= 0.57089 val_acc= 0.87443 time= 0.18511
Epoch: 0058 train_loss= 0.55601 train_acc= 0.87770 val_loss= 0.55611 val_acc= 0.87443 time= 0.16623
Epoch: 0059 train_loss= 0.53789 train_acc= 0.88076 val_loss= 0.54197 val_acc= 0.87443 time= 0.17432
Epoch: 0060 train_loss= 0.52060 train_acc= 0.88195 val_loss= 0.52841 val_acc= 0.87749 time= 0.18265
Epoch: 0061 train_loss= 0.50921 train_acc= 0.88893 val_loss= 0.51524 val_acc= 0.87749 time= 0.17300
Epoch: 0062 train_loss= 0.49491 train_acc= 0.89216 val_loss= 0.50244 val_acc= 0.87749 time= 0.17308
Epoch: 0063 train_loss= 0.47462 train_acc= 0.89760 val_loss= 0.48907 val_acc= 0.88055 time= 0.17595
Epoch: 0064 train_loss= 0.46135 train_acc= 0.89624 val_loss= 0.47633 val_acc= 0.88055 time= 0.16707
Epoch: 0065 train_loss= 0.45327 train_acc= 0.90543 val_loss= 0.46396 val_acc= 0.89127 time= 0.17003
Epoch: 0066 train_loss= 0.43744 train_acc= 0.90900 val_loss= 0.45229 val_acc= 0.89280 time= 0.16901
Epoch: 0067 train_loss= 0.42944 train_acc= 0.90662 val_loss= 0.44123 val_acc= 0.89740 time= 0.17205
Epoch: 0068 train_loss= 0.41472 train_acc= 0.91138 val_loss= 0.43088 val_acc= 0.89893 time= 0.19095
Epoch: 0069 train_loss= 0.40190 train_acc= 0.91359 val_loss= 0.42076 val_acc= 0.89893 time= 0.17901
Epoch: 0070 train_loss= 0.39081 train_acc= 0.91359 val_loss= 0.41126 val_acc= 0.90046 time= 0.17154
Epoch: 0071 train_loss= 0.38334 train_acc= 0.91597 val_loss= 0.40205 val_acc= 0.90352 time= 0.17119
Epoch: 0072 train_loss= 0.36115 train_acc= 0.92073 val_loss= 0.39311 val_acc= 0.90352 time= 0.19000
Epoch: 0073 train_loss= 0.35261 train_acc= 0.92142 val_loss= 0.38447 val_acc= 0.90658 time= 0.17204
Epoch: 0074 train_loss= 0.34629 train_acc= 0.92635 val_loss= 0.37609 val_acc= 0.90658 time= 0.19251
Epoch: 0075 train_loss= 0.33733 train_acc= 0.93060 val_loss= 0.36872 val_acc= 0.90965 time= 0.16951
Epoch: 0076 train_loss= 0.32095 train_acc= 0.93162 val_loss= 0.36215 val_acc= 0.90965 time= 0.17100
Epoch: 0077 train_loss= 0.32013 train_acc= 0.93077 val_loss= 0.35614 val_acc= 0.91118 time= 0.19633
Epoch: 0078 train_loss= 0.30638 train_acc= 0.93672 val_loss= 0.34983 val_acc= 0.91424 time= 0.17053
Epoch: 0079 train_loss= 0.29710 train_acc= 0.93689 val_loss= 0.34329 val_acc= 0.91424 time= 0.16763
Epoch: 0080 train_loss= 0.28934 train_acc= 0.94030 val_loss= 0.33603 val_acc= 0.91577 time= 0.18804
Epoch: 0081 train_loss= 0.27777 train_acc= 0.94353 val_loss= 0.32924 val_acc= 0.91577 time= 0.16706
Epoch: 0082 train_loss= 0.27582 train_acc= 0.94319 val_loss= 0.32339 val_acc= 0.91884 time= 0.17165
Epoch: 0083 train_loss= 0.26719 train_acc= 0.94319 val_loss= 0.31796 val_acc= 0.92190 time= 0.16801
Epoch: 0084 train_loss= 0.26592 train_acc= 0.94421 val_loss= 0.31340 val_acc= 0.92343 time= 0.17104
Epoch: 0085 train_loss= 0.25229 train_acc= 0.94506 val_loss= 0.31006 val_acc= 0.92037 time= 0.17101
Epoch: 0086 train_loss= 0.24546 train_acc= 0.94744 val_loss= 0.30561 val_acc= 0.91884 time= 0.17102
Epoch: 0087 train_loss= 0.23666 train_acc= 0.94999 val_loss= 0.30100 val_acc= 0.92037 time= 0.17003
Epoch: 0088 train_loss= 0.23403 train_acc= 0.95169 val_loss= 0.29626 val_acc= 0.92190 time= 0.17511
Epoch: 0089 train_loss= 0.22687 train_acc= 0.95237 val_loss= 0.29188 val_acc= 0.92802 time= 0.17600
Epoch: 0090 train_loss= 0.21968 train_acc= 0.95458 val_loss= 0.28801 val_acc= 0.92956 time= 0.16810
Epoch: 0091 train_loss= 0.20868 train_acc= 0.95782 val_loss= 0.28411 val_acc= 0.92649 time= 0.17301
Epoch: 0092 train_loss= 0.20863 train_acc= 0.95680 val_loss= 0.28119 val_acc= 0.92649 time= 0.18800
Epoch: 0093 train_loss= 0.20789 train_acc= 0.96003 val_loss= 0.27955 val_acc= 0.92649 time= 0.17200
Epoch: 0094 train_loss= 0.19549 train_acc= 0.96037 val_loss= 0.27868 val_acc= 0.92649 time= 0.18500
Epoch: 0095 train_loss= 0.19209 train_acc= 0.96139 val_loss= 0.27805 val_acc= 0.92649 time= 0.16700
Epoch: 0096 train_loss= 0.18700 train_acc= 0.96309 val_loss= 0.27520 val_acc= 0.92649 time= 0.16800
Epoch: 0097 train_loss= 0.18773 train_acc= 0.96054 val_loss= 0.27105 val_acc= 0.92343 time= 0.18700
Epoch: 0098 train_loss= 0.18501 train_acc= 0.96088 val_loss= 0.26639 val_acc= 0.92496 time= 0.16803
Epoch: 0099 train_loss= 0.17484 train_acc= 0.96615 val_loss= 0.26225 val_acc= 0.92956 time= 0.17100
Epoch: 0100 train_loss= 0.16961 train_acc= 0.96581 val_loss= 0.25916 val_acc= 0.93109 time= 0.19597
Epoch: 0101 train_loss= 0.16416 train_acc= 0.96819 val_loss= 0.25653 val_acc= 0.93415 time= 0.17182
Epoch: 0102 train_loss= 0.15708 train_acc= 0.96904 val_loss= 0.25492 val_acc= 0.93415 time= 0.17400
Epoch: 0103 train_loss= 0.15925 train_acc= 0.96921 val_loss= 0.25394 val_acc= 0.93262 time= 0.18600
Epoch: 0104 train_loss= 0.15467 train_acc= 0.97057 val_loss= 0.25361 val_acc= 0.93568 time= 0.16800
Epoch: 0105 train_loss= 0.15287 train_acc= 0.97074 val_loss= 0.25268 val_acc= 0.93109 time= 0.18600
Epoch: 0106 train_loss= 0.14701 train_acc= 0.97363 val_loss= 0.25158 val_acc= 0.93262 time= 0.16600
Epoch: 0107 train_loss= 0.14290 train_acc= 0.97040 val_loss= 0.24942 val_acc= 0.93109 time= 0.16700
Epoch: 0108 train_loss= 0.14315 train_acc= 0.97176 val_loss= 0.24608 val_acc= 0.93262 time= 0.17009
Epoch: 0109 train_loss= 0.13411 train_acc= 0.97363 val_loss= 0.24370 val_acc= 0.93415 time= 0.17800
Epoch: 0110 train_loss= 0.13425 train_acc= 0.97823 val_loss= 0.24222 val_acc= 0.93721 time= 0.16900
Epoch: 0111 train_loss= 0.12747 train_acc= 0.97653 val_loss= 0.24075 val_acc= 0.93874 time= 0.17104
Epoch: 0112 train_loss= 0.12573 train_acc= 0.97687 val_loss= 0.24006 val_acc= 0.93874 time= 0.18704
Epoch: 0113 train_loss= 0.12472 train_acc= 0.97670 val_loss= 0.23938 val_acc= 0.93721 time= 0.16899
Epoch: 0114 train_loss= 0.12056 train_acc= 0.97789 val_loss= 0.23987 val_acc= 0.93721 time= 0.18796
Epoch: 0115 train_loss= 0.12077 train_acc= 0.97517 val_loss= 0.24014 val_acc= 0.93721 time= 0.16805
Epoch: 0116 train_loss= 0.11135 train_acc= 0.98061 val_loss= 0.23989 val_acc= 0.93568 time= 0.17095
Epoch: 0117 train_loss= 0.11245 train_acc= 0.97687 val_loss= 0.23881 val_acc= 0.93262 time= 0.19201
Epoch: 0118 train_loss= 0.11185 train_acc= 0.97925 val_loss= 0.23739 val_acc= 0.93262 time= 0.16900
Epoch: 0119 train_loss= 0.10901 train_acc= 0.97755 val_loss= 0.23465 val_acc= 0.93109 time= 0.17003
Epoch: 0120 train_loss= 0.10581 train_acc= 0.97942 val_loss= 0.23195 val_acc= 0.93568 time= 0.18601
Epoch: 0121 train_loss= 0.10389 train_acc= 0.98027 val_loss= 0.22917 val_acc= 0.93721 time= 0.16895
Epoch: 0122 train_loss= 0.10606 train_acc= 0.98078 val_loss= 0.22746 val_acc= 0.93721 time= 0.17003
Epoch: 0123 train_loss= 0.10063 train_acc= 0.98112 val_loss= 0.22627 val_acc= 0.93721 time= 0.18501
Epoch: 0124 train_loss= 0.09381 train_acc= 0.98486 val_loss= 0.22617 val_acc= 0.93721 time= 0.17047
Epoch: 0125 train_loss= 0.09653 train_acc= 0.98197 val_loss= 0.22752 val_acc= 0.94028 time= 0.17302
Epoch: 0126 train_loss= 0.09446 train_acc= 0.98180 val_loss= 0.22907 val_acc= 0.93874 time= 0.18700
Epoch: 0127 train_loss= 0.08971 train_acc= 0.98384 val_loss= 0.23059 val_acc= 0.93874 time= 0.16700
Epoch: 0128 train_loss= 0.09090 train_acc= 0.98197 val_loss= 0.23139 val_acc= 0.93874 time= 0.17304
Epoch: 0129 train_loss= 0.09010 train_acc= 0.98435 val_loss= 0.23196 val_acc= 0.93721 time= 0.18501
Epoch: 0130 train_loss= 0.08676 train_acc= 0.98095 val_loss= 0.23025 val_acc= 0.93721 time= 0.16953
Epoch: 0131 train_loss= 0.08277 train_acc= 0.98758 val_loss= 0.22764 val_acc= 0.93874 time= 0.16900
Epoch: 0132 train_loss= 0.08077 train_acc= 0.98605 val_loss= 0.22616 val_acc= 0.93874 time= 0.18500
Epoch: 0133 train_loss= 0.07450 train_acc= 0.99013 val_loss= 0.22558 val_acc= 0.93874 time= 0.17054
Epoch: 0134 train_loss= 0.08117 train_acc= 0.98588 val_loss= 0.22460 val_acc= 0.94028 time= 0.18603
Epoch: 0135 train_loss= 0.07759 train_acc= 0.98843 val_loss= 0.22317 val_acc= 0.94181 time= 0.16819
Epoch: 0136 train_loss= 0.08037 train_acc= 0.98588 val_loss= 0.22278 val_acc= 0.94028 time= 0.16795
Epoch: 0137 train_loss= 0.07444 train_acc= 0.98622 val_loss= 0.22347 val_acc= 0.94181 time= 0.18704
Epoch: 0138 train_loss= 0.07582 train_acc= 0.98826 val_loss= 0.22395 val_acc= 0.94028 time= 0.16700
Epoch: 0139 train_loss= 0.06926 train_acc= 0.98877 val_loss= 0.22441 val_acc= 0.93874 time= 0.16796
Epoch: 0140 train_loss= 0.07005 train_acc= 0.98775 val_loss= 0.22445 val_acc= 0.94181 time= 0.19501
Epoch: 0141 train_loss= 0.06813 train_acc= 0.98911 val_loss= 0.22410 val_acc= 0.94181 time= 0.17051
Epoch: 0142 train_loss= 0.06514 train_acc= 0.98877 val_loss= 0.22408 val_acc= 0.94181 time= 0.17103
Epoch: 0143 train_loss= 0.06740 train_acc= 0.98826 val_loss= 0.22415 val_acc= 0.93874 time= 0.18401
Epoch: 0144 train_loss= 0.06717 train_acc= 0.98622 val_loss= 0.22143 val_acc= 0.94181 time= 0.16700
Epoch: 0145 train_loss= 0.06366 train_acc= 0.98775 val_loss= 0.22069 val_acc= 0.94487 time= 0.18099
Epoch: 0146 train_loss= 0.06292 train_acc= 0.98945 val_loss= 0.22095 val_acc= 0.94487 time= 0.16592
Epoch: 0147 train_loss= 0.06262 train_acc= 0.99030 val_loss= 0.22168 val_acc= 0.94487 time= 0.16697
Epoch: 0148 train_loss= 0.05878 train_acc= 0.98962 val_loss= 0.22348 val_acc= 0.94487 time= 0.19548
Epoch: 0149 train_loss= 0.06011 train_acc= 0.99064 val_loss= 0.22575 val_acc= 0.93874 time= 0.17600
Epoch: 0150 train_loss= 0.05704 train_acc= 0.99218 val_loss= 0.22693 val_acc= 0.93874 time= 0.17104
Epoch: 0151 train_loss= 0.05988 train_acc= 0.99013 val_loss= 0.22743 val_acc= 0.93721 time= 0.17496
Epoch: 0152 train_loss= 0.05759 train_acc= 0.99064 val_loss= 0.22809 val_acc= 0.93721 time= 0.18831
Epoch: 0153 train_loss= 0.05368 train_acc= 0.99133 val_loss= 0.22786 val_acc= 0.93721 time= 0.17060
Epoch: 0154 train_loss= 0.05688 train_acc= 0.99098 val_loss= 0.22646 val_acc= 0.93874 time= 0.18600
Epoch: 0155 train_loss= 0.05472 train_acc= 0.99201 val_loss= 0.22347 val_acc= 0.94181 time= 0.16898
Epoch: 0156 train_loss= 0.05243 train_acc= 0.99116 val_loss= 0.22009 val_acc= 0.94334 time= 0.17200
Epoch: 0157 train_loss= 0.05123 train_acc= 0.99269 val_loss= 0.21891 val_acc= 0.94334 time= 0.19401
Epoch: 0158 train_loss= 0.05208 train_acc= 0.99184 val_loss= 0.21712 val_acc= 0.94487 time= 0.16899
Epoch: 0159 train_loss= 0.04941 train_acc= 0.98945 val_loss= 0.21616 val_acc= 0.94487 time= 0.16800
Epoch: 0160 train_loss= 0.04842 train_acc= 0.99252 val_loss= 0.21619 val_acc= 0.94334 time= 0.18700
Epoch: 0161 train_loss= 0.05042 train_acc= 0.99098 val_loss= 0.21629 val_acc= 0.94334 time= 0.17000
Epoch: 0162 train_loss= 0.04815 train_acc= 0.99167 val_loss= 0.21769 val_acc= 0.94334 time= 0.18700
Epoch: 0163 train_loss= 0.04660 train_acc= 0.99235 val_loss= 0.21905 val_acc= 0.94181 time= 0.16997
Epoch: 0164 train_loss= 0.04658 train_acc= 0.99286 val_loss= 0.22054 val_acc= 0.94028 time= 0.17104
Epoch: 0165 train_loss= 0.04734 train_acc= 0.99320 val_loss= 0.22153 val_acc= 0.94028 time= 0.18958
Epoch: 0166 train_loss= 0.04387 train_acc= 0.99337 val_loss= 0.22101 val_acc= 0.94028 time= 0.16599
Epoch: 0167 train_loss= 0.04571 train_acc= 0.99303 val_loss= 0.21966 val_acc= 0.94181 time= 0.16801
Epoch: 0168 train_loss= 0.04428 train_acc= 0.99252 val_loss= 0.21903 val_acc= 0.94334 time= 0.18900
Epoch: 0169 train_loss= 0.04188 train_acc= 0.99456 val_loss= 0.21980 val_acc= 0.94334 time= 0.16799
Epoch: 0170 train_loss= 0.04566 train_acc= 0.99269 val_loss= 0.22097 val_acc= 0.94487 time= 0.17000
Epoch: 0171 train_loss= 0.04044 train_acc= 0.99456 val_loss= 0.22177 val_acc= 0.94181 time= 0.19200
Epoch: 0172 train_loss= 0.04084 train_acc= 0.99422 val_loss= 0.22228 val_acc= 0.94028 time= 0.17204
Epoch: 0173 train_loss= 0.04082 train_acc= 0.99303 val_loss= 0.22095 val_acc= 0.94028 time= 0.17099
Epoch: 0174 train_loss= 0.03950 train_acc= 0.99490 val_loss= 0.21892 val_acc= 0.94487 time= 0.18800
Epoch: 0175 train_loss= 0.04042 train_acc= 0.99303 val_loss= 0.21686 val_acc= 0.94640 time= 0.16700
Epoch: 0176 train_loss= 0.03857 train_acc= 0.99507 val_loss= 0.21636 val_acc= 0.94640 time= 0.16800
Epoch: 0177 train_loss= 0.03860 train_acc= 0.99439 val_loss= 0.21725 val_acc= 0.94946 time= 0.19000
Epoch: 0178 train_loss= 0.03968 train_acc= 0.99354 val_loss= 0.21907 val_acc= 0.94640 time= 0.16850
Epoch: 0179 train_loss= 0.03716 train_acc= 0.99388 val_loss= 0.22195 val_acc= 0.94640 time= 0.17302
Epoch: 0180 train_loss= 0.03708 train_acc= 0.99439 val_loss= 0.22529 val_acc= 0.94487 time= 0.19300
Epoch: 0181 train_loss= 0.03741 train_acc= 0.99388 val_loss= 0.22729 val_acc= 0.94334 time= 0.17200
Epoch: 0182 train_loss= 0.03416 train_acc= 0.99541 val_loss= 0.22747 val_acc= 0.94334 time= 0.17803
Epoch: 0183 train_loss= 0.03617 train_acc= 0.99422 val_loss= 0.22672 val_acc= 0.94334 time= 0.16700
Epoch: 0184 train_loss= 0.03478 train_acc= 0.99456 val_loss= 0.22634 val_acc= 0.94181 time= 0.16800
Epoch: 0185 train_loss= 0.03545 train_acc= 0.99541 val_loss= 0.22459 val_acc= 0.94334 time= 0.18900
Epoch: 0186 train_loss= 0.03605 train_acc= 0.99507 val_loss= 0.22174 val_acc= 0.94640 time= 0.16700
Epoch: 0187 train_loss= 0.03395 train_acc= 0.99490 val_loss= 0.22038 val_acc= 0.94640 time= 0.17465
Epoch: 0188 train_loss= 0.03385 train_acc= 0.99473 val_loss= 0.22004 val_acc= 0.94640 time= 0.19291
Epoch: 0189 train_loss= 0.03189 train_acc= 0.99524 val_loss= 0.22080 val_acc= 0.94640 time= 0.17000
Epoch: 0190 train_loss= 0.03153 train_acc= 0.99575 val_loss= 0.22144 val_acc= 0.94334 time= 0.17100
Epoch: 0191 train_loss= 0.03206 train_acc= 0.99660 val_loss= 0.22234 val_acc= 0.94181 time= 0.19184
Epoch: 0192 train_loss= 0.03323 train_acc= 0.99541 val_loss= 0.22320 val_acc= 0.94334 time= 0.16804
Epoch: 0193 train_loss= 0.03288 train_acc= 0.99473 val_loss= 0.22266 val_acc= 0.94487 time= 0.16807
Epoch: 0194 train_loss= 0.03391 train_acc= 0.99439 val_loss= 0.22107 val_acc= 0.94640 time= 0.18700
Epoch: 0195 train_loss= 0.02959 train_acc= 0.99473 val_loss= 0.21950 val_acc= 0.94640 time= 0.17348
Epoch: 0196 train_loss= 0.03109 train_acc= 0.99524 val_loss= 0.21933 val_acc= 0.94640 time= 0.19849
Epoch: 0197 train_loss= 0.03077 train_acc= 0.99490 val_loss= 0.22026 val_acc= 0.94487 time= 0.17300
Epoch: 0198 train_loss= 0.02944 train_acc= 0.99694 val_loss= 0.22221 val_acc= 0.94487 time= 0.16704
Epoch: 0199 train_loss= 0.03169 train_acc= 0.99439 val_loss= 0.22330 val_acc= 0.94334 time= 0.19554
Epoch: 0200 train_loss= 0.02870 train_acc= 0.99609 val_loss= 0.22448 val_acc= 0.94334 time= 0.17011
Epoch: 0201 train_loss= 0.02686 train_acc= 0.99609 val_loss= 0.22516 val_acc= 0.94334 time= 0.16900
Epoch: 0202 train_loss= 0.02707 train_acc= 0.99677 val_loss= 0.22697 val_acc= 0.94181 time= 0.18622
Epoch: 0203 train_loss= 0.02829 train_acc= 0.99541 val_loss= 0.22914 val_acc= 0.94028 time= 0.17000
Early stopping...
Optimization Finished!
Test set results: cost= 0.25213 accuracy= 0.93769 time= 0.07900
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.7778    0.8750    0.8235         8
           1     0.5000    0.1667    0.2500         6
           2     0.5000    1.0000    0.6667         1
           3     0.7912    0.9600    0.8675        75
           4     1.0000    1.0000    1.0000         9
           5     0.7692    0.9195    0.8377        87
           6     0.9583    0.9200    0.9388        25
           7     0.7500    0.9231    0.8276        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.6667    0.8000         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    0.9167    0.9565        12
          12     0.8571    0.9917    0.9195       121
          13     1.0000    0.7895    0.8824        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.8636    0.9500    0.9048        20
          22     0.6000    0.6000    0.6000         5
          23     1.0000    1.0000    1.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9668    0.9626    0.9647       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8356    0.7531    0.7922        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9898    0.9839      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     0.0000    0.0000    0.0000         3
          44     0.9000    0.7500    0.8182        12
          45     0.5000    0.1667    0.2500         6
          46     1.0000    0.2857    0.4444         7
          47     0.8750    0.9333    0.9032        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9377      2568
   macro avg     0.7825    0.7079    0.7201      2568
weighted avg     0.9370    0.9377    0.9334      2568

Macro average Test Precision, Recall and F1-Score...
(0.7824918481537436, 0.7078891868264287, 0.7200715115588024, None)
Micro average Test Precision, Recall and F1-Score...
(0.9376947040498442, 0.9376947040498442, 0.9376947040498442, None)
embeddings:
8892 6532 2568
[[ 2.023602   -0.14226973  0.43997374 ... -0.0076412  -0.18649809
  -0.08653565]
 [ 1.0518681   0.29865536  0.37844387 ...  0.6246919   0.05044134
   0.00368173]
 [ 0.6690725   0.04694577  0.18927318 ...  0.41201788 -0.00811251
   0.1756325 ]
 ...
 [ 0.4323391   0.0921829   0.2942234  ...  0.47730735 -0.01923065
   0.00474925]
 [ 0.48160842  0.06137049  0.1435499  ...  0.2972223   0.05240981
   0.11767916]
 [ 0.5159662   0.30331025  0.3415843  ...  0.5169367   0.26907015
   0.24446148]]

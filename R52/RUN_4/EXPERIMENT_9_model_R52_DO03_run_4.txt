(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95117 train_acc= 0.04882 val_loss= 3.89570 val_acc= 0.60643 time= 0.45698
Epoch: 0002 train_loss= 3.89582 train_acc= 0.61082 val_loss= 3.78977 val_acc= 0.59724 time= 0.17500
Epoch: 0003 train_loss= 3.79030 train_acc= 0.59568 val_loss= 3.62585 val_acc= 0.56355 time= 0.17700
Epoch: 0004 train_loss= 3.63150 train_acc= 0.56574 val_loss= 3.40383 val_acc= 0.52833 time= 0.16600
Epoch: 0005 train_loss= 3.41262 train_acc= 0.51658 val_loss= 3.13584 val_acc= 0.49464 time= 0.16921
Epoch: 0006 train_loss= 3.13871 train_acc= 0.47253 val_loss= 2.84969 val_acc= 0.47779 time= 0.18400
Epoch: 0007 train_loss= 2.85496 train_acc= 0.45059 val_loss= 2.58155 val_acc= 0.46554 time= 0.16900
Epoch: 0008 train_loss= 2.58549 train_acc= 0.44429 val_loss= 2.37205 val_acc= 0.46095 time= 0.16573
Epoch: 0009 train_loss= 2.37705 train_acc= 0.43749 val_loss= 2.24666 val_acc= 0.46095 time= 0.19108
Epoch: 0010 train_loss= 2.24340 train_acc= 0.43494 val_loss= 2.18736 val_acc= 0.46248 time= 0.16800
Epoch: 0011 train_loss= 2.19009 train_acc= 0.43664 val_loss= 2.15178 val_acc= 0.46554 time= 0.16900
Epoch: 0012 train_loss= 2.16573 train_acc= 0.43766 val_loss= 2.10671 val_acc= 0.46861 time= 0.18100
Epoch: 0013 train_loss= 2.11731 train_acc= 0.44055 val_loss= 2.04005 val_acc= 0.48239 time= 0.16975
Epoch: 0014 train_loss= 2.05951 train_acc= 0.45229 val_loss= 1.95521 val_acc= 0.50077 time= 0.17000
Epoch: 0015 train_loss= 1.97673 train_acc= 0.48954 val_loss= 1.86358 val_acc= 0.54211 time= 0.17103
Epoch: 0016 train_loss= 1.89145 train_acc= 0.54091 val_loss= 1.77828 val_acc= 0.59418 time= 0.16603
Epoch: 0017 train_loss= 1.81293 train_acc= 0.59755 val_loss= 1.70771 val_acc= 0.62940 time= 0.18300
Epoch: 0018 train_loss= 1.73259 train_acc= 0.62426 val_loss= 1.65119 val_acc= 0.65544 time= 0.16607
Epoch: 0019 train_loss= 1.68220 train_acc= 0.63837 val_loss= 1.60144 val_acc= 0.66922 time= 0.16803
Epoch: 0020 train_loss= 1.63165 train_acc= 0.64926 val_loss= 1.55185 val_acc= 0.67841 time= 0.17001
Epoch: 0021 train_loss= 1.57950 train_acc= 0.65572 val_loss= 1.50067 val_acc= 0.67841 time= 0.19096
Epoch: 0022 train_loss= 1.52753 train_acc= 0.65862 val_loss= 1.44969 val_acc= 0.67994 time= 0.17100
Epoch: 0023 train_loss= 1.47741 train_acc= 0.66321 val_loss= 1.40142 val_acc= 0.68606 time= 0.18103
Epoch: 0024 train_loss= 1.42659 train_acc= 0.66253 val_loss= 1.35753 val_acc= 0.68760 time= 0.16700
Epoch: 0025 train_loss= 1.38406 train_acc= 0.66525 val_loss= 1.31804 val_acc= 0.69066 time= 0.16700
Epoch: 0026 train_loss= 1.34211 train_acc= 0.67018 val_loss= 1.28215 val_acc= 0.69066 time= 0.17101
Epoch: 0027 train_loss= 1.30635 train_acc= 0.67324 val_loss= 1.24886 val_acc= 0.69678 time= 0.18900
Epoch: 0028 train_loss= 1.27464 train_acc= 0.67886 val_loss= 1.21715 val_acc= 0.70138 time= 0.16503
Epoch: 0029 train_loss= 1.23627 train_acc= 0.69059 val_loss= 1.18632 val_acc= 0.70904 time= 0.17463
Epoch: 0030 train_loss= 1.20947 train_acc= 0.69808 val_loss= 1.15591 val_acc= 0.72129 time= 0.16900
Epoch: 0031 train_loss= 1.17415 train_acc= 0.71492 val_loss= 1.12561 val_acc= 0.72435 time= 0.16810
Epoch: 0032 train_loss= 1.14558 train_acc= 0.72750 val_loss= 1.09544 val_acc= 0.73507 time= 0.19403
Epoch: 0033 train_loss= 1.11040 train_acc= 0.74060 val_loss= 1.06558 val_acc= 0.73660 time= 0.16701
Epoch: 0034 train_loss= 1.08388 train_acc= 0.74911 val_loss= 1.03647 val_acc= 0.74885 time= 0.16722
Epoch: 0035 train_loss= 1.05024 train_acc= 0.75659 val_loss= 1.00847 val_acc= 0.75804 time= 0.18900
Epoch: 0036 train_loss= 1.02312 train_acc= 0.76374 val_loss= 0.98163 val_acc= 0.76570 time= 0.16700
Epoch: 0037 train_loss= 0.99495 train_acc= 0.77003 val_loss= 0.95590 val_acc= 0.76876 time= 0.16985
Epoch: 0038 train_loss= 0.96837 train_acc= 0.77513 val_loss= 0.93102 val_acc= 0.78254 time= 0.19700
Epoch: 0039 train_loss= 0.94453 train_acc= 0.78619 val_loss= 0.90672 val_acc= 0.79173 time= 0.16800
Epoch: 0040 train_loss= 0.91972 train_acc= 0.79486 val_loss= 0.88280 val_acc= 0.80245 time= 0.18300
Epoch: 0041 train_loss= 0.89360 train_acc= 0.80558 val_loss= 0.85918 val_acc= 0.81623 time= 0.16714
Epoch: 0042 train_loss= 0.86714 train_acc= 0.81510 val_loss= 0.83595 val_acc= 0.82236 time= 0.16899
Epoch: 0043 train_loss= 0.84612 train_acc= 0.82361 val_loss= 0.81306 val_acc= 0.82848 time= 0.16800
Epoch: 0044 train_loss= 0.81472 train_acc= 0.82939 val_loss= 0.79052 val_acc= 0.83308 time= 0.19001
Epoch: 0045 train_loss= 0.79475 train_acc= 0.83365 val_loss= 0.76829 val_acc= 0.83920 time= 0.16896
Epoch: 0046 train_loss= 0.77388 train_acc= 0.84045 val_loss= 0.74642 val_acc= 0.84380 time= 0.17100
Epoch: 0047 train_loss= 0.75461 train_acc= 0.84436 val_loss= 0.72509 val_acc= 0.84380 time= 0.16900
Epoch: 0048 train_loss= 0.72533 train_acc= 0.85151 val_loss= 0.70426 val_acc= 0.84992 time= 0.16804
Epoch: 0049 train_loss= 0.70215 train_acc= 0.85712 val_loss= 0.68398 val_acc= 0.85605 time= 0.19200
Epoch: 0050 train_loss= 0.68262 train_acc= 0.85984 val_loss= 0.66428 val_acc= 0.86064 time= 0.16800
Epoch: 0051 train_loss= 0.66714 train_acc= 0.86392 val_loss= 0.64507 val_acc= 0.86524 time= 0.16800
Epoch: 0052 train_loss= 0.63887 train_acc= 0.86596 val_loss= 0.62649 val_acc= 0.86677 time= 0.18700
Epoch: 0053 train_loss= 0.61612 train_acc= 0.87294 val_loss= 0.60854 val_acc= 0.86983 time= 0.16997
Epoch: 0054 train_loss= 0.59741 train_acc= 0.87583 val_loss= 0.59119 val_acc= 0.87443 time= 0.17100
Epoch: 0055 train_loss= 0.57784 train_acc= 0.88093 val_loss= 0.57422 val_acc= 0.87443 time= 0.19100
Epoch: 0056 train_loss= 0.56024 train_acc= 0.87957 val_loss= 0.55773 val_acc= 0.87443 time= 0.16803
Epoch: 0057 train_loss= 0.54735 train_acc= 0.88416 val_loss= 0.54190 val_acc= 0.87596 time= 0.17000
Epoch: 0058 train_loss= 0.52296 train_acc= 0.88621 val_loss= 0.52684 val_acc= 0.87749 time= 0.16900
Epoch: 0059 train_loss= 0.50659 train_acc= 0.88961 val_loss= 0.51218 val_acc= 0.88208 time= 0.16900
Epoch: 0060 train_loss= 0.49200 train_acc= 0.89590 val_loss= 0.49795 val_acc= 0.88361 time= 0.16597
Epoch: 0061 train_loss= 0.47167 train_acc= 0.89811 val_loss= 0.48413 val_acc= 0.88515 time= 0.19800
Epoch: 0062 train_loss= 0.45907 train_acc= 0.90458 val_loss= 0.47077 val_acc= 0.88668 time= 0.17000
Epoch: 0063 train_loss= 0.45081 train_acc= 0.90747 val_loss= 0.45790 val_acc= 0.88821 time= 0.18300
Epoch: 0064 train_loss= 0.43154 train_acc= 0.90883 val_loss= 0.44572 val_acc= 0.89433 time= 0.16703
Epoch: 0065 train_loss= 0.41727 train_acc= 0.91274 val_loss= 0.43427 val_acc= 0.89280 time= 0.16700
Epoch: 0066 train_loss= 0.40323 train_acc= 0.91444 val_loss= 0.42310 val_acc= 0.89433 time= 0.18712
Epoch: 0067 train_loss= 0.38410 train_acc= 0.91631 val_loss= 0.41210 val_acc= 0.89740 time= 0.16799
Epoch: 0068 train_loss= 0.37696 train_acc= 0.92193 val_loss= 0.40158 val_acc= 0.90046 time= 0.16797
Epoch: 0069 train_loss= 0.36408 train_acc= 0.92380 val_loss= 0.39147 val_acc= 0.90199 time= 0.17099
Epoch: 0070 train_loss= 0.35177 train_acc= 0.92754 val_loss= 0.38209 val_acc= 0.90352 time= 0.17025
Epoch: 0071 train_loss= 0.34076 train_acc= 0.92822 val_loss= 0.37349 val_acc= 0.90199 time= 0.16903
Epoch: 0072 train_loss= 0.33110 train_acc= 0.93094 val_loss= 0.36564 val_acc= 0.90199 time= 0.19226
Epoch: 0073 train_loss= 0.32156 train_acc= 0.93519 val_loss= 0.35838 val_acc= 0.90352 time= 0.16897
Epoch: 0074 train_loss= 0.31324 train_acc= 0.93553 val_loss= 0.35164 val_acc= 0.90658 time= 0.16803
Epoch: 0075 train_loss= 0.30139 train_acc= 0.93842 val_loss= 0.34488 val_acc= 0.91118 time= 0.18500
Epoch: 0076 train_loss= 0.29324 train_acc= 0.93962 val_loss= 0.33819 val_acc= 0.91271 time= 0.16900
Epoch: 0077 train_loss= 0.28229 train_acc= 0.94319 val_loss= 0.33211 val_acc= 0.91577 time= 0.16900
Epoch: 0078 train_loss= 0.27174 train_acc= 0.94557 val_loss= 0.32586 val_acc= 0.91577 time= 0.19600
Epoch: 0079 train_loss= 0.26419 train_acc= 0.94625 val_loss= 0.31987 val_acc= 0.91577 time= 0.17000
Epoch: 0080 train_loss= 0.25276 train_acc= 0.95305 val_loss= 0.31412 val_acc= 0.91730 time= 0.18500
Epoch: 0081 train_loss= 0.24405 train_acc= 0.95067 val_loss= 0.30877 val_acc= 0.91730 time= 0.16600
Epoch: 0082 train_loss= 0.24099 train_acc= 0.95390 val_loss= 0.30367 val_acc= 0.92037 time= 0.16626
Epoch: 0083 train_loss= 0.23311 train_acc= 0.95612 val_loss= 0.29903 val_acc= 0.92343 time= 0.17098
Epoch: 0084 train_loss= 0.22651 train_acc= 0.95612 val_loss= 0.29506 val_acc= 0.92343 time= 0.19159
Epoch: 0085 train_loss= 0.21792 train_acc= 0.95714 val_loss= 0.29218 val_acc= 0.92190 time= 0.17200
Epoch: 0086 train_loss= 0.21289 train_acc= 0.95765 val_loss= 0.28911 val_acc= 0.92343 time= 0.17704
Epoch: 0087 train_loss= 0.20535 train_acc= 0.96105 val_loss= 0.28560 val_acc= 0.92649 time= 0.16604
Epoch: 0088 train_loss= 0.20071 train_acc= 0.96190 val_loss= 0.28263 val_acc= 0.92496 time= 0.16595
Epoch: 0089 train_loss= 0.19319 train_acc= 0.96462 val_loss= 0.27972 val_acc= 0.92496 time= 0.17168
Epoch: 0090 train_loss= 0.18858 train_acc= 0.96343 val_loss= 0.27671 val_acc= 0.92649 time= 0.18903
Epoch: 0091 train_loss= 0.18013 train_acc= 0.96547 val_loss= 0.27348 val_acc= 0.92802 time= 0.16900
Epoch: 0092 train_loss= 0.17293 train_acc= 0.96734 val_loss= 0.27030 val_acc= 0.92649 time= 0.18304
Epoch: 0093 train_loss= 0.16845 train_acc= 0.96819 val_loss= 0.26745 val_acc= 0.92956 time= 0.17000
Epoch: 0094 train_loss= 0.16297 train_acc= 0.96921 val_loss= 0.26447 val_acc= 0.93262 time= 0.16900
Epoch: 0095 train_loss= 0.16098 train_acc= 0.96972 val_loss= 0.26119 val_acc= 0.93415 time= 0.17900
Epoch: 0096 train_loss= 0.15435 train_acc= 0.97142 val_loss= 0.25846 val_acc= 0.93262 time= 0.16700
Epoch: 0097 train_loss= 0.14814 train_acc= 0.97449 val_loss= 0.25612 val_acc= 0.93262 time= 0.16701
Epoch: 0098 train_loss= 0.14518 train_acc= 0.97432 val_loss= 0.25356 val_acc= 0.93415 time= 0.18599
Epoch: 0099 train_loss= 0.14059 train_acc= 0.97466 val_loss= 0.25180 val_acc= 0.93415 time= 0.16700
Epoch: 0100 train_loss= 0.13983 train_acc= 0.97398 val_loss= 0.25053 val_acc= 0.93415 time= 0.17001
Epoch: 0101 train_loss= 0.13306 train_acc= 0.97432 val_loss= 0.24949 val_acc= 0.93415 time= 0.19599
Epoch: 0102 train_loss= 0.13097 train_acc= 0.97500 val_loss= 0.24764 val_acc= 0.93568 time= 0.16900
Epoch: 0103 train_loss= 0.12405 train_acc= 0.97755 val_loss= 0.24637 val_acc= 0.93568 time= 0.18200
Epoch: 0104 train_loss= 0.11941 train_acc= 0.97789 val_loss= 0.24464 val_acc= 0.93415 time= 0.16800
Epoch: 0105 train_loss= 0.11920 train_acc= 0.97993 val_loss= 0.24302 val_acc= 0.93568 time= 0.16800
Epoch: 0106 train_loss= 0.11080 train_acc= 0.98367 val_loss= 0.24153 val_acc= 0.93568 time= 0.19000
Epoch: 0107 train_loss= 0.10983 train_acc= 0.98282 val_loss= 0.24098 val_acc= 0.93568 time= 0.16600
Epoch: 0108 train_loss= 0.10828 train_acc= 0.98214 val_loss= 0.24037 val_acc= 0.93415 time= 0.17039
Epoch: 0109 train_loss= 0.10381 train_acc= 0.98418 val_loss= 0.23979 val_acc= 0.93262 time= 0.17600
Epoch: 0110 train_loss= 0.10278 train_acc= 0.98367 val_loss= 0.23865 val_acc= 0.93568 time= 0.16800
Epoch: 0111 train_loss= 0.09719 train_acc= 0.98469 val_loss= 0.23755 val_acc= 0.93568 time= 0.16801
Epoch: 0112 train_loss= 0.09498 train_acc= 0.98418 val_loss= 0.23708 val_acc= 0.93721 time= 0.17006
Epoch: 0113 train_loss= 0.09248 train_acc= 0.98571 val_loss= 0.23574 val_acc= 0.93568 time= 0.18896
Epoch: 0114 train_loss= 0.09206 train_acc= 0.98554 val_loss= 0.23374 val_acc= 0.93568 time= 0.16700
Epoch: 0115 train_loss= 0.08672 train_acc= 0.98707 val_loss= 0.23094 val_acc= 0.93874 time= 0.18200
Epoch: 0116 train_loss= 0.08420 train_acc= 0.98758 val_loss= 0.22933 val_acc= 0.93874 time= 0.17000
Epoch: 0117 train_loss= 0.08287 train_acc= 0.98724 val_loss= 0.22919 val_acc= 0.93874 time= 0.17000
Epoch: 0118 train_loss= 0.08146 train_acc= 0.98741 val_loss= 0.22904 val_acc= 0.93874 time= 0.19400
Epoch: 0119 train_loss= 0.07630 train_acc= 0.98996 val_loss= 0.22984 val_acc= 0.94028 time= 0.16600
Epoch: 0120 train_loss= 0.07462 train_acc= 0.98945 val_loss= 0.23040 val_acc= 0.94028 time= 0.16903
Epoch: 0121 train_loss= 0.07257 train_acc= 0.98945 val_loss= 0.23111 val_acc= 0.93874 time= 0.17109
Epoch: 0122 train_loss= 0.07288 train_acc= 0.98775 val_loss= 0.23059 val_acc= 0.93874 time= 0.16900
Epoch: 0123 train_loss= 0.06963 train_acc= 0.99047 val_loss= 0.23065 val_acc= 0.93721 time= 0.16797
Epoch: 0124 train_loss= 0.07064 train_acc= 0.98945 val_loss= 0.22893 val_acc= 0.93721 time= 0.19700
Epoch: 0125 train_loss= 0.06576 train_acc= 0.99030 val_loss= 0.22746 val_acc= 0.94028 time= 0.17100
Epoch: 0126 train_loss= 0.06311 train_acc= 0.98996 val_loss= 0.22592 val_acc= 0.93874 time= 0.18503
Epoch: 0127 train_loss= 0.06419 train_acc= 0.99081 val_loss= 0.22457 val_acc= 0.94181 time= 0.16800
Epoch: 0128 train_loss= 0.06165 train_acc= 0.98996 val_loss= 0.22400 val_acc= 0.94028 time= 0.16700
Epoch: 0129 train_loss= 0.06040 train_acc= 0.99098 val_loss= 0.22495 val_acc= 0.94028 time= 0.17400
Epoch: 0130 train_loss= 0.05738 train_acc= 0.99201 val_loss= 0.22635 val_acc= 0.93721 time= 0.17897
Epoch: 0131 train_loss= 0.05496 train_acc= 0.99167 val_loss= 0.22682 val_acc= 0.93721 time= 0.16803
Epoch: 0132 train_loss= 0.05330 train_acc= 0.99320 val_loss= 0.22648 val_acc= 0.93721 time= 0.16997
Epoch: 0133 train_loss= 0.05489 train_acc= 0.99337 val_loss= 0.22519 val_acc= 0.93721 time= 0.17000
Epoch: 0134 train_loss= 0.05295 train_acc= 0.99184 val_loss= 0.22367 val_acc= 0.94181 time= 0.16903
Epoch: 0135 train_loss= 0.05080 train_acc= 0.99388 val_loss= 0.22194 val_acc= 0.94181 time= 0.18797
Epoch: 0136 train_loss= 0.04890 train_acc= 0.99422 val_loss= 0.22121 val_acc= 0.94181 time= 0.17003
Epoch: 0137 train_loss= 0.04909 train_acc= 0.99303 val_loss= 0.22150 val_acc= 0.94181 time= 0.16600
Epoch: 0138 train_loss= 0.04886 train_acc= 0.99371 val_loss= 0.22209 val_acc= 0.94181 time= 0.19197
Epoch: 0139 train_loss= 0.04728 train_acc= 0.99337 val_loss= 0.22275 val_acc= 0.94181 time= 0.16800
Epoch: 0140 train_loss= 0.04678 train_acc= 0.99320 val_loss= 0.22295 val_acc= 0.94028 time= 0.17200
Epoch: 0141 train_loss= 0.04504 train_acc= 0.99405 val_loss= 0.22354 val_acc= 0.93874 time= 0.19200
Early stopping...
Optimization Finished!
Test set results: cost= 0.25018 accuracy= 0.93808 time= 0.07504
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7826    0.9600    0.8623        75
           4     1.0000    1.0000    1.0000         9
           5     0.8421    0.9195    0.8791        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.5556    0.7143         9
          10     0.9600    0.6667    0.7869        36
          11     1.0000    0.9167    0.9565        12
          12     0.8392    0.9917    0.9091       121
          13     0.9286    0.6842    0.7879        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.5556    0.7143         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8235    0.8235    0.8235        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.7273    0.8421        11
          29     0.9627    0.9641    0.9634       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8250    0.8148    0.8199        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9908    0.9849      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.8889    0.6667    0.7619        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9381      2568
   macro avg     0.7949    0.6930    0.7163      2568
weighted avg     0.9387    0.9381    0.9338      2568

Macro average Test Precision, Recall and F1-Score...
(0.7949442689479468, 0.6930258084242998, 0.71629096960434, None)
Micro average Test Precision, Recall and F1-Score...
(0.9380841121495327, 0.9380841121495327, 0.9380841121495327, None)
embeddings:
8892 6532 2568
[[-0.07044499  0.29160815  0.07808206 ... -0.02939292  0.03849042
  -0.17398515]
 [ 0.02562208  0.30824485 -0.09718123 ...  0.00142027  0.12926221
   0.01895469]
 [-0.05227491 -0.01606209  0.6827617  ...  0.08740669  0.5793202
   0.14463115]
 ...
 [ 0.03459433  0.00966832  0.5398201  ...  0.12134659  0.05616561
   0.13933823]
 [ 0.01689677  0.03282414  0.5250085  ...  0.06438465  0.2057913
   0.11500808]
 [ 0.25835216  0.26911068  0.45804694 ...  0.2839329   0.2742055
   0.25830168]]

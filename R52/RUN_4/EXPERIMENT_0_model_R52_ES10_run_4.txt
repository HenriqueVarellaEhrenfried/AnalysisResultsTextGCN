(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95119 train_acc= 0.01344 val_loss= 3.89205 val_acc= 0.64931 time= 0.46525
Epoch: 0002 train_loss= 3.89316 train_acc= 0.64433 val_loss= 3.78202 val_acc= 0.65084 time= 0.18957
Epoch: 0003 train_loss= 3.78589 train_acc= 0.64042 val_loss= 3.61535 val_acc= 0.64012 time= 0.17100
Epoch: 0004 train_loss= 3.61954 train_acc= 0.62102 val_loss= 3.39318 val_acc= 0.60337 time= 0.17265
Epoch: 0005 train_loss= 3.39452 train_acc= 0.59228 val_loss= 3.12765 val_acc= 0.56355 time= 0.17104
Epoch: 0006 train_loss= 3.14100 train_acc= 0.55503 val_loss= 2.84604 val_acc= 0.52833 time= 0.17436
Epoch: 0007 train_loss= 2.84946 train_acc= 0.52033 val_loss= 2.58429 val_acc= 0.50536 time= 0.19400
Epoch: 0008 train_loss= 2.58164 train_acc= 0.49464 val_loss= 2.38078 val_acc= 0.49158 time= 0.17300
Epoch: 0009 train_loss= 2.39858 train_acc= 0.47474 val_loss= 2.25738 val_acc= 0.47933 time= 0.17200
Epoch: 0010 train_loss= 2.26818 train_acc= 0.45246 val_loss= 2.19644 val_acc= 0.46708 time= 0.19405
Epoch: 0011 train_loss= 2.20072 train_acc= 0.44395 val_loss= 2.15930 val_acc= 0.46554 time= 0.17205
Epoch: 0012 train_loss= 2.16953 train_acc= 0.43715 val_loss= 2.11560 val_acc= 0.46248 time= 0.17157
Epoch: 0013 train_loss= 2.12605 train_acc= 0.43749 val_loss= 2.05232 val_acc= 0.46554 time= 0.19557
Epoch: 0014 train_loss= 2.07591 train_acc= 0.43698 val_loss= 1.97123 val_acc= 0.47014 time= 0.17522
Epoch: 0015 train_loss= 1.99185 train_acc= 0.44804 val_loss= 1.88194 val_acc= 0.49311 time= 0.17562
Epoch: 0016 train_loss= 1.90607 train_acc= 0.47491 val_loss= 1.79777 val_acc= 0.54058 time= 0.18201
Epoch: 0017 train_loss= 1.82268 train_acc= 0.53530 val_loss= 1.72785 val_acc= 0.60643 time= 0.17103
Epoch: 0018 train_loss= 1.75690 train_acc= 0.60010 val_loss= 1.67129 val_acc= 0.64625 time= 0.17405
Epoch: 0019 train_loss= 1.69405 train_acc= 0.62698 val_loss= 1.62124 val_acc= 0.66769 time= 0.17298
Epoch: 0020 train_loss= 1.65472 train_acc= 0.64246 val_loss= 1.57124 val_acc= 0.67534 time= 0.17307
Epoch: 0021 train_loss= 1.59323 train_acc= 0.64501 val_loss= 1.52049 val_acc= 0.67075 time= 0.19797
Epoch: 0022 train_loss= 1.55399 train_acc= 0.64773 val_loss= 1.47025 val_acc= 0.67534 time= 0.17911
Epoch: 0023 train_loss= 1.50490 train_acc= 0.64824 val_loss= 1.42255 val_acc= 0.67381 time= 0.17518
Epoch: 0024 train_loss= 1.46050 train_acc= 0.64960 val_loss= 1.37877 val_acc= 0.67688 time= 0.18959
Epoch: 0025 train_loss= 1.42601 train_acc= 0.65504 val_loss= 1.33912 val_acc= 0.68453 time= 0.17416
Epoch: 0026 train_loss= 1.37368 train_acc= 0.65811 val_loss= 1.30347 val_acc= 0.69219 time= 0.17284
Epoch: 0027 train_loss= 1.33366 train_acc= 0.66406 val_loss= 1.27104 val_acc= 0.69985 time= 0.19223
Epoch: 0028 train_loss= 1.30159 train_acc= 0.67954 val_loss= 1.24100 val_acc= 0.70291 time= 0.17124
Epoch: 0029 train_loss= 1.27947 train_acc= 0.68515 val_loss= 1.21262 val_acc= 0.71363 time= 0.17363
Epoch: 0030 train_loss= 1.24947 train_acc= 0.70505 val_loss= 1.18527 val_acc= 0.71822 time= 0.19830
Epoch: 0031 train_loss= 1.21168 train_acc= 0.71509 val_loss= 1.15839 val_acc= 0.72588 time= 0.17402
Epoch: 0032 train_loss= 1.18366 train_acc= 0.72853 val_loss= 1.13166 val_acc= 0.73201 time= 0.17188
Epoch: 0033 train_loss= 1.15520 train_acc= 0.74009 val_loss= 1.10504 val_acc= 0.73507 time= 0.19537
Epoch: 0034 train_loss= 1.13132 train_acc= 0.74213 val_loss= 1.07882 val_acc= 0.74119 time= 0.17085
Epoch: 0035 train_loss= 1.09590 train_acc= 0.74741 val_loss= 1.05318 val_acc= 0.75191 time= 0.17327
Epoch: 0036 train_loss= 1.07661 train_acc= 0.75506 val_loss= 1.02822 val_acc= 0.76110 time= 0.19041
Epoch: 0037 train_loss= 1.04936 train_acc= 0.76323 val_loss= 1.00379 val_acc= 0.76263 time= 0.17486
Epoch: 0038 train_loss= 1.02860 train_acc= 0.76765 val_loss= 0.97994 val_acc= 0.77642 time= 0.19893
Epoch: 0039 train_loss= 0.99514 train_acc= 0.78194 val_loss= 0.95655 val_acc= 0.78867 time= 0.17355
Epoch: 0040 train_loss= 0.97431 train_acc= 0.78704 val_loss= 0.93344 val_acc= 0.79479 time= 0.17354
Epoch: 0041 train_loss= 0.95438 train_acc= 0.79384 val_loss= 0.91066 val_acc= 0.81011 time= 0.19400
Epoch: 0042 train_loss= 0.92923 train_acc= 0.80558 val_loss= 0.88823 val_acc= 0.81317 time= 0.17299
Epoch: 0043 train_loss= 0.90581 train_acc= 0.81221 val_loss= 0.86616 val_acc= 0.81623 time= 0.17197
Epoch: 0044 train_loss= 0.87777 train_acc= 0.82157 val_loss= 0.84446 val_acc= 0.82083 time= 0.17642
Epoch: 0045 train_loss= 0.86032 train_acc= 0.82344 val_loss= 0.82306 val_acc= 0.82848 time= 0.17441
Epoch: 0046 train_loss= 0.82868 train_acc= 0.82939 val_loss= 0.80193 val_acc= 0.83308 time= 0.19513
Epoch: 0047 train_loss= 0.81218 train_acc= 0.83245 val_loss= 0.78107 val_acc= 0.83614 time= 0.17203
Epoch: 0048 train_loss= 0.79321 train_acc= 0.83858 val_loss= 0.76033 val_acc= 0.84074 time= 0.17244
Epoch: 0049 train_loss= 0.76152 train_acc= 0.84623 val_loss= 0.73988 val_acc= 0.84227 time= 0.19178
Epoch: 0050 train_loss= 0.74385 train_acc= 0.84742 val_loss= 0.71952 val_acc= 0.84686 time= 0.17276
Epoch: 0051 train_loss= 0.72020 train_acc= 0.85287 val_loss= 0.69944 val_acc= 0.85145 time= 0.17404
Epoch: 0052 train_loss= 0.70598 train_acc= 0.85168 val_loss= 0.67982 val_acc= 0.85299 time= 0.19403
Epoch: 0053 train_loss= 0.67886 train_acc= 0.85814 val_loss= 0.66055 val_acc= 0.85911 time= 0.17480
Epoch: 0054 train_loss= 0.66594 train_acc= 0.85814 val_loss= 0.64193 val_acc= 0.86064 time= 0.17402
Epoch: 0055 train_loss= 0.64619 train_acc= 0.86205 val_loss= 0.62390 val_acc= 0.86524 time= 0.19269
Epoch: 0056 train_loss= 0.62355 train_acc= 0.86681 val_loss= 0.60625 val_acc= 0.86677 time= 0.17251
Epoch: 0057 train_loss= 0.60179 train_acc= 0.86971 val_loss= 0.58925 val_acc= 0.87443 time= 0.17572
Epoch: 0058 train_loss= 0.58447 train_acc= 0.87311 val_loss= 0.57322 val_acc= 0.87596 time= 0.18201
Epoch: 0059 train_loss= 0.56927 train_acc= 0.87804 val_loss= 0.55813 val_acc= 0.87749 time= 0.17362
Epoch: 0060 train_loss= 0.54600 train_acc= 0.88297 val_loss= 0.54359 val_acc= 0.87902 time= 0.17601
Epoch: 0061 train_loss= 0.53267 train_acc= 0.88638 val_loss= 0.52943 val_acc= 0.88055 time= 0.19654
Epoch: 0062 train_loss= 0.51456 train_acc= 0.88586 val_loss= 0.51553 val_acc= 0.88208 time= 0.17600
Epoch: 0063 train_loss= 0.49752 train_acc= 0.89063 val_loss= 0.50211 val_acc= 0.88361 time= 0.17300
Epoch: 0064 train_loss= 0.47940 train_acc= 0.89862 val_loss= 0.48831 val_acc= 0.88208 time= 0.18900
Epoch: 0065 train_loss= 0.47015 train_acc= 0.89607 val_loss= 0.47488 val_acc= 0.88361 time= 0.17101
Epoch: 0066 train_loss= 0.45590 train_acc= 0.89811 val_loss= 0.46208 val_acc= 0.88974 time= 0.18803
Epoch: 0067 train_loss= 0.44198 train_acc= 0.90356 val_loss= 0.45008 val_acc= 0.89587 time= 0.17033
Epoch: 0068 train_loss= 0.42632 train_acc= 0.90866 val_loss= 0.43879 val_acc= 0.89893 time= 0.17503
Epoch: 0069 train_loss= 0.41644 train_acc= 0.91121 val_loss= 0.42826 val_acc= 0.90046 time= 0.20818
Epoch: 0070 train_loss= 0.40239 train_acc= 0.91699 val_loss= 0.41783 val_acc= 0.90046 time= 0.18004
Epoch: 0071 train_loss= 0.38579 train_acc= 0.91648 val_loss= 0.40804 val_acc= 0.90046 time= 0.17302
Epoch: 0072 train_loss= 0.37827 train_acc= 0.91988 val_loss= 0.39923 val_acc= 0.90352 time= 0.18659
Epoch: 0073 train_loss= 0.36623 train_acc= 0.92329 val_loss= 0.39120 val_acc= 0.90199 time= 0.16870
Epoch: 0074 train_loss= 0.35795 train_acc= 0.92397 val_loss= 0.38324 val_acc= 0.90352 time= 0.17382
Epoch: 0075 train_loss= 0.34150 train_acc= 0.92992 val_loss= 0.37552 val_acc= 0.90505 time= 0.18759
Epoch: 0076 train_loss= 0.34201 train_acc= 0.92856 val_loss= 0.36801 val_acc= 0.90812 time= 0.17167
Epoch: 0077 train_loss= 0.32297 train_acc= 0.93179 val_loss= 0.36061 val_acc= 0.90658 time= 0.19551
Epoch: 0078 train_loss= 0.31441 train_acc= 0.93247 val_loss= 0.35315 val_acc= 0.90812 time= 0.17167
Epoch: 0079 train_loss= 0.30112 train_acc= 0.93894 val_loss= 0.34578 val_acc= 0.90965 time= 0.16917
Epoch: 0080 train_loss= 0.30022 train_acc= 0.94013 val_loss= 0.33860 val_acc= 0.91424 time= 0.17906
Epoch: 0081 train_loss= 0.29262 train_acc= 0.93740 val_loss= 0.33186 val_acc= 0.91271 time= 0.17208
Epoch: 0082 train_loss= 0.27852 train_acc= 0.94268 val_loss= 0.32630 val_acc= 0.91730 time= 0.17071
Epoch: 0083 train_loss= 0.27597 train_acc= 0.94319 val_loss= 0.32095 val_acc= 0.91884 time= 0.18800
Epoch: 0084 train_loss= 0.26264 train_acc= 0.94676 val_loss= 0.31622 val_acc= 0.91730 time= 0.17354
Epoch: 0085 train_loss= 0.26061 train_acc= 0.94506 val_loss= 0.31171 val_acc= 0.91730 time= 0.17571
Epoch: 0086 train_loss= 0.24725 train_acc= 0.94846 val_loss= 0.30858 val_acc= 0.92037 time= 0.19142
Epoch: 0087 train_loss= 0.24091 train_acc= 0.95322 val_loss= 0.30497 val_acc= 0.92190 time= 0.17000
Epoch: 0088 train_loss= 0.23615 train_acc= 0.95203 val_loss= 0.30078 val_acc= 0.92190 time= 0.17204
Epoch: 0089 train_loss= 0.22748 train_acc= 0.95254 val_loss= 0.29653 val_acc= 0.92190 time= 0.17607
Epoch: 0090 train_loss= 0.22299 train_acc= 0.95526 val_loss= 0.29165 val_acc= 0.92343 time= 0.17004
Epoch: 0091 train_loss= 0.21332 train_acc= 0.95646 val_loss= 0.28752 val_acc= 0.92802 time= 0.17414
Epoch: 0092 train_loss= 0.20905 train_acc= 0.95816 val_loss= 0.28429 val_acc= 0.92956 time= 0.19108
Epoch: 0093 train_loss= 0.20609 train_acc= 0.95680 val_loss= 0.28165 val_acc= 0.92956 time= 0.17366
Epoch: 0094 train_loss= 0.19723 train_acc= 0.96190 val_loss= 0.27979 val_acc= 0.92802 time= 0.18613
Epoch: 0095 train_loss= 0.19483 train_acc= 0.96156 val_loss= 0.27812 val_acc= 0.92649 time= 0.16905
Epoch: 0096 train_loss= 0.18938 train_acc= 0.96292 val_loss= 0.27535 val_acc= 0.92802 time= 0.17000
Epoch: 0097 train_loss= 0.18193 train_acc= 0.96207 val_loss= 0.27282 val_acc= 0.92802 time= 0.19014
Epoch: 0098 train_loss= 0.17825 train_acc= 0.96530 val_loss= 0.27037 val_acc= 0.92649 time= 0.16901
Epoch: 0099 train_loss= 0.17164 train_acc= 0.96853 val_loss= 0.26758 val_acc= 0.92956 time= 0.17265
Epoch: 0100 train_loss= 0.16796 train_acc= 0.96666 val_loss= 0.26482 val_acc= 0.92956 time= 0.20010
Epoch: 0101 train_loss= 0.16890 train_acc= 0.96394 val_loss= 0.26217 val_acc= 0.93262 time= 0.17400
Epoch: 0102 train_loss= 0.16065 train_acc= 0.96819 val_loss= 0.25988 val_acc= 0.93109 time= 0.18213
Epoch: 0103 train_loss= 0.15256 train_acc= 0.96802 val_loss= 0.25743 val_acc= 0.92802 time= 0.17402
Epoch: 0104 train_loss= 0.15383 train_acc= 0.96836 val_loss= 0.25589 val_acc= 0.93109 time= 0.17103
Epoch: 0105 train_loss= 0.14419 train_acc= 0.97040 val_loss= 0.25443 val_acc= 0.93109 time= 0.18904
Epoch: 0106 train_loss= 0.13917 train_acc= 0.97415 val_loss= 0.25336 val_acc= 0.93262 time= 0.17105
Epoch: 0107 train_loss= 0.13940 train_acc= 0.97517 val_loss= 0.25194 val_acc= 0.93415 time= 0.17397
Epoch: 0108 train_loss= 0.13363 train_acc= 0.97568 val_loss= 0.25111 val_acc= 0.93415 time= 0.19577
Epoch: 0109 train_loss= 0.13726 train_acc= 0.97363 val_loss= 0.25008 val_acc= 0.93415 time= 0.17458
Epoch: 0110 train_loss= 0.12505 train_acc= 0.97619 val_loss= 0.24848 val_acc= 0.93568 time= 0.17060
Epoch: 0111 train_loss= 0.12112 train_acc= 0.97636 val_loss= 0.24661 val_acc= 0.93568 time= 0.18800
Epoch: 0112 train_loss= 0.12470 train_acc= 0.97636 val_loss= 0.24540 val_acc= 0.93721 time= 0.16903
Epoch: 0113 train_loss= 0.12026 train_acc= 0.97993 val_loss= 0.24460 val_acc= 0.93721 time= 0.17597
Epoch: 0114 train_loss= 0.11388 train_acc= 0.98112 val_loss= 0.24309 val_acc= 0.93874 time= 0.18608
Epoch: 0115 train_loss= 0.11218 train_acc= 0.98061 val_loss= 0.24230 val_acc= 0.93109 time= 0.17297
Epoch: 0116 train_loss= 0.11300 train_acc= 0.97942 val_loss= 0.24178 val_acc= 0.93109 time= 0.17757
Epoch: 0117 train_loss= 0.11160 train_acc= 0.97976 val_loss= 0.24196 val_acc= 0.93415 time= 0.18722
Epoch: 0118 train_loss= 0.10687 train_acc= 0.97874 val_loss= 0.24257 val_acc= 0.93415 time= 0.17400
Epoch: 0119 train_loss= 0.10347 train_acc= 0.98112 val_loss= 0.24297 val_acc= 0.93415 time= 0.17601
Epoch: 0120 train_loss= 0.10229 train_acc= 0.98044 val_loss= 0.24288 val_acc= 0.93262 time= 0.18804
Epoch: 0121 train_loss= 0.10071 train_acc= 0.98180 val_loss= 0.24124 val_acc= 0.93415 time= 0.17005
Epoch: 0122 train_loss= 0.10252 train_acc= 0.97976 val_loss= 0.23817 val_acc= 0.93721 time= 0.18598
Epoch: 0123 train_loss= 0.09192 train_acc= 0.98401 val_loss= 0.23529 val_acc= 0.93721 time= 0.17495
Epoch: 0124 train_loss= 0.09753 train_acc= 0.98316 val_loss= 0.23377 val_acc= 0.93721 time= 0.17303
Epoch: 0125 train_loss= 0.08855 train_acc= 0.98537 val_loss= 0.23312 val_acc= 0.93721 time= 0.19311
Epoch: 0126 train_loss= 0.09049 train_acc= 0.98469 val_loss= 0.23282 val_acc= 0.93874 time= 0.16999
Epoch: 0127 train_loss= 0.09068 train_acc= 0.98316 val_loss= 0.23289 val_acc= 0.93874 time= 0.17301
Epoch: 0128 train_loss= 0.08051 train_acc= 0.98503 val_loss= 0.23340 val_acc= 0.93874 time= 0.18825
Epoch: 0129 train_loss= 0.08174 train_acc= 0.98418 val_loss= 0.23314 val_acc= 0.94028 time= 0.16906
Epoch: 0130 train_loss= 0.08094 train_acc= 0.98452 val_loss= 0.23171 val_acc= 0.94181 time= 0.17595
Epoch: 0131 train_loss= 0.08190 train_acc= 0.98605 val_loss= 0.22985 val_acc= 0.94181 time= 0.19349
Epoch: 0132 train_loss= 0.07712 train_acc= 0.98741 val_loss= 0.22824 val_acc= 0.94028 time= 0.16966
Epoch: 0133 train_loss= 0.07705 train_acc= 0.98588 val_loss= 0.22709 val_acc= 0.94028 time= 0.19051
Epoch: 0134 train_loss= 0.07728 train_acc= 0.98707 val_loss= 0.22702 val_acc= 0.94028 time= 0.16805
Epoch: 0135 train_loss= 0.06995 train_acc= 0.98928 val_loss= 0.22741 val_acc= 0.93874 time= 0.16704
Epoch: 0136 train_loss= 0.06977 train_acc= 0.98843 val_loss= 0.22836 val_acc= 0.93874 time= 0.16697
Epoch: 0137 train_loss= 0.06957 train_acc= 0.98741 val_loss= 0.22953 val_acc= 0.93874 time= 0.16856
Epoch: 0138 train_loss= 0.06525 train_acc= 0.98928 val_loss= 0.23065 val_acc= 0.93721 time= 0.17095
Early stopping...
Optimization Finished!
Test set results: cost= 0.25453 accuracy= 0.93731 time= 0.09531
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7742    0.9600    0.8571        75
           4     1.0000    1.0000    1.0000         9
           5     0.8100    0.9310    0.8663        87
           6     0.8846    0.9200    0.9020        25
           7     0.7333    0.8462    0.7857        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.3333    0.5000         9
          10     0.8966    0.7222    0.8000        36
          11     1.0000    0.9167    0.9565        12
          12     0.8750    0.9835    0.9261       121
          13     1.0000    0.7368    0.8485        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.7273    0.8421        11
          29     0.9669    0.9641    0.9655       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8442    0.8025    0.8228        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9926    0.9849      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.8889    0.6667    0.7619        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7533    0.6777    0.6957      2568
weighted avg     0.9343    0.9373    0.9321      2568

Macro average Test Precision, Recall and F1-Score...
(0.7533143992672132, 0.6776588462631951, 0.6956884835504463, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[ 0.47482845 -0.04111249  2.0623322  ...  1.3609298   0.92935944
   0.10400449]
 [ 0.8108311   0.15216191  0.6329241  ...  0.84626985  0.03830023
   0.09926128]
 [ 0.68202287 -0.0040771   0.6682751  ...  0.32894334  0.30328786
   0.06990097]
 ...
 [ 0.21121709 -0.00384596  0.52874273 ...  0.11359584  0.35622692
   0.03072639]
 [ 0.31382433  0.0372139   0.47495288 ...  0.20630494  0.299906
   0.06712227]
 [ 0.20043892  0.22227648  0.29565078 ...  0.30530477  0.36147738
   0.27553594]]

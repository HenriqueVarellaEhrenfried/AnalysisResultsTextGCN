(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.00272 val_loss= 3.88162 val_acc= 0.55130 time= 0.54587
Epoch: 0002 train_loss= 3.88165 train_acc= 0.53632 val_loss= 3.72997 val_acc= 0.50230 time= 0.21300
Epoch: 0003 train_loss= 3.73349 train_acc= 0.49345 val_loss= 3.48636 val_acc= 0.48698 time= 0.21204
Epoch: 0004 train_loss= 3.49071 train_acc= 0.46334 val_loss= 3.16251 val_acc= 0.47014 time= 0.23000
Epoch: 0005 train_loss= 3.15785 train_acc= 0.44923 val_loss= 2.80483 val_acc= 0.46401 time= 0.22000
Epoch: 0006 train_loss= 2.81001 train_acc= 0.44123 val_loss= 2.48403 val_acc= 0.46095 time= 0.21096
Epoch: 0007 train_loss= 2.48296 train_acc= 0.43902 val_loss= 2.27604 val_acc= 0.46095 time= 0.21200
Epoch: 0008 train_loss= 2.27814 train_acc= 0.43766 val_loss= 2.19065 val_acc= 0.46554 time= 0.23688
Epoch: 0009 train_loss= 2.19362 train_acc= 0.44395 val_loss= 2.15700 val_acc= 0.46861 time= 0.22504
Epoch: 0010 train_loss= 2.16679 train_acc= 0.44753 val_loss= 2.10771 val_acc= 0.48086 time= 0.20901
Epoch: 0011 train_loss= 2.12417 train_acc= 0.45433 val_loss= 2.02082 val_acc= 0.50383 time= 0.21100
Epoch: 0012 train_loss= 2.03179 train_acc= 0.49175 val_loss= 1.90768 val_acc= 0.55743 time= 0.21299
Epoch: 0013 train_loss= 1.92745 train_acc= 0.55775 val_loss= 1.79215 val_acc= 0.62328 time= 0.23496
Epoch: 0014 train_loss= 1.80997 train_acc= 0.60963 val_loss= 1.69753 val_acc= 0.65697 time= 0.22362
Epoch: 0015 train_loss= 1.73291 train_acc= 0.64144 val_loss= 1.62867 val_acc= 0.67075 time= 0.21304
Epoch: 0016 train_loss= 1.65760 train_acc= 0.64909 val_loss= 1.57071 val_acc= 0.67688 time= 0.20899
Epoch: 0017 train_loss= 1.60350 train_acc= 0.66202 val_loss= 1.50978 val_acc= 0.69219 time= 0.23601
Epoch: 0018 train_loss= 1.53873 train_acc= 0.67273 val_loss= 1.44542 val_acc= 0.69525 time= 0.22396
Epoch: 0019 train_loss= 1.47333 train_acc= 0.67835 val_loss= 1.38304 val_acc= 0.69219 time= 0.21300
Epoch: 0020 train_loss= 1.41754 train_acc= 0.67443 val_loss= 1.32713 val_acc= 0.69678 time= 0.21300
Epoch: 0021 train_loss= 1.35414 train_acc= 0.68260 val_loss= 1.27871 val_acc= 0.70291 time= 0.21600
Epoch: 0022 train_loss= 1.30551 train_acc= 0.68736 val_loss= 1.23631 val_acc= 0.71363 time= 0.23111
Epoch: 0023 train_loss= 1.26527 train_acc= 0.69876 val_loss= 1.19807 val_acc= 0.72129 time= 0.21000
Epoch: 0024 train_loss= 1.22586 train_acc= 0.70862 val_loss= 1.16237 val_acc= 0.72741 time= 0.20897
Epoch: 0025 train_loss= 1.18489 train_acc= 0.72733 val_loss= 1.12820 val_acc= 0.73660 time= 0.21000
Epoch: 0026 train_loss= 1.15194 train_acc= 0.74128 val_loss= 1.09478 val_acc= 0.74426 time= 0.21300
Epoch: 0027 train_loss= 1.11145 train_acc= 0.75404 val_loss= 1.06161 val_acc= 0.75345 time= 0.21300
Epoch: 0028 train_loss= 1.08030 train_acc= 0.76288 val_loss= 1.02853 val_acc= 0.76723 time= 0.23000
Epoch: 0029 train_loss= 1.04145 train_acc= 0.77309 val_loss= 0.99576 val_acc= 0.77489 time= 0.21040
Epoch: 0030 train_loss= 1.00980 train_acc= 0.77564 val_loss= 0.96375 val_acc= 0.77795 time= 0.21101
Epoch: 0031 train_loss= 0.97507 train_acc= 0.78432 val_loss= 0.93271 val_acc= 0.78560 time= 0.23400
Epoch: 0032 train_loss= 0.94572 train_acc= 0.78653 val_loss= 0.90266 val_acc= 0.79939 time= 0.22108
Epoch: 0033 train_loss= 0.91196 train_acc= 0.79929 val_loss= 0.87350 val_acc= 0.80245 time= 0.21600
Epoch: 0034 train_loss= 0.88601 train_acc= 0.80796 val_loss= 0.84516 val_acc= 0.81164 time= 0.21400
Epoch: 0035 train_loss= 0.85488 train_acc= 0.81936 val_loss= 0.81760 val_acc= 0.82542 time= 0.23404
Epoch: 0036 train_loss= 0.82603 train_acc= 0.82633 val_loss= 0.79091 val_acc= 0.83614 time= 0.20905
Epoch: 0037 train_loss= 0.80072 train_acc= 0.83228 val_loss= 0.76500 val_acc= 0.83767 time= 0.22000
Epoch: 0038 train_loss= 0.77209 train_acc= 0.83841 val_loss= 0.73985 val_acc= 0.84686 time= 0.21300
Epoch: 0039 train_loss= 0.74533 train_acc= 0.84385 val_loss= 0.71540 val_acc= 0.85145 time= 0.21500
Epoch: 0040 train_loss= 0.71806 train_acc= 0.84895 val_loss= 0.69169 val_acc= 0.85605 time= 0.23600
Epoch: 0041 train_loss= 0.69370 train_acc= 0.85304 val_loss= 0.66888 val_acc= 0.85299 time= 0.22000
Epoch: 0042 train_loss= 0.67398 train_acc= 0.85644 val_loss= 0.64684 val_acc= 0.85911 time= 0.21000
Epoch: 0043 train_loss= 0.64396 train_acc= 0.86018 val_loss= 0.62553 val_acc= 0.86524 time= 0.21204
Epoch: 0044 train_loss= 0.62043 train_acc= 0.86596 val_loss= 0.60502 val_acc= 0.86830 time= 0.21697
Epoch: 0045 train_loss= 0.59427 train_acc= 0.87022 val_loss= 0.58505 val_acc= 0.87136 time= 0.21469
Epoch: 0046 train_loss= 0.57797 train_acc= 0.87413 val_loss= 0.56547 val_acc= 0.87289 time= 0.21403
Epoch: 0047 train_loss= 0.55678 train_acc= 0.87498 val_loss= 0.54662 val_acc= 0.87596 time= 0.21031
Epoch: 0048 train_loss= 0.53755 train_acc= 0.87872 val_loss= 0.52895 val_acc= 0.87902 time= 0.21097
Epoch: 0049 train_loss= 0.51817 train_acc= 0.88212 val_loss= 0.51276 val_acc= 0.88361 time= 0.23402
Epoch: 0050 train_loss= 0.49411 train_acc= 0.88825 val_loss= 0.49756 val_acc= 0.88515 time= 0.21501
Epoch: 0051 train_loss= 0.47798 train_acc= 0.89114 val_loss= 0.48338 val_acc= 0.88821 time= 0.21500
Epoch: 0052 train_loss= 0.46745 train_acc= 0.89522 val_loss= 0.46981 val_acc= 0.88821 time= 0.21335
Epoch: 0053 train_loss= 0.45118 train_acc= 0.89777 val_loss= 0.45656 val_acc= 0.88668 time= 0.21101
Epoch: 0054 train_loss= 0.42975 train_acc= 0.90100 val_loss= 0.44370 val_acc= 0.88974 time= 0.23600
Epoch: 0055 train_loss= 0.41555 train_acc= 0.90151 val_loss= 0.43106 val_acc= 0.89587 time= 0.21000
Epoch: 0056 train_loss= 0.40244 train_acc= 0.90747 val_loss= 0.41883 val_acc= 0.90046 time= 0.21001
Epoch: 0057 train_loss= 0.38480 train_acc= 0.91274 val_loss= 0.40695 val_acc= 0.90199 time= 0.21198
Epoch: 0058 train_loss= 0.36967 train_acc= 0.91716 val_loss= 0.39522 val_acc= 0.90352 time= 0.23800
Epoch: 0059 train_loss= 0.36021 train_acc= 0.92022 val_loss= 0.38431 val_acc= 0.90505 time= 0.21200
Epoch: 0060 train_loss= 0.34685 train_acc= 0.92635 val_loss= 0.37452 val_acc= 0.90658 time= 0.22600
Epoch: 0061 train_loss= 0.33593 train_acc= 0.92924 val_loss= 0.36564 val_acc= 0.90658 time= 0.21101
Epoch: 0062 train_loss= 0.31976 train_acc= 0.92992 val_loss= 0.35743 val_acc= 0.90812 time= 0.21100
Epoch: 0063 train_loss= 0.31209 train_acc= 0.93570 val_loss= 0.35027 val_acc= 0.90505 time= 0.21100
Epoch: 0064 train_loss= 0.30163 train_acc= 0.93638 val_loss= 0.34358 val_acc= 0.90658 time= 0.22699
Epoch: 0065 train_loss= 0.28784 train_acc= 0.93894 val_loss= 0.33761 val_acc= 0.90658 time= 0.21300
Epoch: 0066 train_loss= 0.28209 train_acc= 0.94387 val_loss= 0.33176 val_acc= 0.90505 time= 0.20900
Epoch: 0067 train_loss= 0.26862 train_acc= 0.94336 val_loss= 0.32608 val_acc= 0.90812 time= 0.21400
Epoch: 0068 train_loss= 0.26334 train_acc= 0.94489 val_loss= 0.32041 val_acc= 0.91271 time= 0.23100
Epoch: 0069 train_loss= 0.25095 train_acc= 0.95016 val_loss= 0.31555 val_acc= 0.91424 time= 0.21004
Epoch: 0070 train_loss= 0.24044 train_acc= 0.95101 val_loss= 0.31057 val_acc= 0.91577 time= 0.21303
Epoch: 0071 train_loss= 0.23884 train_acc= 0.94778 val_loss= 0.30523 val_acc= 0.91730 time= 0.21497
Epoch: 0072 train_loss= 0.22772 train_acc= 0.95237 val_loss= 0.30013 val_acc= 0.91884 time= 0.21403
Epoch: 0073 train_loss= 0.21920 train_acc= 0.95509 val_loss= 0.29467 val_acc= 0.92190 time= 0.21100
Epoch: 0074 train_loss= 0.20908 train_acc= 0.95577 val_loss= 0.29049 val_acc= 0.92037 time= 0.23000
Epoch: 0075 train_loss= 0.19849 train_acc= 0.95833 val_loss= 0.28674 val_acc= 0.92037 time= 0.21000
Epoch: 0076 train_loss= 0.19490 train_acc= 0.95935 val_loss= 0.28273 val_acc= 0.92037 time= 0.21297
Epoch: 0077 train_loss= 0.19069 train_acc= 0.96275 val_loss= 0.27914 val_acc= 0.92190 time= 0.23900
Epoch: 0078 train_loss= 0.18442 train_acc= 0.96156 val_loss= 0.27693 val_acc= 0.92190 time= 0.21903
Epoch: 0079 train_loss= 0.17521 train_acc= 0.96377 val_loss= 0.27430 val_acc= 0.92190 time= 0.21100
Epoch: 0080 train_loss= 0.16945 train_acc= 0.96377 val_loss= 0.27171 val_acc= 0.92343 time= 0.20900
Epoch: 0081 train_loss= 0.16228 train_acc= 0.96836 val_loss= 0.26938 val_acc= 0.92496 time= 0.24012
Epoch: 0082 train_loss= 0.15957 train_acc= 0.96683 val_loss= 0.26694 val_acc= 0.92496 time= 0.20996
Epoch: 0083 train_loss= 0.14681 train_acc= 0.96972 val_loss= 0.26366 val_acc= 0.92649 time= 0.22566
Epoch: 0084 train_loss= 0.14870 train_acc= 0.96700 val_loss= 0.26018 val_acc= 0.92649 time= 0.21292
Epoch: 0085 train_loss= 0.14245 train_acc= 0.97159 val_loss= 0.25691 val_acc= 0.92649 time= 0.21004
Epoch: 0086 train_loss= 0.13600 train_acc= 0.97398 val_loss= 0.25387 val_acc= 0.93109 time= 0.23696
Epoch: 0087 train_loss= 0.13424 train_acc= 0.97432 val_loss= 0.25049 val_acc= 0.93415 time= 0.21700
Epoch: 0088 train_loss= 0.13014 train_acc= 0.97534 val_loss= 0.24821 val_acc= 0.93262 time= 0.21000
Epoch: 0089 train_loss= 0.12457 train_acc= 0.97653 val_loss= 0.24707 val_acc= 0.93109 time= 0.21468
Epoch: 0090 train_loss= 0.11937 train_acc= 0.97704 val_loss= 0.24695 val_acc= 0.92956 time= 0.21701
Epoch: 0091 train_loss= 0.11788 train_acc= 0.97925 val_loss= 0.24704 val_acc= 0.92802 time= 0.21600
Epoch: 0092 train_loss= 0.11380 train_acc= 0.97840 val_loss= 0.24520 val_acc= 0.92802 time= 0.21100
Epoch: 0093 train_loss= 0.11143 train_acc= 0.97806 val_loss= 0.24355 val_acc= 0.92802 time= 0.21307
Epoch: 0094 train_loss= 0.10428 train_acc= 0.97925 val_loss= 0.24262 val_acc= 0.93109 time= 0.21100
Epoch: 0095 train_loss= 0.10077 train_acc= 0.98112 val_loss= 0.24145 val_acc= 0.93415 time= 0.22980
Epoch: 0096 train_loss= 0.10140 train_acc= 0.98214 val_loss= 0.24111 val_acc= 0.93262 time= 0.21600
Epoch: 0097 train_loss= 0.09620 train_acc= 0.98316 val_loss= 0.24089 val_acc= 0.93262 time= 0.21500
Epoch: 0098 train_loss= 0.09360 train_acc= 0.98333 val_loss= 0.24013 val_acc= 0.93415 time= 0.21200
Epoch: 0099 train_loss= 0.09087 train_acc= 0.98503 val_loss= 0.23932 val_acc= 0.93721 time= 0.21000
Epoch: 0100 train_loss= 0.08633 train_acc= 0.98571 val_loss= 0.23804 val_acc= 0.93568 time= 0.21700
Epoch: 0101 train_loss= 0.08499 train_acc= 0.98384 val_loss= 0.23678 val_acc= 0.93568 time= 0.22110
Epoch: 0102 train_loss= 0.08206 train_acc= 0.98571 val_loss= 0.23446 val_acc= 0.93568 time= 0.21300
Epoch: 0103 train_loss= 0.07740 train_acc= 0.98605 val_loss= 0.23152 val_acc= 0.93874 time= 0.21400
Epoch: 0104 train_loss= 0.07824 train_acc= 0.98622 val_loss= 0.22986 val_acc= 0.93874 time= 0.23301
Epoch: 0105 train_loss= 0.07456 train_acc= 0.98588 val_loss= 0.23046 val_acc= 0.93568 time= 0.20999
Epoch: 0106 train_loss= 0.07218 train_acc= 0.98826 val_loss= 0.23177 val_acc= 0.93568 time= 0.22500
Epoch: 0107 train_loss= 0.06884 train_acc= 0.98809 val_loss= 0.23186 val_acc= 0.93721 time= 0.21400
Epoch: 0108 train_loss= 0.06706 train_acc= 0.98860 val_loss= 0.22937 val_acc= 0.93568 time= 0.21275
Epoch: 0109 train_loss= 0.06559 train_acc= 0.98979 val_loss= 0.22662 val_acc= 0.93721 time= 0.21500
Epoch: 0110 train_loss= 0.06581 train_acc= 0.98894 val_loss= 0.22603 val_acc= 0.93721 time= 0.21600
Epoch: 0111 train_loss= 0.06291 train_acc= 0.99081 val_loss= 0.22533 val_acc= 0.93721 time= 0.20804
Epoch: 0112 train_loss= 0.06119 train_acc= 0.98860 val_loss= 0.22493 val_acc= 0.93874 time= 0.20803
Epoch: 0113 train_loss= 0.05852 train_acc= 0.99167 val_loss= 0.22681 val_acc= 0.93415 time= 0.23401
Epoch: 0114 train_loss= 0.05773 train_acc= 0.99081 val_loss= 0.22975 val_acc= 0.93415 time= 0.21100
Early stopping...
Optimization Finished!
Test set results: cost= 0.25531 accuracy= 0.93886 time= 0.09073
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8022    0.9733    0.8795        75
           4     1.0000    1.0000    1.0000         9
           5     0.7757    0.9540    0.8557        87
           6     0.9200    0.9200    0.9200        25
           7     0.7500    0.9231    0.8276        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.7778    0.8750         9
          10     0.8846    0.6389    0.7419        36
          11     1.0000    1.0000    1.0000        12
          12     0.8333    0.9917    0.9057       121
          13     1.0000    0.6842    0.8125        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.5556    0.7143         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9669    0.9655    0.9662       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8493    0.7654    0.8052        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9917    0.9849      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     1.0000    0.8667    0.9286        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9389      2568
   macro avg     0.7664    0.6878    0.7098      2568
weighted avg     0.9362    0.9389    0.9338      2568

Macro average Test Precision, Recall and F1-Score...
(0.7663801388374308, 0.6877895891064358, 0.7097780546261152, None)
Micro average Test Precision, Recall and F1-Score...
(0.9388629283489096, 0.9388629283489096, 0.9388629283489096, None)
embeddings:
8892 6532 2568
[[ 0.18751283  0.9268563   0.78182673 ...  0.37929237  0.34512126
  -0.11437479]
 [ 0.19439776  0.44845495  0.34832942 ...  0.06329649  0.3082298
  -0.08252592]
 [-0.01507485  0.45700625  0.48047096 ...  0.3362645   0.06539452
  -0.09045701]
 ...
 [-0.00769275  0.12606223  0.2656157  ...  0.10039526  0.08724806
  -0.04832421]
 [ 0.02728447  0.15345292  0.19809534 ...  0.2280912   0.05147712
  -0.05084299]
 [ 0.23633738  0.1580679   0.1375351  ...  0.4135679   0.23335183
  -0.08410408]]

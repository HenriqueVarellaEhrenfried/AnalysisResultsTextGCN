(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95122 train_acc= 0.00765 val_loss= 3.93090 val_acc= 0.66156 time= 0.37001
Epoch: 0002 train_loss= 3.93100 train_acc= 0.63531 val_loss= 3.89529 val_acc= 0.67075 time= 0.13000
Epoch: 0003 train_loss= 3.89489 train_acc= 0.65385 val_loss= 3.84273 val_acc= 0.67228 time= 0.15797
Epoch: 0004 train_loss= 3.84376 train_acc= 0.65385 val_loss= 3.77136 val_acc= 0.65391 time= 0.13400
Epoch: 0005 train_loss= 3.77454 train_acc= 0.63718 val_loss= 3.68029 val_acc= 0.63247 time= 0.13600
Epoch: 0006 train_loss= 3.68389 train_acc= 0.61949 val_loss= 3.56979 val_acc= 0.58959 time= 0.14817
Epoch: 0007 train_loss= 3.57925 train_acc= 0.57170 val_loss= 3.44117 val_acc= 0.55130 time= 0.13108
Epoch: 0008 train_loss= 3.43900 train_acc= 0.52526 val_loss= 3.29711 val_acc= 0.50230 time= 0.13805
Epoch: 0009 train_loss= 3.30713 train_acc= 0.50094 val_loss= 3.14188 val_acc= 0.48851 time= 0.13565
Epoch: 0010 train_loss= 3.16236 train_acc= 0.48393 val_loss= 2.98162 val_acc= 0.46861 time= 0.13096
Epoch: 0011 train_loss= 2.98657 train_acc= 0.45875 val_loss= 2.82381 val_acc= 0.46248 time= 0.13200
Epoch: 0012 train_loss= 2.82403 train_acc= 0.45841 val_loss= 2.67559 val_acc= 0.45942 time= 0.14804
Epoch: 0013 train_loss= 2.67985 train_acc= 0.44787 val_loss= 2.54472 val_acc= 0.45789 time= 0.13700
Epoch: 0014 train_loss= 2.54542 train_acc= 0.44378 val_loss= 2.43766 val_acc= 0.45636 time= 0.13452
Epoch: 0015 train_loss= 2.44003 train_acc= 0.45348 val_loss= 2.35735 val_acc= 0.45636 time= 0.15500
Epoch: 0016 train_loss= 2.35669 train_acc= 0.43783 val_loss= 2.30131 val_acc= 0.45636 time= 0.13403
Epoch: 0017 train_loss= 2.31423 train_acc= 0.43562 val_loss= 2.26290 val_acc= 0.45636 time= 0.13400
Epoch: 0018 train_loss= 2.24140 train_acc= 0.43307 val_loss= 2.23337 val_acc= 0.45636 time= 0.14505
Epoch: 0019 train_loss= 2.23778 train_acc= 0.43239 val_loss= 2.20569 val_acc= 0.45636 time= 0.13199
Epoch: 0020 train_loss= 2.22409 train_acc= 0.43239 val_loss= 2.17505 val_acc= 0.45636 time= 0.14200
Epoch: 0021 train_loss= 2.17894 train_acc= 0.43239 val_loss= 2.13877 val_acc= 0.45636 time= 0.13001
Epoch: 0022 train_loss= 2.14703 train_acc= 0.43239 val_loss= 2.09657 val_acc= 0.45636 time= 0.13000
Epoch: 0023 train_loss= 2.12020 train_acc= 0.43239 val_loss= 2.05004 val_acc= 0.45636 time= 0.15398
Epoch: 0024 train_loss= 2.05863 train_acc= 0.43239 val_loss= 2.00071 val_acc= 0.45636 time= 0.13000
Epoch: 0025 train_loss= 2.02304 train_acc= 0.43256 val_loss= 1.95123 val_acc= 0.45636 time= 0.13200
Epoch: 0026 train_loss= 1.97032 train_acc= 0.43239 val_loss= 1.90385 val_acc= 0.45636 time= 0.15792
Epoch: 0027 train_loss= 1.92474 train_acc= 0.43273 val_loss= 1.85975 val_acc= 0.45636 time= 0.14208
Epoch: 0028 train_loss= 1.88813 train_acc= 0.43341 val_loss= 1.81912 val_acc= 0.45789 time= 0.13390
Epoch: 0029 train_loss= 1.84098 train_acc= 0.43630 val_loss= 1.78174 val_acc= 0.48086 time= 0.13397
Epoch: 0030 train_loss= 1.78601 train_acc= 0.46232 val_loss= 1.74656 val_acc= 0.51608 time= 0.13803
Epoch: 0031 train_loss= 1.76531 train_acc= 0.50060 val_loss= 1.71258 val_acc= 0.55896 time= 0.13101
Epoch: 0032 train_loss= 1.73137 train_acc= 0.53632 val_loss= 1.67893 val_acc= 0.61409 time= 0.13100
Epoch: 0033 train_loss= 1.69705 train_acc= 0.59262 val_loss= 1.64532 val_acc= 0.64012 time= 0.15199
Epoch: 0034 train_loss= 1.68241 train_acc= 0.61303 val_loss= 1.61157 val_acc= 0.66003 time= 0.13300
Epoch: 0035 train_loss= 1.63246 train_acc= 0.63837 val_loss= 1.57837 val_acc= 0.66309 time= 0.13206
Epoch: 0036 train_loss= 1.60688 train_acc= 0.62919 val_loss= 1.54611 val_acc= 0.67381 time= 0.12955
Epoch: 0037 train_loss= 1.57977 train_acc= 0.63905 val_loss= 1.51500 val_acc= 0.67534 time= 0.13597
Epoch: 0038 train_loss= 1.55159 train_acc= 0.64960 val_loss= 1.48513 val_acc= 0.67534 time= 0.15300
Epoch: 0039 train_loss= 1.51228 train_acc= 0.64688 val_loss= 1.45689 val_acc= 0.67688 time= 0.13500
Epoch: 0040 train_loss= 1.48318 train_acc= 0.64756 val_loss= 1.43016 val_acc= 0.67688 time= 0.13601
Epoch: 0041 train_loss= 1.45702 train_acc= 0.65470 val_loss= 1.40466 val_acc= 0.67841 time= 0.15003
Epoch: 0042 train_loss= 1.42488 train_acc= 0.66406 val_loss= 1.38008 val_acc= 0.68300 time= 0.13808
Epoch: 0043 train_loss= 1.40988 train_acc= 0.66134 val_loss= 1.35618 val_acc= 0.68760 time= 0.13096
Epoch: 0044 train_loss= 1.36637 train_acc= 0.67716 val_loss= 1.33281 val_acc= 0.68913 time= 0.15504
Epoch: 0045 train_loss= 1.35412 train_acc= 0.67256 val_loss= 1.30982 val_acc= 0.69832 time= 0.13001
Epoch: 0046 train_loss= 1.33016 train_acc= 0.68260 val_loss= 1.28727 val_acc= 0.70444 time= 0.13101
Epoch: 0047 train_loss= 1.31340 train_acc= 0.68770 val_loss= 1.26523 val_acc= 0.70750 time= 0.15329
Epoch: 0048 train_loss= 1.28711 train_acc= 0.69927 val_loss= 1.24364 val_acc= 0.71210 time= 0.13400
Epoch: 0049 train_loss= 1.24832 train_acc= 0.71169 val_loss= 1.22245 val_acc= 0.71669 time= 0.14801
Epoch: 0050 train_loss= 1.24785 train_acc= 0.70607 val_loss= 1.20172 val_acc= 0.71975 time= 0.13400
Epoch: 0051 train_loss= 1.22797 train_acc= 0.71373 val_loss= 1.18144 val_acc= 0.72282 time= 0.13600
Epoch: 0052 train_loss= 1.19480 train_acc= 0.72461 val_loss= 1.16159 val_acc= 0.72741 time= 0.15004
Epoch: 0053 train_loss= 1.18174 train_acc= 0.72291 val_loss= 1.14211 val_acc= 0.73354 time= 0.13200
Epoch: 0054 train_loss= 1.16252 train_acc= 0.73091 val_loss= 1.12287 val_acc= 0.73507 time= 0.13400
Epoch: 0055 train_loss= 1.13616 train_acc= 0.73210 val_loss= 1.10401 val_acc= 0.73966 time= 0.14900
Epoch: 0056 train_loss= 1.12374 train_acc= 0.74315 val_loss= 1.08551 val_acc= 0.74273 time= 0.13307
Epoch: 0057 train_loss= 1.09652 train_acc= 0.74383 val_loss= 1.06740 val_acc= 0.74885 time= 0.13000
Epoch: 0058 train_loss= 1.08320 train_acc= 0.75234 val_loss= 1.04968 val_acc= 0.75804 time= 0.13400
Epoch: 0059 train_loss= 1.05708 train_acc= 0.75710 val_loss= 1.03228 val_acc= 0.76570 time= 0.15528
Epoch: 0060 train_loss= 1.04407 train_acc= 0.76305 val_loss= 1.01517 val_acc= 0.77335 time= 0.13400
Epoch: 0061 train_loss= 1.03820 train_acc= 0.77037 val_loss= 0.99848 val_acc= 0.77642 time= 0.13600
Epoch: 0062 train_loss= 1.03065 train_acc= 0.76901 val_loss= 0.98229 val_acc= 0.78407 time= 0.15300
Epoch: 0063 train_loss= 1.00305 train_acc= 0.77598 val_loss= 0.96659 val_acc= 0.78714 time= 0.14403
Epoch: 0064 train_loss= 0.98729 train_acc= 0.79010 val_loss= 0.95134 val_acc= 0.79632 time= 0.13097
Epoch: 0065 train_loss= 0.96316 train_acc= 0.78585 val_loss= 0.93641 val_acc= 0.79939 time= 0.13100
Epoch: 0066 train_loss= 0.95293 train_acc= 0.78925 val_loss= 0.92183 val_acc= 0.79939 time= 0.15100
Epoch: 0067 train_loss= 0.93422 train_acc= 0.79384 val_loss= 0.90762 val_acc= 0.80092 time= 0.13018
Epoch: 0068 train_loss= 0.92115 train_acc= 0.80048 val_loss= 0.89373 val_acc= 0.80245 time= 0.13004
Epoch: 0069 train_loss= 0.91521 train_acc= 0.79707 val_loss= 0.88012 val_acc= 0.80398 time= 0.15210
Epoch: 0070 train_loss= 0.89729 train_acc= 0.79248 val_loss= 0.86680 val_acc= 0.80551 time= 0.14532
Epoch: 0071 train_loss= 0.87877 train_acc= 0.79775 val_loss= 0.85371 val_acc= 0.80704 time= 0.13478
Epoch: 0072 train_loss= 0.87015 train_acc= 0.80711 val_loss= 0.84083 val_acc= 0.81011 time= 0.13357
Epoch: 0073 train_loss= 0.85033 train_acc= 0.80881 val_loss= 0.82818 val_acc= 0.81164 time= 0.14512
Epoch: 0074 train_loss= 0.84776 train_acc= 0.80660 val_loss= 0.81579 val_acc= 0.81470 time= 0.13200
Epoch: 0075 train_loss= 0.82786 train_acc= 0.81068 val_loss= 0.80372 val_acc= 0.81776 time= 0.13100
Epoch: 0076 train_loss= 0.81628 train_acc= 0.80983 val_loss= 0.79177 val_acc= 0.81930 time= 0.15101
Epoch: 0077 train_loss= 0.81500 train_acc= 0.81749 val_loss= 0.78029 val_acc= 0.82389 time= 0.13000
Epoch: 0078 train_loss= 0.79536 train_acc= 0.82191 val_loss= 0.76899 val_acc= 0.82848 time= 0.14200
Epoch: 0079 train_loss= 0.77009 train_acc= 0.83586 val_loss= 0.75783 val_acc= 0.83155 time= 0.13100
Epoch: 0080 train_loss= 0.77522 train_acc= 0.82446 val_loss= 0.74707 val_acc= 0.83308 time= 0.13200
Epoch: 0081 train_loss= 0.76091 train_acc= 0.83348 val_loss= 0.73672 val_acc= 0.83461 time= 0.14900
Epoch: 0082 train_loss= 0.75977 train_acc= 0.82463 val_loss= 0.72623 val_acc= 0.83767 time= 0.13477
Epoch: 0083 train_loss= 0.74202 train_acc= 0.83194 val_loss= 0.71570 val_acc= 0.83767 time= 0.13558
Epoch: 0084 train_loss= 0.73468 train_acc= 0.83143 val_loss= 0.70553 val_acc= 0.83767 time= 0.15299
Epoch: 0085 train_loss= 0.71938 train_acc= 0.83416 val_loss= 0.69567 val_acc= 0.83614 time= 0.14101
Epoch: 0086 train_loss= 0.70556 train_acc= 0.83569 val_loss= 0.68615 val_acc= 0.83767 time= 0.13200
Epoch: 0087 train_loss= 0.69467 train_acc= 0.83671 val_loss= 0.67666 val_acc= 0.83920 time= 0.13297
Epoch: 0088 train_loss= 0.68389 train_acc= 0.84181 val_loss= 0.66723 val_acc= 0.84686 time= 0.14902
Epoch: 0089 train_loss= 0.67453 train_acc= 0.84691 val_loss= 0.65807 val_acc= 0.84839 time= 0.13200
Epoch: 0090 train_loss= 0.66868 train_acc= 0.84181 val_loss= 0.64931 val_acc= 0.84839 time= 0.13400
Epoch: 0091 train_loss= 0.65713 train_acc= 0.84147 val_loss= 0.64073 val_acc= 0.84839 time= 0.14800
Epoch: 0092 train_loss= 0.64591 train_acc= 0.84589 val_loss= 0.63224 val_acc= 0.84992 time= 0.13400
Epoch: 0093 train_loss= 0.64880 train_acc= 0.84708 val_loss= 0.62418 val_acc= 0.85145 time= 0.13270
Epoch: 0094 train_loss= 0.61697 train_acc= 0.85185 val_loss= 0.61634 val_acc= 0.85452 time= 0.13407
Epoch: 0095 train_loss= 0.62313 train_acc= 0.85576 val_loss= 0.60865 val_acc= 0.85758 time= 0.15700
Epoch: 0096 train_loss= 0.60691 train_acc= 0.85542 val_loss= 0.60107 val_acc= 0.85911 time= 0.13297
Epoch: 0097 train_loss= 0.60487 train_acc= 0.85899 val_loss= 0.59355 val_acc= 0.85911 time= 0.13403
Epoch: 0098 train_loss= 0.59638 train_acc= 0.85695 val_loss= 0.58593 val_acc= 0.85911 time= 0.15000
Epoch: 0099 train_loss= 0.59361 train_acc= 0.86239 val_loss= 0.57812 val_acc= 0.86524 time= 0.13900
Epoch: 0100 train_loss= 0.59225 train_acc= 0.86120 val_loss= 0.57028 val_acc= 0.86983 time= 0.12915
Epoch: 0101 train_loss= 0.58336 train_acc= 0.86358 val_loss= 0.56242 val_acc= 0.87136 time= 0.13196
Epoch: 0102 train_loss= 0.56792 train_acc= 0.86596 val_loss= 0.55485 val_acc= 0.87289 time= 0.15200
Epoch: 0103 train_loss= 0.54592 train_acc= 0.86971 val_loss= 0.54766 val_acc= 0.87289 time= 0.13095
Epoch: 0104 train_loss= 0.54424 train_acc= 0.87158 val_loss= 0.54030 val_acc= 0.87443 time= 0.13300
Epoch: 0105 train_loss= 0.54626 train_acc= 0.86545 val_loss= 0.53304 val_acc= 0.87902 time= 0.15558
Epoch: 0106 train_loss= 0.53117 train_acc= 0.87481 val_loss= 0.52601 val_acc= 0.87902 time= 0.14361
Epoch: 0107 train_loss= 0.52150 train_acc= 0.87940 val_loss= 0.51939 val_acc= 0.88055 time= 0.13300
Epoch: 0108 train_loss= 0.52404 train_acc= 0.87787 val_loss= 0.51313 val_acc= 0.88515 time= 0.13301
Epoch: 0109 train_loss= 0.50426 train_acc= 0.88569 val_loss= 0.50765 val_acc= 0.88515 time= 0.15203
Epoch: 0110 train_loss= 0.50688 train_acc= 0.87787 val_loss= 0.50216 val_acc= 0.88515 time= 0.13101
Epoch: 0111 train_loss= 0.50957 train_acc= 0.88178 val_loss= 0.49650 val_acc= 0.88821 time= 0.13200
Epoch: 0112 train_loss= 0.49738 train_acc= 0.88791 val_loss= 0.49059 val_acc= 0.88974 time= 0.15100
Epoch: 0113 train_loss= 0.48695 train_acc= 0.88638 val_loss= 0.48455 val_acc= 0.89127 time= 0.13104
Epoch: 0114 train_loss= 0.47454 train_acc= 0.88757 val_loss= 0.47815 val_acc= 0.89280 time= 0.14203
Epoch: 0115 train_loss= 0.48133 train_acc= 0.88808 val_loss= 0.47227 val_acc= 0.89433 time= 0.13297
Epoch: 0116 train_loss= 0.47353 train_acc= 0.88893 val_loss= 0.46713 val_acc= 0.89433 time= 0.13500
Epoch: 0117 train_loss= 0.45791 train_acc= 0.89199 val_loss= 0.46248 val_acc= 0.89127 time= 0.15300
Epoch: 0118 train_loss= 0.47031 train_acc= 0.88791 val_loss= 0.45791 val_acc= 0.88974 time= 0.13400
Epoch: 0119 train_loss= 0.44935 train_acc= 0.89794 val_loss= 0.45323 val_acc= 0.89127 time= 0.13600
Epoch: 0120 train_loss= 0.45072 train_acc= 0.89573 val_loss= 0.44857 val_acc= 0.89127 time= 0.15903
Epoch: 0121 train_loss= 0.44452 train_acc= 0.89556 val_loss= 0.44413 val_acc= 0.89127 time= 0.14397
Epoch: 0122 train_loss= 0.44553 train_acc= 0.89760 val_loss= 0.44028 val_acc= 0.89433 time= 0.13200
Epoch: 0123 train_loss= 0.43971 train_acc= 0.89454 val_loss= 0.43713 val_acc= 0.89587 time= 0.15300
Epoch: 0124 train_loss= 0.41955 train_acc= 0.90185 val_loss= 0.43383 val_acc= 0.89740 time= 0.13000
Epoch: 0125 train_loss= 0.41684 train_acc= 0.90645 val_loss= 0.43042 val_acc= 0.89740 time= 0.13307
Epoch: 0126 train_loss= 0.42093 train_acc= 0.90066 val_loss= 0.42659 val_acc= 0.89893 time= 0.15004
Epoch: 0127 train_loss= 0.41015 train_acc= 0.90100 val_loss= 0.42252 val_acc= 0.89893 time= 0.13196
Epoch: 0128 train_loss= 0.41122 train_acc= 0.90458 val_loss= 0.41801 val_acc= 0.90046 time= 0.14900
Epoch: 0129 train_loss= 0.40301 train_acc= 0.90815 val_loss= 0.41345 val_acc= 0.90046 time= 0.13352
Epoch: 0130 train_loss= 0.40309 train_acc= 0.90270 val_loss= 0.40900 val_acc= 0.90199 time= 0.13700
Epoch: 0131 train_loss= 0.40012 train_acc= 0.91002 val_loss= 0.40488 val_acc= 0.90199 time= 0.15200
Epoch: 0132 train_loss= 0.39040 train_acc= 0.90866 val_loss= 0.40132 val_acc= 0.90199 time= 0.13203
Epoch: 0133 train_loss= 0.37790 train_acc= 0.91325 val_loss= 0.39787 val_acc= 0.90199 time= 0.15001
Epoch: 0134 train_loss= 0.38608 train_acc= 0.91223 val_loss= 0.39476 val_acc= 0.90046 time= 0.12996
Epoch: 0135 train_loss= 0.38802 train_acc= 0.90849 val_loss= 0.39156 val_acc= 0.90199 time= 0.13503
Epoch: 0136 train_loss= 0.37949 train_acc= 0.91104 val_loss= 0.38831 val_acc= 0.90046 time= 0.12900
Epoch: 0137 train_loss= 0.36550 train_acc= 0.91733 val_loss= 0.38513 val_acc= 0.89893 time= 0.13097
Epoch: 0138 train_loss= 0.36859 train_acc= 0.91563 val_loss= 0.38213 val_acc= 0.89740 time= 0.15200
Epoch: 0139 train_loss= 0.36252 train_acc= 0.91716 val_loss= 0.37920 val_acc= 0.89740 time= 0.13500
Epoch: 0140 train_loss= 0.35172 train_acc= 0.91954 val_loss= 0.37627 val_acc= 0.89893 time= 0.13400
Epoch: 0141 train_loss= 0.35435 train_acc= 0.91699 val_loss= 0.37354 val_acc= 0.90505 time= 0.15600
Epoch: 0142 train_loss= 0.34958 train_acc= 0.91835 val_loss= 0.37122 val_acc= 0.90505 time= 0.14004
Epoch: 0143 train_loss= 0.34990 train_acc= 0.91529 val_loss= 0.36921 val_acc= 0.90505 time= 0.13099
Epoch: 0144 train_loss= 0.34768 train_acc= 0.92005 val_loss= 0.36719 val_acc= 0.90352 time= 0.12900
Epoch: 0145 train_loss= 0.34237 train_acc= 0.91886 val_loss= 0.36480 val_acc= 0.90352 time= 0.15197
Epoch: 0146 train_loss= 0.34653 train_acc= 0.91393 val_loss= 0.36238 val_acc= 0.90505 time= 0.13103
Epoch: 0147 train_loss= 0.33473 train_acc= 0.92193 val_loss= 0.36008 val_acc= 0.90505 time= 0.13097
Epoch: 0148 train_loss= 0.33875 train_acc= 0.92295 val_loss= 0.35755 val_acc= 0.90352 time= 0.15103
Epoch: 0149 train_loss= 0.32097 train_acc= 0.92550 val_loss= 0.35509 val_acc= 0.90352 time= 0.13904
Epoch: 0150 train_loss= 0.32562 train_acc= 0.92771 val_loss= 0.35270 val_acc= 0.90505 time= 0.13288
Epoch: 0151 train_loss= 0.32909 train_acc= 0.92618 val_loss= 0.35053 val_acc= 0.90352 time= 0.13400
Epoch: 0152 train_loss= 0.31586 train_acc= 0.92516 val_loss= 0.34824 val_acc= 0.90505 time= 0.15557
Epoch: 0153 train_loss= 0.32398 train_acc= 0.92397 val_loss= 0.34625 val_acc= 0.90658 time= 0.13200
Epoch: 0154 train_loss= 0.32779 train_acc= 0.92022 val_loss= 0.34399 val_acc= 0.90505 time= 0.13100
Epoch: 0155 train_loss= 0.30533 train_acc= 0.93009 val_loss= 0.34155 val_acc= 0.90658 time= 0.15100
Epoch: 0156 train_loss= 0.30458 train_acc= 0.92873 val_loss= 0.33925 val_acc= 0.90812 time= 0.13000
Epoch: 0157 train_loss= 0.30602 train_acc= 0.92516 val_loss= 0.33699 val_acc= 0.90658 time= 0.14500
Epoch: 0158 train_loss= 0.30364 train_acc= 0.92533 val_loss= 0.33469 val_acc= 0.90812 time= 0.13207
Epoch: 0159 train_loss= 0.30679 train_acc= 0.92329 val_loss= 0.33248 val_acc= 0.90812 time= 0.15297
Epoch: 0160 train_loss= 0.29624 train_acc= 0.93332 val_loss= 0.33074 val_acc= 0.90965 time= 0.13400
Epoch: 0161 train_loss= 0.29730 train_acc= 0.93315 val_loss= 0.32924 val_acc= 0.90812 time= 0.13400
Epoch: 0162 train_loss= 0.29676 train_acc= 0.92703 val_loss= 0.32822 val_acc= 0.90812 time= 0.15352
Epoch: 0163 train_loss= 0.27396 train_acc= 0.93928 val_loss= 0.32708 val_acc= 0.90658 time= 0.13400
Epoch: 0164 train_loss= 0.29340 train_acc= 0.92720 val_loss= 0.32580 val_acc= 0.90812 time= 0.14551
Epoch: 0165 train_loss= 0.28841 train_acc= 0.93247 val_loss= 0.32471 val_acc= 0.91271 time= 0.13105
Epoch: 0166 train_loss= 0.28004 train_acc= 0.93247 val_loss= 0.32357 val_acc= 0.91424 time= 0.13398
Epoch: 0167 train_loss= 0.27571 train_acc= 0.93859 val_loss= 0.32163 val_acc= 0.91577 time= 0.14697
Epoch: 0168 train_loss= 0.28432 train_acc= 0.93315 val_loss= 0.32018 val_acc= 0.91577 time= 0.13103
Epoch: 0169 train_loss= 0.27253 train_acc= 0.93400 val_loss= 0.31805 val_acc= 0.91271 time= 0.13500
Epoch: 0170 train_loss= 0.27070 train_acc= 0.93894 val_loss= 0.31646 val_acc= 0.91271 time= 0.14901
Epoch: 0171 train_loss= 0.27000 train_acc= 0.93689 val_loss= 0.31545 val_acc= 0.91271 time= 0.13100
Epoch: 0172 train_loss= 0.27163 train_acc= 0.93655 val_loss= 0.31389 val_acc= 0.91271 time= 0.13460
Epoch: 0173 train_loss= 0.26700 train_acc= 0.93842 val_loss= 0.31212 val_acc= 0.91271 time= 0.13616
Epoch: 0174 train_loss= 0.24987 train_acc= 0.94404 val_loss= 0.31011 val_acc= 0.91271 time= 0.15243
Epoch: 0175 train_loss= 0.26977 train_acc= 0.93706 val_loss= 0.30888 val_acc= 0.91118 time= 0.13500
Epoch: 0176 train_loss= 0.26496 train_acc= 0.93519 val_loss= 0.30751 val_acc= 0.91118 time= 0.13500
Epoch: 0177 train_loss= 0.24560 train_acc= 0.94455 val_loss= 0.30582 val_acc= 0.91271 time= 0.14800
Epoch: 0178 train_loss= 0.25091 train_acc= 0.94557 val_loss= 0.30432 val_acc= 0.91424 time= 0.14300
Epoch: 0179 train_loss= 0.25080 train_acc= 0.94030 val_loss= 0.30319 val_acc= 0.91424 time= 0.13000
Epoch: 0180 train_loss= 0.23582 train_acc= 0.94982 val_loss= 0.30186 val_acc= 0.91271 time= 0.13200
Epoch: 0181 train_loss= 0.24537 train_acc= 0.94251 val_loss= 0.30084 val_acc= 0.91271 time= 0.15200
Epoch: 0182 train_loss= 0.24618 train_acc= 0.94234 val_loss= 0.29986 val_acc= 0.91577 time= 0.13214
Epoch: 0183 train_loss= 0.24767 train_acc= 0.93894 val_loss= 0.29798 val_acc= 0.91577 time= 0.13418
Epoch: 0184 train_loss= 0.25371 train_acc= 0.94336 val_loss= 0.29574 val_acc= 0.91577 time= 0.15500
Epoch: 0185 train_loss= 0.24351 train_acc= 0.94404 val_loss= 0.29424 val_acc= 0.91577 time= 0.14200
Epoch: 0186 train_loss= 0.24590 train_acc= 0.93621 val_loss= 0.29287 val_acc= 0.91884 time= 0.13400
Epoch: 0187 train_loss= 0.24030 train_acc= 0.94234 val_loss= 0.29191 val_acc= 0.91730 time= 0.13200
Epoch: 0188 train_loss= 0.22845 train_acc= 0.94387 val_loss= 0.29108 val_acc= 0.91884 time= 0.14000
Epoch: 0189 train_loss= 0.23350 train_acc= 0.94489 val_loss= 0.29081 val_acc= 0.91884 time= 0.13100
Epoch: 0190 train_loss= 0.23408 train_acc= 0.94591 val_loss= 0.29047 val_acc= 0.92037 time= 0.13065
Epoch: 0191 train_loss= 0.22999 train_acc= 0.94591 val_loss= 0.29020 val_acc= 0.92190 time= 0.15600
Epoch: 0192 train_loss= 0.22830 train_acc= 0.94574 val_loss= 0.28975 val_acc= 0.92190 time= 0.13000
Epoch: 0193 train_loss= 0.21620 train_acc= 0.95033 val_loss= 0.28887 val_acc= 0.92343 time= 0.14196
Epoch: 0194 train_loss= 0.22508 train_acc= 0.94472 val_loss= 0.28724 val_acc= 0.92190 time= 0.13300
Epoch: 0195 train_loss= 0.22490 train_acc= 0.94659 val_loss= 0.28581 val_acc= 0.92037 time= 0.13504
Epoch: 0196 train_loss= 0.21563 train_acc= 0.95203 val_loss= 0.28394 val_acc= 0.91577 time= 0.15100
Epoch: 0197 train_loss= 0.22769 train_acc= 0.94812 val_loss= 0.28252 val_acc= 0.91577 time= 0.13304
Epoch: 0198 train_loss= 0.20235 train_acc= 0.95356 val_loss= 0.28105 val_acc= 0.91577 time= 0.13404
Epoch: 0199 train_loss= 0.20582 train_acc= 0.95033 val_loss= 0.27886 val_acc= 0.91577 time= 0.14805
Epoch: 0200 train_loss= 0.21056 train_acc= 0.95084 val_loss= 0.27685 val_acc= 0.91884 time= 0.13900
Epoch: 0201 train_loss= 0.20939 train_acc= 0.94812 val_loss= 0.27583 val_acc= 0.91730 time= 0.13000
Epoch: 0202 train_loss= 0.20645 train_acc= 0.95050 val_loss= 0.27495 val_acc= 0.91884 time= 0.13400
Epoch: 0203 train_loss= 0.21263 train_acc= 0.95458 val_loss= 0.27429 val_acc= 0.91884 time= 0.14108
Epoch: 0204 train_loss= 0.20819 train_acc= 0.94557 val_loss= 0.27379 val_acc= 0.92037 time= 0.13200
Epoch: 0205 train_loss= 0.20267 train_acc= 0.95458 val_loss= 0.27371 val_acc= 0.92343 time= 0.13482
Epoch: 0206 train_loss= 0.19982 train_acc= 0.95407 val_loss= 0.27421 val_acc= 0.92343 time= 0.15399
Epoch: 0207 train_loss= 0.20065 train_acc= 0.95560 val_loss= 0.27541 val_acc= 0.92802 time= 0.13500
Epoch: 0208 train_loss= 0.19992 train_acc= 0.94948 val_loss= 0.27649 val_acc= 0.92496 time= 0.13258
Early stopping...
Optimization Finished!
Test set results: cost= 0.32781 accuracy= 0.91822 time= 0.06300
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.2500    0.4000         8
           1     0.3333    0.3333    0.3333         6
           2     0.0000    0.0000    0.0000         1
           3     0.7527    0.9333    0.8333        75
           4     1.0000    1.0000    1.0000         9
           5     0.8511    0.9195    0.8840        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     1.0000    0.6364    0.7778        11
           9     1.0000    0.2222    0.3636         9
          10     0.9565    0.6111    0.7458        36
          11     1.0000    0.9167    0.9565        12
          12     0.7707    1.0000    0.8705       121
          13     0.6500    0.6842    0.6667        19
          14     0.7500    0.8571    0.8000        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.8333    0.5000    0.6250        10
          19     1.0000    1.0000    1.0000         2
          20     0.8000    0.4444    0.5714         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.5000    0.6471    0.5641        17
          25     0.8000    0.8000    0.8000        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.4167    0.5882        12
          28     0.6364    0.6364    0.6364        11
          29     0.9683    0.9641    0.9662       696
          30     0.9565    1.0000    0.9778        22
          31     0.0000    0.0000    0.0000         3
          32     0.5294    0.9000    0.6667        10
          33     0.0000    0.0000    0.0000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8718    0.8395    0.8553        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9755    0.9917    0.9835      1083
          40     0.0000    0.0000    0.0000         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.6000    0.7500    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.5833    0.9333    0.7179        15
          48     0.8889    0.8889    0.8889         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9182      2568
   macro avg     0.5756    0.4969    0.5059      2568
weighted avg     0.9114    0.9182    0.9082      2568

Macro average Test Precision, Recall and F1-Score...
(0.5755500353306509, 0.49693312764363606, 0.505912821816054, None)
Micro average Test Precision, Recall and F1-Score...
(0.9182242990654206, 0.9182242990654206, 0.9182242990654206, None)
embeddings:
8892 6532 2568
[[ 0.29552487  3.5476534   0.04479244 ... -0.04025282  1.7984984
  -0.03184604]
 [ 0.2547325   1.4037671   0.7215422  ...  0.21301444  0.5653449
   0.01681523]
 [ 0.03422674  1.4064858   0.3292543  ...  0.00764499  0.7202119
   0.1667384 ]
 ...
 [ 0.02438795  0.54924446  0.45833343 ...  0.14416651  0.66793543
   0.05899362]
 [ 0.14022191  0.68942034  0.25383732 ...  0.06151605  0.41413787
   0.14941388]
 [ 0.5306869   0.5568804   0.644347   ...  0.4664471   0.30678543
   0.52814615]]

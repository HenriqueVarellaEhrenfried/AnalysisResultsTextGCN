(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.00527 val_loss= 3.90115 val_acc= 0.67075 time= 0.44206
Epoch: 0002 train_loss= 3.90118 train_acc= 0.65845 val_loss= 3.80566 val_acc= 0.67534 time= 0.19800
Epoch: 0003 train_loss= 3.80688 train_acc= 0.65504 val_loss= 3.65662 val_acc= 0.67075 time= 0.17000
Epoch: 0004 train_loss= 3.65822 train_acc= 0.64977 val_loss= 3.45334 val_acc= 0.67228 time= 0.17000
Epoch: 0005 train_loss= 3.45677 train_acc= 0.64688 val_loss= 3.20460 val_acc= 0.66922 time= 0.16900
Epoch: 0006 train_loss= 3.20408 train_acc= 0.64484 val_loss= 2.93267 val_acc= 0.66922 time= 0.17857
Epoch: 0007 train_loss= 2.93413 train_acc= 0.64297 val_loss= 2.66765 val_acc= 0.66769 time= 0.16907
Epoch: 0008 train_loss= 2.67153 train_acc= 0.64501 val_loss= 2.44587 val_acc= 0.66769 time= 0.16699
Epoch: 0009 train_loss= 2.43988 train_acc= 0.64603 val_loss= 2.29653 val_acc= 0.67075 time= 0.19000
Epoch: 0010 train_loss= 2.29092 train_acc= 0.64773 val_loss= 2.21305 val_acc= 0.63093 time= 0.16700
Epoch: 0011 train_loss= 2.21215 train_acc= 0.61898 val_loss= 2.16146 val_acc= 0.51761 time= 0.16997
Epoch: 0012 train_loss= 2.16344 train_acc= 0.50672 val_loss= 2.11200 val_acc= 0.47473 time= 0.19600
Epoch: 0013 train_loss= 2.12023 train_acc= 0.44753 val_loss= 2.04871 val_acc= 0.46554 time= 0.17000
Epoch: 0014 train_loss= 2.06223 train_acc= 0.43800 val_loss= 1.96708 val_acc= 0.46861 time= 0.17903
Epoch: 0015 train_loss= 1.98575 train_acc= 0.44106 val_loss= 1.87332 val_acc= 0.48086 time= 0.16700
Epoch: 0016 train_loss= 1.89726 train_acc= 0.45688 val_loss= 1.78052 val_acc= 0.52067 time= 0.16817
Epoch: 0017 train_loss= 1.79966 train_acc= 0.51318 val_loss= 1.70152 val_acc= 0.60184 time= 0.19109
Epoch: 0018 train_loss= 1.72444 train_acc= 0.60129 val_loss= 1.63850 val_acc= 0.66309 time= 0.16703
Epoch: 0019 train_loss= 1.66632 train_acc= 0.65504 val_loss= 1.58281 val_acc= 0.69219 time= 0.16800
Epoch: 0020 train_loss= 1.60722 train_acc= 0.67648 val_loss= 1.52644 val_acc= 0.71057 time= 0.19400
Epoch: 0021 train_loss= 1.54895 train_acc= 0.69280 val_loss= 1.46790 val_acc= 0.70444 time= 0.17100
Epoch: 0022 train_loss= 1.48970 train_acc= 0.69655 val_loss= 1.41005 val_acc= 0.70597 time= 0.16800
Epoch: 0023 train_loss= 1.43237 train_acc= 0.69944 val_loss= 1.35592 val_acc= 0.70750 time= 0.16900
Epoch: 0024 train_loss= 1.37732 train_acc= 0.70182 val_loss= 1.30718 val_acc= 0.70904 time= 0.18800
Epoch: 0025 train_loss= 1.32830 train_acc= 0.70556 val_loss= 1.26382 val_acc= 0.71210 time= 0.16700
Epoch: 0026 train_loss= 1.28564 train_acc= 0.71203 val_loss= 1.22501 val_acc= 0.72282 time= 0.18500
Epoch: 0027 train_loss= 1.24705 train_acc= 0.72189 val_loss= 1.18971 val_acc= 0.73201 time= 0.16600
Epoch: 0028 train_loss= 1.21097 train_acc= 0.73159 val_loss= 1.15691 val_acc= 0.73813 time= 0.17030
Epoch: 0029 train_loss= 1.17681 train_acc= 0.74145 val_loss= 1.12578 val_acc= 0.74732 time= 0.19300
Epoch: 0030 train_loss= 1.14402 train_acc= 0.75149 val_loss= 1.09564 val_acc= 0.75498 time= 0.17000
Epoch: 0031 train_loss= 1.11199 train_acc= 0.76016 val_loss= 1.06600 val_acc= 0.76110 time= 0.17003
Epoch: 0032 train_loss= 1.08057 train_acc= 0.76867 val_loss= 1.03664 val_acc= 0.77335 time= 0.17428
Epoch: 0033 train_loss= 1.04931 train_acc= 0.77513 val_loss= 1.00757 val_acc= 0.77795 time= 0.16700
Epoch: 0034 train_loss= 1.01830 train_acc= 0.78432 val_loss= 0.97906 val_acc= 0.78407 time= 0.16604
Epoch: 0035 train_loss= 0.98800 train_acc= 0.79350 val_loss= 0.95142 val_acc= 0.79479 time= 0.19203
Epoch: 0036 train_loss= 0.95909 train_acc= 0.80235 val_loss= 0.92482 val_acc= 0.81164 time= 0.16797
Epoch: 0037 train_loss= 0.93201 train_acc= 0.81085 val_loss= 0.89929 val_acc= 0.81623 time= 0.18800
Epoch: 0038 train_loss= 0.90452 train_acc= 0.81732 val_loss= 0.87471 val_acc= 0.82236 time= 0.17031
Epoch: 0039 train_loss= 0.88325 train_acc= 0.82242 val_loss= 0.85088 val_acc= 0.82848 time= 0.17000
Epoch: 0040 train_loss= 0.85742 train_acc= 0.82803 val_loss= 0.82759 val_acc= 0.83155 time= 0.16903
Epoch: 0041 train_loss= 0.83173 train_acc= 0.83348 val_loss= 0.80473 val_acc= 0.83308 time= 0.18900
Epoch: 0042 train_loss= 0.80795 train_acc= 0.83875 val_loss= 0.78228 val_acc= 0.83767 time= 0.16800
Epoch: 0043 train_loss= 0.78358 train_acc= 0.84079 val_loss= 0.76029 val_acc= 0.83920 time= 0.16700
Epoch: 0044 train_loss= 0.76141 train_acc= 0.84453 val_loss= 0.73878 val_acc= 0.84074 time= 0.16597
Epoch: 0045 train_loss= 0.73374 train_acc= 0.84793 val_loss= 0.71770 val_acc= 0.84380 time= 0.17051
Epoch: 0046 train_loss= 0.71297 train_acc= 0.84997 val_loss= 0.69711 val_acc= 0.85299 time= 0.19400
Epoch: 0047 train_loss= 0.69364 train_acc= 0.85627 val_loss= 0.67703 val_acc= 0.85145 time= 0.17100
Epoch: 0048 train_loss= 0.66979 train_acc= 0.85899 val_loss= 0.65743 val_acc= 0.85758 time= 0.16903
Epoch: 0049 train_loss= 0.64845 train_acc= 0.86239 val_loss= 0.63832 val_acc= 0.86217 time= 0.18800
Epoch: 0050 train_loss= 0.62916 train_acc= 0.86613 val_loss= 0.61979 val_acc= 0.86371 time= 0.16800
Epoch: 0051 train_loss= 0.60974 train_acc= 0.87124 val_loss= 0.60188 val_acc= 0.86983 time= 0.17000
Epoch: 0052 train_loss= 0.58792 train_acc= 0.87481 val_loss= 0.58477 val_acc= 0.86983 time= 0.18800
Epoch: 0053 train_loss= 0.57095 train_acc= 0.87821 val_loss= 0.56854 val_acc= 0.87136 time= 0.16600
Epoch: 0054 train_loss= 0.55159 train_acc= 0.88093 val_loss= 0.55324 val_acc= 0.87443 time= 0.18258
Epoch: 0055 train_loss= 0.53366 train_acc= 0.88382 val_loss= 0.53885 val_acc= 0.87596 time= 0.17000
Epoch: 0056 train_loss= 0.51769 train_acc= 0.88774 val_loss= 0.52526 val_acc= 0.87902 time= 0.17000
Epoch: 0057 train_loss= 0.50160 train_acc= 0.88893 val_loss= 0.51238 val_acc= 0.87902 time= 0.18803
Epoch: 0058 train_loss= 0.48478 train_acc= 0.89250 val_loss= 0.49985 val_acc= 0.88055 time= 0.16700
Epoch: 0059 train_loss= 0.46971 train_acc= 0.89641 val_loss= 0.48755 val_acc= 0.88361 time= 0.16700
Epoch: 0060 train_loss= 0.45537 train_acc= 0.90015 val_loss= 0.47522 val_acc= 0.88208 time= 0.19000
Epoch: 0061 train_loss= 0.44273 train_acc= 0.90253 val_loss= 0.46299 val_acc= 0.88361 time= 0.16600
Epoch: 0062 train_loss= 0.42613 train_acc= 0.90662 val_loss= 0.45091 val_acc= 0.88668 time= 0.17097
Epoch: 0063 train_loss= 0.41335 train_acc= 0.91002 val_loss= 0.43926 val_acc= 0.88974 time= 0.17300
Epoch: 0064 train_loss= 0.40039 train_acc= 0.91240 val_loss= 0.42802 val_acc= 0.89587 time= 0.19400
Epoch: 0065 train_loss= 0.38819 train_acc= 0.91699 val_loss= 0.41731 val_acc= 0.89893 time= 0.16903
Epoch: 0066 train_loss= 0.37812 train_acc= 0.91937 val_loss= 0.40721 val_acc= 0.90046 time= 0.18301
Epoch: 0067 train_loss= 0.36472 train_acc= 0.92380 val_loss= 0.39787 val_acc= 0.90046 time= 0.17196
Epoch: 0068 train_loss= 0.35564 train_acc= 0.92448 val_loss= 0.38920 val_acc= 0.89893 time= 0.17000
Epoch: 0069 train_loss= 0.34247 train_acc= 0.92839 val_loss= 0.38128 val_acc= 0.90199 time= 0.19400
Epoch: 0070 train_loss= 0.33192 train_acc= 0.92924 val_loss= 0.37407 val_acc= 0.90505 time= 0.17000
Epoch: 0071 train_loss= 0.32036 train_acc= 0.93315 val_loss= 0.36726 val_acc= 0.90505 time= 0.18600
Epoch: 0072 train_loss= 0.31048 train_acc= 0.93570 val_loss= 0.36078 val_acc= 0.90505 time= 0.17300
Epoch: 0073 train_loss= 0.30015 train_acc= 0.93791 val_loss= 0.35422 val_acc= 0.90505 time= 0.17200
Epoch: 0074 train_loss= 0.29190 train_acc= 0.93842 val_loss= 0.34755 val_acc= 0.90505 time= 0.17700
Epoch: 0075 train_loss= 0.28077 train_acc= 0.93945 val_loss= 0.34103 val_acc= 0.90812 time= 0.17000
Epoch: 0076 train_loss= 0.27287 train_acc= 0.94285 val_loss= 0.33447 val_acc= 0.90965 time= 0.16900
Epoch: 0077 train_loss= 0.26369 train_acc= 0.94489 val_loss= 0.32796 val_acc= 0.91118 time= 0.18600
Epoch: 0078 train_loss= 0.25463 train_acc= 0.94948 val_loss= 0.32178 val_acc= 0.91424 time= 0.16700
Epoch: 0079 train_loss= 0.24595 train_acc= 0.95152 val_loss= 0.31599 val_acc= 0.91577 time= 0.16700
Epoch: 0080 train_loss= 0.24178 train_acc= 0.95169 val_loss= 0.31071 val_acc= 0.91730 time= 0.19900
Epoch: 0081 train_loss= 0.22909 train_acc= 0.95543 val_loss= 0.30611 val_acc= 0.91730 time= 0.17100
Epoch: 0082 train_loss= 0.22343 train_acc= 0.95612 val_loss= 0.30183 val_acc= 0.91884 time= 0.17000
Epoch: 0083 train_loss= 0.21608 train_acc= 0.95816 val_loss= 0.29762 val_acc= 0.91884 time= 0.19003
Epoch: 0084 train_loss= 0.20869 train_acc= 0.96037 val_loss= 0.29358 val_acc= 0.92190 time= 0.16900
Epoch: 0085 train_loss= 0.20159 train_acc= 0.96156 val_loss= 0.29005 val_acc= 0.92343 time= 0.16700
Epoch: 0086 train_loss= 0.19697 train_acc= 0.96275 val_loss= 0.28673 val_acc= 0.92496 time= 0.19000
Epoch: 0087 train_loss= 0.18974 train_acc= 0.96377 val_loss= 0.28316 val_acc= 0.92496 time= 0.16700
Epoch: 0088 train_loss= 0.18241 train_acc= 0.96547 val_loss= 0.27953 val_acc= 0.92496 time= 0.16997
Epoch: 0089 train_loss= 0.17741 train_acc= 0.96717 val_loss= 0.27619 val_acc= 0.92802 time= 0.18200
Epoch: 0090 train_loss= 0.17075 train_acc= 0.96666 val_loss= 0.27310 val_acc= 0.92802 time= 0.17134
Epoch: 0091 train_loss= 0.16396 train_acc= 0.97023 val_loss= 0.26995 val_acc= 0.92956 time= 0.17100
Epoch: 0092 train_loss= 0.15928 train_acc= 0.96989 val_loss= 0.26695 val_acc= 0.92956 time= 0.18803
Epoch: 0093 train_loss= 0.15474 train_acc= 0.97159 val_loss= 0.26403 val_acc= 0.92956 time= 0.16700
Epoch: 0094 train_loss= 0.14810 train_acc= 0.97312 val_loss= 0.26101 val_acc= 0.93262 time= 0.18400
Epoch: 0095 train_loss= 0.14511 train_acc= 0.97381 val_loss= 0.25783 val_acc= 0.93109 time= 0.16800
Epoch: 0096 train_loss= 0.14030 train_acc= 0.97398 val_loss= 0.25516 val_acc= 0.93262 time= 0.16900
Epoch: 0097 train_loss= 0.13374 train_acc= 0.97721 val_loss= 0.25300 val_acc= 0.93262 time= 0.19097
Epoch: 0098 train_loss= 0.13176 train_acc= 0.97687 val_loss= 0.25163 val_acc= 0.93109 time= 0.17100
Epoch: 0099 train_loss= 0.12554 train_acc= 0.97925 val_loss= 0.25056 val_acc= 0.93109 time= 0.17100
Epoch: 0100 train_loss= 0.12131 train_acc= 0.98078 val_loss= 0.24953 val_acc= 0.93568 time= 0.17503
Epoch: 0101 train_loss= 0.11865 train_acc= 0.97993 val_loss= 0.24799 val_acc= 0.93568 time= 0.16800
Epoch: 0102 train_loss= 0.11337 train_acc= 0.98231 val_loss= 0.24604 val_acc= 0.93415 time= 0.16800
Epoch: 0103 train_loss= 0.11030 train_acc= 0.98231 val_loss= 0.24433 val_acc= 0.93415 time= 0.18600
Epoch: 0104 train_loss= 0.10539 train_acc= 0.98333 val_loss= 0.24288 val_acc= 0.93568 time= 0.16900
Epoch: 0105 train_loss= 0.10272 train_acc= 0.98503 val_loss= 0.24149 val_acc= 0.93721 time= 0.17001
Epoch: 0106 train_loss= 0.09938 train_acc= 0.98605 val_loss= 0.24014 val_acc= 0.93721 time= 0.19045
Epoch: 0107 train_loss= 0.09658 train_acc= 0.98554 val_loss= 0.23889 val_acc= 0.94028 time= 0.17100
Epoch: 0108 train_loss= 0.09407 train_acc= 0.98656 val_loss= 0.23781 val_acc= 0.93874 time= 0.17100
Epoch: 0109 train_loss= 0.09061 train_acc= 0.98656 val_loss= 0.23684 val_acc= 0.93874 time= 0.19475
Epoch: 0110 train_loss= 0.08810 train_acc= 0.98775 val_loss= 0.23597 val_acc= 0.93874 time= 0.16800
Epoch: 0111 train_loss= 0.08489 train_acc= 0.98707 val_loss= 0.23527 val_acc= 0.94028 time= 0.18600
Epoch: 0112 train_loss= 0.08190 train_acc= 0.98860 val_loss= 0.23439 val_acc= 0.94028 time= 0.16900
Epoch: 0113 train_loss= 0.07948 train_acc= 0.98979 val_loss= 0.23375 val_acc= 0.94028 time= 0.16800
Epoch: 0114 train_loss= 0.07765 train_acc= 0.98928 val_loss= 0.23316 val_acc= 0.94028 time= 0.19000
Epoch: 0115 train_loss= 0.07527 train_acc= 0.98962 val_loss= 0.23244 val_acc= 0.94028 time= 0.16897
Epoch: 0116 train_loss= 0.07244 train_acc= 0.99098 val_loss= 0.23159 val_acc= 0.94028 time= 0.17100
Epoch: 0117 train_loss= 0.07035 train_acc= 0.99115 val_loss= 0.23041 val_acc= 0.94028 time= 0.17000
Epoch: 0118 train_loss= 0.06792 train_acc= 0.99064 val_loss= 0.22985 val_acc= 0.94028 time= 0.16903
Epoch: 0119 train_loss= 0.06596 train_acc= 0.99150 val_loss= 0.22929 val_acc= 0.94028 time= 0.16800
Epoch: 0120 train_loss= 0.06415 train_acc= 0.99098 val_loss= 0.22898 val_acc= 0.94028 time= 0.18900
Epoch: 0121 train_loss= 0.06327 train_acc= 0.99184 val_loss= 0.22845 val_acc= 0.94028 time= 0.17000
Epoch: 0122 train_loss= 0.05977 train_acc= 0.99235 val_loss= 0.22821 val_acc= 0.94028 time= 0.16900
Epoch: 0123 train_loss= 0.05892 train_acc= 0.99201 val_loss= 0.22826 val_acc= 0.94028 time= 0.19100
Epoch: 0124 train_loss= 0.05574 train_acc= 0.99320 val_loss= 0.22850 val_acc= 0.94028 time= 0.17009
Epoch: 0125 train_loss= 0.05534 train_acc= 0.99303 val_loss= 0.22884 val_acc= 0.94028 time= 0.17000
Epoch: 0126 train_loss= 0.05375 train_acc= 0.99286 val_loss= 0.22822 val_acc= 0.94028 time= 0.19500
Epoch: 0127 train_loss= 0.05203 train_acc= 0.99337 val_loss= 0.22737 val_acc= 0.94028 time= 0.17003
Epoch: 0128 train_loss= 0.05021 train_acc= 0.99252 val_loss= 0.22651 val_acc= 0.94181 time= 0.18900
Epoch: 0129 train_loss= 0.04941 train_acc= 0.99388 val_loss= 0.22513 val_acc= 0.93874 time= 0.16699
Epoch: 0130 train_loss= 0.04832 train_acc= 0.99422 val_loss= 0.22427 val_acc= 0.93874 time= 0.17000
Epoch: 0131 train_loss= 0.04627 train_acc= 0.99422 val_loss= 0.22395 val_acc= 0.94334 time= 0.19200
Epoch: 0132 train_loss= 0.04514 train_acc= 0.99456 val_loss= 0.22323 val_acc= 0.94028 time= 0.16699
Epoch: 0133 train_loss= 0.04366 train_acc= 0.99507 val_loss= 0.22278 val_acc= 0.93874 time= 0.17198
Epoch: 0134 train_loss= 0.04216 train_acc= 0.99575 val_loss= 0.22249 val_acc= 0.94028 time= 0.17700
Epoch: 0135 train_loss= 0.04114 train_acc= 0.99575 val_loss= 0.22278 val_acc= 0.94181 time= 0.17097
Epoch: 0136 train_loss= 0.04060 train_acc= 0.99541 val_loss= 0.22312 val_acc= 0.94181 time= 0.16903
Epoch: 0137 train_loss= 0.04003 train_acc= 0.99558 val_loss= 0.22418 val_acc= 0.94181 time= 0.19200
Early stopping...
Optimization Finished!
Test set results: cost= 0.25458 accuracy= 0.93692 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     0.3333    1.0000    0.5000         1
           3     0.7717    0.9467    0.8503        75
           4     1.0000    1.0000    1.0000         9
           5     0.8387    0.8966    0.8667        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.6667    0.8000         9
          10     0.8462    0.6111    0.7097        36
          11     1.0000    0.9167    0.9565        12
          12     0.8392    0.9917    0.9091       121
          13     0.9375    0.7895    0.8571        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.6667    0.4444    0.5333         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.7273    0.8421        11
          29     0.9654    0.9626    0.9640       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8193    0.8395    0.8293        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9898    0.9839      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.8889    0.6667    0.7619        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7821    0.6978    0.7117      2568
weighted avg     0.9373    0.9369    0.9330      2568

Macro average Test Precision, Recall and F1-Score...
(0.7820956617640864, 0.6978335525190261, 0.7116697648438491, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[ 0.03109813  0.50208676  0.04953595 ... -0.04419895 -0.09800427
   0.06717863]
 [ 0.07105719  0.25170928 -0.02107788 ... -0.03055964  0.02633312
   0.03235614]
 [ 0.03630793  0.11450161  0.33902192 ...  0.01220292  0.02553395
   0.03718815]
 ...
 [ 0.0587852   0.22362016  0.04306132 ...  0.00757695  0.0060278
   0.02377623]
 [ 0.05711807  0.09940352  0.16591612 ...  0.02868506  0.05771577
   0.05961027]
 [ 0.29062352  0.2629526   0.39559892 ...  0.26530406  0.27627307
   0.24333373]]

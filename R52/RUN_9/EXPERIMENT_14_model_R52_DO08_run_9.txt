(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95138 train_acc= 0.00374 val_loss= 3.90580 val_acc= 0.53905 time= 0.46201
Epoch: 0002 train_loss= 3.90436 train_acc= 0.51284 val_loss= 3.81368 val_acc= 0.50077 time= 0.17837
Epoch: 0003 train_loss= 3.81398 train_acc= 0.48767 val_loss= 3.67200 val_acc= 0.48698 time= 0.17500
Epoch: 0004 train_loss= 3.67221 train_acc= 0.46556 val_loss= 3.47946 val_acc= 0.46554 time= 0.19100
Epoch: 0005 train_loss= 3.46693 train_acc= 0.45399 val_loss= 3.24332 val_acc= 0.46095 time= 0.16900
Epoch: 0006 train_loss= 3.23851 train_acc= 0.44736 val_loss= 2.98210 val_acc= 0.45942 time= 0.16800
Epoch: 0007 train_loss= 2.95491 train_acc= 0.43749 val_loss= 2.72286 val_acc= 0.45636 time= 0.18704
Epoch: 0008 train_loss= 2.72337 train_acc= 0.43749 val_loss= 2.49767 val_acc= 0.45636 time= 0.16695
Epoch: 0009 train_loss= 2.48826 train_acc= 0.43324 val_loss= 2.33670 val_acc= 0.45636 time= 0.16700
Epoch: 0010 train_loss= 2.34437 train_acc= 0.43426 val_loss= 2.24701 val_acc= 0.45636 time= 0.16800
Epoch: 0011 train_loss= 2.27258 train_acc= 0.43511 val_loss= 2.20471 val_acc= 0.45636 time= 0.16900
Epoch: 0012 train_loss= 2.21933 train_acc= 0.43800 val_loss= 2.17228 val_acc= 0.45636 time= 0.17300
Epoch: 0013 train_loss= 2.19652 train_acc= 0.43817 val_loss= 2.12615 val_acc= 0.46401 time= 0.19051
Epoch: 0014 train_loss= 2.12989 train_acc= 0.44582 val_loss= 2.05947 val_acc= 0.47779 time= 0.17000
Epoch: 0015 train_loss= 2.09094 train_acc= 0.47083 val_loss= 1.97604 val_acc= 0.49923 time= 0.17100
Epoch: 0016 train_loss= 1.99197 train_acc= 0.48903 val_loss= 1.88675 val_acc= 0.53599 time= 0.18362
Epoch: 0017 train_loss= 1.91482 train_acc= 0.51522 val_loss= 1.80281 val_acc= 0.59265 time= 0.16801
Epoch: 0018 train_loss= 1.81220 train_acc= 0.57561 val_loss= 1.73300 val_acc= 0.63706 time= 0.17100
Epoch: 0019 train_loss= 1.76785 train_acc= 0.59891 val_loss= 1.67775 val_acc= 0.65544 time= 0.18603
Epoch: 0020 train_loss= 1.70154 train_acc= 0.63446 val_loss= 1.63076 val_acc= 0.66922 time= 0.17101
Epoch: 0021 train_loss= 1.64877 train_acc= 0.64331 val_loss= 1.58454 val_acc= 0.67688 time= 0.17353
Epoch: 0022 train_loss= 1.61243 train_acc= 0.65164 val_loss= 1.53574 val_acc= 0.68913 time= 0.19000
Epoch: 0023 train_loss= 1.59733 train_acc= 0.66457 val_loss= 1.48434 val_acc= 0.68913 time= 0.16827
Epoch: 0024 train_loss= 1.50065 train_acc= 0.67035 val_loss= 1.43489 val_acc= 0.69372 time= 0.18300
Epoch: 0025 train_loss= 1.48007 train_acc= 0.66814 val_loss= 1.38932 val_acc= 0.69985 time= 0.16801
Epoch: 0026 train_loss= 1.40709 train_acc= 0.67682 val_loss= 1.34906 val_acc= 0.70138 time= 0.16611
Epoch: 0027 train_loss= 1.38925 train_acc= 0.68209 val_loss= 1.31309 val_acc= 0.70291 time= 0.18668
Epoch: 0028 train_loss= 1.34064 train_acc= 0.67988 val_loss= 1.28070 val_acc= 0.71057 time= 0.16997
Epoch: 0029 train_loss= 1.31845 train_acc= 0.67903 val_loss= 1.25047 val_acc= 0.71057 time= 0.17100
Epoch: 0030 train_loss= 1.28787 train_acc= 0.69144 val_loss= 1.22153 val_acc= 0.71669 time= 0.17500
Epoch: 0031 train_loss= 1.24790 train_acc= 0.70046 val_loss= 1.19333 val_acc= 0.72129 time= 0.16903
Epoch: 0032 train_loss= 1.21882 train_acc= 0.71577 val_loss= 1.16563 val_acc= 0.72894 time= 0.16801
Epoch: 0033 train_loss= 1.19598 train_acc= 0.71866 val_loss= 1.13841 val_acc= 0.73201 time= 0.18900
Epoch: 0034 train_loss= 1.17363 train_acc= 0.73244 val_loss= 1.11175 val_acc= 0.73507 time= 0.16617
Epoch: 0035 train_loss= 1.12960 train_acc= 0.74043 val_loss= 1.08580 val_acc= 0.74119 time= 0.16714
Epoch: 0036 train_loss= 1.11839 train_acc= 0.74213 val_loss= 1.06079 val_acc= 0.75345 time= 0.18905
Epoch: 0037 train_loss= 1.09055 train_acc= 0.74724 val_loss= 1.03685 val_acc= 0.75345 time= 0.17095
Epoch: 0038 train_loss= 1.06192 train_acc= 0.75574 val_loss= 1.01393 val_acc= 0.75651 time= 0.17351
Epoch: 0039 train_loss= 1.05071 train_acc= 0.75404 val_loss= 0.99180 val_acc= 0.76263 time= 0.18851
Epoch: 0040 train_loss= 1.02647 train_acc= 0.76118 val_loss= 0.97019 val_acc= 0.76876 time= 0.17006
Epoch: 0041 train_loss= 0.99315 train_acc= 0.77360 val_loss= 0.94861 val_acc= 0.77948 time= 0.17099
Epoch: 0042 train_loss= 0.97465 train_acc= 0.78585 val_loss= 0.92748 val_acc= 0.78867 time= 0.16597
Epoch: 0043 train_loss= 0.96245 train_acc= 0.78279 val_loss= 0.90662 val_acc= 0.79632 time= 0.16808
Epoch: 0044 train_loss= 0.93632 train_acc= 0.79520 val_loss= 0.88612 val_acc= 0.80704 time= 0.18607
Epoch: 0045 train_loss= 0.90149 train_acc= 0.80711 val_loss= 0.86592 val_acc= 0.81930 time= 0.16696
Epoch: 0046 train_loss= 0.89296 train_acc= 0.81476 val_loss= 0.84592 val_acc= 0.83155 time= 0.17200
Epoch: 0047 train_loss= 0.86928 train_acc= 0.82004 val_loss= 0.82606 val_acc= 0.83461 time= 0.19300
Epoch: 0048 train_loss= 0.84469 train_acc= 0.82242 val_loss= 0.80626 val_acc= 0.83614 time= 0.17103
Epoch: 0049 train_loss= 0.83234 train_acc= 0.82718 val_loss= 0.78691 val_acc= 0.83920 time= 0.16800
Epoch: 0050 train_loss= 0.81383 train_acc= 0.82395 val_loss= 0.76818 val_acc= 0.84227 time= 0.18700
Epoch: 0051 train_loss= 0.78421 train_acc= 0.83245 val_loss= 0.74974 val_acc= 0.84533 time= 0.16800
Epoch: 0052 train_loss= 0.77329 train_acc= 0.82939 val_loss= 0.73139 val_acc= 0.84533 time= 0.16803
Epoch: 0053 train_loss= 0.75247 train_acc= 0.83518 val_loss= 0.71376 val_acc= 0.84533 time= 0.16797
Epoch: 0054 train_loss= 0.74646 train_acc= 0.83739 val_loss= 0.69653 val_acc= 0.84992 time= 0.16897
Epoch: 0055 train_loss= 0.72339 train_acc= 0.83654 val_loss= 0.67972 val_acc= 0.84839 time= 0.17300
Epoch: 0056 train_loss= 0.71426 train_acc= 0.83943 val_loss= 0.66365 val_acc= 0.85145 time= 0.19100
Epoch: 0057 train_loss= 0.67994 train_acc= 0.85151 val_loss= 0.64834 val_acc= 0.85605 time= 0.17103
Epoch: 0058 train_loss= 0.66937 train_acc= 0.85083 val_loss= 0.63399 val_acc= 0.86371 time= 0.17001
Epoch: 0059 train_loss= 0.66671 train_acc= 0.84895 val_loss= 0.61955 val_acc= 0.86217 time= 0.18440
Epoch: 0060 train_loss= 0.63524 train_acc= 0.85848 val_loss= 0.60541 val_acc= 0.86371 time= 0.16900
Epoch: 0061 train_loss= 0.61656 train_acc= 0.85984 val_loss= 0.59136 val_acc= 0.86677 time= 0.17304
Epoch: 0062 train_loss= 0.60284 train_acc= 0.85899 val_loss= 0.57767 val_acc= 0.86677 time= 0.16796
Epoch: 0063 train_loss= 0.58252 train_acc= 0.86613 val_loss= 0.56406 val_acc= 0.86830 time= 0.17051
Epoch: 0064 train_loss= 0.56434 train_acc= 0.86664 val_loss= 0.55117 val_acc= 0.86983 time= 0.19300
Epoch: 0065 train_loss= 0.56342 train_acc= 0.86852 val_loss= 0.53877 val_acc= 0.87749 time= 0.17100
Epoch: 0066 train_loss= 0.53534 train_acc= 0.87583 val_loss= 0.52716 val_acc= 0.87902 time= 0.17305
Epoch: 0067 train_loss= 0.55126 train_acc= 0.86971 val_loss= 0.51529 val_acc= 0.87902 time= 0.16995
Epoch: 0068 train_loss= 0.52573 train_acc= 0.87957 val_loss= 0.50325 val_acc= 0.88208 time= 0.16704
Epoch: 0069 train_loss= 0.50443 train_acc= 0.88331 val_loss= 0.49157 val_acc= 0.88515 time= 0.17296
Epoch: 0070 train_loss= 0.49062 train_acc= 0.88501 val_loss= 0.48108 val_acc= 0.88668 time= 0.18491
Epoch: 0071 train_loss= 0.48611 train_acc= 0.88825 val_loss= 0.47181 val_acc= 0.88974 time= 0.16800
Epoch: 0072 train_loss= 0.47031 train_acc= 0.89165 val_loss= 0.46321 val_acc= 0.88821 time= 0.17370
Epoch: 0073 train_loss= 0.46835 train_acc= 0.89658 val_loss= 0.45532 val_acc= 0.88515 time= 0.17400
Epoch: 0074 train_loss= 0.44628 train_acc= 0.89505 val_loss= 0.44761 val_acc= 0.88515 time= 0.17000
Epoch: 0075 train_loss= 0.44492 train_acc= 0.90168 val_loss= 0.44025 val_acc= 0.88974 time= 0.17200
Epoch: 0076 train_loss= 0.43812 train_acc= 0.89913 val_loss= 0.43197 val_acc= 0.89127 time= 0.18401
Epoch: 0077 train_loss= 0.43377 train_acc= 0.90117 val_loss= 0.42356 val_acc= 0.89280 time= 0.16699
Epoch: 0078 train_loss= 0.42267 train_acc= 0.90304 val_loss= 0.41529 val_acc= 0.89433 time= 0.17100
Epoch: 0079 train_loss= 0.39937 train_acc= 0.90883 val_loss= 0.40756 val_acc= 0.89740 time= 0.18400
Epoch: 0080 train_loss= 0.40611 train_acc= 0.90373 val_loss= 0.39925 val_acc= 0.90352 time= 0.17101
Epoch: 0081 train_loss= 0.38871 train_acc= 0.91138 val_loss= 0.39154 val_acc= 0.90812 time= 0.17367
Epoch: 0082 train_loss= 0.38955 train_acc= 0.90815 val_loss= 0.38527 val_acc= 0.90659 time= 0.19100
Epoch: 0083 train_loss= 0.38320 train_acc= 0.90662 val_loss= 0.37859 val_acc= 0.90352 time= 0.17003
Epoch: 0084 train_loss= 0.36468 train_acc= 0.91393 val_loss= 0.37255 val_acc= 0.90199 time= 0.17000
Epoch: 0085 train_loss= 0.35866 train_acc= 0.91614 val_loss= 0.36738 val_acc= 0.90352 time= 0.18500
Epoch: 0086 train_loss= 0.36038 train_acc= 0.91631 val_loss= 0.36167 val_acc= 0.90658 time= 0.16699
Epoch: 0087 train_loss= 0.33724 train_acc= 0.92210 val_loss= 0.35612 val_acc= 0.90505 time= 0.18600
Epoch: 0088 train_loss= 0.33820 train_acc= 0.91903 val_loss= 0.35055 val_acc= 0.90812 time= 0.16700
Epoch: 0089 train_loss= 0.33468 train_acc= 0.91750 val_loss= 0.34711 val_acc= 0.90812 time= 0.17097
Epoch: 0090 train_loss= 0.33140 train_acc= 0.92125 val_loss= 0.34533 val_acc= 0.91118 time= 0.19300
Epoch: 0091 train_loss= 0.32312 train_acc= 0.92771 val_loss= 0.34389 val_acc= 0.91118 time= 0.17000
Epoch: 0092 train_loss= 0.29852 train_acc= 0.92618 val_loss= 0.34173 val_acc= 0.90812 time= 0.16803
Epoch: 0093 train_loss= 0.29602 train_acc= 0.93264 val_loss= 0.33931 val_acc= 0.90812 time= 0.16800
Epoch: 0094 train_loss= 0.30710 train_acc= 0.92992 val_loss= 0.33659 val_acc= 0.90505 time= 0.16800
Epoch: 0095 train_loss= 0.28860 train_acc= 0.93672 val_loss= 0.33221 val_acc= 0.90505 time= 0.16700
Epoch: 0096 train_loss= 0.28716 train_acc= 0.93451 val_loss= 0.32604 val_acc= 0.90658 time= 0.19058
Epoch: 0097 train_loss= 0.27406 train_acc= 0.93706 val_loss= 0.31982 val_acc= 0.90812 time= 0.16800
Epoch: 0098 train_loss= 0.28441 train_acc= 0.93315 val_loss= 0.31390 val_acc= 0.91271 time= 0.17040
Epoch: 0099 train_loss= 0.26823 train_acc= 0.93877 val_loss= 0.30797 val_acc= 0.91424 time= 0.19204
Epoch: 0100 train_loss= 0.25871 train_acc= 0.93706 val_loss= 0.30265 val_acc= 0.91424 time= 0.17153
Epoch: 0101 train_loss= 0.26490 train_acc= 0.94098 val_loss= 0.29690 val_acc= 0.91577 time= 0.17001
Epoch: 0102 train_loss= 0.25177 train_acc= 0.93859 val_loss= 0.29254 val_acc= 0.91730 time= 0.18400
Epoch: 0103 train_loss= 0.25195 train_acc= 0.94064 val_loss= 0.28927 val_acc= 0.92037 time= 0.16800
Epoch: 0104 train_loss= 0.24348 train_acc= 0.94217 val_loss= 0.28569 val_acc= 0.92190 time= 0.16897
Epoch: 0105 train_loss= 0.24391 train_acc= 0.94438 val_loss= 0.28336 val_acc= 0.92190 time= 0.17303
Epoch: 0106 train_loss= 0.23302 train_acc= 0.94727 val_loss= 0.28193 val_acc= 0.92496 time= 0.16904
Epoch: 0107 train_loss= 0.23480 train_acc= 0.94285 val_loss= 0.28062 val_acc= 0.92343 time= 0.17448
Epoch: 0108 train_loss= 0.21913 train_acc= 0.95118 val_loss= 0.28038 val_acc= 0.92343 time= 0.18966
Epoch: 0109 train_loss= 0.22174 train_acc= 0.94829 val_loss= 0.28101 val_acc= 0.91730 time= 0.17103
Epoch: 0110 train_loss= 0.22357 train_acc= 0.94693 val_loss= 0.28172 val_acc= 0.91730 time= 0.18600
Epoch: 0111 train_loss= 0.22378 train_acc= 0.94625 val_loss= 0.28019 val_acc= 0.91577 time= 0.16700
Epoch: 0112 train_loss= 0.20752 train_acc= 0.95390 val_loss= 0.27802 val_acc= 0.91884 time= 0.16604
Epoch: 0113 train_loss= 0.20321 train_acc= 0.95577 val_loss= 0.27571 val_acc= 0.92190 time= 0.18804
Epoch: 0114 train_loss= 0.21129 train_acc= 0.94591 val_loss= 0.27265 val_acc= 0.92190 time= 0.16641
Epoch: 0115 train_loss= 0.21333 train_acc= 0.94982 val_loss= 0.26897 val_acc= 0.92649 time= 0.17104
Epoch: 0116 train_loss= 0.20196 train_acc= 0.95441 val_loss= 0.26553 val_acc= 0.92496 time= 0.18800
Epoch: 0117 train_loss= 0.20119 train_acc= 0.94897 val_loss= 0.26242 val_acc= 0.92649 time= 0.17100
Epoch: 0118 train_loss= 0.20239 train_acc= 0.95033 val_loss= 0.26115 val_acc= 0.92649 time= 0.17187
Epoch: 0119 train_loss= 0.19570 train_acc= 0.95305 val_loss= 0.26022 val_acc= 0.92802 time= 0.18497
Epoch: 0120 train_loss= 0.20248 train_acc= 0.95254 val_loss= 0.25858 val_acc= 0.92802 time= 0.16700
Epoch: 0121 train_loss= 0.18666 train_acc= 0.95526 val_loss= 0.25699 val_acc= 0.92649 time= 0.17008
Epoch: 0122 train_loss= 0.18330 train_acc= 0.95782 val_loss= 0.25614 val_acc= 0.92956 time= 0.18600
Epoch: 0123 train_loss= 0.17752 train_acc= 0.96054 val_loss= 0.25578 val_acc= 0.92802 time= 0.16900
Epoch: 0124 train_loss= 0.17151 train_acc= 0.95884 val_loss= 0.25563 val_acc= 0.92802 time= 0.17344
Epoch: 0125 train_loss= 0.18608 train_acc= 0.95765 val_loss= 0.25398 val_acc= 0.92802 time= 0.18985
Epoch: 0126 train_loss= 0.17016 train_acc= 0.95901 val_loss= 0.25141 val_acc= 0.92802 time= 0.17100
Epoch: 0127 train_loss= 0.16858 train_acc= 0.95799 val_loss= 0.24957 val_acc= 0.92956 time= 0.18800
Epoch: 0128 train_loss= 0.16849 train_acc= 0.96207 val_loss= 0.24880 val_acc= 0.92956 time= 0.17100
Epoch: 0129 train_loss= 0.15819 train_acc= 0.95918 val_loss= 0.24815 val_acc= 0.92956 time= 0.16900
Epoch: 0130 train_loss= 0.16713 train_acc= 0.96003 val_loss= 0.24822 val_acc= 0.92649 time= 0.18801
Epoch: 0131 train_loss= 0.16142 train_acc= 0.96037 val_loss= 0.24704 val_acc= 0.92802 time= 0.16699
Epoch: 0132 train_loss= 0.15997 train_acc= 0.96666 val_loss= 0.24639 val_acc= 0.93262 time= 0.16800
Epoch: 0133 train_loss= 0.16324 train_acc= 0.96326 val_loss= 0.24622 val_acc= 0.92956 time= 0.17000
Epoch: 0134 train_loss= 0.14695 train_acc= 0.96938 val_loss= 0.24597 val_acc= 0.93262 time= 0.17200
Epoch: 0135 train_loss= 0.15887 train_acc= 0.96343 val_loss= 0.24605 val_acc= 0.93415 time= 0.17200
Epoch: 0136 train_loss= 0.15016 train_acc= 0.96632 val_loss= 0.24481 val_acc= 0.93109 time= 0.18758
Epoch: 0137 train_loss= 0.14859 train_acc= 0.96581 val_loss= 0.24364 val_acc= 0.92649 time= 0.16700
Epoch: 0138 train_loss= 0.14476 train_acc= 0.96462 val_loss= 0.24181 val_acc= 0.92649 time= 0.17000
Epoch: 0139 train_loss= 0.13950 train_acc= 0.96870 val_loss= 0.23995 val_acc= 0.92802 time= 0.18500
Epoch: 0140 train_loss= 0.14240 train_acc= 0.96564 val_loss= 0.23821 val_acc= 0.92956 time= 0.16606
Epoch: 0141 train_loss= 0.13718 train_acc= 0.96751 val_loss= 0.23682 val_acc= 0.93415 time= 0.17100
Epoch: 0142 train_loss= 0.13934 train_acc= 0.96853 val_loss= 0.23523 val_acc= 0.93262 time= 0.18661
Epoch: 0143 train_loss= 0.13729 train_acc= 0.96836 val_loss= 0.23398 val_acc= 0.93415 time= 0.17074
Epoch: 0144 train_loss= 0.12918 train_acc= 0.97381 val_loss= 0.23438 val_acc= 0.93415 time= 0.17402
Epoch: 0145 train_loss= 0.13801 train_acc= 0.97193 val_loss= 0.23515 val_acc= 0.93262 time= 0.16901
Epoch: 0146 train_loss= 0.13081 train_acc= 0.96921 val_loss= 0.23542 val_acc= 0.93262 time= 0.16703
Epoch: 0147 train_loss= 0.12157 train_acc= 0.97483 val_loss= 0.23448 val_acc= 0.93262 time= 0.18602
Epoch: 0148 train_loss= 0.12493 train_acc= 0.97176 val_loss= 0.23410 val_acc= 0.93415 time= 0.16674
Epoch: 0149 train_loss= 0.13071 train_acc= 0.97142 val_loss= 0.23333 val_acc= 0.93415 time= 0.16699
Epoch: 0150 train_loss= 0.12774 train_acc= 0.97074 val_loss= 0.23361 val_acc= 0.93721 time= 0.18797
Epoch: 0151 train_loss= 0.12239 train_acc= 0.97227 val_loss= 0.23495 val_acc= 0.93721 time= 0.17041
Early stopping...
Optimization Finished!
Test set results: cost= 0.26360 accuracy= 0.93692 time= 0.07553
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     0.6667    0.3333    0.4444         6
           2     0.2500    1.0000    0.4000         1
           3     0.7912    0.9600    0.8675        75
           4     1.0000    1.0000    1.0000         9
           5     0.7981    0.9540    0.8691        87
           6     0.9231    0.9600    0.9412        25
           7     0.6667    0.9231    0.7742        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.2222    0.3636         9
          10     0.9259    0.6944    0.7937        36
          11     1.0000    0.9167    0.9565        12
          12     0.8633    0.9917    0.9231       121
          13     1.0000    0.6842    0.8125        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    0.2500    0.4000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.6500    0.7647    0.7027        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9614    0.9670    0.9642       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    0.6667    0.8000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8590    0.8272    0.8428        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9908    0.9858      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.7778    0.5833    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7343    0.6403    0.6591      2568
weighted avg     0.9349    0.9369    0.9314      2568

Macro average Test Precision, Recall and F1-Score...
(0.7342520403765173, 0.6403446416193097, 0.6591300727545982, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[-0.10487509  0.02731695  0.52155745 ...  0.00250858 -0.14906092
   0.48083237]
 [ 0.01953927 -0.00121598  0.45600256 ...  0.1053547   0.33781093
   0.26892346]
 [ 0.1282396   0.08603963  0.15901434 ... -0.02592113  0.11430589
   0.07368896]
 ...
 [-0.02252758  0.14433846  0.27128124 ...  0.0952981   0.22858697
   0.18223073]
 [ 0.0734001   0.14015645  0.07495025 ...  0.02170227  0.05023452
   0.08215775]
 [ 0.23797318  0.29317287  0.2042898  ...  0.25467515  0.2832476
   0.2598673 ]]

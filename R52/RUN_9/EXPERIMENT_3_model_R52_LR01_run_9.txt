(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.01446 val_loss= 3.42630 val_acc= 0.61715 time= 0.44305
Epoch: 0002 train_loss= 3.43600 train_acc= 0.61422 val_loss= 2.45482 val_acc= 0.62481 time= 0.17520
Epoch: 0003 train_loss= 2.45815 train_acc= 0.61184 val_loss= 2.25022 val_acc= 0.60643 time= 0.19104
Epoch: 0004 train_loss= 2.24668 train_acc= 0.60146 val_loss= 2.06180 val_acc= 0.52374 time= 0.17033
Epoch: 0005 train_loss= 2.08393 train_acc= 0.51250 val_loss= 1.66796 val_acc= 0.66616 time= 0.17100
Epoch: 0006 train_loss= 1.70476 train_acc= 0.65232 val_loss= 1.46840 val_acc= 0.69219 time= 0.16900
Epoch: 0007 train_loss= 1.51994 train_acc= 0.66559 val_loss= 1.36559 val_acc= 0.71669 time= 0.17203
Epoch: 0008 train_loss= 1.40623 train_acc= 0.71458 val_loss= 1.24891 val_acc= 0.72435 time= 0.18500
Epoch: 0009 train_loss= 1.28421 train_acc= 0.71985 val_loss= 1.14934 val_acc= 0.72894 time= 0.16900
Epoch: 0010 train_loss= 1.16544 train_acc= 0.72938 val_loss= 1.07013 val_acc= 0.73813 time= 0.16907
Epoch: 0011 train_loss= 1.08278 train_acc= 0.73992 val_loss= 1.00948 val_acc= 0.74885 time= 0.18497
Epoch: 0012 train_loss= 1.01374 train_acc= 0.75183 val_loss= 0.96244 val_acc= 0.75804 time= 0.16557
Epoch: 0013 train_loss= 0.95121 train_acc= 0.76595 val_loss= 0.90822 val_acc= 0.77642 time= 0.17197
Epoch: 0014 train_loss= 0.90790 train_acc= 0.77530 val_loss= 0.84558 val_acc= 0.78867 time= 0.18900
Epoch: 0015 train_loss= 0.84730 train_acc= 0.79792 val_loss= 0.78688 val_acc= 0.81776 time= 0.17200
Epoch: 0016 train_loss= 0.78861 train_acc= 0.82140 val_loss= 0.73998 val_acc= 0.83767 time= 0.17300
Epoch: 0017 train_loss= 0.73577 train_acc= 0.83654 val_loss= 0.70079 val_acc= 0.84380 time= 0.17531
Epoch: 0018 train_loss= 0.69189 train_acc= 0.84368 val_loss= 0.66412 val_acc= 0.84839 time= 0.16901
Epoch: 0019 train_loss= 0.65327 train_acc= 0.84589 val_loss= 0.62915 val_acc= 0.84992 time= 0.17100
Epoch: 0020 train_loss= 0.61301 train_acc= 0.85389 val_loss= 0.59814 val_acc= 0.85145 time= 0.18500
Epoch: 0021 train_loss= 0.56795 train_acc= 0.86290 val_loss= 0.57059 val_acc= 0.85145 time= 0.16800
Epoch: 0022 train_loss= 0.52614 train_acc= 0.87260 val_loss= 0.54163 val_acc= 0.86217 time= 0.18797
Epoch: 0023 train_loss= 0.50685 train_acc= 0.87855 val_loss= 0.50987 val_acc= 0.87289 time= 0.17100
Epoch: 0024 train_loss= 0.46502 train_acc= 0.88621 val_loss= 0.47850 val_acc= 0.88055 time= 0.17000
Epoch: 0025 train_loss= 0.43486 train_acc= 0.89403 val_loss= 0.45289 val_acc= 0.88055 time= 0.18903
Epoch: 0026 train_loss= 0.40111 train_acc= 0.90066 val_loss= 0.43250 val_acc= 0.88361 time= 0.16700
Epoch: 0027 train_loss= 0.36851 train_acc= 0.90560 val_loss= 0.41512 val_acc= 0.88361 time= 0.16708
Epoch: 0028 train_loss= 0.33824 train_acc= 0.92022 val_loss= 0.39812 val_acc= 0.88821 time= 0.16800
Epoch: 0029 train_loss= 0.32025 train_acc= 0.91767 val_loss= 0.37754 val_acc= 0.89280 time= 0.16707
Epoch: 0030 train_loss= 0.29390 train_acc= 0.92737 val_loss= 0.35832 val_acc= 0.89893 time= 0.16923
Epoch: 0031 train_loss= 0.26695 train_acc= 0.93570 val_loss= 0.34288 val_acc= 0.90812 time= 0.18727
Epoch: 0032 train_loss= 0.24463 train_acc= 0.94149 val_loss= 0.33424 val_acc= 0.90965 time= 0.16991
Epoch: 0033 train_loss= 0.23127 train_acc= 0.94489 val_loss= 0.32867 val_acc= 0.91577 time= 0.17300
Epoch: 0034 train_loss= 0.21575 train_acc= 0.94863 val_loss= 0.32453 val_acc= 0.91577 time= 0.18803
Epoch: 0035 train_loss= 0.19632 train_acc= 0.95356 val_loss= 0.31473 val_acc= 0.91884 time= 0.16700
Epoch: 0036 train_loss= 0.18152 train_acc= 0.95918 val_loss= 0.30450 val_acc= 0.92037 time= 0.16997
Epoch: 0037 train_loss= 0.16425 train_acc= 0.95884 val_loss= 0.30202 val_acc= 0.92343 time= 0.18500
Epoch: 0038 train_loss= 0.15458 train_acc= 0.96054 val_loss= 0.29854 val_acc= 0.92802 time= 0.16709
Epoch: 0039 train_loss= 0.14656 train_acc= 0.96224 val_loss= 0.29890 val_acc= 0.92802 time= 0.17199
Epoch: 0040 train_loss= 0.13144 train_acc= 0.96649 val_loss= 0.29361 val_acc= 0.92802 time= 0.17300
Epoch: 0041 train_loss= 0.12117 train_acc= 0.97108 val_loss= 0.28430 val_acc= 0.92649 time= 0.17101
Epoch: 0042 train_loss= 0.11492 train_acc= 0.97142 val_loss= 0.28294 val_acc= 0.93109 time= 0.17306
Epoch: 0043 train_loss= 0.10715 train_acc= 0.97125 val_loss= 0.27196 val_acc= 0.93415 time= 0.18783
Epoch: 0044 train_loss= 0.09051 train_acc= 0.97840 val_loss= 0.26931 val_acc= 0.92956 time= 0.16803
Epoch: 0045 train_loss= 0.08992 train_acc= 0.97925 val_loss= 0.27380 val_acc= 0.93262 time= 0.18697
Epoch: 0046 train_loss= 0.08096 train_acc= 0.97959 val_loss= 0.27786 val_acc= 0.93262 time= 0.16803
Epoch: 0047 train_loss= 0.07365 train_acc= 0.98401 val_loss= 0.28367 val_acc= 0.92956 time= 0.16700
Epoch: 0048 train_loss= 0.07281 train_acc= 0.98044 val_loss= 0.28603 val_acc= 0.93568 time= 0.18897
Early stopping...
Optimization Finished!
Test set results: cost= 0.30311 accuracy= 0.92991 time= 0.07600
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.5000    0.5000    0.5000         6
           2     0.0000    0.0000    0.0000         1
           3     0.8313    0.9200    0.8734        75
           4     1.0000    1.0000    1.0000         9
           5     0.7193    0.9425    0.8159        87
           6     0.9583    0.9200    0.9388        25
           7     0.8462    0.8462    0.8462        13
           8     1.0000    0.9091    0.9524        11
           9     1.0000    0.3333    0.5000         9
          10     0.8529    0.8056    0.8286        36
          11     1.0000    0.9167    0.9565        12
          12     0.8676    0.9752    0.9183       121
          13     0.7500    0.6316    0.6857        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.8889    0.8000    0.8421        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9751    0.9555    0.9652       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8382    0.7037    0.7651        81
          36     0.8333    0.4167    0.5556        12
          37     0.8000    1.0000    0.8889         4
          38     0.0000    0.0000    0.0000         1
          39     0.9711    0.9926    0.9817      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.6364    0.5833    0.6087        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     0.9000    1.0000    0.9474         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9299      2568
   macro avg     0.7069    0.6356    0.6506      2568
weighted avg     0.9267    0.9299    0.9246      2568

Macro average Test Precision, Recall and F1-Score...
(0.7068724735294725, 0.6355759928310043, 0.6506245304620986, None)
Micro average Test Precision, Recall and F1-Score...
(0.9299065420560748, 0.9299065420560748, 0.9299065420560748, None)
embeddings:
8892 6532 2568
[[-0.33603904  1.3524534   0.3839423  ... -0.69590867 -0.03375169
  -0.60917056]
 [ 0.06723426  0.42963967  0.29807764 ... -0.10662659  0.47595388
  -0.29462188]
 [-0.35303935 -0.11726297  0.4860032  ... -0.33492708  0.5810381
  -0.4089383 ]
 ...
 [-0.10364315  0.19907542  0.1030468  ... -0.04610247  0.39252508
  -0.12772171]
 [-0.13825178 -0.01822857  0.48183176 ... -0.12755421  0.42092112
  -0.22456597]
 [ 0.22731614  0.13484395  0.3506068  ...  0.00809846  0.58762664
  -0.00586685]]

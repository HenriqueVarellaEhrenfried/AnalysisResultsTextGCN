(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95117 train_acc= 0.02960 val_loss= 3.34670 val_acc= 0.67228 time= 0.44367
Epoch: 0002 train_loss= 3.35354 train_acc= 0.65300 val_loss= 2.39956 val_acc= 0.67075 time= 0.20400
Epoch: 0003 train_loss= 2.38953 train_acc= 0.65062 val_loss= 2.22547 val_acc= 0.60643 time= 0.18104
Epoch: 0004 train_loss= 2.22804 train_acc= 0.59398 val_loss= 2.01139 val_acc= 0.49311 time= 0.16800
Epoch: 0005 train_loss= 2.04651 train_acc= 0.47389 val_loss= 1.63010 val_acc= 0.67075 time= 0.16800
Epoch: 0006 train_loss= 1.67805 train_acc= 0.65266 val_loss= 1.46483 val_acc= 0.66922 time= 0.17000
Epoch: 0007 train_loss= 1.51319 train_acc= 0.64807 val_loss= 1.36051 val_acc= 0.69832 time= 0.18900
Epoch: 0008 train_loss= 1.39602 train_acc= 0.68838 val_loss= 1.25533 val_acc= 0.71210 time= 0.16600
Epoch: 0009 train_loss= 1.28612 train_acc= 0.70454 val_loss= 1.16387 val_acc= 0.72129 time= 0.16899
Epoch: 0010 train_loss= 1.18064 train_acc= 0.72189 val_loss= 1.09327 val_acc= 0.73507 time= 0.17105
Epoch: 0011 train_loss= 1.10046 train_acc= 0.73754 val_loss= 1.03548 val_acc= 0.73966 time= 0.16999
Epoch: 0012 train_loss= 1.03136 train_acc= 0.74571 val_loss= 0.97603 val_acc= 0.75038 time= 0.17000
Epoch: 0013 train_loss= 0.97547 train_acc= 0.76050 val_loss= 0.90975 val_acc= 0.76876 time= 0.16700
Epoch: 0014 train_loss= 0.91562 train_acc= 0.77479 val_loss= 0.84475 val_acc= 0.79326 time= 0.16605
Epoch: 0015 train_loss= 0.83773 train_acc= 0.80575 val_loss= 0.78566 val_acc= 0.81164 time= 0.16704
Epoch: 0016 train_loss= 0.79535 train_acc= 0.81800 val_loss= 0.73352 val_acc= 0.83308 time= 0.16707
Epoch: 0017 train_loss= 0.73758 train_acc= 0.83671 val_loss= 0.68915 val_acc= 0.84686 time= 0.16806
Epoch: 0018 train_loss= 0.68891 train_acc= 0.85065 val_loss= 0.65049 val_acc= 0.84686 time= 0.19500
Epoch: 0019 train_loss= 0.63987 train_acc= 0.85746 val_loss= 0.61336 val_acc= 0.85452 time= 0.17100
Epoch: 0020 train_loss= 0.60633 train_acc= 0.86137 val_loss= 0.57752 val_acc= 0.85758 time= 0.17051
Epoch: 0021 train_loss= 0.55638 train_acc= 0.87107 val_loss= 0.54466 val_acc= 0.86524 time= 0.18904
Epoch: 0022 train_loss= 0.51749 train_acc= 0.88059 val_loss= 0.51569 val_acc= 0.86983 time= 0.16538
Epoch: 0023 train_loss= 0.48964 train_acc= 0.89182 val_loss= 0.48878 val_acc= 0.87749 time= 0.16700
Epoch: 0024 train_loss= 0.44244 train_acc= 0.89862 val_loss= 0.46151 val_acc= 0.88055 time= 0.19100
Epoch: 0025 train_loss= 0.41057 train_acc= 0.90338 val_loss= 0.43505 val_acc= 0.88361 time= 0.16812
Epoch: 0026 train_loss= 0.38007 train_acc= 0.90985 val_loss= 0.41055 val_acc= 0.89587 time= 0.16900
Epoch: 0027 train_loss= 0.34405 train_acc= 0.91529 val_loss= 0.38979 val_acc= 0.89587 time= 0.17597
Epoch: 0028 train_loss= 0.32625 train_acc= 0.91648 val_loss= 0.37378 val_acc= 0.89587 time= 0.17100
Epoch: 0029 train_loss= 0.30165 train_acc= 0.92448 val_loss= 0.35960 val_acc= 0.89280 time= 0.17000
Epoch: 0030 train_loss= 0.27774 train_acc= 0.92771 val_loss= 0.34791 val_acc= 0.89893 time= 0.19303
Epoch: 0031 train_loss= 0.25954 train_acc= 0.93604 val_loss= 0.33228 val_acc= 0.90352 time= 0.16600
Epoch: 0032 train_loss= 0.23423 train_acc= 0.94387 val_loss= 0.31543 val_acc= 0.91271 time= 0.18197
Epoch: 0033 train_loss= 0.21618 train_acc= 0.94387 val_loss= 0.30325 val_acc= 0.91884 time= 0.16700
Epoch: 0034 train_loss= 0.19981 train_acc= 0.95135 val_loss= 0.29680 val_acc= 0.92190 time= 0.16600
Epoch: 0035 train_loss= 0.19051 train_acc= 0.95373 val_loss= 0.30037 val_acc= 0.91730 time= 0.16903
Epoch: 0036 train_loss= 0.16717 train_acc= 0.95765 val_loss= 0.30244 val_acc= 0.91884 time= 0.19297
Epoch: 0037 train_loss= 0.16258 train_acc= 0.95714 val_loss= 0.30523 val_acc= 0.92190 time= 0.17100
Early stopping...
Optimization Finished!
Test set results: cost= 0.32450 accuracy= 0.91550 time= 0.07800
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.5000    0.6667         8
           1     0.8000    0.6667    0.7273         6
           2     0.0000    0.0000    0.0000         1
           3     0.8148    0.8800    0.8462        75
           4     1.0000    1.0000    1.0000         9
           5     0.8427    0.8621    0.8523        87
           6     0.9600    0.9600    0.9600        25
           7     0.6111    0.8462    0.7097        13
           8     0.4000    0.1818    0.2500        11
           9     1.0000    0.3333    0.5000         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    0.9167    0.9565        12
          12     0.8108    0.9917    0.8922       121
          13     0.9286    0.6842    0.7879        19
          14     0.7419    0.8214    0.7797        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.2000    0.3333        10
          19     1.0000    1.0000    1.0000         2
          20     0.5556    0.5556    0.5556         9
          21     0.8696    1.0000    0.9302        20
          22     0.3333    0.4000    0.3636         5
          23     0.0000    0.0000    0.0000         1
          24     0.5000    0.5882    0.5405        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.8571    0.5000    0.6316        12
          28     1.0000    0.6364    0.7778        11
          29     0.9736    0.9526    0.9630       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.3333    0.5000         3
          32     0.4444    0.8000    0.5714        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.7667    0.8519    0.8070        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9711    0.9926    0.9817      1083
          40     0.6000    0.6000    0.6000         5
          41     0.0000    0.0000    0.0000         2
          42     0.6154    0.8889    0.7273         9
          43     1.0000    0.3333    0.5000         3
          44     0.6154    0.6667    0.6400        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.5652    0.8667    0.6842        15
          48     0.6923    1.0000    0.8182         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9155      2568
   macro avg     0.6369    0.5418    0.5586      2568
weighted avg     0.9138    0.9155    0.9088      2568

Macro average Test Precision, Recall and F1-Score...
(0.6369035168364948, 0.5418013250766012, 0.5586277835564276, None)
Micro average Test Precision, Recall and F1-Score...
(0.9154984423676013, 0.9154984423676013, 0.9154984423676013, None)
embeddings:
8892 6532 2568
[[-0.41761014  0.00557188  0.27648804 ... -0.36244646 -0.20246883
   1.6702255 ]
 [-0.10947916 -0.25345975  0.5494668  ... -0.2755603  -0.18036324
   0.32436725]
 [ 0.97312343 -0.41453275  0.45702752 ...  0.0347199  -0.33391577
   0.03313795]
 ...
 [ 0.4260876  -0.21669896  0.21607369 ...  0.20542955 -0.11482468
   0.04176901]
 [ 0.2811487  -0.12964246  0.19434932 ...  0.00845302 -0.11193984
   0.06837671]
 [ 0.44419864  0.04650035  0.07943316 ...  0.02846246  0.20591943
  -0.06721603]]

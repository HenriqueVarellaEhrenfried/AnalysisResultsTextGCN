(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95125 train_acc= 0.01123 val_loss= 3.87819 val_acc= 0.56662 time= 0.54452
Epoch: 0002 train_loss= 3.87809 train_acc= 0.56710 val_loss= 3.72310 val_acc= 0.56815 time= 0.22105
Epoch: 0003 train_loss= 3.72236 train_acc= 0.57085 val_loss= 3.47824 val_acc= 0.56049 time= 0.21100
Epoch: 0004 train_loss= 3.47707 train_acc= 0.56353 val_loss= 3.15785 val_acc= 0.54977 time= 0.20996
Epoch: 0005 train_loss= 3.15959 train_acc= 0.53632 val_loss= 2.80805 val_acc= 0.54058 time= 0.21104
Epoch: 0006 train_loss= 2.81050 train_acc= 0.51403 val_loss= 2.49468 val_acc= 0.51914 time= 0.22996
Epoch: 0007 train_loss= 2.51631 train_acc= 0.51335 val_loss= 2.28893 val_acc= 0.50536 time= 0.21301
Epoch: 0008 train_loss= 2.28312 train_acc= 0.48546 val_loss= 2.20109 val_acc= 0.49464 time= 0.21000
Epoch: 0009 train_loss= 2.19284 train_acc= 0.47134 val_loss= 2.16229 val_acc= 0.48086 time= 0.23100
Epoch: 0010 train_loss= 2.16787 train_acc= 0.45841 val_loss= 2.10986 val_acc= 0.47933 time= 0.20911
Epoch: 0011 train_loss= 2.13113 train_acc= 0.45178 val_loss= 2.02351 val_acc= 0.48239 time= 0.20900
Epoch: 0012 train_loss= 2.04679 train_acc= 0.45960 val_loss= 1.91101 val_acc= 0.51455 time= 0.21100
Epoch: 0013 train_loss= 1.93549 train_acc= 0.51624 val_loss= 1.79535 val_acc= 0.58959 time= 0.21357
Epoch: 0014 train_loss= 1.82509 train_acc= 0.57952 val_loss= 1.69939 val_acc= 0.64778 time= 0.23205
Epoch: 0015 train_loss= 1.72284 train_acc= 0.62953 val_loss= 1.62967 val_acc= 0.67228 time= 0.21499
Epoch: 0016 train_loss= 1.64886 train_acc= 0.65470 val_loss= 1.57233 val_acc= 0.67841 time= 0.20900
Epoch: 0017 train_loss= 1.59552 train_acc= 0.66185 val_loss= 1.51298 val_acc= 0.68453 time= 0.21099
Epoch: 0018 train_loss= 1.53323 train_acc= 0.66848 val_loss= 1.44916 val_acc= 0.69066 time= 0.21300
Epoch: 0019 train_loss= 1.48132 train_acc= 0.67546 val_loss= 1.38558 val_acc= 0.69066 time= 0.22601
Epoch: 0020 train_loss= 1.41407 train_acc= 0.67597 val_loss= 1.32754 val_acc= 0.69372 time= 0.22557
Epoch: 0021 train_loss= 1.35321 train_acc= 0.68362 val_loss= 1.27676 val_acc= 0.70597 time= 0.21201
Epoch: 0022 train_loss= 1.30088 train_acc= 0.69008 val_loss= 1.23242 val_acc= 0.71363 time= 0.21200
Epoch: 0023 train_loss= 1.25356 train_acc= 0.70590 val_loss= 1.19304 val_acc= 0.72282 time= 0.23200
Epoch: 0024 train_loss= 1.21652 train_acc= 0.71492 val_loss= 1.15697 val_acc= 0.73047 time= 0.21000
Epoch: 0025 train_loss= 1.17876 train_acc= 0.72801 val_loss= 1.12294 val_acc= 0.73354 time= 0.22000
Epoch: 0026 train_loss= 1.14062 train_acc= 0.74162 val_loss= 1.08984 val_acc= 0.74119 time= 0.20900
Epoch: 0027 train_loss= 1.10548 train_acc= 0.75506 val_loss= 1.05692 val_acc= 0.74426 time= 0.21315
Epoch: 0028 train_loss= 1.07205 train_acc= 0.75795 val_loss= 1.02407 val_acc= 0.76110 time= 0.23254
Epoch: 0029 train_loss= 1.03981 train_acc= 0.76493 val_loss= 0.99169 val_acc= 0.76723 time= 0.22300
Epoch: 0030 train_loss= 1.00677 train_acc= 0.77479 val_loss= 0.96034 val_acc= 0.77489 time= 0.20701
Epoch: 0031 train_loss= 0.97214 train_acc= 0.78517 val_loss= 0.93056 val_acc= 0.78254 time= 0.21000
Epoch: 0032 train_loss= 0.94600 train_acc= 0.78653 val_loss= 0.90229 val_acc= 0.79939 time= 0.23099
Epoch: 0033 train_loss= 0.91374 train_acc= 0.79792 val_loss= 0.87527 val_acc= 0.80551 time= 0.20800
Epoch: 0034 train_loss= 0.88459 train_acc= 0.81136 val_loss= 0.84905 val_acc= 0.81011 time= 0.21262
Epoch: 0035 train_loss= 0.85918 train_acc= 0.81732 val_loss= 0.82337 val_acc= 0.82236 time= 0.21200
Epoch: 0036 train_loss= 0.83398 train_acc= 0.82786 val_loss= 0.79815 val_acc= 0.83614 time= 0.21600
Epoch: 0037 train_loss= 0.80560 train_acc= 0.83279 val_loss= 0.77344 val_acc= 0.84227 time= 0.22606
Epoch: 0038 train_loss= 0.78034 train_acc= 0.83756 val_loss= 0.74932 val_acc= 0.84992 time= 0.21256
Epoch: 0039 train_loss= 0.75511 train_acc= 0.84181 val_loss= 0.72583 val_acc= 0.85452 time= 0.22000
Epoch: 0040 train_loss= 0.72790 train_acc= 0.84674 val_loss= 0.70318 val_acc= 0.85452 time= 0.20900
Epoch: 0041 train_loss= 0.70556 train_acc= 0.84776 val_loss= 0.68137 val_acc= 0.85299 time= 0.21550
Epoch: 0042 train_loss= 0.68441 train_acc= 0.85355 val_loss= 0.66040 val_acc= 0.85605 time= 0.23317
Epoch: 0043 train_loss= 0.65427 train_acc= 0.86001 val_loss= 0.64033 val_acc= 0.85758 time= 0.22200
Epoch: 0044 train_loss= 0.63153 train_acc= 0.86222 val_loss= 0.62120 val_acc= 0.86217 time= 0.21001
Epoch: 0045 train_loss= 0.61076 train_acc= 0.86477 val_loss= 0.60279 val_acc= 0.86677 time= 0.21296
Epoch: 0046 train_loss= 0.58811 train_acc= 0.87056 val_loss= 0.58435 val_acc= 0.87289 time= 0.22512
Epoch: 0047 train_loss= 0.57693 train_acc= 0.87362 val_loss= 0.56606 val_acc= 0.87443 time= 0.20800
Epoch: 0048 train_loss= 0.54978 train_acc= 0.87736 val_loss= 0.54800 val_acc= 0.87596 time= 0.22757
Epoch: 0049 train_loss= 0.53024 train_acc= 0.88127 val_loss= 0.53047 val_acc= 0.87902 time= 0.21301
Epoch: 0050 train_loss= 0.50975 train_acc= 0.88382 val_loss= 0.51395 val_acc= 0.88055 time= 0.21500
Epoch: 0051 train_loss= 0.48841 train_acc= 0.88808 val_loss= 0.49818 val_acc= 0.88668 time= 0.21600
Epoch: 0052 train_loss= 0.47688 train_acc= 0.89182 val_loss= 0.48326 val_acc= 0.88208 time= 0.21600
Epoch: 0053 train_loss= 0.45905 train_acc= 0.89726 val_loss= 0.46918 val_acc= 0.87902 time= 0.21087
Epoch: 0054 train_loss= 0.44175 train_acc= 0.89930 val_loss= 0.45561 val_acc= 0.88208 time= 0.21004
Epoch: 0055 train_loss= 0.42449 train_acc= 0.90236 val_loss= 0.44275 val_acc= 0.88668 time= 0.21296
Epoch: 0056 train_loss= 0.41824 train_acc= 0.90934 val_loss= 0.43003 val_acc= 0.89587 time= 0.21200
Epoch: 0057 train_loss= 0.39646 train_acc= 0.91427 val_loss= 0.41755 val_acc= 0.89893 time= 0.22027
Epoch: 0058 train_loss= 0.37383 train_acc= 0.91801 val_loss= 0.40581 val_acc= 0.90199 time= 0.21200
Epoch: 0059 train_loss= 0.36180 train_acc= 0.92159 val_loss= 0.39505 val_acc= 0.90199 time= 0.21204
Epoch: 0060 train_loss= 0.35352 train_acc= 0.92414 val_loss= 0.38549 val_acc= 0.90658 time= 0.23096
Epoch: 0061 train_loss= 0.34054 train_acc= 0.92839 val_loss= 0.37713 val_acc= 0.90812 time= 0.21100
Epoch: 0062 train_loss= 0.32623 train_acc= 0.93366 val_loss= 0.36908 val_acc= 0.91118 time= 0.22397
Epoch: 0063 train_loss= 0.31608 train_acc= 0.93247 val_loss= 0.36125 val_acc= 0.90965 time= 0.21200
Epoch: 0064 train_loss= 0.30122 train_acc= 0.93706 val_loss= 0.35306 val_acc= 0.90812 time= 0.21600
Epoch: 0065 train_loss= 0.29368 train_acc= 0.94098 val_loss= 0.34503 val_acc= 0.90812 time= 0.22900
Epoch: 0066 train_loss= 0.28194 train_acc= 0.94047 val_loss= 0.33755 val_acc= 0.91118 time= 0.22200
Epoch: 0067 train_loss= 0.27547 train_acc= 0.94353 val_loss= 0.33040 val_acc= 0.91118 time= 0.20900
Epoch: 0068 train_loss= 0.26202 train_acc= 0.94455 val_loss= 0.32398 val_acc= 0.91118 time= 0.21400
Epoch: 0069 train_loss= 0.25992 train_acc= 0.94778 val_loss= 0.31717 val_acc= 0.91577 time= 0.23300
Epoch: 0070 train_loss= 0.24453 train_acc= 0.95288 val_loss= 0.31151 val_acc= 0.92037 time= 0.21380
Epoch: 0071 train_loss= 0.23641 train_acc= 0.95101 val_loss= 0.30616 val_acc= 0.92037 time= 0.22200
Epoch: 0072 train_loss= 0.22613 train_acc= 0.95407 val_loss= 0.30058 val_acc= 0.91884 time= 0.21200
Epoch: 0073 train_loss= 0.21832 train_acc= 0.95339 val_loss= 0.29590 val_acc= 0.91884 time= 0.21010
Epoch: 0074 train_loss= 0.21281 train_acc= 0.95816 val_loss= 0.29218 val_acc= 0.91884 time= 0.21000
Epoch: 0075 train_loss= 0.20554 train_acc= 0.96020 val_loss= 0.28846 val_acc= 0.91884 time= 0.21099
Epoch: 0076 train_loss= 0.19763 train_acc= 0.96105 val_loss= 0.28443 val_acc= 0.92037 time= 0.21400
Epoch: 0077 train_loss= 0.19407 train_acc= 0.96003 val_loss= 0.28020 val_acc= 0.92190 time= 0.21286
Epoch: 0078 train_loss= 0.18130 train_acc= 0.96207 val_loss= 0.27612 val_acc= 0.92649 time= 0.23257
Epoch: 0079 train_loss= 0.17534 train_acc= 0.96428 val_loss= 0.27176 val_acc= 0.92802 time= 0.21101
Epoch: 0080 train_loss= 0.16656 train_acc= 0.96649 val_loss= 0.26700 val_acc= 0.92956 time= 0.21699
Epoch: 0081 train_loss= 0.16508 train_acc= 0.96751 val_loss= 0.26278 val_acc= 0.93262 time= 0.20800
Epoch: 0082 train_loss= 0.16106 train_acc= 0.96530 val_loss= 0.25971 val_acc= 0.93262 time= 0.21100
Epoch: 0083 train_loss= 0.15231 train_acc= 0.97142 val_loss= 0.25752 val_acc= 0.93262 time= 0.23000
Epoch: 0084 train_loss= 0.15108 train_acc= 0.96819 val_loss= 0.25532 val_acc= 0.93262 time= 0.21200
Epoch: 0085 train_loss= 0.14271 train_acc= 0.97346 val_loss= 0.25380 val_acc= 0.92956 time= 0.22535
Epoch: 0086 train_loss= 0.13694 train_acc= 0.97363 val_loss= 0.25253 val_acc= 0.92802 time= 0.21200
Epoch: 0087 train_loss= 0.13244 train_acc= 0.97568 val_loss= 0.25116 val_acc= 0.92956 time= 0.21199
Epoch: 0088 train_loss= 0.12870 train_acc= 0.97449 val_loss= 0.24876 val_acc= 0.93109 time= 0.22700
Epoch: 0089 train_loss= 0.12578 train_acc= 0.97534 val_loss= 0.24624 val_acc= 0.93415 time= 0.22300
Epoch: 0090 train_loss= 0.12061 train_acc= 0.97738 val_loss= 0.24469 val_acc= 0.93568 time= 0.20900
Epoch: 0091 train_loss= 0.11509 train_acc= 0.97874 val_loss= 0.24243 val_acc= 0.93415 time= 0.21508
Epoch: 0092 train_loss= 0.11080 train_acc= 0.97857 val_loss= 0.24055 val_acc= 0.93262 time= 0.23600
Epoch: 0093 train_loss= 0.10994 train_acc= 0.98061 val_loss= 0.23810 val_acc= 0.93415 time= 0.21000
Epoch: 0094 train_loss= 0.10637 train_acc= 0.98214 val_loss= 0.23594 val_acc= 0.93415 time= 0.20900
Epoch: 0095 train_loss= 0.10117 train_acc= 0.98180 val_loss= 0.23415 val_acc= 0.93568 time= 0.21100
Epoch: 0096 train_loss= 0.09705 train_acc= 0.98316 val_loss= 0.23367 val_acc= 0.93109 time= 0.21197
Epoch: 0097 train_loss= 0.09736 train_acc= 0.98231 val_loss= 0.23322 val_acc= 0.93109 time= 0.21903
Epoch: 0098 train_loss= 0.09552 train_acc= 0.98299 val_loss= 0.23361 val_acc= 0.93415 time= 0.21524
Epoch: 0099 train_loss= 0.08907 train_acc= 0.98418 val_loss= 0.23482 val_acc= 0.93262 time= 0.22308
Epoch: 0100 train_loss= 0.08641 train_acc= 0.98367 val_loss= 0.23442 val_acc= 0.93109 time= 0.21196
Epoch: 0101 train_loss= 0.08223 train_acc= 0.98639 val_loss= 0.23223 val_acc= 0.93568 time= 0.21400
Epoch: 0102 train_loss= 0.08231 train_acc= 0.98537 val_loss= 0.23019 val_acc= 0.93874 time= 0.22700
Epoch: 0103 train_loss= 0.07710 train_acc= 0.98860 val_loss= 0.22873 val_acc= 0.93874 time= 0.21800
Epoch: 0104 train_loss= 0.07462 train_acc= 0.98792 val_loss= 0.22802 val_acc= 0.93874 time= 0.21099
Epoch: 0105 train_loss= 0.07420 train_acc= 0.98707 val_loss= 0.22819 val_acc= 0.93874 time= 0.21405
Epoch: 0106 train_loss= 0.07183 train_acc= 0.98758 val_loss= 0.22941 val_acc= 0.93721 time= 0.22996
Epoch: 0107 train_loss= 0.06819 train_acc= 0.98979 val_loss= 0.23132 val_acc= 0.93262 time= 0.21303
Early stopping...
Optimization Finished!
Test set results: cost= 0.26021 accuracy= 0.93614 time= 0.10500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7802    0.9467    0.8554        75
           4     1.0000    1.0000    1.0000         9
           5     0.8119    0.9425    0.8723        87
           6     0.9200    0.9200    0.9200        25
           7     0.7059    0.9231    0.8000        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.3333    0.5000         9
          10     0.9231    0.6667    0.7742        36
          11     1.0000    0.9167    0.9565        12
          12     0.8451    0.9917    0.9125       121
          13     1.0000    0.7895    0.8824        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9724    0.9626    0.9675       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.6667    0.8000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8533    0.7901    0.8205        81
          36     1.0000    0.4167    0.5882        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9917    0.9844      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8182    0.7500    0.7826        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9361      2568
   macro avg     0.7225    0.6536    0.6671      2568
weighted avg     0.9334    0.9361    0.9307      2568

Macro average Test Precision, Recall and F1-Score...
(0.7224902343266119, 0.6535646913092454, 0.6670736405342927, None)
Micro average Test Precision, Recall and F1-Score...
(0.9361370716510904, 0.9361370716510904, 0.9361370716510904, None)
embeddings:
8892 6532 2568
[[-1.7954685e-01  9.6819371e-02  6.3693412e-03 ...  2.5378156e-02
   1.5124568e-01 -1.0018155e-01]
 [ 6.5309510e-02  1.7554611e-02  9.3082897e-05 ...  3.4077015e-02
   7.4201755e-02  1.5831859e-01]
 [ 2.4055758e-02  4.6421025e-02 -3.5118140e-02 ...  2.4911179e-01
   1.8509302e-01  4.4636406e-02]
 ...
 [ 4.9931504e-02  1.5866151e-01  1.3947576e-02 ...  1.0589751e-01
  -3.5024323e-03  1.6615050e-01]
 [ 4.4850867e-02  4.7573220e-02  8.5802022e-03 ...  1.3223402e-01
   7.8119256e-02  5.4982599e-02]
 [ 1.8208422e-01  1.9393118e-01  1.8293147e-01 ...  3.3380499e-01
   1.8627867e-01  1.9053847e-01]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95132 train_acc= 0.00629 val_loss= 3.87891 val_acc= 0.64319 time= 2.35353
Epoch: 0002 train_loss= 3.87945 train_acc= 0.63242 val_loss= 3.72447 val_acc= 0.64625 time= 2.21500
Epoch: 0003 train_loss= 3.72460 train_acc= 0.64076 val_loss= 3.48025 val_acc= 0.64625 time= 2.21300
Epoch: 0004 train_loss= 3.48714 train_acc= 0.63106 val_loss= 3.15957 val_acc= 0.63553 time= 2.19400
Epoch: 0005 train_loss= 3.16734 train_acc= 0.63055 val_loss= 2.80943 val_acc= 0.62634 time= 2.26900
Epoch: 0006 train_loss= 2.81453 train_acc= 0.61592 val_loss= 2.49796 val_acc= 0.60643 time= 2.20056
Epoch: 0007 train_loss= 2.48720 train_acc= 0.60384 val_loss= 2.29624 val_acc= 0.58499 time= 2.18100
Epoch: 0008 train_loss= 2.29947 train_acc= 0.57255 val_loss= 2.21134 val_acc= 0.53446 time= 2.19400
Epoch: 0009 train_loss= 2.21116 train_acc= 0.52917 val_loss= 2.17360 val_acc= 0.48545 time= 2.21600
Epoch: 0010 train_loss= 2.18469 train_acc= 0.46079 val_loss= 2.12376 val_acc= 0.47014 time= 2.20000
Epoch: 0011 train_loss= 2.13953 train_acc= 0.44276 val_loss= 2.04045 val_acc= 0.47167 time= 2.19700
Epoch: 0012 train_loss= 2.05869 train_acc= 0.44838 val_loss= 1.93063 val_acc= 0.49923 time= 2.20400
Epoch: 0013 train_loss= 1.96007 train_acc= 0.48086 val_loss= 1.81603 val_acc= 0.56968 time= 2.22200
Epoch: 0014 train_loss= 1.83846 train_acc= 0.56404 val_loss= 1.71989 val_acc= 0.64625 time= 2.21700
Epoch: 0015 train_loss= 1.74866 train_acc= 0.63327 val_loss= 1.64892 val_acc= 0.66922 time= 2.19600
Epoch: 0016 train_loss= 1.67131 train_acc= 0.65011 val_loss= 1.59112 val_acc= 0.67688 time= 2.21100
Epoch: 0017 train_loss= 1.62452 train_acc= 0.65504 val_loss= 1.53283 val_acc= 0.67381 time= 2.19313
Epoch: 0018 train_loss= 1.56268 train_acc= 0.65640 val_loss= 1.47119 val_acc= 0.67534 time= 2.20600
Epoch: 0019 train_loss= 1.49442 train_acc= 0.66083 val_loss= 1.41019 val_acc= 0.68606 time= 2.21200
Epoch: 0020 train_loss= 1.44090 train_acc= 0.66576 val_loss= 1.35376 val_acc= 0.68913 time= 2.20561
Epoch: 0021 train_loss= 1.38143 train_acc= 0.67511 val_loss= 1.30387 val_acc= 0.69372 time= 2.21600
Epoch: 0022 train_loss= 1.32999 train_acc= 0.67954 val_loss= 1.26024 val_acc= 0.70750 time= 2.19656
Epoch: 0023 train_loss= 1.28599 train_acc= 0.69110 val_loss= 1.22149 val_acc= 0.71516 time= 2.21148
Epoch: 0024 train_loss= 1.24567 train_acc= 0.70080 val_loss= 1.18588 val_acc= 0.72282 time= 2.22000
Epoch: 0025 train_loss= 1.20697 train_acc= 0.71441 val_loss= 1.15210 val_acc= 0.72741 time= 2.20349
Epoch: 0026 train_loss= 1.16879 train_acc= 0.73040 val_loss= 1.11907 val_acc= 0.73201 time= 2.21200
Epoch: 0027 train_loss= 1.13463 train_acc= 0.74128 val_loss= 1.08595 val_acc= 0.73660 time= 2.21200
Epoch: 0028 train_loss= 1.10086 train_acc= 0.74979 val_loss= 1.05242 val_acc= 0.74579 time= 2.20286
Epoch: 0029 train_loss= 1.07013 train_acc= 0.75523 val_loss= 1.01903 val_acc= 0.75804 time= 2.19083
Epoch: 0030 train_loss= 1.03405 train_acc= 0.76476 val_loss= 0.98622 val_acc= 0.76876 time= 2.18800
Epoch: 0031 train_loss= 0.99935 train_acc= 0.77054 val_loss= 0.95469 val_acc= 0.77642 time= 2.22350
Epoch: 0032 train_loss= 0.96493 train_acc= 0.77904 val_loss= 0.92466 val_acc= 0.78561 time= 2.21000
Epoch: 0033 train_loss= 0.93999 train_acc= 0.78534 val_loss= 0.89595 val_acc= 0.79786 time= 2.19957
Epoch: 0034 train_loss= 0.91488 train_acc= 0.79963 val_loss= 0.86837 val_acc= 0.80551 time= 2.22039
Epoch: 0035 train_loss= 0.88287 train_acc= 0.80847 val_loss= 0.84164 val_acc= 0.81623 time= 2.19300
Epoch: 0036 train_loss= 0.85780 train_acc= 0.81697 val_loss= 0.81568 val_acc= 0.83155 time= 2.20200
Epoch: 0037 train_loss= 0.82908 train_acc= 0.82905 val_loss= 0.79042 val_acc= 0.83767 time= 2.20900
Epoch: 0038 train_loss= 0.79502 train_acc= 0.83501 val_loss= 0.76571 val_acc= 0.83767 time= 2.20427
Epoch: 0039 train_loss= 0.77528 train_acc= 0.83977 val_loss= 0.74165 val_acc= 0.83920 time= 2.20900
Epoch: 0040 train_loss= 0.74879 train_acc= 0.84113 val_loss= 0.71806 val_acc= 0.84533 time= 2.19700
Epoch: 0041 train_loss= 0.71892 train_acc= 0.84572 val_loss= 0.69514 val_acc= 0.85146 time= 2.20500
Epoch: 0042 train_loss= 0.69670 train_acc= 0.85167 val_loss= 0.67261 val_acc= 0.85146 time= 2.20604
Epoch: 0043 train_loss= 0.67183 train_acc= 0.85746 val_loss= 0.65073 val_acc= 0.85452 time= 2.19900
Epoch: 0044 train_loss= 0.64534 train_acc= 0.85950 val_loss= 0.62939 val_acc= 0.86064 time= 2.21000
Epoch: 0045 train_loss= 0.62176 train_acc= 0.86103 val_loss= 0.60880 val_acc= 0.86983 time= 2.21428
Epoch: 0046 train_loss= 0.59871 train_acc= 0.86851 val_loss= 0.58886 val_acc= 0.87136 time= 2.22126
Epoch: 0047 train_loss= 0.58120 train_acc= 0.87328 val_loss= 0.56960 val_acc= 0.87289 time= 2.19899
Epoch: 0048 train_loss= 0.56139 train_acc= 0.87787 val_loss= 0.55080 val_acc= 0.87289 time= 2.20601
Epoch: 0049 train_loss= 0.54311 train_acc= 0.88263 val_loss= 0.53315 val_acc= 0.87443 time= 2.21209
Epoch: 0050 train_loss= 0.52651 train_acc= 0.88093 val_loss= 0.51679 val_acc= 0.87443 time= 2.21438
Epoch: 0051 train_loss= 0.49611 train_acc= 0.88705 val_loss= 0.50123 val_acc= 0.87596 time= 2.19200
Epoch: 0052 train_loss= 0.48228 train_acc= 0.88961 val_loss= 0.48682 val_acc= 0.87749 time= 2.19899
Epoch: 0053 train_loss= 0.46484 train_acc= 0.89573 val_loss= 0.47292 val_acc= 0.87902 time= 2.21800
Epoch: 0054 train_loss= 0.44318 train_acc= 0.89879 val_loss= 0.45911 val_acc= 0.88208 time= 2.20285
Epoch: 0055 train_loss= 0.42544 train_acc= 0.90253 val_loss= 0.44554 val_acc= 0.88361 time= 2.21483
Epoch: 0056 train_loss= 0.41716 train_acc= 0.90730 val_loss= 0.43192 val_acc= 0.88974 time= 2.17500
Epoch: 0057 train_loss= 0.40167 train_acc= 0.91138 val_loss= 0.41847 val_acc= 0.89587 time= 2.20499
Epoch: 0058 train_loss= 0.38724 train_acc= 0.91580 val_loss= 0.40568 val_acc= 0.90352 time= 2.19800
Epoch: 0059 train_loss= 0.37091 train_acc= 0.92124 val_loss= 0.39420 val_acc= 0.90352 time= 2.19811
Epoch: 0060 train_loss= 0.36064 train_acc= 0.92090 val_loss= 0.38470 val_acc= 0.90505 time= 2.18500
Epoch: 0061 train_loss= 0.34686 train_acc= 0.92686 val_loss= 0.37660 val_acc= 0.90352 time= 2.21100
Epoch: 0062 train_loss= 0.33559 train_acc= 0.93043 val_loss= 0.36859 val_acc= 0.90352 time= 2.21900
Epoch: 0063 train_loss= 0.32256 train_acc= 0.93162 val_loss= 0.36098 val_acc= 0.90659 time= 2.18700
Epoch: 0064 train_loss= 0.30914 train_acc= 0.93383 val_loss= 0.35356 val_acc= 0.90505 time= 2.21700
Epoch: 0065 train_loss= 0.29678 train_acc= 0.94080 val_loss= 0.34574 val_acc= 0.90659 time= 2.19800
Epoch: 0066 train_loss= 0.28477 train_acc= 0.94251 val_loss= 0.33787 val_acc= 0.90505 time= 2.18456
Epoch: 0067 train_loss= 0.27653 train_acc= 0.94472 val_loss= 0.33052 val_acc= 0.90505 time= 2.19497
Epoch: 0068 train_loss= 0.26794 train_acc= 0.94489 val_loss= 0.32353 val_acc= 0.90965 time= 2.21477
Epoch: 0069 train_loss= 0.25349 train_acc= 0.95033 val_loss= 0.31692 val_acc= 0.91424 time= 2.18006
Epoch: 0070 train_loss= 0.24463 train_acc= 0.95186 val_loss= 0.31082 val_acc= 0.91731 time= 2.18500
Epoch: 0071 train_loss= 0.23347 train_acc= 0.95594 val_loss= 0.30460 val_acc= 0.91577 time= 2.20900
Epoch: 0072 train_loss= 0.22578 train_acc= 0.95594 val_loss= 0.29826 val_acc= 0.91577 time= 2.19800
Epoch: 0073 train_loss= 0.22054 train_acc= 0.95373 val_loss= 0.29281 val_acc= 0.92037 time= 2.22500
Epoch: 0074 train_loss= 0.20917 train_acc= 0.95713 val_loss= 0.28781 val_acc= 0.92037 time= 2.20900
Epoch: 0075 train_loss= 0.20674 train_acc= 0.95764 val_loss= 0.28382 val_acc= 0.92190 time= 2.21300
Epoch: 0076 train_loss= 0.19767 train_acc= 0.95815 val_loss= 0.28122 val_acc= 0.92037 time= 2.20938
Epoch: 0077 train_loss= 0.18973 train_acc= 0.96054 val_loss= 0.27929 val_acc= 0.92343 time= 2.20414
Epoch: 0078 train_loss= 0.18413 train_acc= 0.96173 val_loss= 0.27678 val_acc= 0.92343 time= 2.20400
Epoch: 0079 train_loss= 0.17374 train_acc= 0.96632 val_loss= 0.27492 val_acc= 0.92343 time= 2.19412
Epoch: 0080 train_loss= 0.16859 train_acc= 0.96615 val_loss= 0.27126 val_acc= 0.92496 time= 2.21000
Epoch: 0081 train_loss= 0.16500 train_acc= 0.96734 val_loss= 0.26533 val_acc= 0.92649 time= 2.18791
Epoch: 0082 train_loss= 0.15544 train_acc= 0.97074 val_loss= 0.25963 val_acc= 0.92956 time= 2.21954
Epoch: 0083 train_loss= 0.14852 train_acc= 0.97023 val_loss= 0.25586 val_acc= 0.93262 time= 2.20899
Epoch: 0084 train_loss= 0.14370 train_acc= 0.97108 val_loss= 0.25297 val_acc= 0.93262 time= 2.21000
Epoch: 0085 train_loss= 0.14073 train_acc= 0.97363 val_loss= 0.25012 val_acc= 0.93109 time= 2.19600
Epoch: 0086 train_loss= 0.13364 train_acc= 0.97295 val_loss= 0.24862 val_acc= 0.93109 time= 2.22600
Epoch: 0087 train_loss= 0.13136 train_acc= 0.97380 val_loss= 0.24672 val_acc= 0.92802 time= 2.22900
Epoch: 0088 train_loss= 0.12308 train_acc= 0.97772 val_loss= 0.24396 val_acc= 0.92956 time= 2.19950
Epoch: 0089 train_loss= 0.12109 train_acc= 0.97704 val_loss= 0.23991 val_acc= 0.93415 time= 2.21226
Epoch: 0090 train_loss= 0.11401 train_acc= 0.97908 val_loss= 0.23684 val_acc= 0.93721 time= 2.19499
Epoch: 0091 train_loss= 0.11069 train_acc= 0.98163 val_loss= 0.23599 val_acc= 0.93721 time= 2.21601
Epoch: 0092 train_loss= 0.10766 train_acc= 0.98010 val_loss= 0.23700 val_acc= 0.93568 time= 2.26500
Epoch: 0093 train_loss= 0.10389 train_acc= 0.98265 val_loss= 0.23816 val_acc= 0.93109 time= 2.24300
Epoch: 0094 train_loss= 0.10379 train_acc= 0.98095 val_loss= 0.23780 val_acc= 0.93109 time= 2.22931
Epoch: 0095 train_loss= 0.09597 train_acc= 0.98248 val_loss= 0.23624 val_acc= 0.93568 time= 2.20000
Epoch: 0096 train_loss= 0.09598 train_acc= 0.98452 val_loss= 0.23355 val_acc= 0.93874 time= 2.21900
Epoch: 0097 train_loss= 0.09147 train_acc= 0.98384 val_loss= 0.23119 val_acc= 0.93874 time= 2.19500
Epoch: 0098 train_loss= 0.09156 train_acc= 0.98452 val_loss= 0.22875 val_acc= 0.94181 time= 2.20536
Epoch: 0099 train_loss= 0.08402 train_acc= 0.98571 val_loss= 0.22682 val_acc= 0.94028 time= 2.19400
Epoch: 0100 train_loss= 0.08392 train_acc= 0.98605 val_loss= 0.22680 val_acc= 0.93415 time= 2.20904
Epoch: 0101 train_loss= 0.08002 train_acc= 0.98605 val_loss= 0.22628 val_acc= 0.93721 time= 2.21300
Epoch: 0102 train_loss= 0.07669 train_acc= 0.98809 val_loss= 0.22536 val_acc= 0.93568 time= 2.20900
Epoch: 0103 train_loss= 0.07456 train_acc= 0.98775 val_loss= 0.22436 val_acc= 0.93721 time= 2.19998
Epoch: 0104 train_loss= 0.07486 train_acc= 0.98775 val_loss= 0.22339 val_acc= 0.94028 time= 2.19699
Epoch: 0105 train_loss= 0.06998 train_acc= 0.98775 val_loss= 0.22319 val_acc= 0.93568 time= 2.21800
Epoch: 0106 train_loss= 0.06708 train_acc= 0.98877 val_loss= 0.22358 val_acc= 0.94028 time= 2.22200
Epoch: 0107 train_loss= 0.06797 train_acc= 0.98826 val_loss= 0.22520 val_acc= 0.93721 time= 2.19800
Epoch: 0108 train_loss= 0.06524 train_acc= 0.98758 val_loss= 0.22628 val_acc= 0.93721 time= 2.20900
Early stopping...
Optimization Finished!
Test set results: cost= 0.25295 accuracy= 0.93769 time= 0.72600
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.8020    0.9310    0.8617        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.4444    0.6154         9
          10     0.9200    0.6389    0.7541        36
          11     1.0000    0.9167    0.9565        12
          12     0.8231    1.0000    0.9030       121
          13     0.9375    0.7895    0.8571        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    0.7500    0.8571         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9697    0.9670    0.9683       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8421    0.7901    0.8153        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9817    0.9917    0.9867      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     0.0000    0.0000    0.0000         3
          44     0.8889    0.6667    0.7619        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9377      2568
   macro avg     0.7521    0.6783    0.6899      2568
weighted avg     0.9361    0.9377    0.9325      2568

Macro average Test Precision, Recall and F1-Score...
(0.7521482585613815, 0.6782813949429372, 0.6899038331521342, None)
Micro average Test Precision, Recall and F1-Score...
(0.9376947040498442, 0.9376947040498442, 0.9376947040498442, None)
embeddings:
8892 6532 2568
[[-1.32041544e-01  4.93980721e-02  7.31369078e-01 ... -2.38875225e-02
  -7.79133616e-03  8.48675132e-01]
 [ 6.07498698e-02  1.09719314e-01  4.85687435e-01 ...  1.18900865e-01
   1.60073832e-01  4.01777029e-01]
 [-7.83423781e-02  1.21162958e-01  7.35953331e-01 ... -2.37405002e-02
   1.03185013e-01  3.31059456e-01]
 ...
 [-2.87366174e-02  2.60898590e-01  1.84885293e-01 ...  6.09461144e-02
   4.80840743e-01  3.99440587e-01]
 [-2.50339683e-04  1.42476052e-01  2.98605144e-01 ...  9.15049389e-03
   1.57620326e-01  2.12396428e-01]
 [ 1.80803165e-01  7.56452084e-02  1.40972063e-01 ...  1.88477874e-01
   1.47223502e-01  3.11602205e-01]]

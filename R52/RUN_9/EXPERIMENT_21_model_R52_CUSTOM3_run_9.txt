(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95105 train_acc= 0.03317 val_loss= 3.90343 val_acc= 0.67228 time= 0.45388
Epoch: 0002 train_loss= 3.90423 train_acc= 0.64280 val_loss= 3.80381 val_acc= 0.67381 time= 0.19500
Epoch: 0003 train_loss= 3.80878 train_acc= 0.63344 val_loss= 3.65068 val_acc= 0.67228 time= 0.16703
Epoch: 0004 train_loss= 3.65541 train_acc= 0.63293 val_loss= 3.44559 val_acc= 0.67075 time= 0.16697
Epoch: 0005 train_loss= 3.46161 train_acc= 0.61796 val_loss= 3.20012 val_acc= 0.67075 time= 0.18845
Epoch: 0006 train_loss= 3.20662 train_acc= 0.61592 val_loss= 2.93927 val_acc= 0.66922 time= 0.16600
Epoch: 0007 train_loss= 2.93857 train_acc= 0.61184 val_loss= 2.69278 val_acc= 0.66769 time= 0.16604
Epoch: 0008 train_loss= 2.73045 train_acc= 0.60810 val_loss= 2.49097 val_acc= 0.67228 time= 0.16904
Epoch: 0009 train_loss= 2.45804 train_acc= 0.60691 val_loss= 2.35761 val_acc= 0.66922 time= 0.16901
Epoch: 0010 train_loss= 2.34603 train_acc= 0.59568 val_loss= 2.28818 val_acc= 0.64472 time= 0.16903
Epoch: 0011 train_loss= 2.29406 train_acc= 0.60146 val_loss= 2.25073 val_acc= 0.53905 time= 0.19400
Epoch: 0012 train_loss= 2.24898 train_acc= 0.53121 val_loss= 2.21393 val_acc= 0.46708 time= 0.16701
Epoch: 0013 train_loss= 2.20607 train_acc= 0.47899 val_loss= 2.16248 val_acc= 0.45636 time= 0.16699
Epoch: 0014 train_loss= 2.17906 train_acc= 0.45127 val_loss= 2.09342 val_acc= 0.45636 time= 0.18704
Epoch: 0015 train_loss= 2.09554 train_acc= 0.44889 val_loss= 2.01061 val_acc= 0.45942 time= 0.16700
Epoch: 0016 train_loss= 2.00221 train_acc= 0.44242 val_loss= 1.92211 val_acc= 0.47014 time= 0.16800
Epoch: 0017 train_loss= 1.96697 train_acc= 0.45960 val_loss= 1.83971 val_acc= 0.50995 time= 0.19800
Epoch: 0018 train_loss= 1.87439 train_acc= 0.53070 val_loss= 1.77104 val_acc= 0.60184 time= 0.17051
Epoch: 0019 train_loss= 1.78909 train_acc= 0.59398 val_loss= 1.71503 val_acc= 0.65237 time= 0.19000
Epoch: 0020 train_loss= 1.73322 train_acc= 0.62579 val_loss= 1.66505 val_acc= 0.66922 time= 0.16600
Epoch: 0021 train_loss= 1.74068 train_acc= 0.63803 val_loss= 1.61454 val_acc= 0.66769 time= 0.16912
Epoch: 0022 train_loss= 1.64556 train_acc= 0.63905 val_loss= 1.56318 val_acc= 0.67228 time= 0.18009
Epoch: 0023 train_loss= 1.58457 train_acc= 0.63650 val_loss= 1.51274 val_acc= 0.67534 time= 0.16596
Epoch: 0024 train_loss= 1.53547 train_acc= 0.64024 val_loss= 1.46493 val_acc= 0.67534 time= 0.16805
Epoch: 0025 train_loss= 1.51006 train_acc= 0.64467 val_loss= 1.42068 val_acc= 0.67841 time= 0.18295
Epoch: 0026 train_loss= 1.45349 train_acc= 0.64994 val_loss= 1.38068 val_acc= 0.68606 time= 0.17100
Epoch: 0027 train_loss= 1.43122 train_acc= 0.65283 val_loss= 1.34452 val_acc= 0.69066 time= 0.17100
Epoch: 0028 train_loss= 1.39382 train_acc= 0.65930 val_loss= 1.31172 val_acc= 0.69219 time= 0.17300
Epoch: 0029 train_loss= 1.34652 train_acc= 0.67205 val_loss= 1.28176 val_acc= 0.70444 time= 0.17600
Epoch: 0030 train_loss= 1.33288 train_acc= 0.68124 val_loss= 1.25384 val_acc= 0.70904 time= 0.16805
Epoch: 0031 train_loss= 1.29319 train_acc= 0.69689 val_loss= 1.22732 val_acc= 0.71516 time= 0.18600
Epoch: 0032 train_loss= 1.27779 train_acc= 0.70777 val_loss= 1.20170 val_acc= 0.72282 time= 0.16700
Epoch: 0033 train_loss= 1.24434 train_acc= 0.70301 val_loss= 1.17665 val_acc= 0.72588 time= 0.16703
Epoch: 0034 train_loss= 1.21022 train_acc= 0.71645 val_loss= 1.15223 val_acc= 0.73047 time= 0.19204
Epoch: 0035 train_loss= 1.19686 train_acc= 0.71220 val_loss= 1.12837 val_acc= 0.73507 time= 0.17139
Epoch: 0036 train_loss= 1.15625 train_acc= 0.73397 val_loss= 1.10508 val_acc= 0.73813 time= 0.17497
Epoch: 0037 train_loss= 1.12585 train_acc= 0.74162 val_loss= 1.08232 val_acc= 0.74273 time= 0.18000
Epoch: 0038 train_loss= 1.12279 train_acc= 0.74315 val_loss= 1.06003 val_acc= 0.74426 time= 0.16800
Epoch: 0039 train_loss= 1.09457 train_acc= 0.75013 val_loss= 1.03834 val_acc= 0.75345 time= 0.16600
Epoch: 0040 train_loss= 1.08485 train_acc= 0.74451 val_loss= 1.01710 val_acc= 0.75957 time= 0.17870
Epoch: 0041 train_loss= 1.05474 train_acc= 0.74979 val_loss= 0.99636 val_acc= 0.76570 time= 0.16605
Epoch: 0042 train_loss= 1.02408 train_acc= 0.75608 val_loss= 0.97611 val_acc= 0.77029 time= 0.18600
Epoch: 0043 train_loss= 1.01668 train_acc= 0.76118 val_loss= 0.95622 val_acc= 0.77489 time= 0.16800
Epoch: 0044 train_loss= 1.00709 train_acc= 0.76901 val_loss= 0.93651 val_acc= 0.78101 time= 0.17200
Epoch: 0045 train_loss= 0.96736 train_acc= 0.78449 val_loss= 0.91732 val_acc= 0.79173 time= 0.17266
Epoch: 0046 train_loss= 0.96227 train_acc= 0.78449 val_loss= 0.89861 val_acc= 0.79939 time= 0.19365
Epoch: 0047 train_loss= 0.92571 train_acc= 0.79588 val_loss= 0.88044 val_acc= 0.80858 time= 0.16700
Epoch: 0048 train_loss= 0.93455 train_acc= 0.78704 val_loss= 0.86300 val_acc= 0.81623 time= 0.16507
Epoch: 0049 train_loss= 0.91444 train_acc= 0.80065 val_loss= 0.84572 val_acc= 0.81930 time= 0.16809
Epoch: 0050 train_loss= 0.89603 train_acc= 0.80422 val_loss= 0.82885 val_acc= 0.81776 time= 0.16595
Epoch: 0051 train_loss= 0.87009 train_acc= 0.80881 val_loss= 0.81222 val_acc= 0.81930 time= 0.19204
Epoch: 0052 train_loss= 0.83563 train_acc= 0.81664 val_loss= 0.79584 val_acc= 0.82236 time= 0.16961
Epoch: 0053 train_loss= 0.84263 train_acc= 0.81630 val_loss= 0.77959 val_acc= 0.82695 time= 0.17000
Epoch: 0054 train_loss= 0.79965 train_acc= 0.81868 val_loss= 0.76353 val_acc= 0.82695 time= 0.19000
Epoch: 0055 train_loss= 0.80266 train_acc= 0.81630 val_loss= 0.74791 val_acc= 0.82695 time= 0.17004
Epoch: 0056 train_loss= 0.79393 train_acc= 0.81374 val_loss= 0.73270 val_acc= 0.83155 time= 0.16701
Epoch: 0057 train_loss= 0.77423 train_acc= 0.82327 val_loss= 0.71761 val_acc= 0.83461 time= 0.19195
Epoch: 0058 train_loss= 0.76421 train_acc= 0.82072 val_loss= 0.70284 val_acc= 0.83614 time= 0.16800
Epoch: 0059 train_loss= 0.76312 train_acc= 0.82310 val_loss= 0.68811 val_acc= 0.83767 time= 0.17200
Epoch: 0060 train_loss= 0.71301 train_acc= 0.83416 val_loss= 0.67398 val_acc= 0.84074 time= 0.16800
Epoch: 0061 train_loss= 0.72402 train_acc= 0.83569 val_loss= 0.66054 val_acc= 0.84074 time= 0.17000
Epoch: 0062 train_loss= 0.70019 train_acc= 0.83484 val_loss= 0.64725 val_acc= 0.84839 time= 0.17200
Epoch: 0063 train_loss= 0.67656 train_acc= 0.84164 val_loss= 0.63457 val_acc= 0.85145 time= 0.19306
Epoch: 0064 train_loss= 0.67232 train_acc= 0.84572 val_loss= 0.62215 val_acc= 0.84992 time= 0.16794
Epoch: 0065 train_loss= 0.67775 train_acc= 0.84334 val_loss= 0.61007 val_acc= 0.85452 time= 0.18705
Epoch: 0066 train_loss= 0.64373 train_acc= 0.85270 val_loss= 0.59789 val_acc= 0.85452 time= 0.16813
Epoch: 0067 train_loss= 0.63707 train_acc= 0.84317 val_loss= 0.58605 val_acc= 0.86371 time= 0.16700
Epoch: 0068 train_loss= 0.63380 train_acc= 0.85185 val_loss= 0.57481 val_acc= 0.86677 time= 0.17005
Epoch: 0069 train_loss= 0.61730 train_acc= 0.84912 val_loss= 0.56401 val_acc= 0.86677 time= 0.19092
Epoch: 0070 train_loss= 0.60375 train_acc= 0.85542 val_loss= 0.55385 val_acc= 0.86983 time= 0.17000
Epoch: 0071 train_loss= 0.57806 train_acc= 0.86239 val_loss= 0.54391 val_acc= 0.86983 time= 0.19000
Epoch: 0072 train_loss= 0.58884 train_acc= 0.85610 val_loss= 0.53385 val_acc= 0.87289 time= 0.16900
Epoch: 0073 train_loss= 0.56379 train_acc= 0.85984 val_loss= 0.52433 val_acc= 0.87749 time= 0.16914
Epoch: 0074 train_loss= 0.55904 train_acc= 0.86732 val_loss= 0.51591 val_acc= 0.88361 time= 0.17301
Epoch: 0075 train_loss= 0.56414 train_acc= 0.85950 val_loss= 0.50862 val_acc= 0.88668 time= 0.16700
Epoch: 0076 train_loss= 0.55785 train_acc= 0.86528 val_loss= 0.50075 val_acc= 0.88668 time= 0.16701
Epoch: 0077 train_loss= 0.52969 train_acc= 0.86494 val_loss= 0.49269 val_acc= 0.88668 time= 0.18799
Epoch: 0078 train_loss= 0.54149 train_acc= 0.86222 val_loss= 0.48463 val_acc= 0.88668 time= 0.17000
Epoch: 0079 train_loss= 0.52426 train_acc= 0.87838 val_loss= 0.47576 val_acc= 0.88668 time= 0.16871
Epoch: 0080 train_loss= 0.50012 train_acc= 0.87974 val_loss= 0.46685 val_acc= 0.88974 time= 0.19556
Epoch: 0081 train_loss= 0.48302 train_acc= 0.87753 val_loss= 0.45865 val_acc= 0.89127 time= 0.16804
Epoch: 0082 train_loss= 0.49189 train_acc= 0.87889 val_loss= 0.45110 val_acc= 0.89127 time= 0.18053
Epoch: 0083 train_loss= 0.48948 train_acc= 0.88314 val_loss= 0.44333 val_acc= 0.89280 time= 0.16701
Epoch: 0084 train_loss= 0.46554 train_acc= 0.88689 val_loss= 0.43653 val_acc= 0.89587 time= 0.16705
Epoch: 0085 train_loss= 0.47494 train_acc= 0.88518 val_loss= 0.43036 val_acc= 0.89280 time= 0.18201
Epoch: 0086 train_loss= 0.45949 train_acc= 0.89063 val_loss= 0.42493 val_acc= 0.89280 time= 0.16899
Epoch: 0087 train_loss= 0.44965 train_acc= 0.88910 val_loss= 0.41935 val_acc= 0.89433 time= 0.16996
Epoch: 0088 train_loss= 0.45908 train_acc= 0.88518 val_loss= 0.41318 val_acc= 0.89740 time= 0.18101
Epoch: 0089 train_loss= 0.43582 train_acc= 0.88961 val_loss= 0.40755 val_acc= 0.89893 time= 0.17000
Epoch: 0090 train_loss= 0.43621 train_acc= 0.88995 val_loss= 0.40265 val_acc= 0.90046 time= 0.16856
Epoch: 0091 train_loss= 0.45094 train_acc= 0.88944 val_loss= 0.39817 val_acc= 0.90199 time= 0.16999
Epoch: 0092 train_loss= 0.43759 train_acc= 0.89165 val_loss= 0.39332 val_acc= 0.90199 time= 0.18853
Epoch: 0093 train_loss= 0.43215 train_acc= 0.89777 val_loss= 0.38812 val_acc= 0.90352 time= 0.16704
Epoch: 0094 train_loss= 0.40389 train_acc= 0.90168 val_loss= 0.38256 val_acc= 0.90505 time= 0.18480
Epoch: 0095 train_loss= 0.40131 train_acc= 0.90662 val_loss= 0.37749 val_acc= 0.90505 time= 0.16796
Epoch: 0096 train_loss= 0.39049 train_acc= 0.90066 val_loss= 0.37277 val_acc= 0.90352 time= 0.17300
Epoch: 0097 train_loss= 0.38345 train_acc= 0.90338 val_loss= 0.36910 val_acc= 0.90812 time= 0.19700
Epoch: 0098 train_loss= 0.39811 train_acc= 0.89947 val_loss= 0.36510 val_acc= 0.90965 time= 0.17000
Epoch: 0099 train_loss= 0.38152 train_acc= 0.90509 val_loss= 0.36156 val_acc= 0.90812 time= 0.18000
Epoch: 0100 train_loss= 0.37612 train_acc= 0.90645 val_loss= 0.35832 val_acc= 0.90965 time= 0.16803
Epoch: 0101 train_loss= 0.36425 train_acc= 0.90560 val_loss= 0.35531 val_acc= 0.90965 time= 0.16897
Epoch: 0102 train_loss= 0.36972 train_acc= 0.90441 val_loss= 0.35338 val_acc= 0.91118 time= 0.17100
Epoch: 0103 train_loss= 0.37416 train_acc= 0.90730 val_loss= 0.35145 val_acc= 0.90965 time= 0.18805
Epoch: 0104 train_loss= 0.37423 train_acc= 0.90270 val_loss= 0.34895 val_acc= 0.90965 time= 0.16970
Epoch: 0105 train_loss= 0.35906 train_acc= 0.90798 val_loss= 0.34693 val_acc= 0.90812 time= 0.18700
Epoch: 0106 train_loss= 0.35953 train_acc= 0.90951 val_loss= 0.34290 val_acc= 0.90658 time= 0.17100
Epoch: 0107 train_loss= 0.34536 train_acc= 0.91461 val_loss= 0.33698 val_acc= 0.90812 time= 0.17000
Epoch: 0108 train_loss= 0.32051 train_acc= 0.91937 val_loss= 0.33090 val_acc= 0.90965 time= 0.16914
Epoch: 0109 train_loss= 0.33391 train_acc= 0.91886 val_loss= 0.32648 val_acc= 0.91118 time= 0.18825
Epoch: 0110 train_loss= 0.33279 train_acc= 0.91495 val_loss= 0.32367 val_acc= 0.91118 time= 0.16805
Epoch: 0111 train_loss= 0.31913 train_acc= 0.91767 val_loss= 0.32188 val_acc= 0.91271 time= 0.16703
Epoch: 0112 train_loss= 0.33312 train_acc= 0.91614 val_loss= 0.31949 val_acc= 0.91424 time= 0.16818
Epoch: 0113 train_loss= 0.33993 train_acc= 0.91801 val_loss= 0.31674 val_acc= 0.91577 time= 0.17096
Epoch: 0114 train_loss= 0.32177 train_acc= 0.92227 val_loss= 0.31409 val_acc= 0.91577 time= 0.18958
Epoch: 0115 train_loss= 0.30750 train_acc= 0.92244 val_loss= 0.31090 val_acc= 0.91577 time= 0.17200
Epoch: 0116 train_loss= 0.31804 train_acc= 0.92056 val_loss= 0.30898 val_acc= 0.91730 time= 0.16800
Epoch: 0117 train_loss= 0.29640 train_acc= 0.92992 val_loss= 0.30739 val_acc= 0.92037 time= 0.18804
Epoch: 0118 train_loss= 0.30160 train_acc= 0.92312 val_loss= 0.30739 val_acc= 0.91730 time= 0.16600
Epoch: 0119 train_loss= 0.30710 train_acc= 0.92090 val_loss= 0.30592 val_acc= 0.91730 time= 0.16700
Epoch: 0120 train_loss= 0.30801 train_acc= 0.91903 val_loss= 0.30333 val_acc= 0.91730 time= 0.19198
Epoch: 0121 train_loss= 0.29373 train_acc= 0.93060 val_loss= 0.30022 val_acc= 0.91730 time= 0.16905
Epoch: 0122 train_loss= 0.28325 train_acc= 0.92924 val_loss= 0.29795 val_acc= 0.91730 time= 0.18965
Epoch: 0123 train_loss= 0.29161 train_acc= 0.92907 val_loss= 0.29593 val_acc= 0.91730 time= 0.17120
Epoch: 0124 train_loss= 0.28469 train_acc= 0.92686 val_loss= 0.29322 val_acc= 0.92037 time= 0.17132
Epoch: 0125 train_loss= 0.27104 train_acc= 0.93349 val_loss= 0.29098 val_acc= 0.91730 time= 0.19301
Epoch: 0126 train_loss= 0.27485 train_acc= 0.92805 val_loss= 0.28909 val_acc= 0.91730 time= 0.16600
Epoch: 0127 train_loss= 0.27564 train_acc= 0.93196 val_loss= 0.28744 val_acc= 0.91884 time= 0.16700
Epoch: 0128 train_loss= 0.28434 train_acc= 0.92618 val_loss= 0.28622 val_acc= 0.92037 time= 0.17712
Epoch: 0129 train_loss= 0.25553 train_acc= 0.93621 val_loss= 0.28477 val_acc= 0.92037 time= 0.16700
Epoch: 0130 train_loss= 0.27099 train_acc= 0.93111 val_loss= 0.28349 val_acc= 0.92037 time= 0.16900
Epoch: 0131 train_loss= 0.26378 train_acc= 0.93791 val_loss= 0.28272 val_acc= 0.91884 time= 0.17326
Epoch: 0132 train_loss= 0.26598 train_acc= 0.93400 val_loss= 0.28179 val_acc= 0.92190 time= 0.19269
Epoch: 0133 train_loss= 0.27285 train_acc= 0.93128 val_loss= 0.28097 val_acc= 0.92343 time= 0.17200
Epoch: 0134 train_loss= 0.25627 train_acc= 0.93315 val_loss= 0.27925 val_acc= 0.92496 time= 0.18200
Epoch: 0135 train_loss= 0.24996 train_acc= 0.93468 val_loss= 0.27771 val_acc= 0.92649 time= 0.16800
Epoch: 0136 train_loss= 0.24949 train_acc= 0.93740 val_loss= 0.27586 val_acc= 0.92343 time= 0.16700
Epoch: 0137 train_loss= 0.25433 train_acc= 0.93570 val_loss= 0.27481 val_acc= 0.92649 time= 0.19100
Epoch: 0138 train_loss= 0.23951 train_acc= 0.93604 val_loss= 0.27395 val_acc= 0.92343 time= 0.16700
Epoch: 0139 train_loss= 0.23943 train_acc= 0.93655 val_loss= 0.27376 val_acc= 0.92649 time= 0.17258
Epoch: 0140 train_loss= 0.22169 train_acc= 0.94880 val_loss= 0.27463 val_acc= 0.92496 time= 0.17300
Epoch: 0141 train_loss= 0.23635 train_acc= 0.93723 val_loss= 0.27376 val_acc= 0.92343 time= 0.17000
Epoch: 0142 train_loss= 0.23909 train_acc= 0.94030 val_loss= 0.27253 val_acc= 0.92190 time= 0.16900
Epoch: 0143 train_loss= 0.22959 train_acc= 0.94183 val_loss= 0.27054 val_acc= 0.92190 time= 0.19539
Epoch: 0144 train_loss= 0.23198 train_acc= 0.93962 val_loss= 0.26873 val_acc= 0.92037 time= 0.16804
Epoch: 0145 train_loss= 0.22677 train_acc= 0.94013 val_loss= 0.26792 val_acc= 0.92037 time= 0.18300
Epoch: 0146 train_loss= 0.22582 train_acc= 0.94200 val_loss= 0.26592 val_acc= 0.92190 time= 0.16803
Epoch: 0147 train_loss= 0.22189 train_acc= 0.94268 val_loss= 0.26423 val_acc= 0.92496 time= 0.16900
Epoch: 0148 train_loss= 0.22469 train_acc= 0.93945 val_loss= 0.26297 val_acc= 0.92496 time= 0.17400
Epoch: 0149 train_loss= 0.23460 train_acc= 0.93945 val_loss= 0.26153 val_acc= 0.92496 time= 0.19400
Epoch: 0150 train_loss= 0.22730 train_acc= 0.94557 val_loss= 0.26042 val_acc= 0.92496 time= 0.17000
Epoch: 0151 train_loss= 0.21630 train_acc= 0.94880 val_loss= 0.25932 val_acc= 0.92802 time= 0.18200
Epoch: 0152 train_loss= 0.21324 train_acc= 0.94795 val_loss= 0.25847 val_acc= 0.92649 time= 0.16700
Epoch: 0153 train_loss= 0.22856 train_acc= 0.94557 val_loss= 0.25785 val_acc= 0.92649 time= 0.16900
Epoch: 0154 train_loss= 0.21344 train_acc= 0.94540 val_loss= 0.25764 val_acc= 0.92649 time= 0.19103
Epoch: 0155 train_loss= 0.21928 train_acc= 0.94064 val_loss= 0.25688 val_acc= 0.92649 time= 0.16800
Epoch: 0156 train_loss= 0.21612 train_acc= 0.94625 val_loss= 0.25523 val_acc= 0.92343 time= 0.17000
Epoch: 0157 train_loss= 0.21861 train_acc= 0.94387 val_loss= 0.25486 val_acc= 0.92343 time= 0.18567
Epoch: 0158 train_loss= 0.20211 train_acc= 0.95016 val_loss= 0.25369 val_acc= 0.92190 time= 0.16900
Epoch: 0159 train_loss= 0.20016 train_acc= 0.94999 val_loss= 0.25395 val_acc= 0.92343 time= 0.17100
Epoch: 0160 train_loss= 0.20584 train_acc= 0.94608 val_loss= 0.25380 val_acc= 0.92496 time= 0.19503
Epoch: 0161 train_loss= 0.21519 train_acc= 0.94098 val_loss= 0.25401 val_acc= 0.92802 time= 0.16709
Epoch: 0162 train_loss= 0.18886 train_acc= 0.95339 val_loss= 0.25511 val_acc= 0.92496 time= 0.18301
Epoch: 0163 train_loss= 0.19292 train_acc= 0.95560 val_loss= 0.25624 val_acc= 0.92649 time= 0.16700
Early stopping...
Optimization Finished!
Test set results: cost= 0.29162 accuracy= 0.92757 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     1.0000    0.1667    0.2857         6
           2     1.0000    1.0000    1.0000         1
           3     0.7955    0.9333    0.8589        75
           4     1.0000    1.0000    1.0000         9
           5     0.7545    0.9540    0.8426        87
           6     0.9231    0.9600    0.9412        25
           7     0.7333    0.8462    0.7857        13
           8     1.0000    0.6364    0.7778        11
           9     1.0000    0.1111    0.2000         9
          10     0.9524    0.5556    0.7018        36
          11     1.0000    0.9167    0.9565        12
          12     0.8264    0.9835    0.8981       121
          13     0.8667    0.6842    0.7647        19
          14     0.8065    0.8929    0.8475        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.7500    0.3333    0.4615         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.5200    0.7647    0.6190        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.5000    0.6667        12
          28     1.0000    0.7273    0.8421        11
          29     0.9630    0.9727    0.9678       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.6667    0.8000         3
          32     0.6250    1.0000    0.7692        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8857    0.7654    0.8212        81
          36     0.8000    0.3333    0.4706        12
          37     1.0000    0.5000    0.6667         4
          38     0.0000    0.0000    0.0000         1
          39     0.9817    0.9908    0.9862      1083
          40     1.0000    0.8000    0.8889         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.7143    0.8333    0.7692        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.6250    1.0000    0.7692        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.6000    0.7500    0.6667         4

    accuracy                         0.9276      2568
   macro avg     0.6893    0.5888    0.6050      2568
weighted avg     0.9254    0.9276    0.9193      2568

Macro average Test Precision, Recall and F1-Score...
(0.6893162371969154, 0.5887549817183204, 0.6050256319452301, None)
Micro average Test Precision, Recall and F1-Score...
(0.927570093457944, 0.927570093457944, 0.927570093457944, None)
embeddings:
8892 6532 2568
[[-1.75413683e-01 -2.33878288e-03 -7.11151138e-02 ... -1.17670977e-02
   1.29574335e+00  6.61189079e-01]
 [ 5.62723100e-01  6.35507144e-03  1.25369251e-01 ... -4.08221297e-02
   4.10368264e-01  5.06308556e-01]
 [ 1.14342384e-01 -6.90014958e-02 -5.65477982e-02 ...  6.08904213e-02
   7.79734373e-01  3.57980788e-01]
 ...
 [ 3.32942545e-01 -1.04357209e-03 -9.50732734e-03 ... -1.72871910e-02
   2.23053813e-01  2.91684180e-01]
 [ 3.10170539e-02  2.15086862e-02  1.44349849e-02 ...  6.02795146e-02
   2.81217128e-01  2.39917204e-01]
 [ 2.22507462e-01  2.51544356e-01  2.33712301e-01 ...  2.20451057e-01
   2.42323443e-01  1.53371558e-01]]

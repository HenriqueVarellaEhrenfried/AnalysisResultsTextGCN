(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95116 train_acc= 0.03895 val_loss= 3.89536 val_acc= 0.65084 time= 0.44166
Epoch: 0002 train_loss= 3.89601 train_acc= 0.62562 val_loss= 3.79037 val_acc= 0.62328 time= 0.19809
Epoch: 0003 train_loss= 3.79173 train_acc= 0.60878 val_loss= 3.62860 val_acc= 0.61103 time= 0.17996
Epoch: 0004 train_loss= 3.63131 train_acc= 0.60282 val_loss= 3.41161 val_acc= 0.60490 time= 0.16900
Epoch: 0005 train_loss= 3.41681 train_acc= 0.59636 val_loss= 3.15196 val_acc= 0.59724 time= 0.17000
Epoch: 0006 train_loss= 3.15508 train_acc= 0.59007 val_loss= 2.87747 val_acc= 0.59265 time= 0.18904
Epoch: 0007 train_loss= 2.87947 train_acc= 0.58564 val_loss= 2.62235 val_acc= 0.58959 time= 0.16899
Epoch: 0008 train_loss= 2.62229 train_acc= 0.57901 val_loss= 2.42284 val_acc= 0.58806 time= 0.16701
Epoch: 0009 train_loss= 2.41871 train_acc= 0.57867 val_loss= 2.29841 val_acc= 0.60949 time= 0.17795
Epoch: 0010 train_loss= 2.29697 train_acc= 0.59687 val_loss= 2.22870 val_acc= 0.67075 time= 0.16805
Epoch: 0011 train_loss= 2.23023 train_acc= 0.65028 val_loss= 2.18102 val_acc= 0.48392 time= 0.16695
Epoch: 0012 train_loss= 2.18802 train_acc= 0.45535 val_loss= 2.13439 val_acc= 0.45636 time= 0.17900
Epoch: 0013 train_loss= 2.14400 train_acc= 0.43273 val_loss= 2.07478 val_acc= 0.45636 time= 0.17103
Epoch: 0014 train_loss= 2.08969 train_acc= 0.43239 val_loss= 1.99660 val_acc= 0.45636 time= 0.16997
Epoch: 0015 train_loss= 2.01757 train_acc= 0.43239 val_loss= 1.90535 val_acc= 0.45789 time= 0.16903
Epoch: 0016 train_loss= 1.93114 train_acc= 0.43375 val_loss= 1.81442 val_acc= 0.47933 time= 0.16696
Epoch: 0017 train_loss= 1.84019 train_acc= 0.44991 val_loss= 1.73604 val_acc= 0.54671 time= 0.17000
Epoch: 0018 train_loss= 1.76336 train_acc= 0.54788 val_loss= 1.67281 val_acc= 0.65544 time= 0.18404
Epoch: 0019 train_loss= 1.69781 train_acc= 0.63820 val_loss= 1.61842 val_acc= 0.67841 time= 0.16996
Epoch: 0020 train_loss= 1.64606 train_acc= 0.65896 val_loss= 1.56516 val_acc= 0.67994 time= 0.17403
Epoch: 0021 train_loss= 1.58993 train_acc= 0.66032 val_loss= 1.51006 val_acc= 0.67228 time= 0.19200
Epoch: 0022 train_loss= 1.53609 train_acc= 0.66236 val_loss= 1.45394 val_acc= 0.68300 time= 0.17101
Epoch: 0023 train_loss= 1.47609 train_acc= 0.66984 val_loss= 1.39945 val_acc= 0.68606 time= 0.16899
Epoch: 0024 train_loss= 1.42189 train_acc= 0.67563 val_loss= 1.34864 val_acc= 0.70138 time= 0.18501
Epoch: 0025 train_loss= 1.37066 train_acc= 0.68957 val_loss= 1.30243 val_acc= 0.71210 time= 0.16600
Epoch: 0026 train_loss= 1.32597 train_acc= 0.70420 val_loss= 1.26056 val_acc= 0.72741 time= 0.18407
Epoch: 0027 train_loss= 1.28179 train_acc= 0.71781 val_loss= 1.22229 val_acc= 0.73507 time= 0.16797
Epoch: 0028 train_loss= 1.24435 train_acc= 0.73023 val_loss= 1.18663 val_acc= 0.74119 time= 0.17002
Epoch: 0029 train_loss= 1.20671 train_acc= 0.74145 val_loss= 1.15268 val_acc= 0.74273 time= 0.19300
Epoch: 0030 train_loss= 1.17168 train_acc= 0.75387 val_loss= 1.11968 val_acc= 0.75038 time= 0.16900
Epoch: 0031 train_loss= 1.13521 train_acc= 0.76135 val_loss= 1.08719 val_acc= 0.75804 time= 0.16900
Epoch: 0032 train_loss= 1.10174 train_acc= 0.76969 val_loss= 1.05506 val_acc= 0.76263 time= 0.17800
Epoch: 0033 train_loss= 1.06677 train_acc= 0.77598 val_loss= 1.02352 val_acc= 0.77182 time= 0.16699
Epoch: 0034 train_loss= 1.03610 train_acc= 0.77972 val_loss= 0.99289 val_acc= 0.78407 time= 0.16700
Epoch: 0035 train_loss= 1.00389 train_acc= 0.78585 val_loss= 0.96339 val_acc= 0.78867 time= 0.18700
Epoch: 0036 train_loss= 0.97242 train_acc= 0.79027 val_loss= 0.93511 val_acc= 0.79479 time= 0.16507
Epoch: 0037 train_loss= 0.94361 train_acc= 0.79639 val_loss= 0.90791 val_acc= 0.79939 time= 0.17097
Epoch: 0038 train_loss= 0.91532 train_acc= 0.80320 val_loss= 0.88159 val_acc= 0.80092 time= 0.18603
Epoch: 0039 train_loss= 0.88777 train_acc= 0.81289 val_loss= 0.85599 val_acc= 0.81164 time= 0.17000
Epoch: 0040 train_loss= 0.86080 train_acc= 0.81953 val_loss= 0.83100 val_acc= 0.82083 time= 0.17000
Epoch: 0041 train_loss= 0.83716 train_acc= 0.82701 val_loss= 0.80665 val_acc= 0.82542 time= 0.18301
Epoch: 0042 train_loss= 0.81035 train_acc= 0.83194 val_loss= 0.78298 val_acc= 0.83461 time= 0.16905
Epoch: 0043 train_loss= 0.78773 train_acc= 0.83824 val_loss= 0.76003 val_acc= 0.84533 time= 0.17000
Epoch: 0044 train_loss= 0.76119 train_acc= 0.84283 val_loss= 0.73777 val_acc= 0.84992 time= 0.18450
Epoch: 0045 train_loss= 0.73978 train_acc= 0.84657 val_loss= 0.71613 val_acc= 0.84992 time= 0.16995
Epoch: 0046 train_loss= 0.71440 train_acc= 0.85168 val_loss= 0.69521 val_acc= 0.85452 time= 0.17500
Epoch: 0047 train_loss= 0.69127 train_acc= 0.85627 val_loss= 0.67484 val_acc= 0.85911 time= 0.19000
Epoch: 0048 train_loss= 0.66857 train_acc= 0.85848 val_loss= 0.65501 val_acc= 0.86830 time= 0.16805
Epoch: 0049 train_loss= 0.64693 train_acc= 0.86375 val_loss= 0.63569 val_acc= 0.86983 time= 0.18900
Epoch: 0050 train_loss= 0.62492 train_acc= 0.86852 val_loss= 0.61704 val_acc= 0.87443 time= 0.16701
Epoch: 0051 train_loss= 0.60717 train_acc= 0.86766 val_loss= 0.59908 val_acc= 0.87596 time= 0.16699
Epoch: 0052 train_loss= 0.58505 train_acc= 0.87192 val_loss= 0.58177 val_acc= 0.87596 time= 0.18803
Epoch: 0053 train_loss= 0.56576 train_acc= 0.87532 val_loss= 0.56519 val_acc= 0.87596 time= 0.16600
Epoch: 0054 train_loss= 0.54769 train_acc= 0.87787 val_loss= 0.54945 val_acc= 0.87443 time= 0.17100
Epoch: 0055 train_loss= 0.52954 train_acc= 0.88365 val_loss= 0.53446 val_acc= 0.88055 time= 0.17700
Epoch: 0056 train_loss= 0.51296 train_acc= 0.88621 val_loss= 0.52014 val_acc= 0.88208 time= 0.17000
Epoch: 0057 train_loss= 0.49364 train_acc= 0.89148 val_loss= 0.50643 val_acc= 0.88055 time= 0.17100
Epoch: 0058 train_loss= 0.47782 train_acc= 0.89233 val_loss= 0.49319 val_acc= 0.88055 time= 0.18500
Epoch: 0059 train_loss= 0.46276 train_acc= 0.89522 val_loss= 0.48014 val_acc= 0.88055 time= 0.16800
Epoch: 0060 train_loss= 0.44732 train_acc= 0.89845 val_loss= 0.46721 val_acc= 0.88055 time= 0.16899
Epoch: 0061 train_loss= 0.43272 train_acc= 0.90219 val_loss= 0.45439 val_acc= 0.88208 time= 0.18500
Epoch: 0062 train_loss= 0.41781 train_acc= 0.90645 val_loss= 0.44203 val_acc= 0.88668 time= 0.16700
Epoch: 0063 train_loss= 0.40551 train_acc= 0.91138 val_loss= 0.43020 val_acc= 0.88974 time= 0.17500
Epoch: 0064 train_loss= 0.39090 train_acc= 0.91546 val_loss= 0.41903 val_acc= 0.89280 time= 0.17900
Epoch: 0065 train_loss= 0.37974 train_acc= 0.91937 val_loss= 0.40854 val_acc= 0.89587 time= 0.17000
Epoch: 0066 train_loss= 0.36680 train_acc= 0.91988 val_loss= 0.39880 val_acc= 0.89740 time= 0.17100
Epoch: 0067 train_loss= 0.35393 train_acc= 0.92584 val_loss= 0.38974 val_acc= 0.89740 time= 0.16700
Epoch: 0068 train_loss= 0.34350 train_acc= 0.92941 val_loss= 0.38130 val_acc= 0.89893 time= 0.16601
Epoch: 0069 train_loss= 0.33229 train_acc= 0.93094 val_loss= 0.37353 val_acc= 0.89740 time= 0.18700
Epoch: 0070 train_loss= 0.32021 train_acc= 0.93349 val_loss= 0.36606 val_acc= 0.90046 time= 0.16799
Epoch: 0071 train_loss= 0.30870 train_acc= 0.93587 val_loss= 0.35888 val_acc= 0.89893 time= 0.17001
Epoch: 0072 train_loss= 0.29925 train_acc= 0.93757 val_loss= 0.35174 val_acc= 0.90352 time= 0.19301
Epoch: 0073 train_loss= 0.29033 train_acc= 0.94047 val_loss= 0.34484 val_acc= 0.90505 time= 0.16899
Epoch: 0074 train_loss= 0.27919 train_acc= 0.94438 val_loss= 0.33849 val_acc= 0.90505 time= 0.16900
Epoch: 0075 train_loss= 0.26949 train_acc= 0.94540 val_loss= 0.33250 val_acc= 0.90505 time= 0.16800
Epoch: 0076 train_loss= 0.26077 train_acc= 0.94795 val_loss= 0.32636 val_acc= 0.90812 time= 0.16701
Epoch: 0077 train_loss= 0.25237 train_acc= 0.94948 val_loss= 0.32038 val_acc= 0.91118 time= 0.17006
Epoch: 0078 train_loss= 0.24478 train_acc= 0.95152 val_loss= 0.31456 val_acc= 0.91424 time= 0.17397
Epoch: 0079 train_loss= 0.23492 train_acc= 0.95407 val_loss= 0.30907 val_acc= 0.91730 time= 0.16600
Epoch: 0080 train_loss= 0.22712 train_acc= 0.95646 val_loss= 0.30445 val_acc= 0.91577 time= 0.17300
Epoch: 0081 train_loss= 0.21973 train_acc= 0.95748 val_loss= 0.30022 val_acc= 0.91730 time= 0.18900
Epoch: 0082 train_loss= 0.21223 train_acc= 0.95884 val_loss= 0.29619 val_acc= 0.91577 time= 0.16803
Epoch: 0083 train_loss= 0.20312 train_acc= 0.96071 val_loss= 0.29261 val_acc= 0.91577 time= 0.17000
Epoch: 0084 train_loss= 0.19664 train_acc= 0.96258 val_loss= 0.28926 val_acc= 0.91730 time= 0.17700
Epoch: 0085 train_loss= 0.19160 train_acc= 0.96326 val_loss= 0.28590 val_acc= 0.91730 time= 0.16800
Epoch: 0086 train_loss= 0.18302 train_acc= 0.96496 val_loss= 0.28185 val_acc= 0.91730 time= 0.16901
Epoch: 0087 train_loss= 0.17594 train_acc= 0.96802 val_loss= 0.27767 val_acc= 0.92037 time= 0.18399
Epoch: 0088 train_loss= 0.17157 train_acc= 0.96768 val_loss= 0.27360 val_acc= 0.92802 time= 0.16897
Epoch: 0089 train_loss= 0.16422 train_acc= 0.96972 val_loss= 0.26982 val_acc= 0.92956 time= 0.18200
Epoch: 0090 train_loss= 0.15673 train_acc= 0.97244 val_loss= 0.26634 val_acc= 0.92956 time= 0.17000
Epoch: 0091 train_loss= 0.15246 train_acc= 0.97125 val_loss= 0.26340 val_acc= 0.92956 time= 0.16903
Epoch: 0092 train_loss= 0.14814 train_acc= 0.97312 val_loss= 0.26126 val_acc= 0.92956 time= 0.18800
Epoch: 0093 train_loss= 0.14121 train_acc= 0.97500 val_loss= 0.25963 val_acc= 0.92802 time= 0.16745
Epoch: 0094 train_loss= 0.13748 train_acc= 0.97738 val_loss= 0.25853 val_acc= 0.92956 time= 0.16781
Epoch: 0095 train_loss= 0.13170 train_acc= 0.97619 val_loss= 0.25681 val_acc= 0.92802 time= 0.18800
Epoch: 0096 train_loss= 0.12665 train_acc= 0.97857 val_loss= 0.25432 val_acc= 0.92802 time= 0.16803
Epoch: 0097 train_loss= 0.12241 train_acc= 0.98180 val_loss= 0.25133 val_acc= 0.92802 time= 0.17100
Epoch: 0098 train_loss= 0.11899 train_acc= 0.98095 val_loss= 0.24863 val_acc= 0.92802 time= 0.19200
Epoch: 0099 train_loss= 0.11308 train_acc= 0.98180 val_loss= 0.24648 val_acc= 0.92956 time= 0.16900
Epoch: 0100 train_loss= 0.10900 train_acc= 0.98418 val_loss= 0.24465 val_acc= 0.93415 time= 0.16700
Epoch: 0101 train_loss= 0.10629 train_acc= 0.98350 val_loss= 0.24330 val_acc= 0.93262 time= 0.16800
Epoch: 0102 train_loss= 0.10255 train_acc= 0.98486 val_loss= 0.24255 val_acc= 0.93109 time= 0.16612
Epoch: 0103 train_loss= 0.09865 train_acc= 0.98469 val_loss= 0.24161 val_acc= 0.92956 time= 0.16694
Epoch: 0104 train_loss= 0.09632 train_acc= 0.98639 val_loss= 0.24079 val_acc= 0.92956 time= 0.17305
Epoch: 0105 train_loss= 0.09169 train_acc= 0.98656 val_loss= 0.24004 val_acc= 0.92802 time= 0.16995
Epoch: 0106 train_loss= 0.08941 train_acc= 0.98707 val_loss= 0.23866 val_acc= 0.92802 time= 0.17138
Epoch: 0107 train_loss= 0.08429 train_acc= 0.98826 val_loss= 0.23739 val_acc= 0.93109 time= 0.17000
Epoch: 0108 train_loss= 0.08231 train_acc= 0.98775 val_loss= 0.23611 val_acc= 0.92956 time= 0.16900
Epoch: 0109 train_loss= 0.08008 train_acc= 0.98826 val_loss= 0.23502 val_acc= 0.92956 time= 0.17000
Epoch: 0110 train_loss= 0.07763 train_acc= 0.98945 val_loss= 0.23422 val_acc= 0.92956 time= 0.18372
Epoch: 0111 train_loss= 0.07510 train_acc= 0.98928 val_loss= 0.23328 val_acc= 0.93109 time= 0.16807
Epoch: 0112 train_loss= 0.07254 train_acc= 0.98962 val_loss= 0.23238 val_acc= 0.93262 time= 0.16953
Epoch: 0113 train_loss= 0.06977 train_acc= 0.99081 val_loss= 0.23215 val_acc= 0.93109 time= 0.18345
Epoch: 0114 train_loss= 0.06862 train_acc= 0.99081 val_loss= 0.23181 val_acc= 0.92956 time= 0.16995
Epoch: 0115 train_loss= 0.06509 train_acc= 0.99081 val_loss= 0.23229 val_acc= 0.93109 time= 0.17400
Epoch: 0116 train_loss= 0.06290 train_acc= 0.99098 val_loss= 0.23263 val_acc= 0.93262 time= 0.17400
Epoch: 0117 train_loss= 0.06153 train_acc= 0.99116 val_loss= 0.23254 val_acc= 0.93109 time= 0.16962
Epoch: 0118 train_loss= 0.05929 train_acc= 0.99201 val_loss= 0.23237 val_acc= 0.93568 time= 0.16999
Epoch: 0119 train_loss= 0.05754 train_acc= 0.99218 val_loss= 0.23180 val_acc= 0.93568 time= 0.16897
Epoch: 0120 train_loss= 0.05652 train_acc= 0.99201 val_loss= 0.23031 val_acc= 0.93262 time= 0.16803
Epoch: 0121 train_loss= 0.05442 train_acc= 0.99252 val_loss= 0.22897 val_acc= 0.93262 time= 0.16909
Epoch: 0122 train_loss= 0.05278 train_acc= 0.99320 val_loss= 0.22789 val_acc= 0.93262 time= 0.18400
Epoch: 0123 train_loss= 0.05083 train_acc= 0.99286 val_loss= 0.22749 val_acc= 0.93262 time= 0.17214
Epoch: 0124 train_loss= 0.05000 train_acc= 0.99354 val_loss= 0.22663 val_acc= 0.93262 time= 0.18901
Epoch: 0125 train_loss= 0.04765 train_acc= 0.99371 val_loss= 0.22599 val_acc= 0.93262 time= 0.17000
Epoch: 0126 train_loss= 0.04643 train_acc= 0.99422 val_loss= 0.22624 val_acc= 0.93415 time= 0.16800
Epoch: 0127 train_loss= 0.04462 train_acc= 0.99439 val_loss= 0.22685 val_acc= 0.93721 time= 0.18900
Epoch: 0128 train_loss= 0.04411 train_acc= 0.99456 val_loss= 0.22762 val_acc= 0.93721 time= 0.16800
Epoch: 0129 train_loss= 0.04324 train_acc= 0.99490 val_loss= 0.22842 val_acc= 0.93721 time= 0.16705
Early stopping...
Optimization Finished!
Test set results: cost= 0.25303 accuracy= 0.93769 time= 0.07697
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7978    0.9467    0.8659        75
           4     1.0000    1.0000    1.0000         9
           5     0.8587    0.9080    0.8827        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.5556    0.7143         9
          10     0.9048    0.5278    0.6667        36
          11     1.0000    0.9167    0.9565        12
          12     0.8219    0.9917    0.8989       121
          13     0.9375    0.7895    0.8571        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.8750    0.8235    0.8485        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9669    0.9641    0.9655       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.7882    0.8272    0.8072        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9091    0.8333    0.8696        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9377      2568
   macro avg     0.7685    0.6795    0.7025      2568
weighted avg     0.9366    0.9377    0.9332      2568

Macro average Test Precision, Recall and F1-Score...
(0.768549358740303, 0.6794820548426567, 0.7025146913519763, None)
Micro average Test Precision, Recall and F1-Score...
(0.9376947040498442, 0.9376947040498442, 0.9376947040498442, None)
embeddings:
8892 6532 2568
[[ 0.09813803  0.24046023 -0.11670828 ...  0.00948594  0.13359234
  -0.11817425]
 [ 0.04684523 -0.16391805  0.0111234  ...  0.3985358   0.03989193
   0.01362817]
 [-0.01196091  0.26482677  0.00215344 ...  0.33828124  0.02414935
   0.05916561]
 ...
 [ 0.07863901  0.44627863  0.04380639 ...  0.31632656  0.05817019
   0.07715418]
 [ 0.05646865  0.2991392   0.04099257 ...  0.1384059   0.03666142
   0.07463998]
 [ 0.2824018   0.3260948   0.2797026  ...  0.24829453  0.24938051
   0.27907178]]

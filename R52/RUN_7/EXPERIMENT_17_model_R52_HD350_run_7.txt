(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95095 train_acc= 0.26739 val_loss= 3.86191 val_acc= 0.54211 time= 0.53600
Epoch: 0002 train_loss= 3.86333 train_acc= 0.52407 val_loss= 3.67918 val_acc= 0.51302 time= 0.21583
Epoch: 0003 train_loss= 3.68362 train_acc= 0.49413 val_loss= 3.39636 val_acc= 0.49617 time= 0.21200
Epoch: 0004 train_loss= 3.41024 train_acc= 0.48274 val_loss= 3.03572 val_acc= 0.48698 time= 0.23500
Epoch: 0005 train_loss= 3.04912 train_acc= 0.46266 val_loss= 2.66360 val_acc= 0.47933 time= 0.21000
Epoch: 0006 train_loss= 2.66746 train_acc= 0.45399 val_loss= 2.36870 val_acc= 0.47167 time= 0.20800
Epoch: 0007 train_loss= 2.38156 train_acc= 0.44667 val_loss= 2.21420 val_acc= 0.47626 time= 0.20913
Epoch: 0008 train_loss= 2.21619 train_acc= 0.45365 val_loss= 2.16104 val_acc= 0.48545 time= 0.23510
Epoch: 0009 train_loss= 2.17283 train_acc= 0.46232 val_loss= 2.12209 val_acc= 0.49464 time= 0.21400
Epoch: 0010 train_loss= 2.12600 train_acc= 0.46896 val_loss= 2.05020 val_acc= 0.51608 time= 0.22704
Epoch: 0011 train_loss= 2.06586 train_acc= 0.50876 val_loss= 1.94473 val_acc= 0.56662 time= 0.20784
Epoch: 0012 train_loss= 1.96597 train_acc= 0.56047 val_loss= 1.82701 val_acc= 0.62175 time= 0.21200
Epoch: 0013 train_loss= 1.84597 train_acc= 0.61371 val_loss= 1.72371 val_acc= 0.65544 time= 0.21100
Epoch: 0014 train_loss= 1.74790 train_acc= 0.64331 val_loss= 1.64856 val_acc= 0.66769 time= 0.21800
Epoch: 0015 train_loss= 1.67839 train_acc= 0.64943 val_loss= 1.59054 val_acc= 0.67534 time= 0.21100
Epoch: 0016 train_loss= 1.61988 train_acc= 0.65164 val_loss= 1.53272 val_acc= 0.67994 time= 0.21200
Epoch: 0017 train_loss= 1.55885 train_acc= 0.65589 val_loss= 1.47119 val_acc= 0.68147 time= 0.21500
Epoch: 0018 train_loss= 1.50463 train_acc= 0.65981 val_loss= 1.41034 val_acc= 0.68300 time= 0.23100
Epoch: 0019 train_loss= 1.44150 train_acc= 0.66015 val_loss= 1.35553 val_acc= 0.68453 time= 0.21007
Epoch: 0020 train_loss= 1.38564 train_acc= 0.66287 val_loss= 1.30906 val_acc= 0.68913 time= 0.21000
Epoch: 0021 train_loss= 1.33761 train_acc= 0.66610 val_loss= 1.26990 val_acc= 0.69219 time= 0.20901
Epoch: 0022 train_loss= 1.29862 train_acc= 0.67307 val_loss= 1.23597 val_acc= 0.69832 time= 0.24095
Epoch: 0023 train_loss= 1.26618 train_acc= 0.67716 val_loss= 1.20515 val_acc= 0.70444 time= 0.21300
Epoch: 0024 train_loss= 1.22918 train_acc= 0.69246 val_loss= 1.17599 val_acc= 0.71210 time= 0.21105
Epoch: 0025 train_loss= 1.20010 train_acc= 0.70267 val_loss= 1.14756 val_acc= 0.72435 time= 0.21099
Epoch: 0026 train_loss= 1.16769 train_acc= 0.72172 val_loss= 1.11927 val_acc= 0.72588 time= 0.21499
Epoch: 0027 train_loss= 1.13648 train_acc= 0.73261 val_loss= 1.09075 val_acc= 0.73047 time= 0.22996
Epoch: 0028 train_loss= 1.10891 train_acc= 0.73839 val_loss= 1.06205 val_acc= 0.74119 time= 0.21912
Epoch: 0029 train_loss= 1.07507 train_acc= 0.74502 val_loss= 1.03340 val_acc= 0.74885 time= 0.21453
Epoch: 0030 train_loss= 1.04397 train_acc= 0.75098 val_loss= 1.00523 val_acc= 0.75345 time= 0.21300
Epoch: 0031 train_loss= 1.01676 train_acc= 0.75744 val_loss= 0.97794 val_acc= 0.76110 time= 0.21503
Epoch: 0032 train_loss= 0.98901 train_acc= 0.76220 val_loss= 0.95157 val_acc= 0.76876 time= 0.23397
Epoch: 0033 train_loss= 0.96044 train_acc= 0.77071 val_loss= 0.92575 val_acc= 0.78101 time= 0.21635
Epoch: 0034 train_loss= 0.93694 train_acc= 0.78568 val_loss= 0.90011 val_acc= 0.79020 time= 0.21303
Epoch: 0035 train_loss= 0.91531 train_acc= 0.79554 val_loss= 0.87471 val_acc= 0.81164 time= 0.21008
Epoch: 0036 train_loss= 0.88704 train_acc= 0.80898 val_loss= 0.84969 val_acc= 0.82083 time= 0.22390
Epoch: 0037 train_loss= 0.86476 train_acc= 0.82038 val_loss= 0.82537 val_acc= 0.83155 time= 0.22100
Epoch: 0038 train_loss= 0.83659 train_acc= 0.82939 val_loss= 0.80170 val_acc= 0.83614 time= 0.21000
Epoch: 0039 train_loss= 0.81774 train_acc= 0.83313 val_loss= 0.77882 val_acc= 0.83767 time= 0.21100
Epoch: 0040 train_loss= 0.78870 train_acc= 0.83960 val_loss= 0.75636 val_acc= 0.84533 time= 0.21400
Epoch: 0041 train_loss= 0.76223 train_acc= 0.84589 val_loss= 0.73427 val_acc= 0.85145 time= 0.23102
Epoch: 0042 train_loss= 0.73844 train_acc= 0.84623 val_loss= 0.71263 val_acc= 0.85145 time= 0.21000
Epoch: 0043 train_loss= 0.71447 train_acc= 0.85168 val_loss= 0.69129 val_acc= 0.85299 time= 0.21600
Epoch: 0044 train_loss= 0.68972 train_acc= 0.85219 val_loss= 0.67046 val_acc= 0.85299 time= 0.21400
Epoch: 0045 train_loss= 0.67166 train_acc= 0.85627 val_loss= 0.65009 val_acc= 0.85452 time= 0.21104
Epoch: 0046 train_loss= 0.65046 train_acc= 0.85984 val_loss= 0.63027 val_acc= 0.85299 time= 0.21201
Epoch: 0047 train_loss= 0.62600 train_acc= 0.86307 val_loss= 0.61085 val_acc= 0.85911 time= 0.22501
Epoch: 0048 train_loss= 0.60618 train_acc= 0.86732 val_loss= 0.59179 val_acc= 0.86064 time= 0.21195
Epoch: 0049 train_loss= 0.58275 train_acc= 0.87209 val_loss= 0.57339 val_acc= 0.86371 time= 0.21310
Epoch: 0050 train_loss= 0.56704 train_acc= 0.87192 val_loss= 0.55574 val_acc= 0.86983 time= 0.23730
Epoch: 0051 train_loss= 0.53935 train_acc= 0.88008 val_loss= 0.53902 val_acc= 0.87443 time= 0.21802
Epoch: 0052 train_loss= 0.52324 train_acc= 0.88280 val_loss= 0.52316 val_acc= 0.87596 time= 0.21149
Epoch: 0053 train_loss= 0.49978 train_acc= 0.88621 val_loss= 0.50785 val_acc= 0.88055 time= 0.21096
Epoch: 0054 train_loss= 0.48146 train_acc= 0.89284 val_loss= 0.49252 val_acc= 0.88515 time= 0.22703
Epoch: 0055 train_loss= 0.46775 train_acc= 0.89556 val_loss= 0.47743 val_acc= 0.88515 time= 0.21200
Epoch: 0056 train_loss= 0.45100 train_acc= 0.89505 val_loss= 0.46295 val_acc= 0.88668 time= 0.22804
Epoch: 0057 train_loss= 0.44016 train_acc= 0.90304 val_loss= 0.44923 val_acc= 0.88821 time= 0.21473
Epoch: 0058 train_loss= 0.42221 train_acc= 0.90509 val_loss= 0.43618 val_acc= 0.89280 time= 0.21000
Epoch: 0059 train_loss= 0.40816 train_acc= 0.91257 val_loss= 0.42354 val_acc= 0.89587 time= 0.22749
Epoch: 0060 train_loss= 0.38869 train_acc= 0.91699 val_loss= 0.41156 val_acc= 0.89893 time= 0.21900
Epoch: 0061 train_loss= 0.38161 train_acc= 0.91801 val_loss= 0.40063 val_acc= 0.90046 time= 0.20874
Epoch: 0062 train_loss= 0.35890 train_acc= 0.92635 val_loss= 0.39071 val_acc= 0.90046 time= 0.20900
Epoch: 0063 train_loss= 0.35196 train_acc= 0.92397 val_loss= 0.38121 val_acc= 0.89893 time= 0.21400
Epoch: 0064 train_loss= 0.34119 train_acc= 0.93009 val_loss= 0.37283 val_acc= 0.90199 time= 0.23603
Epoch: 0065 train_loss= 0.32736 train_acc= 0.93383 val_loss= 0.36524 val_acc= 0.90199 time= 0.21301
Epoch: 0066 train_loss= 0.31435 train_acc= 0.93536 val_loss= 0.35876 val_acc= 0.90046 time= 0.21099
Epoch: 0067 train_loss= 0.30695 train_acc= 0.93366 val_loss= 0.35209 val_acc= 0.90199 time= 0.21000
Epoch: 0068 train_loss= 0.29631 train_acc= 0.93689 val_loss= 0.34482 val_acc= 0.90505 time= 0.23500
Epoch: 0069 train_loss= 0.28387 train_acc= 0.94285 val_loss= 0.33766 val_acc= 0.90812 time= 0.21500
Epoch: 0070 train_loss= 0.27577 train_acc= 0.94472 val_loss= 0.33040 val_acc= 0.90812 time= 0.21297
Epoch: 0071 train_loss= 0.26625 train_acc= 0.94183 val_loss= 0.32378 val_acc= 0.91118 time= 0.21403
Epoch: 0072 train_loss= 0.25586 train_acc= 0.94829 val_loss= 0.31782 val_acc= 0.91271 time= 0.21100
Epoch: 0073 train_loss= 0.24867 train_acc= 0.94931 val_loss= 0.31241 val_acc= 0.91577 time= 0.23408
Epoch: 0074 train_loss= 0.24397 train_acc= 0.94965 val_loss= 0.30826 val_acc= 0.91577 time= 0.21108
Epoch: 0075 train_loss= 0.23042 train_acc= 0.95322 val_loss= 0.30391 val_acc= 0.91730 time= 0.21000
Epoch: 0076 train_loss= 0.22169 train_acc= 0.95492 val_loss= 0.29992 val_acc= 0.91884 time= 0.21100
Epoch: 0077 train_loss= 0.21412 train_acc= 0.95475 val_loss= 0.29572 val_acc= 0.92037 time= 0.23889
Epoch: 0078 train_loss= 0.20758 train_acc= 0.95714 val_loss= 0.29069 val_acc= 0.92343 time= 0.21300
Epoch: 0079 train_loss= 0.19683 train_acc= 0.95884 val_loss= 0.28554 val_acc= 0.92343 time= 0.22332
Epoch: 0080 train_loss= 0.19382 train_acc= 0.95986 val_loss= 0.28152 val_acc= 0.92496 time= 0.21297
Epoch: 0081 train_loss= 0.18939 train_acc= 0.96020 val_loss= 0.27800 val_acc= 0.92496 time= 0.21209
Epoch: 0082 train_loss= 0.17777 train_acc= 0.96292 val_loss= 0.27412 val_acc= 0.92496 time= 0.23299
Epoch: 0083 train_loss= 0.17477 train_acc= 0.96513 val_loss= 0.26989 val_acc= 0.92802 time= 0.21703
Epoch: 0084 train_loss= 0.16887 train_acc= 0.96547 val_loss= 0.26651 val_acc= 0.92956 time= 0.21391
Epoch: 0085 train_loss= 0.15938 train_acc= 0.96751 val_loss= 0.26368 val_acc= 0.93109 time= 0.21300
Epoch: 0086 train_loss= 0.15750 train_acc= 0.96768 val_loss= 0.26194 val_acc= 0.92802 time= 0.21501
Epoch: 0087 train_loss= 0.15129 train_acc= 0.96734 val_loss= 0.26012 val_acc= 0.93262 time= 0.23099
Epoch: 0088 train_loss= 0.14878 train_acc= 0.97023 val_loss= 0.25973 val_acc= 0.92956 time= 0.21210
Epoch: 0089 train_loss= 0.14270 train_acc= 0.97244 val_loss= 0.25881 val_acc= 0.92956 time= 0.21007
Epoch: 0090 train_loss= 0.13624 train_acc= 0.97500 val_loss= 0.25730 val_acc= 0.92802 time= 0.20819
Epoch: 0091 train_loss= 0.13420 train_acc= 0.97432 val_loss= 0.25350 val_acc= 0.92956 time= 0.23930
Epoch: 0092 train_loss= 0.12476 train_acc= 0.97789 val_loss= 0.24937 val_acc= 0.93568 time= 0.21300
Epoch: 0093 train_loss= 0.12528 train_acc= 0.97670 val_loss= 0.24595 val_acc= 0.93415 time= 0.21100
Epoch: 0094 train_loss= 0.11765 train_acc= 0.97993 val_loss= 0.24369 val_acc= 0.93415 time= 0.20901
Epoch: 0095 train_loss= 0.11716 train_acc= 0.97925 val_loss= 0.24280 val_acc= 0.93721 time= 0.23307
Epoch: 0096 train_loss= 0.11106 train_acc= 0.98027 val_loss= 0.24333 val_acc= 0.93568 time= 0.21000
Epoch: 0097 train_loss= 0.10607 train_acc= 0.98129 val_loss= 0.24349 val_acc= 0.93415 time= 0.21997
Epoch: 0098 train_loss= 0.10646 train_acc= 0.98112 val_loss= 0.24386 val_acc= 0.93262 time= 0.21476
Epoch: 0099 train_loss= 0.10150 train_acc= 0.98129 val_loss= 0.24409 val_acc= 0.93262 time= 0.21200
Epoch: 0100 train_loss= 0.09818 train_acc= 0.98231 val_loss= 0.24203 val_acc= 0.93109 time= 0.21400
Epoch: 0101 train_loss= 0.09437 train_acc= 0.98384 val_loss= 0.23921 val_acc= 0.93568 time= 0.23303
Epoch: 0102 train_loss= 0.08937 train_acc= 0.98520 val_loss= 0.23735 val_acc= 0.93721 time= 0.21700
Epoch: 0103 train_loss= 0.08734 train_acc= 0.98401 val_loss= 0.23704 val_acc= 0.93568 time= 0.20900
Epoch: 0104 train_loss= 0.08458 train_acc= 0.98673 val_loss= 0.23594 val_acc= 0.93568 time= 0.21197
Epoch: 0105 train_loss= 0.08481 train_acc= 0.98435 val_loss= 0.23589 val_acc= 0.93721 time= 0.22520
Epoch: 0106 train_loss= 0.08307 train_acc= 0.98520 val_loss= 0.23792 val_acc= 0.93415 time= 0.22100
Epoch: 0107 train_loss= 0.07846 train_acc= 0.98673 val_loss= 0.23809 val_acc= 0.93415 time= 0.21103
Epoch: 0108 train_loss= 0.07869 train_acc= 0.98707 val_loss= 0.23620 val_acc= 0.93568 time= 0.20900
Epoch: 0109 train_loss= 0.07496 train_acc= 0.98656 val_loss= 0.23271 val_acc= 0.93721 time= 0.21403
Epoch: 0110 train_loss= 0.07149 train_acc= 0.98928 val_loss= 0.23010 val_acc= 0.93874 time= 0.23100
Epoch: 0111 train_loss= 0.07191 train_acc= 0.98775 val_loss= 0.22825 val_acc= 0.93874 time= 0.20900
Epoch: 0112 train_loss= 0.06954 train_acc= 0.98758 val_loss= 0.22715 val_acc= 0.94028 time= 0.21449
Epoch: 0113 train_loss= 0.06644 train_acc= 0.98996 val_loss= 0.22679 val_acc= 0.94028 time= 0.21100
Epoch: 0114 train_loss= 0.06488 train_acc= 0.99030 val_loss= 0.22729 val_acc= 0.93721 time= 0.23400
Epoch: 0115 train_loss= 0.06431 train_acc= 0.98945 val_loss= 0.22745 val_acc= 0.93568 time= 0.21208
Epoch: 0116 train_loss= 0.06292 train_acc= 0.98962 val_loss= 0.22890 val_acc= 0.93568 time= 0.21300
Epoch: 0117 train_loss= 0.05986 train_acc= 0.99081 val_loss= 0.22909 val_acc= 0.93568 time= 0.21001
Epoch: 0118 train_loss= 0.05878 train_acc= 0.99116 val_loss= 0.23035 val_acc= 0.93874 time= 0.20906
Early stopping...
Optimization Finished!
Test set results: cost= 0.25141 accuracy= 0.93575 time= 0.11497
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7717    0.9467    0.8503        75
           4     1.0000    1.0000    1.0000         9
           5     0.8061    0.9080    0.8541        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.5556    0.7143         9
          10     0.8800    0.6111    0.7213        36
          11     1.0000    0.9167    0.9565        12
          12     0.8345    1.0000    0.9098       121
          13     0.9333    0.7368    0.8235        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.8571    0.7059    0.7742        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.7273    0.8421        11
          29     0.9655    0.9655    0.9655       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8375    0.8272    0.8323        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9817    0.9898    0.9857      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8333    0.8333    0.8333        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9357      2568
   macro avg     0.7657    0.6645    0.6906      2568
weighted avg     0.9356    0.9357    0.9316      2568

Macro average Test Precision, Recall and F1-Score...
(0.7656548070030719, 0.6644606897636761, 0.6905734409219911, None)
Micro average Test Precision, Recall and F1-Score...
(0.9357476635514018, 0.9357476635514018, 0.9357476635514018, None)
embeddings:
8892 6532 2568
[[ 0.40649223  0.12031274 -0.05142216 ...  0.4740543   0.06068572
   0.01173535]
 [ 0.00354603  0.01107689  0.5475873  ...  0.53891736  0.15848456
   0.3562633 ]
 [ 0.66525257  0.49815655  0.48754394 ...  0.22739767  0.00425703
   0.31506178]
 ...
 [ 0.15071641  0.03834711  0.04002695 ...  0.14981909  0.00910248
   0.16350833]
 [ 0.3569758   0.26146722  0.10239182 ...  0.0802047   0.03233741
   0.05941266]
 [ 0.30486056  0.39246348  0.2404296  ...  0.12289831  0.19299756
   0.3091723 ]]

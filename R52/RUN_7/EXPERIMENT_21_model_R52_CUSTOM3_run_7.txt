(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.01310 val_loss= 3.91236 val_acc= 0.65084 time= 0.45888
Epoch: 0002 train_loss= 3.91141 train_acc= 0.61677 val_loss= 3.82775 val_acc= 0.65544 time= 0.17379
Epoch: 0003 train_loss= 3.83258 train_acc= 0.60180 val_loss= 3.69704 val_acc= 0.65237 time= 0.16703
Epoch: 0004 train_loss= 3.69567 train_acc= 0.62681 val_loss= 3.51978 val_acc= 0.65237 time= 0.19697
Epoch: 0005 train_loss= 3.52756 train_acc= 0.61541 val_loss= 3.30258 val_acc= 0.65237 time= 0.17000
Epoch: 0006 train_loss= 3.29928 train_acc= 0.62545 val_loss= 3.06234 val_acc= 0.64931 time= 0.17700
Epoch: 0007 train_loss= 3.07336 train_acc= 0.59721 val_loss= 2.82354 val_acc= 0.64472 time= 0.16803
Epoch: 0008 train_loss= 2.84820 train_acc= 0.60401 val_loss= 2.61067 val_acc= 0.63706 time= 0.16700
Epoch: 0009 train_loss= 2.52251 train_acc= 0.59517 val_loss= 2.44695 val_acc= 0.64625 time= 0.17000
Epoch: 0010 train_loss= 2.43493 train_acc= 0.59279 val_loss= 2.34390 val_acc= 0.66922 time= 0.18908
Epoch: 0011 train_loss= 2.39297 train_acc= 0.59721 val_loss= 2.28765 val_acc= 0.61256 time= 0.16800
Epoch: 0012 train_loss= 2.33603 train_acc= 0.56013 val_loss= 2.25157 val_acc= 0.46861 time= 0.18504
Epoch: 0013 train_loss= 2.24623 train_acc= 0.49668 val_loss= 2.21420 val_acc= 0.45636 time= 0.17100
Epoch: 0014 train_loss= 2.21391 train_acc= 0.44327 val_loss= 2.16553 val_acc= 0.45636 time= 0.17000
Epoch: 0015 train_loss= 2.19510 train_acc= 0.43664 val_loss= 2.10122 val_acc= 0.45636 time= 0.17403
Epoch: 0016 train_loss= 2.15170 train_acc= 0.43817 val_loss= 2.02275 val_acc= 0.45636 time= 0.18901
Epoch: 0017 train_loss= 2.07418 train_acc= 0.43749 val_loss= 1.93930 val_acc= 0.46248 time= 0.16900
Epoch: 0018 train_loss= 1.97646 train_acc= 0.44395 val_loss= 1.86068 val_acc= 0.48086 time= 0.17100
Epoch: 0019 train_loss= 1.89732 train_acc= 0.48716 val_loss= 1.79295 val_acc= 0.53752 time= 0.16796
Epoch: 0020 train_loss= 1.84524 train_acc= 0.52696 val_loss= 1.73539 val_acc= 0.62634 time= 0.16804
Epoch: 0021 train_loss= 1.74467 train_acc= 0.59670 val_loss= 1.68501 val_acc= 0.66003 time= 0.19700
Epoch: 0022 train_loss= 1.73155 train_acc= 0.62187 val_loss= 1.63646 val_acc= 0.67688 time= 0.17100
Epoch: 0023 train_loss= 1.69025 train_acc= 0.63820 val_loss= 1.58723 val_acc= 0.67841 time= 0.17200
Epoch: 0024 train_loss= 1.62687 train_acc= 0.64382 val_loss= 1.53792 val_acc= 0.67841 time= 0.18503
Epoch: 0025 train_loss= 1.58006 train_acc= 0.64450 val_loss= 1.48980 val_acc= 0.67841 time= 0.16699
Epoch: 0026 train_loss= 1.54731 train_acc= 0.63888 val_loss= 1.44365 val_acc= 0.68453 time= 0.16701
Epoch: 0027 train_loss= 1.48294 train_acc= 0.65930 val_loss= 1.40131 val_acc= 0.69066 time= 0.19206
Epoch: 0028 train_loss= 1.44279 train_acc= 0.66814 val_loss= 1.36269 val_acc= 0.69372 time= 0.16768
Epoch: 0029 train_loss= 1.40869 train_acc= 0.66899 val_loss= 1.32755 val_acc= 0.70444 time= 0.18202
Epoch: 0030 train_loss= 1.36984 train_acc= 0.68396 val_loss= 1.29500 val_acc= 0.70750 time= 0.17195
Epoch: 0031 train_loss= 1.32025 train_acc= 0.68872 val_loss= 1.26431 val_acc= 0.71669 time= 0.17000
Epoch: 0032 train_loss= 1.30828 train_acc= 0.69383 val_loss= 1.23474 val_acc= 0.71975 time= 0.17300
Epoch: 0033 train_loss= 1.27154 train_acc= 0.70556 val_loss= 1.20591 val_acc= 0.72588 time= 0.17203
Epoch: 0034 train_loss= 1.24102 train_acc= 0.71594 val_loss= 1.17785 val_acc= 0.72894 time= 0.16705
Epoch: 0035 train_loss= 1.21350 train_acc= 0.71679 val_loss= 1.15055 val_acc= 0.73660 time= 0.17400
Epoch: 0036 train_loss= 1.18810 train_acc= 0.72921 val_loss= 1.12413 val_acc= 0.73660 time= 0.16900
Epoch: 0037 train_loss= 1.17357 train_acc= 0.72784 val_loss= 1.09862 val_acc= 0.74119 time= 0.16900
Epoch: 0038 train_loss= 1.12378 train_acc= 0.73754 val_loss= 1.07377 val_acc= 0.74579 time= 0.17000
Epoch: 0039 train_loss= 1.11740 train_acc= 0.73652 val_loss= 1.04966 val_acc= 0.75345 time= 0.19400
Epoch: 0040 train_loss= 1.08322 train_acc= 0.74690 val_loss= 1.02627 val_acc= 0.75957 time= 0.17200
Epoch: 0041 train_loss= 1.06171 train_acc= 0.75183 val_loss= 1.00351 val_acc= 0.76723 time= 0.18500
Epoch: 0042 train_loss= 1.05586 train_acc= 0.75336 val_loss= 0.98150 val_acc= 0.77029 time= 0.16701
Epoch: 0043 train_loss= 1.02648 train_acc= 0.76748 val_loss= 0.96030 val_acc= 0.77335 time= 0.16600
Epoch: 0044 train_loss= 0.99669 train_acc= 0.77564 val_loss= 0.93952 val_acc= 0.79173 time= 0.19099
Epoch: 0045 train_loss= 0.98823 train_acc= 0.77598 val_loss= 0.91945 val_acc= 0.79632 time= 0.16600
Epoch: 0046 train_loss= 0.95056 train_acc= 0.78908 val_loss= 0.89955 val_acc= 0.80704 time= 0.17100
Epoch: 0047 train_loss= 0.93542 train_acc= 0.79265 val_loss= 0.87970 val_acc= 0.81776 time= 0.17597
Epoch: 0048 train_loss= 0.91995 train_acc= 0.80354 val_loss= 0.86037 val_acc= 0.82542 time= 0.17003
Epoch: 0049 train_loss= 0.90192 train_acc= 0.80762 val_loss= 0.84152 val_acc= 0.82695 time= 0.17044
Epoch: 0050 train_loss= 0.88521 train_acc= 0.80422 val_loss= 0.82321 val_acc= 0.82848 time= 0.19201
Epoch: 0051 train_loss= 0.86709 train_acc= 0.80847 val_loss= 0.80497 val_acc= 0.82848 time= 0.16700
Epoch: 0052 train_loss= 0.85260 train_acc= 0.80847 val_loss= 0.78702 val_acc= 0.83002 time= 0.18201
Epoch: 0053 train_loss= 0.83600 train_acc= 0.81442 val_loss= 0.76958 val_acc= 0.83155 time= 0.16608
Epoch: 0054 train_loss= 0.83052 train_acc= 0.80915 val_loss= 0.75252 val_acc= 0.83614 time= 0.16800
Epoch: 0055 train_loss= 0.81210 train_acc= 0.81715 val_loss= 0.73591 val_acc= 0.83920 time= 0.17896
Epoch: 0056 train_loss= 0.78610 train_acc= 0.82055 val_loss= 0.71979 val_acc= 0.83920 time= 0.17100
Epoch: 0057 train_loss= 0.77795 train_acc= 0.81408 val_loss= 0.70443 val_acc= 0.83767 time= 0.17000
Epoch: 0058 train_loss= 0.74673 train_acc= 0.82395 val_loss= 0.68931 val_acc= 0.84074 time= 0.17634
Epoch: 0059 train_loss= 0.74107 train_acc= 0.83194 val_loss= 0.67511 val_acc= 0.85145 time= 0.16705
Epoch: 0060 train_loss= 0.73329 train_acc= 0.83586 val_loss= 0.66154 val_acc= 0.85299 time= 0.16800
Epoch: 0061 train_loss= 0.70565 train_acc= 0.83365 val_loss= 0.64819 val_acc= 0.85605 time= 0.17096
Epoch: 0062 train_loss= 0.68886 train_acc= 0.84096 val_loss= 0.63499 val_acc= 0.85911 time= 0.18822
Epoch: 0063 train_loss= 0.69890 train_acc= 0.83858 val_loss= 0.62168 val_acc= 0.86064 time= 0.16800
Epoch: 0064 train_loss= 0.64413 train_acc= 0.84980 val_loss= 0.60770 val_acc= 0.86371 time= 0.18500
Epoch: 0065 train_loss= 0.65237 train_acc= 0.85151 val_loss= 0.59347 val_acc= 0.86983 time= 0.16958
Epoch: 0066 train_loss= 0.64701 train_acc= 0.84776 val_loss= 0.57990 val_acc= 0.87289 time= 0.16999
Epoch: 0067 train_loss= 0.62941 train_acc= 0.84742 val_loss= 0.56750 val_acc= 0.87596 time= 0.19400
Epoch: 0068 train_loss= 0.60653 train_acc= 0.86239 val_loss= 0.55621 val_acc= 0.87596 time= 0.16701
Epoch: 0069 train_loss= 0.58901 train_acc= 0.86222 val_loss= 0.54543 val_acc= 0.87749 time= 0.16999
Epoch: 0070 train_loss= 0.58072 train_acc= 0.86664 val_loss= 0.53521 val_acc= 0.87749 time= 0.16700
Epoch: 0071 train_loss= 0.58538 train_acc= 0.85406 val_loss= 0.52556 val_acc= 0.87749 time= 0.16800
Epoch: 0072 train_loss= 0.55675 train_acc= 0.86426 val_loss= 0.51593 val_acc= 0.88055 time= 0.16815
Epoch: 0073 train_loss= 0.57126 train_acc= 0.86069 val_loss= 0.50661 val_acc= 0.88361 time= 0.19496
Epoch: 0074 train_loss= 0.53768 train_acc= 0.86749 val_loss= 0.49805 val_acc= 0.88361 time= 0.17000
Epoch: 0075 train_loss= 0.53249 train_acc= 0.87141 val_loss= 0.48894 val_acc= 0.88361 time= 0.19111
Epoch: 0076 train_loss= 0.53175 train_acc= 0.87294 val_loss= 0.47971 val_acc= 0.88515 time= 0.16693
Epoch: 0077 train_loss= 0.49624 train_acc= 0.88229 val_loss= 0.47012 val_acc= 0.88821 time= 0.16900
Epoch: 0078 train_loss= 0.51643 train_acc= 0.87600 val_loss= 0.46083 val_acc= 0.88974 time= 0.16997
Epoch: 0079 train_loss= 0.49455 train_acc= 0.88297 val_loss= 0.45188 val_acc= 0.89127 time= 0.18913
Epoch: 0080 train_loss= 0.49631 train_acc= 0.87855 val_loss= 0.44390 val_acc= 0.89280 time= 0.16807
Epoch: 0081 train_loss= 0.48402 train_acc= 0.87872 val_loss= 0.43615 val_acc= 0.89740 time= 0.16895
Epoch: 0082 train_loss= 0.45777 train_acc= 0.88552 val_loss= 0.42876 val_acc= 0.89893 time= 0.17200
Epoch: 0083 train_loss= 0.46073 train_acc= 0.89097 val_loss= 0.42199 val_acc= 0.90046 time= 0.17100
Epoch: 0084 train_loss= 0.45277 train_acc= 0.89420 val_loss= 0.41595 val_acc= 0.90046 time= 0.19005
Epoch: 0085 train_loss= 0.45473 train_acc= 0.88978 val_loss= 0.41024 val_acc= 0.89893 time= 0.16699
Epoch: 0086 train_loss= 0.44227 train_acc= 0.89216 val_loss= 0.40488 val_acc= 0.89893 time= 0.16800
Epoch: 0087 train_loss= 0.41390 train_acc= 0.89794 val_loss= 0.39989 val_acc= 0.89893 time= 0.18797
Epoch: 0088 train_loss= 0.41700 train_acc= 0.89862 val_loss= 0.39560 val_acc= 0.90046 time= 0.16611
Epoch: 0089 train_loss= 0.43185 train_acc= 0.89624 val_loss= 0.39191 val_acc= 0.90505 time= 0.16905
Epoch: 0090 train_loss= 0.41145 train_acc= 0.90066 val_loss= 0.38714 val_acc= 0.90505 time= 0.19900
Epoch: 0091 train_loss= 0.41434 train_acc= 0.89998 val_loss= 0.38170 val_acc= 0.90352 time= 0.17100
Epoch: 0092 train_loss= 0.39555 train_acc= 0.90287 val_loss= 0.37568 val_acc= 0.90505 time= 0.18204
Epoch: 0093 train_loss= 0.40339 train_acc= 0.90424 val_loss= 0.36925 val_acc= 0.90505 time= 0.16800
Epoch: 0094 train_loss= 0.38117 train_acc= 0.90543 val_loss= 0.36321 val_acc= 0.90505 time= 0.16707
Epoch: 0095 train_loss= 0.38308 train_acc= 0.90645 val_loss= 0.35820 val_acc= 0.90505 time= 0.17072
Epoch: 0096 train_loss= 0.38127 train_acc= 0.90747 val_loss= 0.35340 val_acc= 0.90352 time= 0.18811
Epoch: 0097 train_loss= 0.36194 train_acc= 0.90985 val_loss= 0.34889 val_acc= 0.90352 time= 0.16800
Epoch: 0098 train_loss= 0.37009 train_acc= 0.90781 val_loss= 0.34470 val_acc= 0.90658 time= 0.17500
Epoch: 0099 train_loss= 0.37694 train_acc= 0.90083 val_loss= 0.34063 val_acc= 0.90658 time= 0.17100
Epoch: 0100 train_loss= 0.36526 train_acc= 0.90390 val_loss= 0.33706 val_acc= 0.90812 time= 0.17100
Epoch: 0101 train_loss= 0.36818 train_acc= 0.90185 val_loss= 0.33303 val_acc= 0.90812 time= 0.17100
Epoch: 0102 train_loss= 0.32817 train_acc= 0.91733 val_loss= 0.33009 val_acc= 0.91424 time= 0.18807
Epoch: 0103 train_loss= 0.33790 train_acc= 0.91529 val_loss= 0.32872 val_acc= 0.91577 time= 0.16896
Epoch: 0104 train_loss= 0.32308 train_acc= 0.91699 val_loss= 0.32863 val_acc= 0.91271 time= 0.18303
Epoch: 0105 train_loss= 0.33000 train_acc= 0.91852 val_loss= 0.32784 val_acc= 0.91118 time= 0.16600
Epoch: 0106 train_loss= 0.32059 train_acc= 0.92142 val_loss= 0.32546 val_acc= 0.91271 time= 0.16797
Epoch: 0107 train_loss= 0.30590 train_acc= 0.92295 val_loss= 0.32162 val_acc= 0.91118 time= 0.19800
Epoch: 0108 train_loss= 0.31516 train_acc= 0.91988 val_loss= 0.31780 val_acc= 0.91424 time= 0.16921
Epoch: 0109 train_loss= 0.31380 train_acc= 0.92346 val_loss= 0.31534 val_acc= 0.91577 time= 0.17003
Epoch: 0110 train_loss= 0.29903 train_acc= 0.92839 val_loss= 0.31268 val_acc= 0.91271 time= 0.16710
Epoch: 0111 train_loss= 0.30123 train_acc= 0.92448 val_loss= 0.30886 val_acc= 0.91577 time= 0.16799
Epoch: 0112 train_loss= 0.30385 train_acc= 0.92431 val_loss= 0.30525 val_acc= 0.91730 time= 0.16900
Epoch: 0113 train_loss= 0.31603 train_acc= 0.91988 val_loss= 0.30226 val_acc= 0.91884 time= 0.19006
Epoch: 0114 train_loss= 0.30757 train_acc= 0.92108 val_loss= 0.29977 val_acc= 0.91577 time= 0.16700
Epoch: 0115 train_loss= 0.29095 train_acc= 0.92482 val_loss= 0.29734 val_acc= 0.91577 time= 0.18696
Epoch: 0116 train_loss= 0.29510 train_acc= 0.92448 val_loss= 0.29530 val_acc= 0.91424 time= 0.17089
Epoch: 0117 train_loss= 0.28910 train_acc= 0.93128 val_loss= 0.29257 val_acc= 0.91577 time= 0.17251
Epoch: 0118 train_loss= 0.29876 train_acc= 0.92193 val_loss= 0.29118 val_acc= 0.91424 time= 0.17200
Epoch: 0119 train_loss= 0.28664 train_acc= 0.92533 val_loss= 0.28989 val_acc= 0.91577 time= 0.18904
Epoch: 0120 train_loss= 0.28192 train_acc= 0.93213 val_loss= 0.28873 val_acc= 0.91577 time= 0.16900
Epoch: 0121 train_loss= 0.26813 train_acc= 0.93077 val_loss= 0.28840 val_acc= 0.91577 time= 0.16900
Epoch: 0122 train_loss= 0.28563 train_acc= 0.93111 val_loss= 0.28890 val_acc= 0.91884 time= 0.16800
Epoch: 0123 train_loss= 0.26513 train_acc= 0.93213 val_loss= 0.29074 val_acc= 0.91577 time= 0.16795
Epoch: 0124 train_loss= 0.26793 train_acc= 0.93315 val_loss= 0.29127 val_acc= 0.91730 time= 0.18300
Epoch: 0125 train_loss= 0.26196 train_acc= 0.93247 val_loss= 0.29003 val_acc= 0.92190 time= 0.17031
Epoch: 0126 train_loss= 0.25982 train_acc= 0.93332 val_loss= 0.28742 val_acc= 0.92037 time= 0.17004
Epoch: 0127 train_loss= 0.25483 train_acc= 0.93894 val_loss= 0.28473 val_acc= 0.91730 time= 0.18501
Epoch: 0128 train_loss= 0.25549 train_acc= 0.93638 val_loss= 0.28125 val_acc= 0.92190 time= 0.16699
Epoch: 0129 train_loss= 0.26495 train_acc= 0.93468 val_loss= 0.27740 val_acc= 0.92037 time= 0.16702
Epoch: 0130 train_loss= 0.23555 train_acc= 0.93894 val_loss= 0.27372 val_acc= 0.92037 time= 0.19099
Epoch: 0131 train_loss= 0.24159 train_acc= 0.93911 val_loss= 0.27006 val_acc= 0.92343 time= 0.16801
Epoch: 0132 train_loss= 0.23057 train_acc= 0.93877 val_loss= 0.26671 val_acc= 0.92343 time= 0.17195
Epoch: 0133 train_loss= 0.24872 train_acc= 0.93281 val_loss= 0.26439 val_acc= 0.92496 time= 0.17000
Epoch: 0134 train_loss= 0.23948 train_acc= 0.93979 val_loss= 0.26289 val_acc= 0.92496 time= 0.16970
Epoch: 0135 train_loss= 0.23837 train_acc= 0.93757 val_loss= 0.26314 val_acc= 0.92496 time= 0.16896
Epoch: 0136 train_loss= 0.24339 train_acc= 0.93553 val_loss= 0.26545 val_acc= 0.92649 time= 0.18898
Epoch: 0137 train_loss= 0.24136 train_acc= 0.93928 val_loss= 0.26702 val_acc= 0.92190 time= 0.16803
Epoch: 0138 train_loss= 0.22380 train_acc= 0.93808 val_loss= 0.26874 val_acc= 0.92037 time= 0.18600
Epoch: 0139 train_loss= 0.23242 train_acc= 0.94030 val_loss= 0.26967 val_acc= 0.92037 time= 0.16700
Early stopping...
Optimization Finished!
Test set results: cost= 0.30655 accuracy= 0.92718 time= 0.07300
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.6667    0.3333    0.4444         6
           2     0.0000    0.0000    0.0000         1
           3     0.8140    0.9333    0.8696        75
           4     1.0000    1.0000    1.0000         9
           5     0.7941    0.9310    0.8571        87
           6     0.9200    0.9200    0.9200        25
           7     0.6471    0.8462    0.7333        13
           8     0.8889    0.7273    0.8000        11
           9     0.0000    0.0000    0.0000         9
          10     0.9000    0.7500    0.8182        36
          11     1.0000    0.9167    0.9565        12
          12     0.8207    0.9835    0.8947       121
          13     0.7778    0.7368    0.7568        19
          14     0.8462    0.7857    0.8148        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.8636    0.9500    0.9048        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.5455    0.7059    0.6154        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.5000    0.6667        12
          28     1.0000    0.7273    0.8421        11
          29     0.9740    0.9684    0.9712       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.3333    0.5000         3
          32     0.5556    1.0000    0.7143        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8608    0.8395    0.8500        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9746    0.9926    0.9835      1083
          40     1.0000    0.2000    0.3333         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.7143    0.8333    0.7692        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.5769    1.0000    0.7317        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9272      2568
   macro avg     0.6400    0.5453    0.5639      2568
weighted avg     0.9196    0.9272    0.9186      2568

Macro average Test Precision, Recall and F1-Score...
(0.6399887222799614, 0.5452966063882777, 0.5638810923515089, None)
Micro average Test Precision, Recall and F1-Score...
(0.9271806853582555, 0.9271806853582555, 0.9271806853582555, None)
embeddings:
8892 6532 2568
[[-1.9030057e-02  8.8388873e-03 -1.0104913e-01 ...  1.4167317e-02
   1.5995108e+00  1.4404763e+00]
 [-1.6016867e-03  4.1567117e-02 -3.0682759e-02 ...  2.7508864e-01
   1.0063293e+00  7.8113723e-01]
 [ 1.5583225e-01 -1.2238631e-03  1.7472142e-01 ...  5.2590270e-02
   2.4877608e-01  1.5427960e-01]
 ...
 [-1.6212473e-02  8.0553941e-02  1.1930871e-01 ...  1.3372697e-01
   1.2571381e-01  1.7523056e-01]
 [ 1.0851777e-01  3.9612710e-02  1.0643567e-01 ...  3.6986679e-02
   2.5425076e-01  2.9280391e-01]
 [ 2.4387528e-01  2.5110397e-01  2.3004936e-01 ...  2.5101689e-01
   2.1300530e-01  1.9768700e-01]]

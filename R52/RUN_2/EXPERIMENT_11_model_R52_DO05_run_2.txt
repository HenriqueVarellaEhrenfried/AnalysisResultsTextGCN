(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95137 train_acc= 0.00272 val_loss= 3.90251 val_acc= 0.67381 time= 0.44956
Epoch: 0002 train_loss= 3.90254 train_acc= 0.64739 val_loss= 3.80653 val_acc= 0.66922 time= 0.17696
Epoch: 0003 train_loss= 3.81116 train_acc= 0.64433 val_loss= 3.65734 val_acc= 0.66922 time= 0.19204
Epoch: 0004 train_loss= 3.65908 train_acc= 0.64195 val_loss= 3.45491 val_acc= 0.66922 time= 0.17009
Epoch: 0005 train_loss= 3.45863 train_acc= 0.64076 val_loss= 3.20860 val_acc= 0.67075 time= 0.16791
Epoch: 0006 train_loss= 3.20896 train_acc= 0.64212 val_loss= 2.94048 val_acc= 0.66769 time= 0.17398
Epoch: 0007 train_loss= 2.94405 train_acc= 0.64365 val_loss= 2.67898 val_acc= 0.66462 time= 0.20104
Epoch: 0008 train_loss= 2.67933 train_acc= 0.64535 val_loss= 2.45812 val_acc= 0.66769 time= 0.16700
Epoch: 0009 train_loss= 2.44559 train_acc= 0.64331 val_loss= 2.30836 val_acc= 0.66616 time= 0.16800
Epoch: 0010 train_loss= 2.30011 train_acc= 0.64212 val_loss= 2.22702 val_acc= 0.60031 time= 0.19500
Epoch: 0011 train_loss= 2.21740 train_acc= 0.59942 val_loss= 2.18254 val_acc= 0.48392 time= 0.16800
Epoch: 0012 train_loss= 2.19476 train_acc= 0.47032 val_loss= 2.14369 val_acc= 0.46248 time= 0.16800
Epoch: 0013 train_loss= 2.16204 train_acc= 0.43528 val_loss= 2.09147 val_acc= 0.45636 time= 0.20169
Epoch: 0014 train_loss= 2.10493 train_acc= 0.43375 val_loss= 2.01952 val_acc= 0.46248 time= 0.17058
Epoch: 0015 train_loss= 2.04847 train_acc= 0.43545 val_loss= 1.93313 val_acc= 0.47014 time= 0.18601
Epoch: 0016 train_loss= 1.96167 train_acc= 0.44208 val_loss= 1.84378 val_acc= 0.50383 time= 0.16799
Epoch: 0017 train_loss= 1.87155 train_acc= 0.49702 val_loss= 1.76342 val_acc= 0.58040 time= 0.16910
Epoch: 0018 train_loss= 1.79575 train_acc= 0.57714 val_loss= 1.69772 val_acc= 0.64778 time= 0.16796
Epoch: 0019 train_loss= 1.72733 train_acc= 0.63395 val_loss= 1.64417 val_acc= 0.66922 time= 0.17305
Epoch: 0020 train_loss= 1.66424 train_acc= 0.64790 val_loss= 1.59604 val_acc= 0.67688 time= 0.16862
Epoch: 0021 train_loss= 1.61991 train_acc= 0.65079 val_loss= 1.54791 val_acc= 0.67994 time= 0.17395
Epoch: 0022 train_loss= 1.56858 train_acc= 0.65981 val_loss= 1.49796 val_acc= 0.67534 time= 0.17392
Epoch: 0023 train_loss= 1.52097 train_acc= 0.66032 val_loss= 1.44781 val_acc= 0.67841 time= 0.17100
Epoch: 0024 train_loss= 1.47542 train_acc= 0.66508 val_loss= 1.39991 val_acc= 0.68453 time= 0.17108
Epoch: 0025 train_loss= 1.42480 train_acc= 0.66797 val_loss= 1.35589 val_acc= 0.68913 time= 0.19364
Epoch: 0026 train_loss= 1.37468 train_acc= 0.67256 val_loss= 1.31609 val_acc= 0.69832 time= 0.16800
Epoch: 0027 train_loss= 1.33492 train_acc= 0.67988 val_loss= 1.28010 val_acc= 0.70444 time= 0.18318
Epoch: 0028 train_loss= 1.29811 train_acc= 0.68889 val_loss= 1.24710 val_acc= 0.71057 time= 0.16800
Epoch: 0029 train_loss= 1.27092 train_acc= 0.69961 val_loss= 1.21604 val_acc= 0.71822 time= 0.17300
Epoch: 0030 train_loss= 1.23553 train_acc= 0.71152 val_loss= 1.18624 val_acc= 0.72282 time= 0.17297
Epoch: 0031 train_loss= 1.21012 train_acc= 0.71832 val_loss= 1.15706 val_acc= 0.73201 time= 0.19403
Epoch: 0032 train_loss= 1.17719 train_acc= 0.72989 val_loss= 1.12820 val_acc= 0.73660 time= 0.18000
Epoch: 0033 train_loss= 1.14255 train_acc= 0.73533 val_loss= 1.09946 val_acc= 0.74273 time= 0.16700
Epoch: 0034 train_loss= 1.11193 train_acc= 0.75081 val_loss= 1.07098 val_acc= 0.74885 time= 0.16900
Epoch: 0035 train_loss= 1.07964 train_acc= 0.75693 val_loss= 1.04286 val_acc= 0.75651 time= 0.17100
Epoch: 0036 train_loss= 1.05767 train_acc= 0.76408 val_loss= 1.01549 val_acc= 0.76876 time= 0.18600
Epoch: 0037 train_loss= 1.02935 train_acc= 0.77105 val_loss= 0.98902 val_acc= 0.77489 time= 0.17300
Epoch: 0038 train_loss= 1.00117 train_acc= 0.78006 val_loss= 0.96349 val_acc= 0.78254 time= 0.17448
Epoch: 0039 train_loss= 0.97789 train_acc= 0.78857 val_loss= 0.93889 val_acc= 0.79632 time= 0.17103
Epoch: 0040 train_loss= 0.94859 train_acc= 0.80184 val_loss= 0.91498 val_acc= 0.80551 time= 0.16800
Epoch: 0041 train_loss= 0.92654 train_acc= 0.80881 val_loss= 0.89141 val_acc= 0.81011 time= 0.17101
Epoch: 0042 train_loss= 0.90476 train_acc= 0.81323 val_loss= 0.86834 val_acc= 0.81317 time= 0.18099
Epoch: 0043 train_loss= 0.87802 train_acc= 0.82174 val_loss= 0.84566 val_acc= 0.82389 time= 0.16898
Epoch: 0044 train_loss= 0.85297 train_acc= 0.82684 val_loss= 0.82334 val_acc= 0.83614 time= 0.18399
Epoch: 0045 train_loss= 0.83195 train_acc= 0.83262 val_loss= 0.80137 val_acc= 0.83614 time= 0.17251
Epoch: 0046 train_loss= 0.81060 train_acc= 0.83824 val_loss= 0.77986 val_acc= 0.83767 time= 0.17103
Epoch: 0047 train_loss= 0.78778 train_acc= 0.84096 val_loss= 0.75887 val_acc= 0.83920 time= 0.19701
Epoch: 0048 train_loss= 0.76085 train_acc= 0.83960 val_loss= 0.73812 val_acc= 0.84227 time= 0.16796
Epoch: 0049 train_loss= 0.74264 train_acc= 0.84215 val_loss= 0.71776 val_acc= 0.84533 time= 0.17100
Epoch: 0050 train_loss= 0.71381 train_acc= 0.84793 val_loss= 0.69785 val_acc= 0.84992 time= 0.17303
Epoch: 0051 train_loss= 0.69652 train_acc= 0.84759 val_loss= 0.67859 val_acc= 0.85452 time= 0.17000
Epoch: 0052 train_loss= 0.67116 train_acc= 0.85797 val_loss= 0.65978 val_acc= 0.86064 time= 0.16997
Epoch: 0053 train_loss= 0.65676 train_acc= 0.86307 val_loss= 0.64120 val_acc= 0.86524 time= 0.17351
Epoch: 0054 train_loss= 0.63528 train_acc= 0.86562 val_loss= 0.62320 val_acc= 0.86830 time= 0.19503
Epoch: 0055 train_loss= 0.60993 train_acc= 0.86869 val_loss= 0.60548 val_acc= 0.86983 time= 0.18197
Epoch: 0056 train_loss= 0.59989 train_acc= 0.87328 val_loss= 0.58810 val_acc= 0.87136 time= 0.18554
Epoch: 0057 train_loss= 0.57880 train_acc= 0.87872 val_loss= 0.57141 val_acc= 0.87596 time= 0.17003
Epoch: 0058 train_loss= 0.55258 train_acc= 0.87804 val_loss= 0.55538 val_acc= 0.87749 time= 0.16999
Epoch: 0059 train_loss= 0.53890 train_acc= 0.88059 val_loss= 0.53988 val_acc= 0.87749 time= 0.18052
Epoch: 0060 train_loss= 0.52181 train_acc= 0.88314 val_loss= 0.52466 val_acc= 0.87902 time= 0.17301
Epoch: 0061 train_loss= 0.51071 train_acc= 0.88791 val_loss= 0.50984 val_acc= 0.88208 time= 0.19101
Epoch: 0062 train_loss= 0.49014 train_acc= 0.89386 val_loss= 0.49543 val_acc= 0.88361 time= 0.17156
Epoch: 0063 train_loss= 0.46675 train_acc= 0.90202 val_loss= 0.48146 val_acc= 0.88668 time= 0.16900
Epoch: 0064 train_loss= 0.46134 train_acc= 0.89539 val_loss= 0.46832 val_acc= 0.89587 time= 0.19540
Epoch: 0065 train_loss= 0.44168 train_acc= 0.90747 val_loss= 0.45578 val_acc= 0.89433 time= 0.16956
Epoch: 0066 train_loss= 0.42596 train_acc= 0.91342 val_loss= 0.44400 val_acc= 0.89740 time= 0.18000
Epoch: 0067 train_loss= 0.41243 train_acc= 0.91274 val_loss= 0.43263 val_acc= 0.89740 time= 0.17092
Epoch: 0068 train_loss= 0.40108 train_acc= 0.91359 val_loss= 0.42188 val_acc= 0.89740 time= 0.18000
Epoch: 0069 train_loss= 0.39311 train_acc= 0.92056 val_loss= 0.41158 val_acc= 0.89587 time= 0.18635
Epoch: 0070 train_loss= 0.37732 train_acc= 0.92159 val_loss= 0.40140 val_acc= 0.89740 time= 0.18000
Epoch: 0071 train_loss= 0.35881 train_acc= 0.92346 val_loss= 0.39165 val_acc= 0.89740 time= 0.17016
Epoch: 0072 train_loss= 0.35293 train_acc= 0.92312 val_loss= 0.38259 val_acc= 0.90199 time= 0.17084
Epoch: 0073 train_loss= 0.34143 train_acc= 0.92873 val_loss= 0.37379 val_acc= 0.90505 time= 0.17050
Epoch: 0074 train_loss= 0.33314 train_acc= 0.93026 val_loss= 0.36554 val_acc= 0.90812 time= 0.17104
Epoch: 0075 train_loss= 0.32221 train_acc= 0.93604 val_loss= 0.35785 val_acc= 0.90965 time= 0.17300
Epoch: 0076 train_loss= 0.31202 train_acc= 0.93825 val_loss= 0.35043 val_acc= 0.91118 time= 0.20000
Epoch: 0077 train_loss= 0.29836 train_acc= 0.94013 val_loss= 0.34368 val_acc= 0.91271 time= 0.17200
Epoch: 0078 train_loss= 0.28954 train_acc= 0.93962 val_loss= 0.33746 val_acc= 0.91424 time= 0.18700
Epoch: 0079 train_loss= 0.28292 train_acc= 0.94234 val_loss= 0.33208 val_acc= 0.91730 time= 0.17000
Epoch: 0080 train_loss= 0.27362 train_acc= 0.94421 val_loss= 0.32711 val_acc= 0.91730 time= 0.17203
Epoch: 0081 train_loss= 0.26588 train_acc= 0.94387 val_loss= 0.32310 val_acc= 0.92037 time= 0.18100
Epoch: 0082 train_loss= 0.26562 train_acc= 0.94489 val_loss= 0.31905 val_acc= 0.91884 time= 0.16700
Epoch: 0083 train_loss= 0.25114 train_acc= 0.94931 val_loss= 0.31497 val_acc= 0.92037 time= 0.18697
Epoch: 0084 train_loss= 0.24520 train_acc= 0.95118 val_loss= 0.31059 val_acc= 0.91884 time= 0.17100
Epoch: 0085 train_loss= 0.23972 train_acc= 0.95322 val_loss= 0.30483 val_acc= 0.92343 time= 0.16905
Epoch: 0086 train_loss= 0.23507 train_acc= 0.95629 val_loss= 0.29778 val_acc= 0.92496 time= 0.16900
Epoch: 0087 train_loss= 0.22667 train_acc= 0.95492 val_loss= 0.29228 val_acc= 0.92649 time= 0.19500
Epoch: 0088 train_loss= 0.21684 train_acc= 0.95407 val_loss= 0.28815 val_acc= 0.92649 time= 0.17000
Epoch: 0089 train_loss= 0.21051 train_acc= 0.95714 val_loss= 0.28528 val_acc= 0.92802 time= 0.16801
Epoch: 0090 train_loss= 0.20315 train_acc= 0.95901 val_loss= 0.28325 val_acc= 0.92802 time= 0.16700
Epoch: 0091 train_loss= 0.20376 train_acc= 0.95935 val_loss= 0.28181 val_acc= 0.92956 time= 0.17096
Epoch: 0092 train_loss= 0.19569 train_acc= 0.95952 val_loss= 0.28021 val_acc= 0.92649 time= 0.20000
Epoch: 0093 train_loss= 0.18419 train_acc= 0.96326 val_loss= 0.27833 val_acc= 0.92496 time= 0.16704
Epoch: 0094 train_loss= 0.18048 train_acc= 0.96496 val_loss= 0.27557 val_acc= 0.92496 time= 0.16800
Epoch: 0095 train_loss= 0.17983 train_acc= 0.96292 val_loss= 0.27157 val_acc= 0.92343 time= 0.18799
Epoch: 0096 train_loss= 0.17904 train_acc= 0.96207 val_loss= 0.26651 val_acc= 0.92802 time= 0.16902
Epoch: 0097 train_loss= 0.17355 train_acc= 0.96428 val_loss= 0.26280 val_acc= 0.92802 time= 0.16500
Epoch: 0098 train_loss= 0.16231 train_acc= 0.96853 val_loss= 0.25923 val_acc= 0.92956 time= 0.19796
Epoch: 0099 train_loss= 0.16520 train_acc= 0.96360 val_loss= 0.25659 val_acc= 0.93262 time= 0.17102
Epoch: 0100 train_loss= 0.15536 train_acc= 0.97142 val_loss= 0.25484 val_acc= 0.93262 time= 0.17503
Epoch: 0101 train_loss= 0.15218 train_acc= 0.96989 val_loss= 0.25391 val_acc= 0.93262 time= 0.16704
Epoch: 0102 train_loss= 0.14928 train_acc= 0.96955 val_loss= 0.25369 val_acc= 0.93415 time= 0.16700
Epoch: 0103 train_loss= 0.14076 train_acc= 0.96972 val_loss= 0.25361 val_acc= 0.93415 time= 0.16800
Epoch: 0104 train_loss= 0.13718 train_acc= 0.97381 val_loss= 0.25294 val_acc= 0.93415 time= 0.19399
Epoch: 0105 train_loss= 0.13852 train_acc= 0.97244 val_loss= 0.25213 val_acc= 0.93415 time= 0.16702
Epoch: 0106 train_loss= 0.13335 train_acc= 0.97261 val_loss= 0.25115 val_acc= 0.93262 time= 0.18793
Epoch: 0107 train_loss= 0.13020 train_acc= 0.97653 val_loss= 0.24850 val_acc= 0.93415 time= 0.17000
Epoch: 0108 train_loss= 0.13706 train_acc= 0.97500 val_loss= 0.24517 val_acc= 0.93415 time= 0.16804
Epoch: 0109 train_loss= 0.12221 train_acc= 0.97653 val_loss= 0.24200 val_acc= 0.93721 time= 0.16801
Epoch: 0110 train_loss= 0.12034 train_acc= 0.97772 val_loss= 0.24000 val_acc= 0.93568 time= 0.19699
Epoch: 0111 train_loss= 0.11431 train_acc= 0.97891 val_loss= 0.23890 val_acc= 0.93568 time= 0.16701
Epoch: 0112 train_loss= 0.11685 train_acc= 0.97619 val_loss= 0.23900 val_acc= 0.93568 time= 0.17800
Epoch: 0113 train_loss= 0.11105 train_acc= 0.98010 val_loss= 0.23968 val_acc= 0.93568 time= 0.16796
Epoch: 0114 train_loss= 0.10812 train_acc= 0.97925 val_loss= 0.24011 val_acc= 0.93568 time= 0.17061
Epoch: 0115 train_loss= 0.10359 train_acc= 0.98231 val_loss= 0.23937 val_acc= 0.93415 time= 0.17300
Epoch: 0116 train_loss= 0.10156 train_acc= 0.98265 val_loss= 0.23871 val_acc= 0.93568 time= 0.19105
Epoch: 0117 train_loss= 0.10165 train_acc= 0.98061 val_loss= 0.23851 val_acc= 0.93568 time= 0.16895
Epoch: 0118 train_loss= 0.09827 train_acc= 0.98180 val_loss= 0.23714 val_acc= 0.93415 time= 0.17443
Epoch: 0119 train_loss= 0.09717 train_acc= 0.98299 val_loss= 0.23568 val_acc= 0.93415 time= 0.16700
Epoch: 0120 train_loss= 0.09447 train_acc= 0.98265 val_loss= 0.23486 val_acc= 0.93415 time= 0.16700
Epoch: 0121 train_loss= 0.09463 train_acc= 0.98197 val_loss= 0.23422 val_acc= 0.93262 time= 0.17295
Epoch: 0122 train_loss= 0.08998 train_acc= 0.98367 val_loss= 0.23309 val_acc= 0.93568 time= 0.19951
Epoch: 0123 train_loss= 0.08718 train_acc= 0.98469 val_loss= 0.23244 val_acc= 0.93721 time= 0.18204
Epoch: 0124 train_loss= 0.08668 train_acc= 0.98367 val_loss= 0.23237 val_acc= 0.93415 time= 0.16701
Epoch: 0125 train_loss= 0.08373 train_acc= 0.98401 val_loss= 0.23347 val_acc= 0.93568 time= 0.17000
Epoch: 0126 train_loss= 0.08184 train_acc= 0.98588 val_loss= 0.23491 val_acc= 0.93568 time= 0.16842
Epoch: 0127 train_loss= 0.08370 train_acc= 0.98690 val_loss= 0.23540 val_acc= 0.93721 time= 0.18400
Early stopping...
Optimization Finished!
Test set results: cost= 0.26209 accuracy= 0.93692 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.7500    0.5000    0.6000         6
           2     0.5000    1.0000    0.6667         1
           3     0.8022    0.9733    0.8795        75
           4     1.0000    1.0000    1.0000         9
           5     0.7714    0.9310    0.8438        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.4444    0.6154         9
          10     0.9231    0.6667    0.7742        36
          11     1.0000    0.9167    0.9565        12
          12     0.8108    0.9917    0.8922       121
          13     0.9286    0.6842    0.7879        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7647    0.7647    0.7647        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9712    0.9684    0.9698       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8630    0.7778    0.8182        81
          36     1.0000    0.3333    0.5000        12
          37     0.8000    1.0000    0.8889         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9000    0.7500    0.8182        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9369      2568
   macro avg     0.7648    0.6772    0.6917      2568
weighted avg     0.9371    0.9369    0.9319      2568

Macro average Test Precision, Recall and F1-Score...
(0.7648480022884748, 0.677184735687441, 0.6917090348421617, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[ 0.00808704 -0.1310455  -0.10841196 ...  1.4867307  -0.01624903
   1.4654288 ]
 [ 0.11345708 -0.09004055  0.4068952  ...  0.5388367   0.6214626
   0.82317835]
 [ 0.31334022 -0.1041109   0.09132799 ...  0.78535724  0.5610662
   0.623939  ]
 ...
 [ 0.0491931  -0.05478314  0.12311009 ...  0.4180518   0.17748228
   0.0979668 ]
 [ 0.16330034 -0.05952455  0.07260004 ...  0.3985526   0.19144447
   0.15881486]
 [ 0.4193666  -0.0925929   0.26341072 ...  0.20230797  0.3442738
   0.4112997 ]]

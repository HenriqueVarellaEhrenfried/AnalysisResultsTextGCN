(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95126 train_acc= 0.01973 val_loss= 3.89947 val_acc= 0.65237 time= 0.44293
Epoch: 0002 train_loss= 3.89895 train_acc= 0.63038 val_loss= 3.80040 val_acc= 0.64931 time= 0.17798
Epoch: 0003 train_loss= 3.79996 train_acc= 0.62341 val_loss= 3.64722 val_acc= 0.64319 time= 0.17300
Epoch: 0004 train_loss= 3.65078 train_acc= 0.62290 val_loss= 3.44000 val_acc= 0.63553 time= 0.17300
Epoch: 0005 train_loss= 3.44189 train_acc= 0.61864 val_loss= 3.19038 val_acc= 0.62940 time= 0.17000
Epoch: 0006 train_loss= 3.18049 train_acc= 0.61065 val_loss= 2.92253 val_acc= 0.62328 time= 0.19203
Epoch: 0007 train_loss= 2.92485 train_acc= 0.61337 val_loss= 2.66618 val_acc= 0.61868 time= 0.17300
Epoch: 0008 train_loss= 2.67317 train_acc= 0.61065 val_loss= 2.45555 val_acc= 0.61409 time= 0.16559
Epoch: 0009 train_loss= 2.46425 train_acc= 0.59840 val_loss= 2.31617 val_acc= 0.62940 time= 0.16699
Epoch: 0010 train_loss= 2.30215 train_acc= 0.61592 val_loss= 2.23748 val_acc= 0.67075 time= 0.17197
Epoch: 0011 train_loss= 2.23606 train_acc= 0.64467 val_loss= 2.18687 val_acc= 0.55436 time= 0.17700
Epoch: 0012 train_loss= 2.19007 train_acc= 0.54873 val_loss= 2.13979 val_acc= 0.47473 time= 0.17151
Epoch: 0013 train_loss= 2.15520 train_acc= 0.44736 val_loss= 2.08118 val_acc= 0.46401 time= 0.16931
Epoch: 0014 train_loss= 2.10600 train_acc= 0.43630 val_loss= 2.00524 val_acc= 0.46401 time= 0.16799
Epoch: 0015 train_loss= 2.02410 train_acc= 0.43732 val_loss= 1.91556 val_acc= 0.47320 time= 0.16701
Epoch: 0016 train_loss= 1.94225 train_acc= 0.44770 val_loss= 1.82345 val_acc= 0.50689 time= 0.19600
Epoch: 0017 train_loss= 1.85058 train_acc= 0.48903 val_loss= 1.74124 val_acc= 0.58652 time= 0.16800
Epoch: 0018 train_loss= 1.76643 train_acc= 0.58037 val_loss= 1.67408 val_acc= 0.65544 time= 0.16999
Epoch: 0019 train_loss= 1.70194 train_acc= 0.63633 val_loss= 1.61841 val_acc= 0.67534 time= 0.18800
Epoch: 0020 train_loss= 1.64687 train_acc= 0.65079 val_loss= 1.56682 val_acc= 0.68147 time= 0.16900
Epoch: 0021 train_loss= 1.59378 train_acc= 0.65862 val_loss= 1.51460 val_acc= 0.67841 time= 0.16814
Epoch: 0022 train_loss= 1.54017 train_acc= 0.66219 val_loss= 1.46159 val_acc= 0.68147 time= 0.19601
Epoch: 0023 train_loss= 1.47709 train_acc= 0.66967 val_loss= 1.40975 val_acc= 0.67994 time= 0.16600
Epoch: 0024 train_loss= 1.43204 train_acc= 0.66780 val_loss= 1.36112 val_acc= 0.69372 time= 0.18600
Epoch: 0025 train_loss= 1.38955 train_acc= 0.67733 val_loss= 1.31635 val_acc= 0.70138 time= 0.16801
Epoch: 0026 train_loss= 1.34299 train_acc= 0.68838 val_loss= 1.27581 val_acc= 0.71516 time= 0.17095
Epoch: 0027 train_loss= 1.30234 train_acc= 0.69927 val_loss= 1.23934 val_acc= 0.71975 time= 0.17087
Epoch: 0028 train_loss= 1.26700 train_acc= 0.70641 val_loss= 1.20635 val_acc= 0.72741 time= 0.20203
Epoch: 0029 train_loss= 1.22995 train_acc= 0.72189 val_loss= 1.17579 val_acc= 0.72741 time= 0.16801
Epoch: 0030 train_loss= 1.19462 train_acc= 0.73737 val_loss= 1.14658 val_acc= 0.73201 time= 0.16699
Epoch: 0031 train_loss= 1.16131 train_acc= 0.74281 val_loss= 1.11796 val_acc= 0.73966 time= 0.16700
Epoch: 0032 train_loss= 1.13489 train_acc= 0.74741 val_loss= 1.08945 val_acc= 0.74273 time= 0.16803
Epoch: 0033 train_loss= 1.10270 train_acc= 0.75149 val_loss= 1.06109 val_acc= 0.75038 time= 0.16900
Epoch: 0034 train_loss= 1.07702 train_acc= 0.75778 val_loss= 1.03324 val_acc= 0.75804 time= 0.18897
Epoch: 0035 train_loss= 1.04983 train_acc= 0.76084 val_loss= 1.00636 val_acc= 0.76876 time= 0.17090
Epoch: 0036 train_loss= 1.01797 train_acc= 0.77394 val_loss= 0.98038 val_acc= 0.77795 time= 0.18504
Epoch: 0037 train_loss= 0.98782 train_acc= 0.77734 val_loss= 0.95528 val_acc= 0.78254 time= 0.16801
Epoch: 0038 train_loss= 0.96620 train_acc= 0.78840 val_loss= 0.93086 val_acc= 0.79632 time= 0.16736
Epoch: 0039 train_loss= 0.94040 train_acc= 0.79912 val_loss= 0.90692 val_acc= 0.80551 time= 0.17000
Epoch: 0040 train_loss= 0.91843 train_acc= 0.80592 val_loss= 0.88341 val_acc= 0.81164 time= 0.19297
Epoch: 0041 train_loss= 0.89441 train_acc= 0.81442 val_loss= 0.86033 val_acc= 0.81470 time= 0.17203
Epoch: 0042 train_loss= 0.86867 train_acc= 0.82089 val_loss= 0.83774 val_acc= 0.83308 time= 0.17278
Epoch: 0043 train_loss= 0.85001 train_acc= 0.82531 val_loss= 0.81572 val_acc= 0.83767 time= 0.17100
Epoch: 0044 train_loss= 0.82998 train_acc= 0.83654 val_loss= 0.79422 val_acc= 0.83920 time= 0.16600
Epoch: 0045 train_loss= 0.80537 train_acc= 0.83569 val_loss= 0.77287 val_acc= 0.84074 time= 0.17000
Epoch: 0046 train_loss= 0.78201 train_acc= 0.84113 val_loss= 0.75171 val_acc= 0.83767 time= 0.19297
Epoch: 0047 train_loss= 0.75280 train_acc= 0.84487 val_loss= 0.73086 val_acc= 0.84074 time= 0.18303
Epoch: 0048 train_loss= 0.73276 train_acc= 0.84691 val_loss= 0.71032 val_acc= 0.84380 time= 0.16625
Epoch: 0049 train_loss= 0.71497 train_acc= 0.84861 val_loss= 0.69039 val_acc= 0.84839 time= 0.17098
Epoch: 0050 train_loss= 0.69665 train_acc= 0.85151 val_loss= 0.67138 val_acc= 0.85145 time= 0.17100
Epoch: 0051 train_loss= 0.66960 train_acc= 0.85797 val_loss= 0.65307 val_acc= 0.85452 time= 0.19600
Epoch: 0052 train_loss= 0.65118 train_acc= 0.85984 val_loss= 0.63520 val_acc= 0.86064 time= 0.16797
Epoch: 0053 train_loss= 0.63653 train_acc= 0.86239 val_loss= 0.61799 val_acc= 0.86371 time= 0.17003
Epoch: 0054 train_loss= 0.61116 train_acc= 0.86613 val_loss= 0.60175 val_acc= 0.86524 time= 0.16900
Epoch: 0055 train_loss= 0.59577 train_acc= 0.87209 val_loss= 0.58617 val_acc= 0.86371 time= 0.16900
Epoch: 0056 train_loss= 0.58131 train_acc= 0.87073 val_loss= 0.57138 val_acc= 0.86830 time= 0.17000
Epoch: 0057 train_loss= 0.56670 train_acc= 0.87345 val_loss= 0.55676 val_acc= 0.87289 time= 0.19500
Epoch: 0058 train_loss= 0.54334 train_acc= 0.87651 val_loss= 0.54260 val_acc= 0.87749 time= 0.16946
Epoch: 0059 train_loss= 0.53370 train_acc= 0.88212 val_loss= 0.52859 val_acc= 0.87902 time= 0.18897
Epoch: 0060 train_loss= 0.51257 train_acc= 0.88672 val_loss= 0.51511 val_acc= 0.88055 time= 0.16703
Epoch: 0061 train_loss= 0.49853 train_acc= 0.88689 val_loss= 0.50184 val_acc= 0.88208 time= 0.16621
Epoch: 0062 train_loss= 0.48079 train_acc= 0.89114 val_loss= 0.48928 val_acc= 0.88515 time= 0.19600
Epoch: 0063 train_loss= 0.46791 train_acc= 0.89437 val_loss= 0.47712 val_acc= 0.88668 time= 0.16700
Epoch: 0064 train_loss= 0.45610 train_acc= 0.89658 val_loss= 0.46501 val_acc= 0.88821 time= 0.18300
Epoch: 0065 train_loss= 0.44476 train_acc= 0.90287 val_loss= 0.45263 val_acc= 0.89280 time= 0.17176
Epoch: 0066 train_loss= 0.42921 train_acc= 0.90287 val_loss= 0.44133 val_acc= 0.89433 time= 0.17163
Epoch: 0067 train_loss= 0.41536 train_acc= 0.90934 val_loss= 0.43117 val_acc= 0.89433 time= 0.16797
Epoch: 0068 train_loss= 0.40302 train_acc= 0.91325 val_loss= 0.42171 val_acc= 0.89740 time= 0.19504
Epoch: 0069 train_loss= 0.39237 train_acc= 0.91393 val_loss= 0.41272 val_acc= 0.89893 time= 0.16800
Epoch: 0070 train_loss= 0.37855 train_acc= 0.91767 val_loss= 0.40409 val_acc= 0.89893 time= 0.16933
Epoch: 0071 train_loss= 0.36771 train_acc= 0.92210 val_loss= 0.39611 val_acc= 0.89893 time= 0.17132
Epoch: 0072 train_loss= 0.35680 train_acc= 0.92482 val_loss= 0.38911 val_acc= 0.90352 time= 0.16896
Epoch: 0073 train_loss= 0.35300 train_acc= 0.92635 val_loss= 0.38282 val_acc= 0.90352 time= 0.19759
Epoch: 0074 train_loss= 0.34020 train_acc= 0.92550 val_loss= 0.37670 val_acc= 0.90352 time= 0.16900
Epoch: 0075 train_loss= 0.33120 train_acc= 0.92992 val_loss= 0.37009 val_acc= 0.90199 time= 0.16699
Epoch: 0076 train_loss= 0.31953 train_acc= 0.93604 val_loss= 0.36315 val_acc= 0.90505 time= 0.18499
Epoch: 0077 train_loss= 0.30858 train_acc= 0.93281 val_loss= 0.35604 val_acc= 0.90812 time= 0.16932
Epoch: 0078 train_loss= 0.30225 train_acc= 0.93672 val_loss= 0.34965 val_acc= 0.90812 time= 0.16701
Epoch: 0079 train_loss= 0.29611 train_acc= 0.93945 val_loss= 0.34356 val_acc= 0.90965 time= 0.17000
Epoch: 0080 train_loss= 0.28643 train_acc= 0.93962 val_loss= 0.33739 val_acc= 0.91424 time= 0.19563
Epoch: 0081 train_loss= 0.27637 train_acc= 0.94387 val_loss= 0.33146 val_acc= 0.91884 time= 0.18606
Epoch: 0082 train_loss= 0.26578 train_acc= 0.94642 val_loss= 0.32622 val_acc= 0.91730 time= 0.16836
Epoch: 0083 train_loss= 0.26820 train_acc= 0.94642 val_loss= 0.32140 val_acc= 0.91884 time= 0.16900
Epoch: 0084 train_loss= 0.25764 train_acc= 0.94829 val_loss= 0.31631 val_acc= 0.92037 time= 0.16797
Epoch: 0085 train_loss= 0.25159 train_acc= 0.94710 val_loss= 0.31128 val_acc= 0.92343 time= 0.19803
Epoch: 0086 train_loss= 0.24432 train_acc= 0.94965 val_loss= 0.30581 val_acc= 0.92190 time= 0.16915
Epoch: 0087 train_loss= 0.24051 train_acc= 0.94948 val_loss= 0.30191 val_acc= 0.92496 time= 0.16800
Epoch: 0088 train_loss= 0.23073 train_acc= 0.95373 val_loss= 0.29873 val_acc= 0.92496 time= 0.17217
Epoch: 0089 train_loss= 0.22912 train_acc= 0.95118 val_loss= 0.29535 val_acc= 0.92343 time= 0.17314
Epoch: 0090 train_loss= 0.22242 train_acc= 0.95373 val_loss= 0.29258 val_acc= 0.92190 time= 0.17300
Epoch: 0091 train_loss= 0.21398 train_acc= 0.95646 val_loss= 0.29007 val_acc= 0.92649 time= 0.19803
Epoch: 0092 train_loss= 0.20403 train_acc= 0.95799 val_loss= 0.28727 val_acc= 0.92343 time= 0.16897
Epoch: 0093 train_loss= 0.20217 train_acc= 0.95918 val_loss= 0.28328 val_acc= 0.92496 time= 0.18003
Epoch: 0094 train_loss= 0.19907 train_acc= 0.96003 val_loss= 0.27934 val_acc= 0.92496 time= 0.16900
Epoch: 0095 train_loss= 0.19279 train_acc= 0.96122 val_loss= 0.27587 val_acc= 0.92802 time= 0.17297
Epoch: 0096 train_loss= 0.17915 train_acc= 0.96530 val_loss= 0.27245 val_acc= 0.92802 time= 0.17300
Epoch: 0097 train_loss= 0.17904 train_acc= 0.96173 val_loss= 0.26982 val_acc= 0.92956 time= 0.20000
Epoch: 0098 train_loss= 0.17649 train_acc= 0.96377 val_loss= 0.26729 val_acc= 0.92956 time= 0.18400
Epoch: 0099 train_loss= 0.16998 train_acc= 0.96496 val_loss= 0.26422 val_acc= 0.92956 time= 0.17200
Epoch: 0100 train_loss= 0.16394 train_acc= 0.96445 val_loss= 0.26207 val_acc= 0.93109 time= 0.16803
Epoch: 0101 train_loss= 0.16351 train_acc= 0.96853 val_loss= 0.26053 val_acc= 0.93109 time= 0.16897
Epoch: 0102 train_loss= 0.15545 train_acc= 0.96802 val_loss= 0.25905 val_acc= 0.93262 time= 0.19500
Epoch: 0103 train_loss= 0.15801 train_acc= 0.96836 val_loss= 0.25709 val_acc= 0.93415 time= 0.16900
Epoch: 0104 train_loss= 0.14784 train_acc= 0.97125 val_loss= 0.25493 val_acc= 0.93568 time= 0.17058
Epoch: 0105 train_loss= 0.14669 train_acc= 0.97278 val_loss= 0.25258 val_acc= 0.93568 time= 0.16900
Epoch: 0106 train_loss= 0.14468 train_acc= 0.97006 val_loss= 0.25053 val_acc= 0.93568 time= 0.16703
Epoch: 0107 train_loss= 0.13808 train_acc= 0.97227 val_loss= 0.24943 val_acc= 0.93415 time= 0.19600
Epoch: 0108 train_loss= 0.13176 train_acc= 0.97500 val_loss= 0.24812 val_acc= 0.93568 time= 0.16900
Epoch: 0109 train_loss= 0.13088 train_acc= 0.97398 val_loss= 0.24729 val_acc= 0.93415 time= 0.16797
Epoch: 0110 train_loss= 0.12796 train_acc= 0.97483 val_loss= 0.24606 val_acc= 0.93568 time= 0.18800
Epoch: 0111 train_loss= 0.12676 train_acc= 0.97381 val_loss= 0.24437 val_acc= 0.93109 time= 0.17100
Epoch: 0112 train_loss= 0.12047 train_acc= 0.97602 val_loss= 0.24328 val_acc= 0.93568 time= 0.17018
Epoch: 0113 train_loss= 0.11577 train_acc= 0.97806 val_loss= 0.24246 val_acc= 0.93568 time= 0.19903
Epoch: 0114 train_loss= 0.11385 train_acc= 0.97959 val_loss= 0.24200 val_acc= 0.93415 time= 0.16797
Epoch: 0115 train_loss= 0.11450 train_acc= 0.97959 val_loss= 0.24081 val_acc= 0.93415 time= 0.18800
Epoch: 0116 train_loss= 0.10970 train_acc= 0.97908 val_loss= 0.24002 val_acc= 0.93415 time= 0.16803
Epoch: 0117 train_loss= 0.10676 train_acc= 0.98061 val_loss= 0.23892 val_acc= 0.93415 time= 0.16801
Epoch: 0118 train_loss= 0.10053 train_acc= 0.98197 val_loss= 0.23926 val_acc= 0.93721 time= 0.16797
Epoch: 0119 train_loss= 0.10368 train_acc= 0.98163 val_loss= 0.23856 val_acc= 0.93415 time= 0.17500
Epoch: 0120 train_loss= 0.09917 train_acc= 0.98112 val_loss= 0.23709 val_acc= 0.93568 time= 0.17003
Epoch: 0121 train_loss= 0.09772 train_acc= 0.98231 val_loss= 0.23604 val_acc= 0.93721 time= 0.18497
Epoch: 0122 train_loss= 0.09470 train_acc= 0.98418 val_loss= 0.23572 val_acc= 0.93568 time= 0.16803
Epoch: 0123 train_loss= 0.09424 train_acc= 0.98214 val_loss= 0.23616 val_acc= 0.93415 time= 0.16997
Epoch: 0124 train_loss= 0.09122 train_acc= 0.98350 val_loss= 0.23626 val_acc= 0.93721 time= 0.16800
Epoch: 0125 train_loss= 0.08840 train_acc= 0.98622 val_loss= 0.23492 val_acc= 0.93874 time= 0.19903
Epoch: 0126 train_loss= 0.08522 train_acc= 0.98673 val_loss= 0.23329 val_acc= 0.94181 time= 0.17097
Epoch: 0127 train_loss= 0.08155 train_acc= 0.98656 val_loss= 0.23160 val_acc= 0.94028 time= 0.18203
Epoch: 0128 train_loss= 0.08221 train_acc= 0.98605 val_loss= 0.22998 val_acc= 0.94028 time= 0.16797
Epoch: 0129 train_loss= 0.07815 train_acc= 0.98724 val_loss= 0.22872 val_acc= 0.93874 time= 0.16800
Epoch: 0130 train_loss= 0.07904 train_acc= 0.98724 val_loss= 0.22831 val_acc= 0.93721 time= 0.19300
Epoch: 0131 train_loss= 0.07813 train_acc= 0.98724 val_loss= 0.22763 val_acc= 0.93874 time= 0.16900
Epoch: 0132 train_loss= 0.07666 train_acc= 0.98588 val_loss= 0.22753 val_acc= 0.93874 time= 0.17100
Epoch: 0133 train_loss= 0.07361 train_acc= 0.98605 val_loss= 0.22803 val_acc= 0.93874 time= 0.17300
Epoch: 0134 train_loss= 0.07236 train_acc= 0.98690 val_loss= 0.22975 val_acc= 0.94028 time= 0.17000
Epoch: 0135 train_loss= 0.07275 train_acc= 0.98928 val_loss= 0.23124 val_acc= 0.93568 time= 0.16900
Early stopping...
Optimization Finished!
Test set results: cost= 0.25615 accuracy= 0.93731 time= 0.07360
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7935    0.9733    0.8743        75
           4     1.0000    1.0000    1.0000         9
           5     0.8100    0.9310    0.8663        87
           6     0.9200    0.9200    0.9200        25
           7     0.7059    0.9231    0.8000        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.3333    0.5000         9
          10     0.9032    0.7778    0.8358        36
          11     1.0000    0.9167    0.9565        12
          12     0.8623    0.9835    0.9189       121
          13     1.0000    0.6842    0.8125        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.7273    0.8421        11
          29     0.9654    0.9626    0.9640       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8333    0.8025    0.8176        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9764    0.9917    0.9840      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8000    0.6667    0.7273        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7580    0.6775    0.6965      2568
weighted avg     0.9344    0.9373    0.9318      2568

Macro average Test Precision, Recall and F1-Score...
(0.7579681114764214, 0.6774823124982189, 0.6965314868543213, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[-0.05874934  0.98344713  1.1988055  ... -0.15467937 -0.10698316
   0.9336059 ]
 [ 0.4830664   0.4240668   0.6788122  ...  0.03500346  0.05048714
   0.03154023]
 [ 0.49000558  0.28077757  0.29434514 ...  0.01603498 -0.02666538
   0.91458267]
 ...
 [ 0.42069024  0.33348176  0.15923528 ...  0.01623415  0.02452886
   0.0827675 ]
 [ 0.12774369  0.17593116  0.22205509 ...  0.05464002  0.04225086
   0.3153685 ]
 [ 0.44746286  0.16300145  0.19488737 ...  0.27541628  0.25909573
   0.3234962 ]]

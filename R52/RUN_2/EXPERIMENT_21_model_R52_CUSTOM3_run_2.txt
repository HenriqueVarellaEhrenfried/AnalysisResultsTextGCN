(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95124 train_acc= 0.01293 val_loss= 3.90918 val_acc= 0.63706 time= 0.44217
Epoch: 0002 train_loss= 3.90822 train_acc= 0.60708 val_loss= 3.81824 val_acc= 0.62175 time= 0.17400
Epoch: 0003 train_loss= 3.82216 train_acc= 0.61218 val_loss= 3.67866 val_acc= 0.60796 time= 0.18200
Epoch: 0004 train_loss= 3.68638 train_acc= 0.59432 val_loss= 3.49136 val_acc= 0.60184 time= 0.16634
Epoch: 0005 train_loss= 3.48641 train_acc= 0.59466 val_loss= 3.26529 val_acc= 0.59112 time= 0.16979
Epoch: 0006 train_loss= 3.28251 train_acc= 0.57935 val_loss= 3.01965 val_acc= 0.58652 time= 0.17262
Epoch: 0007 train_loss= 3.03266 train_acc= 0.57969 val_loss= 2.78137 val_acc= 0.58193 time= 0.19600
Epoch: 0008 train_loss= 2.78945 train_acc= 0.56200 val_loss= 2.57436 val_acc= 0.57580 time= 0.17499
Epoch: 0009 train_loss= 2.56768 train_acc= 0.56574 val_loss= 2.42243 val_acc= 0.58193 time= 0.16800
Epoch: 0010 train_loss= 2.44737 train_acc= 0.56234 val_loss= 2.33076 val_acc= 0.60184 time= 0.16802
Epoch: 0011 train_loss= 2.32295 train_acc= 0.59330 val_loss= 2.27701 val_acc= 0.67075 time= 0.16799
Epoch: 0012 train_loss= 2.28585 train_acc= 0.58496 val_loss= 2.23549 val_acc= 0.47320 time= 0.17700
Epoch: 0013 train_loss= 2.24623 train_acc= 0.49294 val_loss= 2.19126 val_acc= 0.45636 time= 0.16899
Epoch: 0014 train_loss= 2.21836 train_acc= 0.43868 val_loss= 2.13761 val_acc= 0.45636 time= 0.17000
Epoch: 0015 train_loss= 2.15590 train_acc= 0.43647 val_loss= 2.06974 val_acc= 0.45636 time= 0.16700
Epoch: 0016 train_loss= 2.10690 train_acc= 0.43443 val_loss= 1.99094 val_acc= 0.45636 time= 0.16609
Epoch: 0017 train_loss= 2.03237 train_acc= 0.43834 val_loss= 1.90859 val_acc= 0.45942 time= 0.17000
Epoch: 0018 train_loss= 1.94786 train_acc= 0.45008 val_loss= 1.83112 val_acc= 0.48851 time= 0.19200
Epoch: 0019 train_loss= 1.84881 train_acc= 0.49600 val_loss= 1.76498 val_acc= 0.57887 time= 0.16901
Epoch: 0020 train_loss= 1.76663 train_acc= 0.56421 val_loss= 1.71044 val_acc= 0.65391 time= 0.18199
Epoch: 0021 train_loss= 1.75254 train_acc= 0.61609 val_loss= 1.66153 val_acc= 0.67228 time= 0.16895
Epoch: 0022 train_loss= 1.68998 train_acc= 0.62681 val_loss= 1.61315 val_acc= 0.67534 time= 0.16800
Epoch: 0023 train_loss= 1.64145 train_acc= 0.63599 val_loss= 1.56419 val_acc= 0.66462 time= 0.16900
Epoch: 0024 train_loss= 1.57018 train_acc= 0.64280 val_loss= 1.51611 val_acc= 0.66616 time= 0.19200
Epoch: 0025 train_loss= 1.57121 train_acc= 0.63939 val_loss= 1.46907 val_acc= 0.66616 time= 0.17200
Epoch: 0026 train_loss= 1.48814 train_acc= 0.64977 val_loss= 1.42522 val_acc= 0.66922 time= 0.17106
Epoch: 0027 train_loss= 1.48878 train_acc= 0.64841 val_loss= 1.38450 val_acc= 0.68147 time= 0.17397
Epoch: 0028 train_loss= 1.45220 train_acc= 0.65028 val_loss= 1.34732 val_acc= 0.68913 time= 0.17400
Epoch: 0029 train_loss= 1.37985 train_acc= 0.67188 val_loss= 1.31390 val_acc= 0.69372 time= 0.20700
Epoch: 0030 train_loss= 1.36710 train_acc= 0.67001 val_loss= 1.28341 val_acc= 0.70291 time= 0.17503
Epoch: 0031 train_loss= 1.31984 train_acc= 0.68107 val_loss= 1.25529 val_acc= 0.71210 time= 0.17500
Epoch: 0032 train_loss= 1.29957 train_acc= 0.69417 val_loss= 1.22859 val_acc= 0.71822 time= 0.16900
Epoch: 0033 train_loss= 1.27721 train_acc= 0.69740 val_loss= 1.20289 val_acc= 0.72282 time= 0.16701
Epoch: 0034 train_loss= 1.23712 train_acc= 0.70590 val_loss= 1.17777 val_acc= 0.72435 time= 0.16696
Epoch: 0035 train_loss= 1.21040 train_acc= 0.71509 val_loss= 1.15281 val_acc= 0.72741 time= 0.20200
Epoch: 0036 train_loss= 1.18321 train_acc= 0.72546 val_loss= 1.12826 val_acc= 0.73354 time= 0.17485
Epoch: 0037 train_loss= 1.16944 train_acc= 0.72716 val_loss= 1.10471 val_acc= 0.73966 time= 0.18900
Epoch: 0038 train_loss= 1.16100 train_acc= 0.73499 val_loss= 1.08233 val_acc= 0.74119 time= 0.17103
Epoch: 0039 train_loss= 1.13760 train_acc= 0.73057 val_loss= 1.06088 val_acc= 0.74426 time= 0.17300
Epoch: 0040 train_loss= 1.10151 train_acc= 0.73584 val_loss= 1.03995 val_acc= 0.74885 time= 0.17400
Epoch: 0041 train_loss= 1.07937 train_acc= 0.74724 val_loss= 1.01945 val_acc= 0.75804 time= 0.19400
Epoch: 0042 train_loss= 1.04807 train_acc= 0.76118 val_loss= 0.99909 val_acc= 0.76417 time= 0.17800
Epoch: 0043 train_loss= 1.03642 train_acc= 0.75727 val_loss= 0.97880 val_acc= 0.77182 time= 0.17026
Epoch: 0044 train_loss= 1.02403 train_acc= 0.76425 val_loss= 0.95879 val_acc= 0.77642 time= 0.17000
Epoch: 0045 train_loss= 1.00554 train_acc= 0.76867 val_loss= 0.93911 val_acc= 0.79173 time= 0.16803
Epoch: 0046 train_loss= 0.98141 train_acc= 0.77564 val_loss= 0.91980 val_acc= 0.79632 time= 0.19500
Epoch: 0047 train_loss= 0.95765 train_acc= 0.78364 val_loss= 0.90098 val_acc= 0.80245 time= 0.16800
Epoch: 0048 train_loss= 0.94367 train_acc= 0.79741 val_loss= 0.88254 val_acc= 0.81164 time= 0.16600
Epoch: 0049 train_loss= 0.93476 train_acc= 0.79673 val_loss= 0.86425 val_acc= 0.81776 time= 0.16699
Epoch: 0050 train_loss= 0.91599 train_acc= 0.79775 val_loss= 0.84594 val_acc= 0.81930 time= 0.16700
Epoch: 0051 train_loss= 0.89098 train_acc= 0.80609 val_loss= 0.82783 val_acc= 0.82083 time= 0.18597
Epoch: 0052 train_loss= 0.88236 train_acc= 0.80354 val_loss= 0.80990 val_acc= 0.82389 time= 0.17100
Epoch: 0053 train_loss= 0.84714 train_acc= 0.81170 val_loss= 0.79237 val_acc= 0.82695 time= 0.16803
Epoch: 0054 train_loss= 0.83249 train_acc= 0.82276 val_loss= 0.77468 val_acc= 0.82848 time= 0.18400
Epoch: 0055 train_loss= 0.82012 train_acc= 0.81579 val_loss= 0.75713 val_acc= 0.83002 time= 0.16800
Epoch: 0056 train_loss= 0.80316 train_acc= 0.82157 val_loss= 0.74021 val_acc= 0.83308 time= 0.16800
Epoch: 0057 train_loss= 0.78145 train_acc= 0.82242 val_loss= 0.72372 val_acc= 0.83614 time= 0.17000
Epoch: 0058 train_loss= 0.78937 train_acc= 0.82089 val_loss= 0.70769 val_acc= 0.83614 time= 0.19296
Epoch: 0059 train_loss= 0.75211 train_acc= 0.82854 val_loss= 0.69211 val_acc= 0.83767 time= 0.17400
Epoch: 0060 train_loss= 0.70985 train_acc= 0.83841 val_loss= 0.67689 val_acc= 0.84227 time= 0.17500
Epoch: 0061 train_loss= 0.73361 train_acc= 0.83501 val_loss= 0.66217 val_acc= 0.85145 time= 0.16704
Epoch: 0062 train_loss= 0.71826 train_acc= 0.83399 val_loss= 0.64822 val_acc= 0.85758 time= 0.16699
Epoch: 0063 train_loss= 0.71085 train_acc= 0.83041 val_loss= 0.63498 val_acc= 0.86524 time= 0.16900
Epoch: 0064 train_loss= 0.67509 train_acc= 0.84266 val_loss= 0.62233 val_acc= 0.86677 time= 0.19201
Epoch: 0065 train_loss= 0.67643 train_acc= 0.83722 val_loss= 0.60986 val_acc= 0.86677 time= 0.17799
Epoch: 0066 train_loss= 0.65440 train_acc= 0.84402 val_loss= 0.59797 val_acc= 0.87136 time= 0.16880
Epoch: 0067 train_loss= 0.65333 train_acc= 0.84572 val_loss= 0.58612 val_acc= 0.86677 time= 0.17000
Epoch: 0068 train_loss= 0.62373 train_acc= 0.84453 val_loss= 0.57446 val_acc= 0.86983 time= 0.16864
Epoch: 0069 train_loss= 0.60324 train_acc= 0.85406 val_loss= 0.56378 val_acc= 0.87289 time= 0.19418
Epoch: 0070 train_loss= 0.61536 train_acc= 0.85661 val_loss= 0.55327 val_acc= 0.87596 time= 0.16700
Epoch: 0071 train_loss= 0.61150 train_acc= 0.85525 val_loss= 0.54212 val_acc= 0.87749 time= 0.16800
Epoch: 0072 train_loss= 0.57726 train_acc= 0.85729 val_loss= 0.53069 val_acc= 0.88361 time= 0.16700
Epoch: 0073 train_loss= 0.57986 train_acc= 0.85576 val_loss= 0.51947 val_acc= 0.88515 time= 0.16698
Epoch: 0074 train_loss= 0.56309 train_acc= 0.87481 val_loss= 0.50937 val_acc= 0.88515 time= 0.17204
Epoch: 0075 train_loss= 0.54091 train_acc= 0.87056 val_loss= 0.50026 val_acc= 0.88668 time= 0.19720
Epoch: 0076 train_loss= 0.53380 train_acc= 0.87039 val_loss= 0.49188 val_acc= 0.88668 time= 0.16900
Epoch: 0077 train_loss= 0.56531 train_acc= 0.86358 val_loss= 0.48411 val_acc= 0.88974 time= 0.18200
Epoch: 0078 train_loss= 0.51313 train_acc= 0.87243 val_loss= 0.47645 val_acc= 0.89280 time= 0.16700
Epoch: 0079 train_loss= 0.50606 train_acc= 0.88076 val_loss= 0.46901 val_acc= 0.89433 time= 0.16800
Epoch: 0080 train_loss= 0.50647 train_acc= 0.87736 val_loss= 0.46140 val_acc= 0.89587 time= 0.16800
Epoch: 0081 train_loss= 0.47682 train_acc= 0.88399 val_loss= 0.45411 val_acc= 0.89587 time= 0.19397
Epoch: 0082 train_loss= 0.47771 train_acc= 0.88621 val_loss= 0.44625 val_acc= 0.89433 time= 0.17406
Epoch: 0083 train_loss= 0.47193 train_acc= 0.88706 val_loss= 0.43866 val_acc= 0.89587 time= 0.16900
Epoch: 0084 train_loss= 0.45870 train_acc= 0.88280 val_loss= 0.43063 val_acc= 0.89587 time= 0.16700
Epoch: 0085 train_loss= 0.46256 train_acc= 0.88331 val_loss= 0.42324 val_acc= 0.89587 time= 0.17000
Epoch: 0086 train_loss= 0.46699 train_acc= 0.88535 val_loss= 0.41619 val_acc= 0.89587 time= 0.19801
Epoch: 0087 train_loss= 0.44022 train_acc= 0.88825 val_loss= 0.40977 val_acc= 0.89740 time= 0.16700
Epoch: 0088 train_loss= 0.44822 train_acc= 0.88774 val_loss= 0.40331 val_acc= 0.89893 time= 0.18599
Epoch: 0089 train_loss= 0.43645 train_acc= 0.89165 val_loss= 0.39799 val_acc= 0.89893 time= 0.16988
Epoch: 0090 train_loss= 0.42372 train_acc= 0.89437 val_loss= 0.39344 val_acc= 0.89740 time= 0.17004
Epoch: 0091 train_loss= 0.41135 train_acc= 0.89505 val_loss= 0.39009 val_acc= 0.89740 time= 0.16800
Epoch: 0092 train_loss= 0.40943 train_acc= 0.90202 val_loss= 0.38851 val_acc= 0.90352 time= 0.19600
Epoch: 0093 train_loss= 0.41728 train_acc= 0.89709 val_loss= 0.38696 val_acc= 0.90199 time= 0.16628
Epoch: 0094 train_loss= 0.39495 train_acc= 0.89828 val_loss= 0.38397 val_acc= 0.90352 time= 0.17501
Epoch: 0095 train_loss= 0.39828 train_acc= 0.90015 val_loss= 0.37879 val_acc= 0.90352 time= 0.16700
Epoch: 0096 train_loss= 0.40977 train_acc= 0.90168 val_loss= 0.37309 val_acc= 0.90658 time= 0.16900
Epoch: 0097 train_loss= 0.38198 train_acc= 0.91223 val_loss= 0.36667 val_acc= 0.90505 time= 0.19997
Epoch: 0098 train_loss= 0.37252 train_acc= 0.90679 val_loss= 0.36103 val_acc= 0.90199 time= 0.16960
Epoch: 0099 train_loss= 0.35561 train_acc= 0.91801 val_loss= 0.35654 val_acc= 0.90505 time= 0.17000
Epoch: 0100 train_loss= 0.36235 train_acc= 0.90934 val_loss= 0.35219 val_acc= 0.90505 time= 0.17903
Epoch: 0101 train_loss= 0.37147 train_acc= 0.91036 val_loss= 0.34867 val_acc= 0.90505 time= 0.16697
Epoch: 0102 train_loss= 0.35291 train_acc= 0.91342 val_loss= 0.34457 val_acc= 0.90812 time= 0.16703
Epoch: 0103 train_loss= 0.35482 train_acc= 0.90849 val_loss= 0.34072 val_acc= 0.90812 time= 0.18100
Epoch: 0104 train_loss= 0.33696 train_acc= 0.91801 val_loss= 0.33792 val_acc= 0.90965 time= 0.16707
Epoch: 0105 train_loss= 0.34358 train_acc= 0.91138 val_loss= 0.33538 val_acc= 0.90812 time= 0.18350
Epoch: 0106 train_loss= 0.33424 train_acc= 0.91750 val_loss= 0.33402 val_acc= 0.90965 time= 0.17104
Epoch: 0107 train_loss= 0.33235 train_acc= 0.91427 val_loss= 0.33151 val_acc= 0.90812 time= 0.16801
Epoch: 0108 train_loss= 0.34101 train_acc= 0.91393 val_loss= 0.32862 val_acc= 0.90658 time= 0.16695
Epoch: 0109 train_loss= 0.32451 train_acc= 0.91767 val_loss= 0.32455 val_acc= 0.91118 time= 0.19600
Epoch: 0110 train_loss= 0.32428 train_acc= 0.92176 val_loss= 0.32018 val_acc= 0.91424 time= 0.16705
Epoch: 0111 train_loss= 0.32262 train_acc= 0.91971 val_loss= 0.31653 val_acc= 0.91424 time= 0.16895
Epoch: 0112 train_loss= 0.31312 train_acc= 0.92567 val_loss= 0.31424 val_acc= 0.91424 time= 0.16900
Epoch: 0113 train_loss= 0.29801 train_acc= 0.92465 val_loss= 0.31323 val_acc= 0.91271 time= 0.17000
Epoch: 0114 train_loss= 0.32604 train_acc= 0.92056 val_loss= 0.31124 val_acc= 0.91118 time= 0.19600
Epoch: 0115 train_loss= 0.28958 train_acc= 0.93315 val_loss= 0.31040 val_acc= 0.91424 time= 0.16700
Epoch: 0116 train_loss= 0.28811 train_acc= 0.93366 val_loss= 0.31041 val_acc= 0.91577 time= 0.16805
Epoch: 0117 train_loss= 0.28959 train_acc= 0.92550 val_loss= 0.30940 val_acc= 0.91577 time= 0.18895
Epoch: 0118 train_loss= 0.28496 train_acc= 0.92958 val_loss= 0.30669 val_acc= 0.91271 time= 0.16905
Epoch: 0119 train_loss= 0.29007 train_acc= 0.93145 val_loss= 0.30423 val_acc= 0.91577 time= 0.16640
Epoch: 0120 train_loss= 0.27166 train_acc= 0.92805 val_loss= 0.30134 val_acc= 0.91424 time= 0.17203
Epoch: 0121 train_loss= 0.28822 train_acc= 0.92720 val_loss= 0.29776 val_acc= 0.91424 time= 0.19861
Epoch: 0122 train_loss= 0.27333 train_acc= 0.93281 val_loss= 0.29435 val_acc= 0.91424 time= 0.17031
Epoch: 0123 train_loss= 0.27408 train_acc= 0.93077 val_loss= 0.29231 val_acc= 0.91884 time= 0.16696
Epoch: 0124 train_loss= 0.26892 train_acc= 0.93281 val_loss= 0.29075 val_acc= 0.92190 time= 0.16805
Epoch: 0125 train_loss= 0.25378 train_acc= 0.93536 val_loss= 0.28926 val_acc= 0.92190 time= 0.16800
Epoch: 0126 train_loss= 0.25768 train_acc= 0.93655 val_loss= 0.28711 val_acc= 0.92037 time= 0.17395
Epoch: 0127 train_loss= 0.26875 train_acc= 0.92907 val_loss= 0.28644 val_acc= 0.92037 time= 0.16727
Epoch: 0128 train_loss= 0.25524 train_acc= 0.93434 val_loss= 0.28483 val_acc= 0.91884 time= 0.19300
Epoch: 0129 train_loss= 0.25641 train_acc= 0.93689 val_loss= 0.28251 val_acc= 0.92190 time= 0.17001
Epoch: 0130 train_loss= 0.25926 train_acc= 0.93689 val_loss= 0.27947 val_acc= 0.92343 time= 0.16799
Epoch: 0131 train_loss= 0.23946 train_acc= 0.93740 val_loss= 0.27681 val_acc= 0.92496 time= 0.16800
Epoch: 0132 train_loss= 0.24545 train_acc= 0.94642 val_loss= 0.27449 val_acc= 0.92649 time= 0.19601
Epoch: 0133 train_loss= 0.24737 train_acc= 0.93825 val_loss= 0.27388 val_acc= 0.92649 time= 0.16806
Epoch: 0134 train_loss= 0.23509 train_acc= 0.93979 val_loss= 0.27357 val_acc= 0.92649 time= 0.16895
Epoch: 0135 train_loss= 0.22791 train_acc= 0.94132 val_loss= 0.27296 val_acc= 0.92343 time= 0.17000
Epoch: 0136 train_loss= 0.24606 train_acc= 0.93808 val_loss= 0.27323 val_acc= 0.92037 time= 0.16956
Epoch: 0137 train_loss= 0.23304 train_acc= 0.94319 val_loss= 0.27392 val_acc= 0.91730 time= 0.19501
Epoch: 0138 train_loss= 0.23869 train_acc= 0.93979 val_loss= 0.27459 val_acc= 0.92037 time= 0.17004
Epoch: 0139 train_loss= 0.23973 train_acc= 0.94030 val_loss= 0.27435 val_acc= 0.91884 time= 0.17301
Epoch: 0140 train_loss= 0.23305 train_acc= 0.94404 val_loss= 0.27343 val_acc= 0.91884 time= 0.19095
Epoch: 0141 train_loss= 0.21740 train_acc= 0.94455 val_loss= 0.27181 val_acc= 0.92037 time= 0.16904
Epoch: 0142 train_loss= 0.22965 train_acc= 0.94234 val_loss= 0.27046 val_acc= 0.92343 time= 0.17102
Epoch: 0143 train_loss= 0.21851 train_acc= 0.94302 val_loss= 0.26842 val_acc= 0.92037 time= 0.20196
Epoch: 0144 train_loss= 0.23025 train_acc= 0.94404 val_loss= 0.26507 val_acc= 0.92343 time= 0.17000
Epoch: 0145 train_loss= 0.21384 train_acc= 0.94557 val_loss= 0.26231 val_acc= 0.92037 time= 0.18105
Epoch: 0146 train_loss= 0.20237 train_acc= 0.94693 val_loss= 0.26029 val_acc= 0.92343 time= 0.16896
Epoch: 0147 train_loss= 0.21014 train_acc= 0.94506 val_loss= 0.25890 val_acc= 0.92496 time= 0.16805
Epoch: 0148 train_loss= 0.22016 train_acc= 0.94166 val_loss= 0.25820 val_acc= 0.92496 time= 0.16799
Epoch: 0149 train_loss= 0.20529 train_acc= 0.94744 val_loss= 0.25718 val_acc= 0.92649 time= 0.19501
Epoch: 0150 train_loss= 0.21115 train_acc= 0.94795 val_loss= 0.25559 val_acc= 0.92649 time= 0.16796
Epoch: 0151 train_loss= 0.20793 train_acc= 0.94693 val_loss= 0.25484 val_acc= 0.92802 time= 0.17109
Epoch: 0152 train_loss= 0.20524 train_acc= 0.94914 val_loss= 0.25425 val_acc= 0.92956 time= 0.16900
Epoch: 0153 train_loss= 0.19178 train_acc= 0.95629 val_loss= 0.25379 val_acc= 0.93109 time= 0.16800
Epoch: 0154 train_loss= 0.20904 train_acc= 0.94829 val_loss= 0.25281 val_acc= 0.92802 time= 0.17100
Epoch: 0155 train_loss= 0.18830 train_acc= 0.95731 val_loss= 0.25197 val_acc= 0.92956 time= 0.17901
Epoch: 0156 train_loss= 0.20281 train_acc= 0.94965 val_loss= 0.25161 val_acc= 0.92496 time= 0.16699
Epoch: 0157 train_loss= 0.20306 train_acc= 0.94727 val_loss= 0.25065 val_acc= 0.92956 time= 0.18697
Epoch: 0158 train_loss= 0.17671 train_acc= 0.95339 val_loss= 0.25001 val_acc= 0.93109 time= 0.17003
Epoch: 0159 train_loss= 0.18564 train_acc= 0.95663 val_loss= 0.24803 val_acc= 0.93262 time= 0.17103
Epoch: 0160 train_loss= 0.18035 train_acc= 0.95288 val_loss= 0.24475 val_acc= 0.93109 time= 0.17300
Epoch: 0161 train_loss= 0.20001 train_acc= 0.94710 val_loss= 0.24130 val_acc= 0.93109 time= 0.19300
Epoch: 0162 train_loss= 0.20728 train_acc= 0.94523 val_loss= 0.24011 val_acc= 0.93262 time= 0.16900
Epoch: 0163 train_loss= 0.18604 train_acc= 0.95407 val_loss= 0.23973 val_acc= 0.93415 time= 0.16600
Epoch: 0164 train_loss= 0.17471 train_acc= 0.95509 val_loss= 0.24091 val_acc= 0.93415 time= 0.16600
Epoch: 0165 train_loss= 0.17569 train_acc= 0.95390 val_loss= 0.24319 val_acc= 0.93262 time= 0.16900
Epoch: 0166 train_loss= 0.17518 train_acc= 0.95390 val_loss= 0.24506 val_acc= 0.93262 time= 0.20104
Early stopping...
Optimization Finished!
Test set results: cost= 0.27763 accuracy= 0.93146 time= 0.07499
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     1.0000    0.3333    0.5000         6
           2     0.0000    0.0000    0.0000         1
           3     0.8090    0.9600    0.8780        75
           4     1.0000    1.0000    1.0000         9
           5     0.8265    0.9310    0.8757        87
           6     0.9200    0.9200    0.9200        25
           7     0.6875    0.8462    0.7586        13
           8     1.0000    0.5455    0.7059        11
           9     0.0000    0.0000    0.0000         9
          10     0.8889    0.6667    0.7619        36
          11     1.0000    0.9167    0.9565        12
          12     0.8478    0.9669    0.9035       121
          13     0.8667    0.6842    0.7647        19
          14     0.8333    0.8929    0.8621        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.8261    0.9500    0.8837        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.5000    0.7647    0.6047        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9602    0.9713    0.9657       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.6667    0.8000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8519    0.8519    0.8519        81
          36     0.8000    0.3333    0.4706        12
          37     1.0000    1.0000    1.0000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9917    0.9849      1083
          40     1.0000    0.8000    0.8889         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8000    0.6667    0.7273        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.6667    0.9333    0.7778        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9315      2568
   macro avg     0.6836    0.5904    0.6148      2568
weighted avg     0.9241    0.9315    0.9237      2568

Macro average Test Precision, Recall and F1-Score...
(0.6835930030871634, 0.5904430855171073, 0.6148300845953619, None)
Micro average Test Precision, Recall and F1-Score...
(0.9314641744548287, 0.9314641744548287, 0.9314641744548287, None)
embeddings:
8892 6532 2568
[[ 1.5007882  -0.01321977 -0.10303494 ...  0.19925478 -0.07567796
   0.04987734]
 [ 0.61452895  0.2530422  -0.06112233 ...  0.00494937  0.22950044
   0.3430809 ]
 [ 0.8301395   0.15695208 -0.08168114 ...  0.32107222  0.06836636
   0.6069986 ]
 ...
 [ 0.13065581  0.00947063 -0.03982111 ... -0.05806585  0.01458799
   0.21711254]
 [ 0.41458052  0.1464612  -0.04221389 ...  0.13900025  0.04874001
   0.26640573]
 [ 0.15190934  0.22662191 -0.06238096 ... -0.00757242  0.29306063
   0.15422152]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95125 train_acc= 0.02364 val_loss= 3.89782 val_acc= 0.65697 time= 0.44502
Epoch: 0002 train_loss= 3.89774 train_acc= 0.63531 val_loss= 3.79731 val_acc= 0.65237 time= 0.19661
Epoch: 0003 train_loss= 3.79721 train_acc= 0.62749 val_loss= 3.64200 val_acc= 0.65084 time= 0.17735
Epoch: 0004 train_loss= 3.64307 train_acc= 0.62800 val_loss= 3.43215 val_acc= 0.64931 time= 0.17001
Epoch: 0005 train_loss= 3.43273 train_acc= 0.62647 val_loss= 3.17960 val_acc= 0.64012 time= 0.16604
Epoch: 0006 train_loss= 3.18192 train_acc= 0.61745 val_loss= 2.90927 val_acc= 0.63247 time= 0.16756
Epoch: 0007 train_loss= 2.91094 train_acc= 0.61677 val_loss= 2.65121 val_acc= 0.62940 time= 0.16703
Epoch: 0008 train_loss= 2.64970 train_acc= 0.61014 val_loss= 2.44056 val_acc= 0.62481 time= 0.16630
Epoch: 0009 train_loss= 2.43630 train_acc= 0.61184 val_loss= 2.30240 val_acc= 0.64472 time= 0.18804
Epoch: 0010 train_loss= 2.29526 train_acc= 0.62409 val_loss= 2.22446 val_acc= 0.67075 time= 0.17209
Epoch: 0011 train_loss= 2.22397 train_acc= 0.65164 val_loss= 2.17350 val_acc= 0.54364 time= 0.17100
Epoch: 0012 train_loss= 2.17550 train_acc= 0.53955 val_loss= 2.12488 val_acc= 0.47626 time= 0.16900
Epoch: 0013 train_loss= 2.13450 train_acc= 0.45025 val_loss= 2.06400 val_acc= 0.46554 time= 0.16999
Epoch: 0014 train_loss= 2.08330 train_acc= 0.43919 val_loss= 1.98565 val_acc= 0.46708 time= 0.16800
Epoch: 0015 train_loss= 2.00421 train_acc= 0.44174 val_loss= 1.89428 val_acc= 0.48086 time= 0.16700
Epoch: 0016 train_loss= 1.92162 train_acc= 0.45892 val_loss= 1.80197 val_acc= 0.52067 time= 0.16704
Epoch: 0017 train_loss= 1.82747 train_acc= 0.52424 val_loss= 1.72040 val_acc= 0.61103 time= 0.16772
Epoch: 0018 train_loss= 1.74646 train_acc= 0.60640 val_loss= 1.65365 val_acc= 0.66003 time= 0.17954
Epoch: 0019 train_loss= 1.68226 train_acc= 0.64875 val_loss= 1.59756 val_acc= 0.67534 time= 0.17703
Epoch: 0020 train_loss= 1.62475 train_acc= 0.65759 val_loss= 1.54520 val_acc= 0.68147 time= 0.16900
Epoch: 0021 train_loss= 1.57389 train_acc= 0.66134 val_loss= 1.49227 val_acc= 0.68300 time= 0.18600
Epoch: 0022 train_loss= 1.51624 train_acc= 0.66304 val_loss= 1.43903 val_acc= 0.68606 time= 0.16833
Epoch: 0023 train_loss= 1.46150 train_acc= 0.66916 val_loss= 1.38727 val_acc= 0.68913 time= 0.16700
Epoch: 0024 train_loss= 1.40669 train_acc= 0.67426 val_loss= 1.33889 val_acc= 0.69832 time= 0.19300
Epoch: 0025 train_loss= 1.36344 train_acc= 0.68260 val_loss= 1.29467 val_acc= 0.70750 time= 0.16806
Epoch: 0026 train_loss= 1.31608 train_acc= 0.69468 val_loss= 1.25480 val_acc= 0.72129 time= 0.17556
Epoch: 0027 train_loss= 1.27602 train_acc= 0.70590 val_loss= 1.21877 val_acc= 0.72129 time= 0.17477
Epoch: 0028 train_loss= 1.23950 train_acc= 0.71798 val_loss= 1.18576 val_acc= 0.72894 time= 0.16696
Epoch: 0029 train_loss= 1.20368 train_acc= 0.72784 val_loss= 1.15477 val_acc= 0.73354 time= 0.16776
Epoch: 0030 train_loss= 1.17323 train_acc= 0.73975 val_loss= 1.12496 val_acc= 0.73354 time= 0.17200
Epoch: 0031 train_loss= 1.13980 train_acc= 0.74758 val_loss= 1.09569 val_acc= 0.74426 time= 0.19300
Epoch: 0032 train_loss= 1.10955 train_acc= 0.75387 val_loss= 1.06671 val_acc= 0.74885 time= 0.17905
Epoch: 0033 train_loss= 1.08062 train_acc= 0.75931 val_loss= 1.03816 val_acc= 0.76110 time= 0.17066
Epoch: 0034 train_loss= 1.05206 train_acc= 0.76442 val_loss= 1.01031 val_acc= 0.76876 time= 0.17244
Epoch: 0035 train_loss= 1.02019 train_acc= 0.77020 val_loss= 0.98332 val_acc= 0.77335 time= 0.17004
Epoch: 0036 train_loss= 0.99216 train_acc= 0.78194 val_loss= 0.95729 val_acc= 0.77642 time= 0.17896
Epoch: 0037 train_loss= 0.96669 train_acc= 0.78874 val_loss= 0.93228 val_acc= 0.79020 time= 0.16903
Epoch: 0038 train_loss= 0.94011 train_acc= 0.79673 val_loss= 0.90808 val_acc= 0.79939 time= 0.16797
Epoch: 0039 train_loss= 0.91660 train_acc= 0.80711 val_loss= 0.88442 val_acc= 0.81011 time= 0.16905
Epoch: 0040 train_loss= 0.89210 train_acc= 0.81544 val_loss= 0.86118 val_acc= 0.81776 time= 0.16811
Epoch: 0041 train_loss= 0.86916 train_acc= 0.82123 val_loss= 0.83830 val_acc= 0.82236 time= 0.18092
Epoch: 0042 train_loss= 0.84301 train_acc= 0.82769 val_loss= 0.81576 val_acc= 0.83155 time= 0.17457
Epoch: 0043 train_loss= 0.82346 train_acc= 0.82956 val_loss= 0.79359 val_acc= 0.83308 time= 0.16956
Epoch: 0044 train_loss= 0.79816 train_acc= 0.83824 val_loss= 0.77180 val_acc= 0.83767 time= 0.18400
Epoch: 0045 train_loss= 0.77285 train_acc= 0.84062 val_loss= 0.75033 val_acc= 0.83767 time= 0.16700
Epoch: 0046 train_loss= 0.75059 train_acc= 0.84334 val_loss= 0.72931 val_acc= 0.84227 time= 0.16696
Epoch: 0047 train_loss= 0.72503 train_acc= 0.84623 val_loss= 0.70885 val_acc= 0.84380 time= 0.16900
Epoch: 0048 train_loss= 0.70741 train_acc= 0.84946 val_loss= 0.68886 val_acc= 0.85299 time= 0.19608
Epoch: 0049 train_loss= 0.68499 train_acc= 0.85457 val_loss= 0.66940 val_acc= 0.85758 time= 0.17200
Epoch: 0050 train_loss= 0.66425 train_acc= 0.85695 val_loss= 0.65047 val_acc= 0.85758 time= 0.17000
Epoch: 0051 train_loss= 0.64439 train_acc= 0.86324 val_loss= 0.63215 val_acc= 0.86064 time= 0.16703
Epoch: 0052 train_loss= 0.62407 train_acc= 0.86562 val_loss= 0.61442 val_acc= 0.86371 time= 0.16701
Epoch: 0053 train_loss= 0.60449 train_acc= 0.86783 val_loss= 0.59749 val_acc= 0.86830 time= 0.18400
Epoch: 0054 train_loss= 0.58403 train_acc= 0.87311 val_loss= 0.58131 val_acc= 0.87136 time= 0.16800
Epoch: 0055 train_loss= 0.56873 train_acc= 0.87838 val_loss= 0.56574 val_acc= 0.87443 time= 0.18197
Epoch: 0056 train_loss= 0.54946 train_acc= 0.87855 val_loss= 0.55090 val_acc= 0.87749 time= 0.16961
Epoch: 0057 train_loss= 0.53189 train_acc= 0.88450 val_loss= 0.53648 val_acc= 0.88055 time= 0.17157
Epoch: 0058 train_loss= 0.51250 train_acc= 0.88672 val_loss= 0.52264 val_acc= 0.88361 time= 0.16897
Epoch: 0059 train_loss= 0.50091 train_acc= 0.89097 val_loss= 0.50906 val_acc= 0.88208 time= 0.19404
Epoch: 0060 train_loss= 0.48021 train_acc= 0.89352 val_loss= 0.49577 val_acc= 0.88361 time= 0.16699
Epoch: 0061 train_loss= 0.46678 train_acc= 0.89760 val_loss= 0.48263 val_acc= 0.88515 time= 0.16700
Epoch: 0062 train_loss= 0.45065 train_acc= 0.89896 val_loss= 0.46962 val_acc= 0.88668 time= 0.16702
Epoch: 0063 train_loss= 0.43786 train_acc= 0.90321 val_loss= 0.45715 val_acc= 0.88974 time= 0.16698
Epoch: 0064 train_loss= 0.42470 train_acc= 0.90849 val_loss= 0.44521 val_acc= 0.89127 time= 0.19704
Epoch: 0065 train_loss= 0.41266 train_acc= 0.91240 val_loss= 0.43362 val_acc= 0.89433 time= 0.17000
Epoch: 0066 train_loss= 0.39759 train_acc= 0.91818 val_loss= 0.42278 val_acc= 0.89587 time= 0.16797
Epoch: 0067 train_loss= 0.38590 train_acc= 0.91852 val_loss= 0.41299 val_acc= 0.89740 time= 0.18808
Epoch: 0068 train_loss= 0.37525 train_acc= 0.92431 val_loss= 0.40398 val_acc= 0.89893 time= 0.16900
Epoch: 0069 train_loss= 0.36294 train_acc= 0.92312 val_loss= 0.39565 val_acc= 0.90199 time= 0.16700
Epoch: 0070 train_loss= 0.35330 train_acc= 0.92856 val_loss= 0.38754 val_acc= 0.90199 time= 0.19206
Epoch: 0071 train_loss= 0.34126 train_acc= 0.93060 val_loss= 0.37957 val_acc= 0.90352 time= 0.16895
Epoch: 0072 train_loss= 0.32896 train_acc= 0.93349 val_loss= 0.37188 val_acc= 0.90658 time= 0.17351
Epoch: 0073 train_loss= 0.31689 train_acc= 0.93604 val_loss= 0.36442 val_acc= 0.91118 time= 0.17100
Epoch: 0074 train_loss= 0.31031 train_acc= 0.93468 val_loss= 0.35747 val_acc= 0.90965 time= 0.16700
Epoch: 0075 train_loss= 0.29883 train_acc= 0.94098 val_loss= 0.35046 val_acc= 0.91118 time= 0.16804
Epoch: 0076 train_loss= 0.28787 train_acc= 0.94268 val_loss= 0.34404 val_acc= 0.91271 time= 0.17896
Epoch: 0077 train_loss= 0.27746 train_acc= 0.94268 val_loss= 0.33791 val_acc= 0.91424 time= 0.16869
Epoch: 0078 train_loss= 0.27340 train_acc= 0.94455 val_loss= 0.33229 val_acc= 0.91271 time= 0.18200
Epoch: 0079 train_loss= 0.26406 train_acc= 0.94948 val_loss= 0.32717 val_acc= 0.92343 time= 0.16840
Epoch: 0080 train_loss= 0.25467 train_acc= 0.94982 val_loss= 0.32239 val_acc= 0.92496 time= 0.17100
Epoch: 0081 train_loss= 0.25034 train_acc= 0.95050 val_loss= 0.31736 val_acc= 0.92496 time= 0.17000
Epoch: 0082 train_loss= 0.23824 train_acc= 0.95356 val_loss= 0.31284 val_acc= 0.92343 time= 0.18400
Epoch: 0083 train_loss= 0.23607 train_acc= 0.95254 val_loss= 0.30799 val_acc= 0.92496 time= 0.16705
Epoch: 0084 train_loss= 0.22429 train_acc= 0.95594 val_loss= 0.30309 val_acc= 0.92649 time= 0.17000
Epoch: 0085 train_loss= 0.21792 train_acc= 0.95594 val_loss= 0.29817 val_acc= 0.92649 time= 0.16799
Epoch: 0086 train_loss= 0.21207 train_acc= 0.95867 val_loss= 0.29378 val_acc= 0.92802 time= 0.16700
Epoch: 0087 train_loss= 0.20666 train_acc= 0.96020 val_loss= 0.29038 val_acc= 0.92956 time= 0.17396
Epoch: 0088 train_loss= 0.19672 train_acc= 0.96360 val_loss= 0.28728 val_acc= 0.92956 time= 0.19043
Epoch: 0089 train_loss= 0.19407 train_acc= 0.96224 val_loss= 0.28423 val_acc= 0.92802 time= 0.17000
Epoch: 0090 train_loss= 0.18745 train_acc= 0.96309 val_loss= 0.28092 val_acc= 0.92956 time= 0.18600
Epoch: 0091 train_loss= 0.18003 train_acc= 0.96717 val_loss= 0.27745 val_acc= 0.92956 time= 0.16700
Epoch: 0092 train_loss= 0.17357 train_acc= 0.96785 val_loss= 0.27390 val_acc= 0.93109 time= 0.16600
Epoch: 0093 train_loss= 0.16821 train_acc= 0.96870 val_loss= 0.26995 val_acc= 0.93109 time= 0.17100
Epoch: 0094 train_loss= 0.16606 train_acc= 0.96870 val_loss= 0.26685 val_acc= 0.93109 time= 0.19300
Epoch: 0095 train_loss= 0.15752 train_acc= 0.96921 val_loss= 0.26407 val_acc= 0.93109 time= 0.17143
Epoch: 0096 train_loss= 0.15104 train_acc= 0.97125 val_loss= 0.26184 val_acc= 0.93109 time= 0.17151
Epoch: 0097 train_loss= 0.14839 train_acc= 0.97415 val_loss= 0.26020 val_acc= 0.93262 time= 0.16747
Epoch: 0098 train_loss= 0.14257 train_acc= 0.97551 val_loss= 0.25892 val_acc= 0.93262 time= 0.16700
Epoch: 0099 train_loss= 0.14028 train_acc= 0.97500 val_loss= 0.25724 val_acc= 0.93415 time= 0.19600
Epoch: 0100 train_loss= 0.13458 train_acc= 0.97619 val_loss= 0.25498 val_acc= 0.93568 time= 0.16701
Epoch: 0101 train_loss= 0.13214 train_acc= 0.97823 val_loss= 0.25223 val_acc= 0.93415 time= 0.18500
Epoch: 0102 train_loss= 0.12746 train_acc= 0.97942 val_loss= 0.24924 val_acc= 0.93568 time= 0.16899
Epoch: 0103 train_loss= 0.12436 train_acc= 0.97806 val_loss= 0.24632 val_acc= 0.93262 time= 0.17100
Epoch: 0104 train_loss= 0.12050 train_acc= 0.98061 val_loss= 0.24435 val_acc= 0.93262 time= 0.16900
Epoch: 0105 train_loss= 0.11682 train_acc= 0.97993 val_loss= 0.24326 val_acc= 0.93262 time= 0.19600
Epoch: 0106 train_loss= 0.11526 train_acc= 0.98146 val_loss= 0.24283 val_acc= 0.93415 time= 0.16800
Epoch: 0107 train_loss= 0.10706 train_acc= 0.98350 val_loss= 0.24275 val_acc= 0.93568 time= 0.16801
Epoch: 0108 train_loss= 0.10389 train_acc= 0.98333 val_loss= 0.24180 val_acc= 0.93568 time= 0.16799
Epoch: 0109 train_loss= 0.10241 train_acc= 0.98452 val_loss= 0.24041 val_acc= 0.93568 time= 0.16701
Epoch: 0110 train_loss= 0.09870 train_acc= 0.98401 val_loss= 0.23865 val_acc= 0.93721 time= 0.17400
Epoch: 0111 train_loss= 0.09771 train_acc= 0.98622 val_loss= 0.23673 val_acc= 0.93568 time= 0.19903
Epoch: 0112 train_loss= 0.09403 train_acc= 0.98741 val_loss= 0.23554 val_acc= 0.93415 time= 0.16800
Epoch: 0113 train_loss= 0.09217 train_acc= 0.98673 val_loss= 0.23461 val_acc= 0.93568 time= 0.18400
Epoch: 0114 train_loss= 0.08857 train_acc= 0.98673 val_loss= 0.23391 val_acc= 0.93721 time= 0.16600
Epoch: 0115 train_loss= 0.08511 train_acc= 0.98843 val_loss= 0.23339 val_acc= 0.93874 time= 0.16599
Epoch: 0116 train_loss= 0.08355 train_acc= 0.98724 val_loss= 0.23236 val_acc= 0.93874 time= 0.17000
Epoch: 0117 train_loss= 0.08021 train_acc= 0.98775 val_loss= 0.23121 val_acc= 0.94028 time= 0.18900
Epoch: 0118 train_loss= 0.07773 train_acc= 0.98945 val_loss= 0.23064 val_acc= 0.94028 time= 0.18397
Epoch: 0119 train_loss= 0.07736 train_acc= 0.98877 val_loss= 0.23011 val_acc= 0.93721 time= 0.17105
Epoch: 0120 train_loss= 0.07314 train_acc= 0.98843 val_loss= 0.22934 val_acc= 0.93874 time= 0.16699
Epoch: 0121 train_loss= 0.07119 train_acc= 0.98962 val_loss= 0.22892 val_acc= 0.93874 time= 0.16699
Epoch: 0122 train_loss= 0.06922 train_acc= 0.98996 val_loss= 0.22872 val_acc= 0.94028 time= 0.19700
Epoch: 0123 train_loss= 0.06857 train_acc= 0.99030 val_loss= 0.22812 val_acc= 0.93874 time= 0.16700
Epoch: 0124 train_loss= 0.06685 train_acc= 0.99081 val_loss= 0.22714 val_acc= 0.94028 time= 0.17100
Epoch: 0125 train_loss= 0.06416 train_acc= 0.99098 val_loss= 0.22599 val_acc= 0.94028 time= 0.17000
Epoch: 0126 train_loss= 0.06174 train_acc= 0.99235 val_loss= 0.22526 val_acc= 0.93874 time= 0.17100
Epoch: 0127 train_loss= 0.06106 train_acc= 0.99098 val_loss= 0.22476 val_acc= 0.94028 time= 0.17001
Epoch: 0128 train_loss= 0.05858 train_acc= 0.99201 val_loss= 0.22444 val_acc= 0.94028 time= 0.19499
Epoch: 0129 train_loss= 0.05686 train_acc= 0.99235 val_loss= 0.22429 val_acc= 0.93721 time= 0.16900
Epoch: 0130 train_loss= 0.05642 train_acc= 0.99201 val_loss= 0.22425 val_acc= 0.93874 time= 0.18500
Epoch: 0131 train_loss= 0.05481 train_acc= 0.99235 val_loss= 0.22431 val_acc= 0.93721 time= 0.16700
Epoch: 0132 train_loss= 0.05295 train_acc= 0.99320 val_loss= 0.22388 val_acc= 0.94181 time= 0.16700
Epoch: 0133 train_loss= 0.05230 train_acc= 0.99371 val_loss= 0.22334 val_acc= 0.93874 time= 0.19757
Epoch: 0134 train_loss= 0.05138 train_acc= 0.99167 val_loss= 0.22325 val_acc= 0.93874 time= 0.16999
Epoch: 0135 train_loss= 0.05035 train_acc= 0.99388 val_loss= 0.22291 val_acc= 0.93874 time= 0.16797
Epoch: 0136 train_loss= 0.04862 train_acc= 0.99388 val_loss= 0.22268 val_acc= 0.93874 time= 0.16704
Epoch: 0137 train_loss= 0.04669 train_acc= 0.99422 val_loss= 0.22237 val_acc= 0.93874 time= 0.16700
Epoch: 0138 train_loss= 0.04610 train_acc= 0.99354 val_loss= 0.22261 val_acc= 0.94028 time= 0.16800
Epoch: 0139 train_loss= 0.04556 train_acc= 0.99439 val_loss= 0.22224 val_acc= 0.94181 time= 0.19200
Epoch: 0140 train_loss= 0.04374 train_acc= 0.99558 val_loss= 0.22126 val_acc= 0.94334 time= 0.16700
Epoch: 0141 train_loss= 0.04454 train_acc= 0.99405 val_loss= 0.21958 val_acc= 0.94334 time= 0.19100
Epoch: 0142 train_loss= 0.04282 train_acc= 0.99439 val_loss= 0.21794 val_acc= 0.94487 time= 0.16955
Epoch: 0143 train_loss= 0.04040 train_acc= 0.99524 val_loss= 0.21751 val_acc= 0.94640 time= 0.16800
Epoch: 0144 train_loss= 0.04001 train_acc= 0.99456 val_loss= 0.21783 val_acc= 0.94640 time= 0.16600
Epoch: 0145 train_loss= 0.03808 train_acc= 0.99592 val_loss= 0.21874 val_acc= 0.94487 time= 0.19552
Epoch: 0146 train_loss= 0.03861 train_acc= 0.99558 val_loss= 0.21975 val_acc= 0.94334 time= 0.16804
Epoch: 0147 train_loss= 0.03641 train_acc= 0.99507 val_loss= 0.22111 val_acc= 0.94334 time= 0.16796
Early stopping...
Optimization Finished!
Test set results: cost= 0.24594 accuracy= 0.93847 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8750    0.8750    0.8750         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.8140    0.9333    0.8696        75
           4     1.0000    1.0000    1.0000         9
           5     0.8333    0.9195    0.8743        87
           6     0.9200    0.9200    0.9200        25
           7     0.8571    0.9231    0.8889        13
           8     1.0000    1.0000    1.0000        11
           9     1.0000    0.4444    0.6154         9
          10     0.8966    0.7222    0.8000        36
          11     1.0000    0.9167    0.9565        12
          12     0.8451    0.9917    0.9125       121
          13     0.9333    0.7368    0.8235        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    1.0000    1.0000         4
          16     1.0000    0.2500    0.4000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     1.0000    1.0000    1.0000         1
          24     0.6667    0.8235    0.7368        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9695    0.9598    0.9646       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8272    0.8272    0.8272        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9917    0.9849      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8333    0.8333    0.8333        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9385      2568
   macro avg     0.8019    0.7133    0.7292      2568
weighted avg     0.9383    0.9385    0.9346      2568

Macro average Test Precision, Recall and F1-Score...
(0.8019257477445423, 0.713324530599484, 0.7292495087128241, None)
Micro average Test Precision, Recall and F1-Score...
(0.9384735202492211, 0.9384735202492211, 0.9384735202492211, None)
embeddings:
8892 6532 2568
[[-0.0413365   1.0668484   1.2426566  ... -0.15822393 -0.09397022
   1.0237703 ]
 [ 0.5206815   0.46296117  0.6821279  ...  0.04352853  0.06077456
  -0.13101056]
 [ 0.55158424  0.31726104  0.30529204 ...  0.00760385 -0.01953388
   0.99956125]
 ...
 [ 0.50795305  0.345128    0.16468455 ...  0.01364956  0.03520748
   0.11727084]
 [ 0.17386535  0.19338122  0.22860815 ...  0.05182869  0.04899043
   0.4546688 ]
 [ 0.47423524  0.16885935  0.19924073 ...  0.2670895   0.27611154
   0.2933592 ]]

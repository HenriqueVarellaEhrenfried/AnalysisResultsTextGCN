(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95120 train_acc= 0.02347 val_loss= 3.90894 val_acc= 0.65084 time= 0.46897
Epoch: 0002 train_loss= 3.90793 train_acc= 0.62358 val_loss= 3.81608 val_acc= 0.65697 time= 0.17399
Epoch: 0003 train_loss= 3.81794 train_acc= 0.62613 val_loss= 3.67233 val_acc= 0.66003 time= 0.16996
Epoch: 0004 train_loss= 3.67889 train_acc= 0.62613 val_loss= 3.47790 val_acc= 0.65544 time= 0.19458
Epoch: 0005 train_loss= 3.47275 train_acc= 0.62221 val_loss= 3.24218 val_acc= 0.65391 time= 0.17000
Epoch: 0006 train_loss= 3.27344 train_acc= 0.61269 val_loss= 2.98688 val_acc= 0.64625 time= 0.16800
Epoch: 0007 train_loss= 3.01062 train_acc= 0.60640 val_loss= 2.74098 val_acc= 0.64319 time= 0.16600
Epoch: 0008 train_loss= 2.75206 train_acc= 0.61796 val_loss= 2.53186 val_acc= 0.64319 time= 0.19505
Epoch: 0009 train_loss= 2.50345 train_acc= 0.60231 val_loss= 2.38460 val_acc= 0.65237 time= 0.16700
Epoch: 0010 train_loss= 2.39951 train_acc= 0.58905 val_loss= 2.30088 val_acc= 0.67228 time= 0.18195
Epoch: 0011 train_loss= 2.33200 train_acc= 0.59517 val_loss= 2.25578 val_acc= 0.58652 time= 0.16700
Epoch: 0012 train_loss= 2.26727 train_acc= 0.57935 val_loss= 2.21841 val_acc= 0.47167 time= 0.17100
Epoch: 0013 train_loss= 2.23753 train_acc= 0.48478 val_loss= 2.17177 val_acc= 0.46095 time= 0.17151
Epoch: 0014 train_loss= 2.22406 train_acc= 0.44701 val_loss= 2.10916 val_acc= 0.45636 time= 0.19500
Epoch: 0015 train_loss= 2.15608 train_acc= 0.44021 val_loss= 2.03288 val_acc= 0.45636 time= 0.16600
Epoch: 0016 train_loss= 2.05350 train_acc= 0.44872 val_loss= 1.94749 val_acc= 0.46708 time= 0.16905
Epoch: 0017 train_loss= 2.01820 train_acc= 0.45399 val_loss= 1.86371 val_acc= 0.49923 time= 0.16695
Epoch: 0018 train_loss= 1.90113 train_acc= 0.49855 val_loss= 1.78886 val_acc= 0.57121 time= 0.16700
Epoch: 0019 train_loss= 1.82124 train_acc= 0.56727 val_loss= 1.72547 val_acc= 0.64472 time= 0.20102
Epoch: 0020 train_loss= 1.76740 train_acc= 0.60231 val_loss= 1.67096 val_acc= 0.66769 time= 0.17000
Epoch: 0021 train_loss= 1.72028 train_acc= 0.64161 val_loss= 1.61985 val_acc= 0.67534 time= 0.16851
Epoch: 0022 train_loss= 1.66446 train_acc= 0.63701 val_loss= 1.56926 val_acc= 0.67688 time= 0.18700
Epoch: 0023 train_loss= 1.62667 train_acc= 0.64161 val_loss= 1.51871 val_acc= 0.68147 time= 0.16600
Epoch: 0024 train_loss= 1.54837 train_acc= 0.65300 val_loss= 1.47027 val_acc= 0.67841 time= 0.16800
Epoch: 0025 train_loss= 1.50887 train_acc= 0.65113 val_loss= 1.42531 val_acc= 0.68147 time= 0.17404
Epoch: 0026 train_loss= 1.49246 train_acc= 0.64875 val_loss= 1.38419 val_acc= 0.68760 time= 0.17397
Epoch: 0027 train_loss= 1.41171 train_acc= 0.65368 val_loss= 1.34784 val_acc= 0.69525 time= 0.17258
Epoch: 0028 train_loss= 1.38539 train_acc= 0.66219 val_loss= 1.31505 val_acc= 0.69832 time= 0.17336
Epoch: 0029 train_loss= 1.35217 train_acc= 0.67614 val_loss= 1.28512 val_acc= 0.70291 time= 0.17000
Epoch: 0030 train_loss= 1.33468 train_acc= 0.67409 val_loss= 1.25718 val_acc= 0.71057 time= 0.16600
Epoch: 0031 train_loss= 1.28715 train_acc= 0.68634 val_loss= 1.23073 val_acc= 0.71822 time= 0.18004
Epoch: 0032 train_loss= 1.25819 train_acc= 0.70828 val_loss= 1.20546 val_acc= 0.72282 time= 0.16596
Epoch: 0033 train_loss= 1.24644 train_acc= 0.70522 val_loss= 1.18108 val_acc= 0.72894 time= 0.18400
Epoch: 0034 train_loss= 1.21530 train_acc= 0.71985 val_loss= 1.15741 val_acc= 0.73354 time= 0.16700
Epoch: 0035 train_loss= 1.21385 train_acc= 0.72019 val_loss= 1.13438 val_acc= 0.73660 time= 0.17000
Epoch: 0036 train_loss= 1.17380 train_acc= 0.72784 val_loss= 1.11186 val_acc= 0.74119 time= 0.17101
Epoch: 0037 train_loss= 1.15949 train_acc= 0.73584 val_loss= 1.09008 val_acc= 0.73966 time= 0.18601
Epoch: 0038 train_loss= 1.12884 train_acc= 0.73686 val_loss= 1.06900 val_acc= 0.73966 time= 0.16704
Epoch: 0039 train_loss= 1.11278 train_acc= 0.73652 val_loss= 1.04881 val_acc= 0.74426 time= 0.17800
Epoch: 0040 train_loss= 1.07625 train_acc= 0.75030 val_loss= 1.02935 val_acc= 0.75191 time= 0.16800
Epoch: 0041 train_loss= 1.05737 train_acc= 0.75013 val_loss= 1.01004 val_acc= 0.75498 time= 0.16595
Epoch: 0042 train_loss= 1.05298 train_acc= 0.74758 val_loss= 0.99110 val_acc= 0.75957 time= 0.16900
Epoch: 0043 train_loss= 1.04214 train_acc= 0.75319 val_loss= 0.97255 val_acc= 0.76263 time= 0.18301
Epoch: 0044 train_loss= 1.01267 train_acc= 0.75778 val_loss= 0.95444 val_acc= 0.77489 time= 0.16904
Epoch: 0045 train_loss= 1.00488 train_acc= 0.76816 val_loss= 0.93675 val_acc= 0.78101 time= 0.16796
Epoch: 0046 train_loss= 0.97607 train_acc= 0.76918 val_loss= 0.91948 val_acc= 0.79939 time= 0.16832
Epoch: 0047 train_loss= 0.96093 train_acc= 0.78057 val_loss= 0.90275 val_acc= 0.80398 time= 0.16697
Epoch: 0048 train_loss= 0.94409 train_acc= 0.78006 val_loss= 0.88624 val_acc= 0.80704 time= 0.17300
Epoch: 0049 train_loss= 0.94003 train_acc= 0.78109 val_loss= 0.86984 val_acc= 0.81164 time= 0.19304
Epoch: 0050 train_loss= 0.90535 train_acc= 0.80014 val_loss= 0.85378 val_acc= 0.81164 time= 0.16999
Epoch: 0051 train_loss= 0.92424 train_acc= 0.78279 val_loss= 0.83798 val_acc= 0.81317 time= 0.18601
Epoch: 0052 train_loss= 0.87676 train_acc= 0.80388 val_loss= 0.82236 val_acc= 0.81317 time= 0.16900
Epoch: 0053 train_loss= 0.86521 train_acc= 0.79741 val_loss= 0.80713 val_acc= 0.81011 time= 0.16800
Epoch: 0054 train_loss= 0.84037 train_acc= 0.79980 val_loss= 0.79173 val_acc= 0.81317 time= 0.19400
Epoch: 0055 train_loss= 0.83089 train_acc= 0.80388 val_loss= 0.77641 val_acc= 0.81470 time= 0.16900
Epoch: 0056 train_loss= 0.82751 train_acc= 0.80932 val_loss= 0.76102 val_acc= 0.81776 time= 0.16600
Epoch: 0057 train_loss= 0.80004 train_acc= 0.80728 val_loss= 0.74593 val_acc= 0.82542 time= 0.16700
Epoch: 0058 train_loss= 0.80214 train_acc= 0.80677 val_loss= 0.73175 val_acc= 0.82848 time= 0.16973
Epoch: 0059 train_loss= 0.76993 train_acc= 0.81766 val_loss= 0.71801 val_acc= 0.83002 time= 0.16985
Epoch: 0060 train_loss= 0.75825 train_acc= 0.82174 val_loss= 0.70460 val_acc= 0.83155 time= 0.19600
Epoch: 0061 train_loss= 0.72976 train_acc= 0.83177 val_loss= 0.69073 val_acc= 0.83767 time= 0.16801
Epoch: 0062 train_loss= 0.73121 train_acc= 0.82344 val_loss= 0.67656 val_acc= 0.84227 time= 0.18399
Epoch: 0063 train_loss= 0.71297 train_acc= 0.82803 val_loss= 0.66279 val_acc= 0.84839 time= 0.16707
Epoch: 0064 train_loss= 0.70964 train_acc= 0.83262 val_loss= 0.64983 val_acc= 0.84992 time= 0.16702
Epoch: 0065 train_loss= 0.68814 train_acc= 0.83688 val_loss= 0.63806 val_acc= 0.85145 time= 0.16796
Epoch: 0066 train_loss= 0.67236 train_acc= 0.84453 val_loss= 0.62702 val_acc= 0.85452 time= 0.19664
Epoch: 0067 train_loss= 0.67529 train_acc= 0.83739 val_loss= 0.61635 val_acc= 0.85452 time= 0.16900
Epoch: 0068 train_loss= 0.65237 train_acc= 0.84317 val_loss= 0.60621 val_acc= 0.85605 time= 0.16703
Epoch: 0069 train_loss= 0.65569 train_acc= 0.84589 val_loss= 0.59583 val_acc= 0.86217 time= 0.16801
Epoch: 0070 train_loss= 0.62859 train_acc= 0.85729 val_loss= 0.58494 val_acc= 0.86217 time= 0.16599
Epoch: 0071 train_loss= 0.60560 train_acc= 0.85355 val_loss= 0.57360 val_acc= 0.87289 time= 0.17097
Epoch: 0072 train_loss= 0.61100 train_acc= 0.84980 val_loss= 0.56302 val_acc= 0.87443 time= 0.19503
Epoch: 0073 train_loss= 0.59730 train_acc= 0.85440 val_loss= 0.55137 val_acc= 0.87443 time= 0.17221
Epoch: 0074 train_loss= 0.58042 train_acc= 0.85031 val_loss= 0.54018 val_acc= 0.88055 time= 0.18827
Epoch: 0075 train_loss= 0.56613 train_acc= 0.86443 val_loss= 0.52960 val_acc= 0.88515 time= 0.16896
Epoch: 0076 train_loss= 0.56251 train_acc= 0.86290 val_loss= 0.51995 val_acc= 0.88668 time= 0.16896
Epoch: 0077 train_loss= 0.57236 train_acc= 0.85882 val_loss= 0.51020 val_acc= 0.88974 time= 0.17200
Epoch: 0078 train_loss= 0.55907 train_acc= 0.86835 val_loss= 0.50086 val_acc= 0.89433 time= 0.19400
Epoch: 0079 train_loss= 0.53702 train_acc= 0.86222 val_loss= 0.49252 val_acc= 0.89433 time= 0.17721
Epoch: 0080 train_loss= 0.53617 train_acc= 0.87073 val_loss= 0.48424 val_acc= 0.89433 time= 0.16701
Epoch: 0081 train_loss= 0.51816 train_acc= 0.87634 val_loss= 0.47682 val_acc= 0.89433 time= 0.17131
Epoch: 0082 train_loss= 0.51018 train_acc= 0.87566 val_loss= 0.46969 val_acc= 0.89433 time= 0.16997
Epoch: 0083 train_loss= 0.51461 train_acc= 0.86937 val_loss= 0.46294 val_acc= 0.89587 time= 0.19203
Epoch: 0084 train_loss= 0.49550 train_acc= 0.87192 val_loss= 0.45660 val_acc= 0.89740 time= 0.16800
Epoch: 0085 train_loss= 0.48206 train_acc= 0.88127 val_loss= 0.45029 val_acc= 0.89740 time= 0.17100
Epoch: 0086 train_loss= 0.47670 train_acc= 0.87923 val_loss= 0.44336 val_acc= 0.90046 time= 0.16700
Epoch: 0087 train_loss= 0.47629 train_acc= 0.88621 val_loss= 0.43672 val_acc= 0.89740 time= 0.16601
Epoch: 0088 train_loss= 0.46349 train_acc= 0.89063 val_loss= 0.43069 val_acc= 0.89740 time= 0.16999
Epoch: 0089 train_loss= 0.46035 train_acc= 0.88859 val_loss= 0.42469 val_acc= 0.90046 time= 0.19257
Epoch: 0090 train_loss= 0.45624 train_acc= 0.88059 val_loss= 0.41877 val_acc= 0.90199 time= 0.16905
Epoch: 0091 train_loss= 0.44812 train_acc= 0.88433 val_loss= 0.41284 val_acc= 0.90352 time= 0.18199
Epoch: 0092 train_loss= 0.44789 train_acc= 0.88893 val_loss= 0.40758 val_acc= 0.90352 time= 0.16801
Epoch: 0093 train_loss= 0.42461 train_acc= 0.89930 val_loss= 0.40169 val_acc= 0.90352 time= 0.16799
Epoch: 0094 train_loss= 0.43416 train_acc= 0.89012 val_loss= 0.39596 val_acc= 0.90046 time= 0.16900
Epoch: 0095 train_loss= 0.41084 train_acc= 0.90270 val_loss= 0.39036 val_acc= 0.90046 time= 0.17899
Epoch: 0096 train_loss= 0.41316 train_acc= 0.89896 val_loss= 0.38452 val_acc= 0.90199 time= 0.17519
Epoch: 0097 train_loss= 0.41191 train_acc= 0.89573 val_loss= 0.37949 val_acc= 0.90352 time= 0.18135
Epoch: 0098 train_loss= 0.40398 train_acc= 0.90134 val_loss= 0.37433 val_acc= 0.90505 time= 0.16803
Epoch: 0099 train_loss= 0.39853 train_acc= 0.90509 val_loss= 0.36935 val_acc= 0.90505 time= 0.16801
Epoch: 0100 train_loss= 0.39575 train_acc= 0.90185 val_loss= 0.36510 val_acc= 0.90352 time= 0.16899
Epoch: 0101 train_loss= 0.38064 train_acc= 0.91104 val_loss= 0.36179 val_acc= 0.90658 time= 0.19728
Epoch: 0102 train_loss= 0.37436 train_acc= 0.90185 val_loss= 0.35901 val_acc= 0.90505 time= 0.17800
Epoch: 0103 train_loss= 0.35142 train_acc= 0.91597 val_loss= 0.35655 val_acc= 0.90046 time= 0.16600
Epoch: 0104 train_loss= 0.36940 train_acc= 0.90968 val_loss= 0.35427 val_acc= 0.90199 time= 0.17300
Epoch: 0105 train_loss= 0.37091 train_acc= 0.91121 val_loss= 0.35205 val_acc= 0.90199 time= 0.16988
Epoch: 0106 train_loss= 0.36248 train_acc= 0.91155 val_loss= 0.35072 val_acc= 0.90199 time= 0.19605
Epoch: 0107 train_loss= 0.34951 train_acc= 0.91529 val_loss= 0.35014 val_acc= 0.90199 time= 0.16700
Epoch: 0108 train_loss= 0.34657 train_acc= 0.91597 val_loss= 0.34851 val_acc= 0.90352 time= 0.16760
Epoch: 0109 train_loss= 0.33318 train_acc= 0.91971 val_loss= 0.34414 val_acc= 0.90352 time= 0.16705
Epoch: 0110 train_loss= 0.35884 train_acc= 0.91478 val_loss= 0.33924 val_acc= 0.90658 time= 0.16795
Epoch: 0111 train_loss= 0.33637 train_acc= 0.92056 val_loss= 0.33423 val_acc= 0.90965 time= 0.17041
Epoch: 0112 train_loss= 0.34579 train_acc= 0.91886 val_loss= 0.32858 val_acc= 0.90812 time= 0.19741
Epoch: 0113 train_loss= 0.33668 train_acc= 0.92022 val_loss= 0.32399 val_acc= 0.90505 time= 0.17003
Epoch: 0114 train_loss= 0.32913 train_acc= 0.92244 val_loss= 0.32047 val_acc= 0.90812 time= 0.17914
Epoch: 0115 train_loss= 0.33388 train_acc= 0.91699 val_loss= 0.31693 val_acc= 0.91271 time= 0.16697
Epoch: 0116 train_loss= 0.31545 train_acc= 0.92295 val_loss= 0.31248 val_acc= 0.91271 time= 0.16604
Epoch: 0117 train_loss= 0.33239 train_acc= 0.91257 val_loss= 0.30843 val_acc= 0.91884 time= 0.17074
Epoch: 0118 train_loss= 0.29798 train_acc= 0.92856 val_loss= 0.30562 val_acc= 0.92190 time= 0.19403
Epoch: 0119 train_loss= 0.30914 train_acc= 0.92227 val_loss= 0.30378 val_acc= 0.91577 time= 0.17800
Epoch: 0120 train_loss= 0.30566 train_acc= 0.91869 val_loss= 0.30311 val_acc= 0.91577 time= 0.17003
Epoch: 0121 train_loss= 0.31219 train_acc= 0.92550 val_loss= 0.30266 val_acc= 0.91424 time= 0.16800
Epoch: 0122 train_loss= 0.28557 train_acc= 0.92992 val_loss= 0.30269 val_acc= 0.91730 time= 0.16700
Epoch: 0123 train_loss= 0.29819 train_acc= 0.92958 val_loss= 0.30203 val_acc= 0.91884 time= 0.18297
Epoch: 0124 train_loss= 0.30225 train_acc= 0.92941 val_loss= 0.30024 val_acc= 0.91884 time= 0.16721
Epoch: 0125 train_loss= 0.28930 train_acc= 0.92873 val_loss= 0.29740 val_acc= 0.92037 time= 0.18300
Epoch: 0126 train_loss= 0.29538 train_acc= 0.92686 val_loss= 0.29371 val_acc= 0.92037 time= 0.16600
Epoch: 0127 train_loss= 0.28123 train_acc= 0.93315 val_loss= 0.29051 val_acc= 0.92190 time= 0.17200
Epoch: 0128 train_loss= 0.28249 train_acc= 0.93400 val_loss= 0.28749 val_acc= 0.92343 time= 0.17004
Epoch: 0129 train_loss= 0.29471 train_acc= 0.92397 val_loss= 0.28603 val_acc= 0.92190 time= 0.19500
Epoch: 0130 train_loss= 0.28188 train_acc= 0.92652 val_loss= 0.28583 val_acc= 0.91884 time= 0.16700
Epoch: 0131 train_loss= 0.27056 train_acc= 0.92975 val_loss= 0.28729 val_acc= 0.92037 time= 0.16700
Epoch: 0132 train_loss= 0.25814 train_acc= 0.93519 val_loss= 0.28670 val_acc= 0.91884 time= 0.16600
Epoch: 0133 train_loss= 0.26245 train_acc= 0.92924 val_loss= 0.28342 val_acc= 0.91577 time= 0.16701
Epoch: 0134 train_loss= 0.26136 train_acc= 0.93332 val_loss= 0.27987 val_acc= 0.91577 time= 0.17300
Epoch: 0135 train_loss= 0.26664 train_acc= 0.93281 val_loss= 0.27719 val_acc= 0.92190 time= 0.19800
Epoch: 0136 train_loss= 0.25457 train_acc= 0.93179 val_loss= 0.27602 val_acc= 0.92496 time= 0.16800
Epoch: 0137 train_loss= 0.24479 train_acc= 0.93689 val_loss= 0.27454 val_acc= 0.92649 time= 0.18100
Epoch: 0138 train_loss= 0.25655 train_acc= 0.93332 val_loss= 0.27302 val_acc= 0.92496 time= 0.16600
Epoch: 0139 train_loss= 0.24821 train_acc= 0.93706 val_loss= 0.27157 val_acc= 0.92496 time= 0.16800
Epoch: 0140 train_loss= 0.24962 train_acc= 0.93536 val_loss= 0.27015 val_acc= 0.92343 time= 0.17200
Epoch: 0141 train_loss= 0.22895 train_acc= 0.94200 val_loss= 0.26907 val_acc= 0.92343 time= 0.19297
Epoch: 0142 train_loss= 0.24458 train_acc= 0.93536 val_loss= 0.26843 val_acc= 0.92037 time= 0.18454
Epoch: 0143 train_loss= 0.24375 train_acc= 0.93587 val_loss= 0.26742 val_acc= 0.92190 time= 0.17000
Epoch: 0144 train_loss= 0.24325 train_acc= 0.94200 val_loss= 0.26584 val_acc= 0.92190 time= 0.16701
Epoch: 0145 train_loss= 0.24025 train_acc= 0.94234 val_loss= 0.26419 val_acc= 0.92190 time= 0.16799
Epoch: 0146 train_loss= 0.22255 train_acc= 0.94336 val_loss= 0.26378 val_acc= 0.92343 time= 0.17040
Epoch: 0147 train_loss= 0.22288 train_acc= 0.94047 val_loss= 0.26421 val_acc= 0.92343 time= 0.16700
Epoch: 0148 train_loss= 0.21421 train_acc= 0.94676 val_loss= 0.26375 val_acc= 0.92496 time= 0.18700
Epoch: 0149 train_loss= 0.22855 train_acc= 0.94438 val_loss= 0.26435 val_acc= 0.92649 time= 0.16697
Epoch: 0150 train_loss= 0.22055 train_acc= 0.94438 val_loss= 0.26336 val_acc= 0.92496 time= 0.17158
Epoch: 0151 train_loss= 0.21953 train_acc= 0.94047 val_loss= 0.26239 val_acc= 0.92496 time= 0.17003
Epoch: 0152 train_loss= 0.21350 train_acc= 0.94676 val_loss= 0.26130 val_acc= 0.92649 time= 0.19297
Epoch: 0153 train_loss= 0.21940 train_acc= 0.94404 val_loss= 0.26056 val_acc= 0.92496 time= 0.16900
Epoch: 0154 train_loss= 0.21370 train_acc= 0.94693 val_loss= 0.25951 val_acc= 0.92649 time= 0.18400
Epoch: 0155 train_loss= 0.20143 train_acc= 0.95135 val_loss= 0.25880 val_acc= 0.92649 time= 0.17032
Epoch: 0156 train_loss= 0.21543 train_acc= 0.94166 val_loss= 0.25740 val_acc= 0.92956 time= 0.16700
Epoch: 0157 train_loss= 0.21376 train_acc= 0.94489 val_loss= 0.25502 val_acc= 0.93109 time= 0.17197
Epoch: 0158 train_loss= 0.21122 train_acc= 0.94744 val_loss= 0.25327 val_acc= 0.92802 time= 0.20060
Epoch: 0159 train_loss= 0.20267 train_acc= 0.94744 val_loss= 0.25251 val_acc= 0.93109 time= 0.17100
Epoch: 0160 train_loss= 0.19148 train_acc= 0.95373 val_loss= 0.25318 val_acc= 0.93262 time= 0.16600
Epoch: 0161 train_loss= 0.19026 train_acc= 0.95169 val_loss= 0.25396 val_acc= 0.92956 time= 0.16797
Epoch: 0162 train_loss= 0.19253 train_acc= 0.95271 val_loss= 0.25321 val_acc= 0.92956 time= 0.16603
Epoch: 0163 train_loss= 0.18670 train_acc= 0.95288 val_loss= 0.25147 val_acc= 0.93262 time= 0.18400
Epoch: 0164 train_loss= 0.18968 train_acc= 0.95339 val_loss= 0.24865 val_acc= 0.92956 time= 0.16500
Epoch: 0165 train_loss= 0.19211 train_acc= 0.95237 val_loss= 0.24558 val_acc= 0.92956 time= 0.18769
Epoch: 0166 train_loss= 0.18729 train_acc= 0.95373 val_loss= 0.24391 val_acc= 0.92496 time= 0.16999
Epoch: 0167 train_loss= 0.19177 train_acc= 0.95050 val_loss= 0.24444 val_acc= 0.92649 time= 0.16701
Epoch: 0168 train_loss= 0.19065 train_acc= 0.95186 val_loss= 0.24550 val_acc= 0.92649 time= 0.16699
Epoch: 0169 train_loss= 0.18812 train_acc= 0.95254 val_loss= 0.24661 val_acc= 0.92649 time= 0.17101
Epoch: 0170 train_loss= 0.19222 train_acc= 0.94846 val_loss= 0.24706 val_acc= 0.92802 time= 0.16600
Epoch: 0171 train_loss= 0.19194 train_acc= 0.94829 val_loss= 0.24689 val_acc= 0.92802 time= 0.18200
Epoch: 0172 train_loss= 0.19266 train_acc= 0.95305 val_loss= 0.24584 val_acc= 0.92956 time= 0.16900
Epoch: 0173 train_loss= 0.18764 train_acc= 0.95203 val_loss= 0.24519 val_acc= 0.93109 time= 0.17139
Epoch: 0174 train_loss= 0.18217 train_acc= 0.95203 val_loss= 0.24601 val_acc= 0.92956 time= 0.17003
Early stopping...
Optimization Finished!
Test set results: cost= 0.28529 accuracy= 0.92835 time= 0.10000
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     0.6667    0.3333    0.4444         6
           2     0.0000    0.0000    0.0000         1
           3     0.8022    0.9733    0.8795        75
           4     1.0000    1.0000    1.0000         9
           5     0.7830    0.9540    0.8601        87
           6     0.9200    0.9200    0.9200        25
           7     0.6875    0.8462    0.7586        13
           8     0.9091    0.9091    0.9091        11
           9     1.0000    0.1111    0.2000         9
          10     0.9048    0.5278    0.6667        36
          11     1.0000    0.9167    0.9565        12
          12     0.7778    0.9835    0.8686       121
          13     0.7778    0.7368    0.7568        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.2500    0.4000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.3333    0.5000         9
          21     0.8261    0.9500    0.8837        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.6190    0.7647    0.6842        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.5000    0.6667        12
          28     0.9000    0.8182    0.8571        11
          29     0.9642    0.9684    0.9663       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.6667    0.8000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8947    0.8395    0.8662        81
          36     1.0000    0.2500    0.4000        12
          37     1.0000    0.5000    0.6667         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9926    0.9849      1083
          40     1.0000    0.2000    0.3333         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.8889    0.6667    0.7619        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.6842    0.8667    0.7647        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9283      2568
   macro avg     0.7015    0.5642    0.5943      2568
weighted avg     0.9263    0.9283    0.9198      2568

Macro average Test Precision, Recall and F1-Score...
(0.7015467696241439, 0.5641857933225964, 0.5942981424766907, None)
Micro average Test Precision, Recall and F1-Score...
(0.9283489096573209, 0.9283489096573209, 0.9283489096573209, None)
embeddings:
8892 6532 2568
[[-0.07281242  1.3013958  -0.40697777 ... -0.2591187  -0.200192
  -0.16289535]
 [ 0.19132549  0.53355247  0.52328795 ...  0.14511625 -0.0740217
   0.12354495]
 [ 0.03554833  0.24234481  0.55063766 ...  0.5121962  -0.0057169
  -0.01217508]
 ...
 [ 0.21796523  0.21085861  0.46560857 ...  0.25265074  0.02767488
  -0.00994965]
 [ 0.0520756   0.24201894  0.15429676 ...  0.21465482  0.06035643
   0.03409491]
 [ 0.18061325  0.2850018   0.1192622  ...  0.04703905  0.24082205
   0.27424437]]

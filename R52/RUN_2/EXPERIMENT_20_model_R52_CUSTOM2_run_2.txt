(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95135 train_acc= 0.00272 val_loss= 3.40302 val_acc= 0.56508 time= 0.47080
Epoch: 0002 train_loss= 3.40533 train_acc= 0.56098 val_loss= 2.42693 val_acc= 0.60949 time= 0.17199
Epoch: 0003 train_loss= 2.41193 train_acc= 0.60895 val_loss= 2.22486 val_acc= 0.61256 time= 0.16602
Epoch: 0004 train_loss= 2.22301 train_acc= 0.60844 val_loss= 2.02175 val_acc= 0.50536 time= 0.18998
Epoch: 0005 train_loss= 2.04427 train_acc= 0.48427 val_loss= 1.63975 val_acc= 0.66616 time= 0.16897
Epoch: 0006 train_loss= 1.67208 train_acc= 0.65198 val_loss= 1.47201 val_acc= 0.68913 time= 0.18901
Epoch: 0007 train_loss= 1.50687 train_acc= 0.67103 val_loss= 1.36139 val_acc= 0.71210 time= 0.16803
Epoch: 0008 train_loss= 1.39260 train_acc= 0.70930 val_loss= 1.23630 val_acc= 0.71363 time= 0.16801
Epoch: 0009 train_loss= 1.26594 train_acc= 0.70420 val_loss= 1.13807 val_acc= 0.72282 time= 0.16699
Epoch: 0010 train_loss= 1.15395 train_acc= 0.72359 val_loss= 1.06297 val_acc= 0.74119 time= 0.19505
Epoch: 0011 train_loss= 1.07604 train_acc= 0.73907 val_loss= 1.01066 val_acc= 0.74426 time= 0.16797
Epoch: 0012 train_loss= 1.01098 train_acc= 0.75132 val_loss= 0.96283 val_acc= 0.74732 time= 0.16703
Epoch: 0013 train_loss= 0.96464 train_acc= 0.75982 val_loss= 0.90429 val_acc= 0.76263 time= 0.17026
Epoch: 0014 train_loss= 0.90828 train_acc= 0.77156 val_loss= 0.84190 val_acc= 0.78560 time= 0.17054
Epoch: 0015 train_loss= 0.83547 train_acc= 0.79758 val_loss= 0.78181 val_acc= 0.81470 time= 0.19703
Epoch: 0016 train_loss= 0.78030 train_acc= 0.81647 val_loss= 0.73189 val_acc= 0.83920 time= 0.16700
Epoch: 0017 train_loss= 0.73513 train_acc= 0.83484 val_loss= 0.69237 val_acc= 0.85605 time= 0.16812
Epoch: 0018 train_loss= 0.69999 train_acc= 0.84130 val_loss= 0.65621 val_acc= 0.84992 time= 0.18497
Epoch: 0019 train_loss= 0.65368 train_acc= 0.85355 val_loss= 0.62083 val_acc= 0.85452 time= 0.16800
Epoch: 0020 train_loss= 0.59937 train_acc= 0.86205 val_loss= 0.58946 val_acc= 0.85145 time= 0.16900
Epoch: 0021 train_loss= 0.55608 train_acc= 0.86783 val_loss= 0.56248 val_acc= 0.85911 time= 0.17454
Epoch: 0022 train_loss= 0.52200 train_acc= 0.87328 val_loss= 0.53848 val_acc= 0.86983 time= 0.19703
Epoch: 0023 train_loss= 0.48584 train_acc= 0.88621 val_loss= 0.51181 val_acc= 0.87136 time= 0.17900
Epoch: 0024 train_loss= 0.46193 train_acc= 0.89369 val_loss= 0.48295 val_acc= 0.87596 time= 0.16700
Epoch: 0025 train_loss= 0.43615 train_acc= 0.89726 val_loss= 0.45634 val_acc= 0.87596 time= 0.16800
Epoch: 0026 train_loss= 0.39382 train_acc= 0.90526 val_loss= 0.43480 val_acc= 0.87902 time= 0.16600
Epoch: 0027 train_loss= 0.36377 train_acc= 0.91087 val_loss= 0.42013 val_acc= 0.88515 time= 0.19500
Epoch: 0028 train_loss= 0.33231 train_acc= 0.91869 val_loss= 0.40704 val_acc= 0.89280 time= 0.16772
Epoch: 0029 train_loss= 0.31581 train_acc= 0.92278 val_loss= 0.39040 val_acc= 0.89740 time= 0.17500
Epoch: 0030 train_loss= 0.28389 train_acc= 0.93111 val_loss= 0.37315 val_acc= 0.90199 time= 0.16903
Epoch: 0031 train_loss= 0.26615 train_acc= 0.93706 val_loss= 0.35740 val_acc= 0.90505 time= 0.16700
Epoch: 0032 train_loss= 0.24439 train_acc= 0.94285 val_loss= 0.34112 val_acc= 0.90965 time= 0.16799
Epoch: 0033 train_loss= 0.23441 train_acc= 0.94302 val_loss= 0.32567 val_acc= 0.91424 time= 0.19701
Epoch: 0034 train_loss= 0.21584 train_acc= 0.94761 val_loss= 0.31809 val_acc= 0.91424 time= 0.16999
Epoch: 0035 train_loss= 0.18908 train_acc= 0.95288 val_loss= 0.31563 val_acc= 0.91271 time= 0.17900
Epoch: 0036 train_loss= 0.17801 train_acc= 0.95577 val_loss= 0.31629 val_acc= 0.91118 time= 0.17097
Epoch: 0037 train_loss= 0.16175 train_acc= 0.95952 val_loss= 0.31501 val_acc= 0.91271 time= 0.16956
Epoch: 0038 train_loss= 0.14686 train_acc= 0.96139 val_loss= 0.31112 val_acc= 0.92037 time= 0.17103
Epoch: 0039 train_loss= 0.13986 train_acc= 0.96734 val_loss= 0.30045 val_acc= 0.92343 time= 0.19320
Epoch: 0040 train_loss= 0.12427 train_acc= 0.96887 val_loss= 0.29584 val_acc= 0.92037 time= 0.16800
Epoch: 0041 train_loss= 0.11855 train_acc= 0.97057 val_loss= 0.29345 val_acc= 0.92343 time= 0.16705
Epoch: 0042 train_loss= 0.10904 train_acc= 0.97074 val_loss= 0.28469 val_acc= 0.92956 time= 0.16807
Epoch: 0043 train_loss= 0.09167 train_acc= 0.97789 val_loss= 0.27688 val_acc= 0.93109 time= 0.16700
Epoch: 0044 train_loss= 0.09320 train_acc= 0.97602 val_loss= 0.27687 val_acc= 0.93262 time= 0.17103
Epoch: 0045 train_loss= 0.08177 train_acc= 0.97857 val_loss= 0.28046 val_acc= 0.92956 time= 0.17016
Epoch: 0046 train_loss= 0.08052 train_acc= 0.98027 val_loss= 0.27468 val_acc= 0.92956 time= 0.18099
Epoch: 0047 train_loss= 0.07534 train_acc= 0.98078 val_loss= 0.27470 val_acc= 0.92956 time= 0.16997
Epoch: 0048 train_loss= 0.06908 train_acc= 0.98282 val_loss= 0.27131 val_acc= 0.93109 time= 0.16804
Epoch: 0049 train_loss= 0.06118 train_acc= 0.98605 val_loss= 0.27069 val_acc= 0.93568 time= 0.16699
Epoch: 0050 train_loss= 0.05909 train_acc= 0.98367 val_loss= 0.27236 val_acc= 0.92956 time= 0.19300
Epoch: 0051 train_loss= 0.05547 train_acc= 0.98537 val_loss= 0.27544 val_acc= 0.93262 time= 0.17107
Early stopping...
Optimization Finished!
Test set results: cost= 0.30067 accuracy= 0.93146 time= 0.07497
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.3333    0.1667    0.2222         6
           2     0.5000    1.0000    0.6667         1
           3     0.7976    0.8933    0.8428        75
           4     1.0000    1.0000    1.0000         9
           5     0.7670    0.9080    0.8316        87
           6     0.9600    0.9600    0.9600        25
           7     0.6667    0.9231    0.7742        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.4444    0.6154         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    1.0000    1.0000        12
          12     0.8521    1.0000    0.9202       121
          13     0.9231    0.6316    0.7500        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.8889    0.8000    0.8421        10
          19     1.0000    1.0000    1.0000         2
          20     0.5714    0.4444    0.5000         9
          21     0.8636    0.9500    0.9048        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7857    0.6471    0.7097        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9167    0.9167    0.9167        12
          28     1.0000    0.8182    0.9000        11
          29     0.9629    0.9684    0.9656       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8451    0.7407    0.7895        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9826    0.9926    0.9876      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.6667    0.6667    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7647    0.8667    0.8125        15
          48     0.8182    1.0000    0.9000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9315      2568
   macro avg     0.7029    0.6403    0.6510      2568
weighted avg     0.9273    0.9315    0.9256      2568

Macro average Test Precision, Recall and F1-Score...
(0.7028623173072907, 0.640348073935498, 0.6509592581969699, None)
Micro average Test Precision, Recall and F1-Score...
(0.9314641744548287, 0.9314641744548287, 0.9314641744548287, None)
embeddings:
8892 6532 2568
[[-0.09565009 -0.22432628 -0.5154778  ... -0.35119426 -0.72062147
  -0.65298325]
 [-0.09202954  0.69762546 -0.24330552 ... -0.26909414 -0.18507668
  -0.40229955]
 [ 0.17733008  0.37215    -0.42457098 ... -0.34327123 -0.3765785
  -0.4976945 ]
 ...
 [ 0.0666363   0.5423733  -0.211747   ... -0.12773283 -0.17338672
  -0.2681683 ]
 [ 0.19792476  0.11542894 -0.18658122 ... -0.1270832  -0.1415973
  -0.27651048]
 [ 0.53680694  0.44713774  0.0711996  ...  0.16660053  0.15627842
  -0.3597028 ]]

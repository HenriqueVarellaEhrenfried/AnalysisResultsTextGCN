(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95142 train_acc= 0.00221 val_loss= 3.88161 val_acc= 0.66922 time= 0.53011
Epoch: 0002 train_loss= 3.88184 train_acc= 0.64586 val_loss= 3.73341 val_acc= 0.66003 time= 0.23517
Epoch: 0003 train_loss= 3.73759 train_acc= 0.63412 val_loss= 3.49858 val_acc= 0.65237 time= 0.20700
Epoch: 0004 train_loss= 3.50505 train_acc= 0.62613 val_loss= 3.18855 val_acc= 0.64778 time= 0.21000
Epoch: 0005 train_loss= 3.19347 train_acc= 0.62783 val_loss= 2.84483 val_acc= 0.64931 time= 0.21402
Epoch: 0006 train_loss= 2.86096 train_acc= 0.62528 val_loss= 2.52857 val_acc= 0.64931 time= 0.24322
Epoch: 0007 train_loss= 2.52523 train_acc= 0.63242 val_loss= 2.30792 val_acc= 0.66462 time= 0.21201
Epoch: 0008 train_loss= 2.30371 train_acc= 0.63803 val_loss= 2.20204 val_acc= 0.65391 time= 0.21004
Epoch: 0009 train_loss= 2.19728 train_acc= 0.63293 val_loss= 2.15500 val_acc= 0.51761 time= 0.21102
Epoch: 0010 train_loss= 2.16347 train_acc= 0.50621 val_loss= 2.10975 val_acc= 0.47320 time= 0.21195
Epoch: 0011 train_loss= 2.11744 train_acc= 0.44753 val_loss= 2.03646 val_acc= 0.47014 time= 0.20912
Epoch: 0012 train_loss= 2.05336 train_acc= 0.44429 val_loss= 1.93361 val_acc= 0.48851 time= 0.21355
Epoch: 0013 train_loss= 1.96000 train_acc= 0.46675 val_loss= 1.82062 val_acc= 0.55130 time= 0.21104
Epoch: 0014 train_loss= 1.84188 train_acc= 0.54601 val_loss= 1.72149 val_acc= 0.63553 time= 0.21200
Epoch: 0015 train_loss= 1.75253 train_acc= 0.61898 val_loss= 1.64750 val_acc= 0.66462 time= 0.21400
Epoch: 0016 train_loss= 1.67787 train_acc= 0.64688 val_loss= 1.59017 val_acc= 0.67381 time= 0.21501
Epoch: 0017 train_loss= 1.62148 train_acc= 0.65164 val_loss= 1.53469 val_acc= 0.67841 time= 0.21204
Epoch: 0018 train_loss= 1.56516 train_acc= 0.65300 val_loss= 1.47534 val_acc= 0.67075 time= 0.21354
Epoch: 0019 train_loss= 1.50306 train_acc= 0.65708 val_loss= 1.41490 val_acc= 0.67841 time= 0.21400
Epoch: 0020 train_loss= 1.44594 train_acc= 0.66032 val_loss= 1.35782 val_acc= 0.67994 time= 0.23700
Epoch: 0021 train_loss= 1.38297 train_acc= 0.66882 val_loss= 1.30662 val_acc= 0.69066 time= 0.21001
Epoch: 0022 train_loss= 1.33409 train_acc= 0.67392 val_loss= 1.26176 val_acc= 0.69985 time= 0.20800
Epoch: 0023 train_loss= 1.28648 train_acc= 0.68447 val_loss= 1.22223 val_acc= 0.71057 time= 0.20999
Epoch: 0024 train_loss= 1.24386 train_acc= 0.70012 val_loss= 1.18659 val_acc= 0.71669 time= 0.24203
Epoch: 0025 train_loss= 1.21129 train_acc= 0.70828 val_loss= 1.15351 val_acc= 0.72588 time= 0.22200
Epoch: 0026 train_loss= 1.17628 train_acc= 0.72767 val_loss= 1.12172 val_acc= 0.73354 time= 0.20901
Epoch: 0027 train_loss= 1.13758 train_acc= 0.73924 val_loss= 1.09022 val_acc= 0.74119 time= 0.21199
Epoch: 0028 train_loss= 1.10540 train_acc= 0.75336 val_loss= 1.05845 val_acc= 0.75651 time= 0.21497
Epoch: 0029 train_loss= 1.07181 train_acc= 0.75897 val_loss= 1.02635 val_acc= 0.76417 time= 0.23903
Epoch: 0030 train_loss= 1.04074 train_acc= 0.76663 val_loss= 0.99451 val_acc= 0.77029 time= 0.21204
Epoch: 0031 train_loss= 1.00533 train_acc= 0.77377 val_loss= 0.96353 val_acc= 0.77795 time= 0.21603
Epoch: 0032 train_loss= 0.97161 train_acc= 0.78330 val_loss= 0.93399 val_acc= 0.78714 time= 0.21101
Epoch: 0033 train_loss= 0.94226 train_acc= 0.78653 val_loss= 0.90605 val_acc= 0.79173 time= 0.23700
Epoch: 0034 train_loss= 0.91444 train_acc= 0.79333 val_loss= 0.87930 val_acc= 0.79632 time= 0.21200
Epoch: 0035 train_loss= 0.88810 train_acc= 0.79792 val_loss= 0.85307 val_acc= 0.80245 time= 0.21000
Epoch: 0036 train_loss= 0.85979 train_acc= 0.80898 val_loss= 0.82723 val_acc= 0.81011 time= 0.21054
Epoch: 0037 train_loss= 0.83201 train_acc= 0.82123 val_loss= 0.80167 val_acc= 0.82083 time= 0.21603
Epoch: 0038 train_loss= 0.80891 train_acc= 0.82769 val_loss= 0.77668 val_acc= 0.83155 time= 0.23400
Epoch: 0039 train_loss= 0.77821 train_acc= 0.83433 val_loss= 0.75255 val_acc= 0.83767 time= 0.20900
Epoch: 0040 train_loss= 0.75550 train_acc= 0.83960 val_loss= 0.72922 val_acc= 0.84686 time= 0.20997
Epoch: 0041 train_loss= 0.72806 train_acc= 0.84742 val_loss= 0.70645 val_acc= 0.85145 time= 0.21203
Epoch: 0042 train_loss= 0.70662 train_acc= 0.85065 val_loss= 0.68436 val_acc= 0.85299 time= 0.21257
Epoch: 0043 train_loss= 0.67958 train_acc= 0.85678 val_loss= 0.66286 val_acc= 0.85758 time= 0.22600
Epoch: 0044 train_loss= 0.65118 train_acc= 0.86154 val_loss= 0.64172 val_acc= 0.86064 time= 0.20999
Epoch: 0045 train_loss= 0.63467 train_acc= 0.86392 val_loss= 0.62107 val_acc= 0.86524 time= 0.21400
Epoch: 0046 train_loss= 0.60905 train_acc= 0.86800 val_loss= 0.60105 val_acc= 0.86524 time= 0.20901
Epoch: 0047 train_loss= 0.59015 train_acc= 0.87243 val_loss= 0.58153 val_acc= 0.86830 time= 0.23599
Epoch: 0048 train_loss= 0.56837 train_acc= 0.87566 val_loss= 0.56271 val_acc= 0.86983 time= 0.22600
Epoch: 0049 train_loss= 0.54712 train_acc= 0.88161 val_loss= 0.54502 val_acc= 0.87443 time= 0.21458
Epoch: 0050 train_loss= 0.52872 train_acc= 0.88263 val_loss= 0.52844 val_acc= 0.87749 time= 0.21200
Epoch: 0051 train_loss= 0.50419 train_acc= 0.88621 val_loss= 0.51278 val_acc= 0.87749 time= 0.22100
Epoch: 0052 train_loss= 0.49117 train_acc= 0.89029 val_loss= 0.49771 val_acc= 0.88055 time= 0.22100
Epoch: 0053 train_loss= 0.47293 train_acc= 0.89318 val_loss= 0.48322 val_acc= 0.88208 time= 0.21202
Epoch: 0054 train_loss= 0.45191 train_acc= 0.89794 val_loss= 0.46916 val_acc= 0.88361 time= 0.21495
Epoch: 0055 train_loss= 0.43854 train_acc= 0.89658 val_loss= 0.45516 val_acc= 0.88208 time= 0.21303
Epoch: 0056 train_loss= 0.42220 train_acc= 0.90611 val_loss= 0.44083 val_acc= 0.88515 time= 0.23901
Epoch: 0057 train_loss= 0.40604 train_acc= 0.90594 val_loss= 0.42674 val_acc= 0.89280 time= 0.20996
Epoch: 0058 train_loss= 0.39093 train_acc= 0.91325 val_loss= 0.41316 val_acc= 0.89587 time= 0.21079
Epoch: 0059 train_loss= 0.37159 train_acc= 0.92056 val_loss= 0.40074 val_acc= 0.90199 time= 0.21000
Epoch: 0060 train_loss= 0.36326 train_acc= 0.92108 val_loss= 0.38977 val_acc= 0.90352 time= 0.24379
Epoch: 0061 train_loss= 0.34632 train_acc= 0.92601 val_loss= 0.38001 val_acc= 0.90352 time= 0.21207
Epoch: 0062 train_loss= 0.33696 train_acc= 0.92856 val_loss= 0.37112 val_acc= 0.90199 time= 0.21100
Epoch: 0063 train_loss= 0.32379 train_acc= 0.93043 val_loss= 0.36307 val_acc= 0.90199 time= 0.21201
Epoch: 0064 train_loss= 0.30732 train_acc= 0.93315 val_loss= 0.35544 val_acc= 0.90046 time= 0.20999
Epoch: 0065 train_loss= 0.30146 train_acc= 0.93366 val_loss= 0.34842 val_acc= 0.90352 time= 0.21000
Epoch: 0066 train_loss= 0.28322 train_acc= 0.94319 val_loss= 0.34107 val_acc= 0.90199 time= 0.22300
Epoch: 0067 train_loss= 0.28074 train_acc= 0.94115 val_loss= 0.33372 val_acc= 0.90658 time= 0.21161
Epoch: 0068 train_loss= 0.26970 train_acc= 0.94438 val_loss= 0.32707 val_acc= 0.91118 time= 0.21100
Epoch: 0069 train_loss= 0.25803 train_acc= 0.94591 val_loss= 0.32082 val_acc= 0.91271 time= 0.21100
Epoch: 0070 train_loss= 0.25089 train_acc= 0.94846 val_loss= 0.31435 val_acc= 0.91577 time= 0.21414
Epoch: 0071 train_loss= 0.23902 train_acc= 0.95169 val_loss= 0.30812 val_acc= 0.91884 time= 0.20904
Epoch: 0072 train_loss= 0.22907 train_acc= 0.95271 val_loss= 0.30217 val_acc= 0.92037 time= 0.21096
Epoch: 0073 train_loss= 0.21832 train_acc= 0.95577 val_loss= 0.29617 val_acc= 0.91884 time= 0.21309
Epoch: 0074 train_loss= 0.21141 train_acc= 0.95407 val_loss= 0.29094 val_acc= 0.92037 time= 0.21705
Epoch: 0075 train_loss= 0.20524 train_acc= 0.95816 val_loss= 0.28645 val_acc= 0.92343 time= 0.22600
Epoch: 0076 train_loss= 0.19409 train_acc= 0.96071 val_loss= 0.28257 val_acc= 0.92649 time= 0.21104
Epoch: 0077 train_loss= 0.19037 train_acc= 0.96173 val_loss= 0.27894 val_acc= 0.92649 time= 0.21251
Epoch: 0078 train_loss= 0.18657 train_acc= 0.96258 val_loss= 0.27510 val_acc= 0.92343 time= 0.21200
Epoch: 0079 train_loss= 0.17707 train_acc= 0.96615 val_loss= 0.27187 val_acc= 0.92190 time= 0.21399
Epoch: 0080 train_loss= 0.17144 train_acc= 0.96632 val_loss= 0.26902 val_acc= 0.92343 time= 0.22500
Epoch: 0081 train_loss= 0.16436 train_acc= 0.96632 val_loss= 0.26549 val_acc= 0.92343 time= 0.21403
Epoch: 0082 train_loss= 0.15921 train_acc= 0.96751 val_loss= 0.26272 val_acc= 0.92802 time= 0.21001
Epoch: 0083 train_loss= 0.15397 train_acc= 0.96768 val_loss= 0.26002 val_acc= 0.92649 time= 0.21199
Epoch: 0084 train_loss= 0.14546 train_acc= 0.97278 val_loss= 0.25701 val_acc= 0.92649 time= 0.23897
Epoch: 0085 train_loss= 0.14134 train_acc= 0.97278 val_loss= 0.25401 val_acc= 0.93109 time= 0.21200
Epoch: 0086 train_loss= 0.13280 train_acc= 0.97483 val_loss= 0.25063 val_acc= 0.93262 time= 0.21601
Epoch: 0087 train_loss= 0.13077 train_acc= 0.97329 val_loss= 0.24806 val_acc= 0.93109 time= 0.21119
Epoch: 0088 train_loss= 0.12553 train_acc= 0.97738 val_loss= 0.24611 val_acc= 0.93109 time= 0.23599
Epoch: 0089 train_loss= 0.11886 train_acc= 0.97993 val_loss= 0.24507 val_acc= 0.93109 time= 0.21101
Epoch: 0090 train_loss= 0.11774 train_acc= 0.97670 val_loss= 0.24419 val_acc= 0.93109 time= 0.21099
Epoch: 0091 train_loss= 0.11368 train_acc= 0.97806 val_loss= 0.24385 val_acc= 0.93415 time= 0.21587
Epoch: 0092 train_loss= 0.10689 train_acc= 0.98061 val_loss= 0.24303 val_acc= 0.93415 time= 0.21257
Epoch: 0093 train_loss= 0.10459 train_acc= 0.98095 val_loss= 0.24161 val_acc= 0.93415 time= 0.24500
Epoch: 0094 train_loss= 0.10057 train_acc= 0.98282 val_loss= 0.23957 val_acc= 0.93721 time= 0.21100
Epoch: 0095 train_loss= 0.09661 train_acc= 0.98299 val_loss= 0.23772 val_acc= 0.93874 time= 0.21200
Epoch: 0096 train_loss= 0.09122 train_acc= 0.98537 val_loss= 0.23581 val_acc= 0.93874 time= 0.21001
Epoch: 0097 train_loss= 0.08887 train_acc= 0.98588 val_loss= 0.23345 val_acc= 0.93874 time= 0.21714
Epoch: 0098 train_loss= 0.08937 train_acc= 0.98401 val_loss= 0.23167 val_acc= 0.93874 time= 0.21700
Epoch: 0099 train_loss= 0.08363 train_acc= 0.98537 val_loss= 0.23078 val_acc= 0.94028 time= 0.21900
Epoch: 0100 train_loss= 0.08032 train_acc= 0.98571 val_loss= 0.23191 val_acc= 0.94181 time= 0.21300
Epoch: 0101 train_loss= 0.07642 train_acc= 0.98673 val_loss= 0.23287 val_acc= 0.93874 time= 0.21005
Epoch: 0102 train_loss= 0.07784 train_acc= 0.98724 val_loss= 0.23395 val_acc= 0.93721 time= 0.21343
Epoch: 0103 train_loss= 0.07667 train_acc= 0.98758 val_loss= 0.23384 val_acc= 0.93874 time= 0.23910
Epoch: 0104 train_loss= 0.07233 train_acc= 0.98758 val_loss= 0.23393 val_acc= 0.93874 time= 0.21300
Epoch: 0105 train_loss= 0.06862 train_acc= 0.98962 val_loss= 0.23201 val_acc= 0.93721 time= 0.21200
Epoch: 0106 train_loss= 0.06747 train_acc= 0.98928 val_loss= 0.22964 val_acc= 0.94028 time= 0.21000
Epoch: 0107 train_loss= 0.06346 train_acc= 0.98996 val_loss= 0.22648 val_acc= 0.94181 time= 0.23500
Epoch: 0108 train_loss= 0.06059 train_acc= 0.99030 val_loss= 0.22483 val_acc= 0.94028 time= 0.21000
Epoch: 0109 train_loss= 0.05779 train_acc= 0.99184 val_loss= 0.22382 val_acc= 0.94028 time= 0.21300
Epoch: 0110 train_loss= 0.05668 train_acc= 0.99098 val_loss= 0.22328 val_acc= 0.94028 time= 0.21473
Epoch: 0111 train_loss= 0.05622 train_acc= 0.99235 val_loss= 0.22390 val_acc= 0.94028 time= 0.23900
Epoch: 0112 train_loss= 0.05337 train_acc= 0.99064 val_loss= 0.22535 val_acc= 0.94181 time= 0.22300
Epoch: 0113 train_loss= 0.05296 train_acc= 0.99013 val_loss= 0.22597 val_acc= 0.94181 time= 0.21701
Epoch: 0114 train_loss= 0.04913 train_acc= 0.99371 val_loss= 0.22681 val_acc= 0.94181 time= 0.21600
Epoch: 0115 train_loss= 0.05046 train_acc= 0.99269 val_loss= 0.22803 val_acc= 0.94181 time= 0.21200
Early stopping...
Optimization Finished!
Test set results: cost= 0.24943 accuracy= 0.93847 time= 0.09597
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8046    0.9333    0.8642        75
           4     1.0000    1.0000    1.0000         9
           5     0.8351    0.9310    0.8804        87
           6     0.9200    0.9200    0.9200        25
           7     0.8000    0.9231    0.8571        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.5556    0.7143         9
          10     0.9167    0.6111    0.7333        36
          11     1.0000    0.9167    0.9565        12
          12     0.8403    1.0000    0.9132       121
          13     0.9375    0.7895    0.8571        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.8235    0.8235    0.8235        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9668    0.9626    0.9647       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8395    0.8395    0.8395        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9782    0.9926    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9385      2568
   macro avg     0.7728    0.6908    0.7114      2568
weighted avg     0.9361    0.9385    0.9337      2568

Macro average Test Precision, Recall and F1-Score...
(0.7728467753338983, 0.6907825808337429, 0.711414277239035, None)
Micro average Test Precision, Recall and F1-Score...
(0.9384735202492211, 0.9384735202492211, 0.9384735202492211, None)
embeddings:
8892 6532 2568
[[ 1.489579    0.1858135   0.02162184 ... -0.01558481 -0.04549537
  -0.05150595]
 [ 0.6766104   0.3112466   0.20238936 ...  0.3717784   0.18785964
   0.197815  ]
 [ 0.64069235  0.35619053  0.2830796  ...  0.4723413   0.28462583
   0.32120743]
 ...
 [ 0.10665062  0.20603995  0.15321863 ... -0.03623619  0.2178791
   0.17000613]
 [ 0.32513398  0.24613445  0.13597713 ...  0.04913074  0.19188002
   0.15039621]
 [ 0.16510512  0.21012513  0.1416645  ...  0.2251337   0.23870996
   0.14727369]]

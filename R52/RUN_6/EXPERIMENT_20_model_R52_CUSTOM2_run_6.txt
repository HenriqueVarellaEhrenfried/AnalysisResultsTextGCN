(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95119 train_acc= 0.01344 val_loss= 3.31766 val_acc= 0.64931 time= 0.47618
Epoch: 0002 train_loss= 3.32645 train_acc= 0.64365 val_loss= 2.31503 val_acc= 0.62021 time= 0.17404
Epoch: 0003 train_loss= 2.32516 train_acc= 0.61167 val_loss= 2.20556 val_acc= 0.53446 time= 0.17096
Epoch: 0004 train_loss= 2.22123 train_acc= 0.52492 val_loss= 1.96357 val_acc= 0.57887 time= 0.16900
Epoch: 0005 train_loss= 1.98341 train_acc= 0.57459 val_loss= 1.61039 val_acc= 0.66922 time= 0.18763
Epoch: 0006 train_loss= 1.64438 train_acc= 0.65096 val_loss= 1.48121 val_acc= 0.67994 time= 0.16600
Epoch: 0007 train_loss= 1.52027 train_acc= 0.65691 val_loss= 1.38854 val_acc= 0.69219 time= 0.18197
Epoch: 0008 train_loss= 1.42392 train_acc= 0.67750 val_loss= 1.27552 val_acc= 0.69832 time= 0.17000
Epoch: 0009 train_loss= 1.30603 train_acc= 0.68685 val_loss= 1.18272 val_acc= 0.71516 time= 0.17100
Epoch: 0010 train_loss= 1.21541 train_acc= 0.70760 val_loss= 1.11121 val_acc= 0.73047 time= 0.17503
Epoch: 0011 train_loss= 1.13094 train_acc= 0.73244 val_loss= 1.06226 val_acc= 0.74579 time= 0.16700
Epoch: 0012 train_loss= 1.06966 train_acc= 0.75064 val_loss= 1.02044 val_acc= 0.75345 time= 0.16900
Epoch: 0013 train_loss= 1.01390 train_acc= 0.76220 val_loss= 0.96921 val_acc= 0.76570 time= 0.17700
Epoch: 0014 train_loss= 0.96142 train_acc= 0.77173 val_loss= 0.91001 val_acc= 0.77948 time= 0.16700
Epoch: 0015 train_loss= 0.90234 train_acc= 0.78551 val_loss= 0.85113 val_acc= 0.80398 time= 0.16697
Epoch: 0016 train_loss= 0.85159 train_acc= 0.79912 val_loss= 0.79595 val_acc= 0.82083 time= 0.19600
Epoch: 0017 train_loss= 0.79181 train_acc= 0.82276 val_loss= 0.74563 val_acc= 0.83767 time= 0.17000
Epoch: 0018 train_loss= 0.74997 train_acc= 0.83807 val_loss= 0.70278 val_acc= 0.84533 time= 0.18403
Epoch: 0019 train_loss= 0.70736 train_acc= 0.84793 val_loss= 0.66745 val_acc= 0.84533 time= 0.16497
Epoch: 0020 train_loss= 0.66374 train_acc= 0.85287 val_loss= 0.63558 val_acc= 0.84686 time= 0.16703
Epoch: 0021 train_loss= 0.61573 train_acc= 0.86154 val_loss= 0.60298 val_acc= 0.84839 time= 0.16897
Epoch: 0022 train_loss= 0.58131 train_acc= 0.86630 val_loss= 0.56959 val_acc= 0.85299 time= 0.18803
Epoch: 0023 train_loss= 0.54281 train_acc= 0.86835 val_loss= 0.53656 val_acc= 0.85758 time= 0.16600
Epoch: 0024 train_loss= 0.50987 train_acc= 0.87787 val_loss= 0.50507 val_acc= 0.85911 time= 0.17009
Epoch: 0025 train_loss= 0.47279 train_acc= 0.88740 val_loss= 0.47702 val_acc= 0.87443 time= 0.16900
Epoch: 0026 train_loss= 0.44130 train_acc= 0.89029 val_loss= 0.45572 val_acc= 0.87902 time= 0.17000
Epoch: 0027 train_loss= 0.40903 train_acc= 0.89998 val_loss= 0.43778 val_acc= 0.88208 time= 0.17803
Epoch: 0028 train_loss= 0.38083 train_acc= 0.90577 val_loss= 0.42153 val_acc= 0.88821 time= 0.16801
Epoch: 0029 train_loss= 0.36286 train_acc= 0.91325 val_loss= 0.40446 val_acc= 0.88974 time= 0.16600
Epoch: 0030 train_loss= 0.32659 train_acc= 0.92022 val_loss= 0.38573 val_acc= 0.89587 time= 0.16700
Epoch: 0031 train_loss= 0.30023 train_acc= 0.92635 val_loss= 0.36796 val_acc= 0.90505 time= 0.16600
Epoch: 0032 train_loss= 0.28090 train_acc= 0.93451 val_loss= 0.35659 val_acc= 0.90505 time= 0.16797
Epoch: 0033 train_loss= 0.25558 train_acc= 0.94132 val_loss= 0.34772 val_acc= 0.90658 time= 0.18200
Epoch: 0034 train_loss= 0.24081 train_acc= 0.94438 val_loss= 0.33903 val_acc= 0.90505 time= 0.16900
Epoch: 0035 train_loss= 0.22285 train_acc= 0.94948 val_loss= 0.33443 val_acc= 0.90659 time= 0.16900
Epoch: 0036 train_loss= 0.21014 train_acc= 0.94778 val_loss= 0.32874 val_acc= 0.90965 time= 0.18703
Epoch: 0037 train_loss= 0.19827 train_acc= 0.95016 val_loss= 0.31949 val_acc= 0.91884 time= 0.16700
Epoch: 0038 train_loss= 0.18002 train_acc= 0.95731 val_loss= 0.31157 val_acc= 0.91577 time= 0.16600
Epoch: 0039 train_loss= 0.16530 train_acc= 0.95612 val_loss= 0.30606 val_acc= 0.92649 time= 0.19100
Epoch: 0040 train_loss= 0.14359 train_acc= 0.96292 val_loss= 0.30881 val_acc= 0.92956 time= 0.16600
Epoch: 0041 train_loss= 0.14350 train_acc= 0.96309 val_loss= 0.30833 val_acc= 0.92649 time= 0.17331
Epoch: 0042 train_loss= 0.13497 train_acc= 0.96445 val_loss= 0.30527 val_acc= 0.92956 time= 0.17100
Epoch: 0043 train_loss= 0.12836 train_acc= 0.96751 val_loss= 0.30777 val_acc= 0.92956 time= 0.16800
Epoch: 0044 train_loss= 0.11390 train_acc= 0.97227 val_loss= 0.30875 val_acc= 0.92649 time= 0.16703
Early stopping...
Optimization Finished!
Test set results: cost= 0.30256 accuracy= 0.92484 time= 0.07597
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     0.7500    0.5000    0.6000         6
           2     0.0000    0.0000    0.0000         1
           3     0.7727    0.9067    0.8344        75
           4     1.0000    1.0000    1.0000         9
           5     0.8298    0.8966    0.8619        87
           6     1.0000    0.9200    0.9583        25
           7     0.8125    1.0000    0.8966        13
           8     1.0000    0.8182    0.9000        11
           9     1.0000    0.4444    0.6154         9
          10     0.7073    0.8056    0.7532        36
          11     1.0000    0.9167    0.9565        12
          12     0.8657    0.9587    0.9098       121
          13     0.7647    0.6842    0.7222        19
          14     0.6757    0.8929    0.7692        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.8000    0.8889        10
          19     1.0000    1.0000    1.0000         2
          20     0.6667    0.4444    0.5333         9
          21     0.9000    0.9000    0.9000        20
          22     0.2857    0.4000    0.3333         5
          23     0.0000    0.0000    0.0000         1
          24     0.6000    0.7059    0.6486        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.2500    0.4000        12
          28     0.8889    0.7273    0.8000        11
          29     0.9738    0.9612    0.9675       696
          30     0.8462    1.0000    0.9167        22
          31     1.0000    0.6667    0.8000         3
          32     0.5625    0.9000    0.6923        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.7952    0.8148    0.8049        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9746    0.9926    0.9835      1083
          40     0.7500    0.6000    0.6667         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.6667    0.6667    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7059    0.8000    0.7500        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9248      2568
   macro avg     0.6659    0.5756    0.5987      2568
weighted avg     0.9208    0.9248    0.9187      2568

Macro average Test Precision, Recall and F1-Score...
(0.665918949170594, 0.5756022388422661, 0.5986809007549853, None)
Micro average Test Precision, Recall and F1-Score...
(0.9248442367601246, 0.9248442367601246, 0.9248442367601246, None)
embeddings:
8892 6532 2568
[[ 0.48440105 -0.46066526  3.3632226  ...  1.632371    0.02896513
   0.37090495]
 [ 1.2506661  -0.15225211  1.3689092  ...  1.0478331  -0.08187986
   0.46141288]
 [ 1.4641271  -0.40479434  0.32246816 ...  0.06643121  0.2569294
   0.36418626]
 ...
 [ 0.07121477 -0.17571472  0.67622155 ...  0.02718313  0.38643214
   0.13658814]
 [ 0.5156853  -0.12697758  0.3815499  ...  0.11469233  0.2087674
   0.32112345]
 [ 0.14090222  0.14383219  0.46100685 ...  0.21743602  0.37552178
   0.7455002 ]]

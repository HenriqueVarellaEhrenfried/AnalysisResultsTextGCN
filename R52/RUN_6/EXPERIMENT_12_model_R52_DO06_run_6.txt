(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95110 train_acc= 0.02432 val_loss= 3.89400 val_acc= 0.60796 time= 0.44637
Epoch: 0002 train_loss= 3.89355 train_acc= 0.60997 val_loss= 3.78511 val_acc= 0.62634 time= 0.19600
Epoch: 0003 train_loss= 3.78724 train_acc= 0.61932 val_loss= 3.61845 val_acc= 0.62787 time= 0.16700
Epoch: 0004 train_loss= 3.62543 train_acc= 0.61337 val_loss= 3.39531 val_acc= 0.62481 time= 0.18222
Epoch: 0005 train_loss= 3.39846 train_acc= 0.60895 val_loss= 3.12948 val_acc= 0.60337 time= 0.16529
Epoch: 0006 train_loss= 3.13728 train_acc= 0.58820 val_loss= 2.84953 val_acc= 0.58193 time= 0.16800
Epoch: 0007 train_loss= 2.83224 train_acc= 0.56268 val_loss= 2.58922 val_acc= 0.56202 time= 0.19700
Epoch: 0008 train_loss= 2.59601 train_acc= 0.55707 val_loss= 2.38786 val_acc= 0.55283 time= 0.17000
Epoch: 0009 train_loss= 2.38151 train_acc= 0.53955 val_loss= 2.26708 val_acc= 0.51914 time= 0.16700
Epoch: 0010 train_loss= 2.27009 train_acc= 0.50859 val_loss= 2.20726 val_acc= 0.48698 time= 0.18500
Epoch: 0011 train_loss= 2.22353 train_acc= 0.47236 val_loss= 2.16823 val_acc= 0.46708 time= 0.16600
Epoch: 0012 train_loss= 2.17546 train_acc= 0.44991 val_loss= 2.12209 val_acc= 0.46248 time= 0.16903
Epoch: 0013 train_loss= 2.13913 train_acc= 0.43596 val_loss= 2.05667 val_acc= 0.46248 time= 0.19100
Epoch: 0014 train_loss= 2.08224 train_acc= 0.43868 val_loss= 1.97339 val_acc= 0.47014 time= 0.16997
Epoch: 0015 train_loss= 2.00290 train_acc= 0.43953 val_loss= 1.88261 val_acc= 0.49158 time= 0.18600
Epoch: 0016 train_loss= 1.92160 train_acc= 0.48018 val_loss= 1.79755 val_acc= 0.54671 time= 0.17000
Epoch: 0017 train_loss= 1.82427 train_acc= 0.56200 val_loss= 1.72681 val_acc= 0.62021 time= 0.16700
Epoch: 0018 train_loss= 1.75898 train_acc= 0.60282 val_loss= 1.66987 val_acc= 0.65850 time= 0.17100
Epoch: 0019 train_loss= 1.70065 train_acc= 0.63973 val_loss= 1.62041 val_acc= 0.66922 time= 0.18900
Epoch: 0020 train_loss= 1.65211 train_acc= 0.64620 val_loss= 1.57267 val_acc= 0.66769 time= 0.16700
Epoch: 0021 train_loss= 1.61108 train_acc= 0.64382 val_loss= 1.52394 val_acc= 0.67228 time= 0.16700
Epoch: 0022 train_loss= 1.56589 train_acc= 0.63922 val_loss= 1.47481 val_acc= 0.66922 time= 0.16600
Epoch: 0023 train_loss= 1.49171 train_acc= 0.64552 val_loss= 1.42840 val_acc= 0.67381 time= 0.17100
Epoch: 0024 train_loss= 1.45596 train_acc= 0.64467 val_loss= 1.38571 val_acc= 0.67994 time= 0.19900
Epoch: 0025 train_loss= 1.41027 train_acc= 0.65453 val_loss= 1.34723 val_acc= 0.68453 time= 0.16800
Epoch: 0026 train_loss= 1.37989 train_acc= 0.65283 val_loss= 1.31252 val_acc= 0.69372 time= 0.16800
Epoch: 0027 train_loss= 1.34410 train_acc= 0.66236 val_loss= 1.28081 val_acc= 0.69372 time= 0.18700
Epoch: 0028 train_loss= 1.31365 train_acc= 0.67086 val_loss= 1.25131 val_acc= 0.69985 time= 0.16807
Epoch: 0029 train_loss= 1.28170 train_acc= 0.68107 val_loss= 1.22323 val_acc= 0.71363 time= 0.16607
Epoch: 0030 train_loss= 1.25047 train_acc= 0.69297 val_loss= 1.19588 val_acc= 0.71669 time= 0.19100
Epoch: 0031 train_loss= 1.22419 train_acc= 0.70573 val_loss= 1.16873 val_acc= 0.72129 time= 0.16946
Epoch: 0032 train_loss= 1.19611 train_acc= 0.71577 val_loss= 1.14160 val_acc= 0.72894 time= 0.17300
Epoch: 0033 train_loss= 1.17352 train_acc= 0.72648 val_loss= 1.11474 val_acc= 0.73354 time= 0.17003
Epoch: 0034 train_loss= 1.13627 train_acc= 0.74264 val_loss= 1.08834 val_acc= 0.74426 time= 0.16797
Epoch: 0035 train_loss= 1.11512 train_acc= 0.74588 val_loss= 1.06266 val_acc= 0.74579 time= 0.16937
Epoch: 0036 train_loss= 1.08886 train_acc= 0.75081 val_loss= 1.03783 val_acc= 0.75345 time= 0.19201
Epoch: 0037 train_loss= 1.05835 train_acc= 0.75795 val_loss= 1.01361 val_acc= 0.75804 time= 0.16717
Epoch: 0038 train_loss= 1.03535 train_acc= 0.76203 val_loss= 0.98987 val_acc= 0.76876 time= 0.18400
Epoch: 0039 train_loss= 1.01019 train_acc= 0.76867 val_loss= 0.96652 val_acc= 0.77642 time= 0.17012
Epoch: 0040 train_loss= 0.98497 train_acc= 0.77581 val_loss= 0.94324 val_acc= 0.78407 time= 0.17100
Epoch: 0041 train_loss= 0.96560 train_acc= 0.78228 val_loss= 0.91992 val_acc= 0.79326 time= 0.17103
Epoch: 0042 train_loss= 0.93569 train_acc= 0.79554 val_loss= 0.89671 val_acc= 0.80551 time= 0.18900
Epoch: 0043 train_loss= 0.91474 train_acc= 0.80405 val_loss= 0.87379 val_acc= 0.81623 time= 0.16728
Epoch: 0044 train_loss= 0.89496 train_acc= 0.81425 val_loss= 0.85110 val_acc= 0.82236 time= 0.16620
Epoch: 0045 train_loss= 0.86439 train_acc= 0.81868 val_loss= 0.82883 val_acc= 0.81930 time= 0.16606
Epoch: 0046 train_loss= 0.85008 train_acc= 0.82242 val_loss= 0.80708 val_acc= 0.82542 time= 0.16800
Epoch: 0047 train_loss= 0.82228 train_acc= 0.82582 val_loss= 0.78582 val_acc= 0.83002 time= 0.19800
Epoch: 0048 train_loss= 0.80520 train_acc= 0.83109 val_loss= 0.76499 val_acc= 0.83308 time= 0.16900
Epoch: 0049 train_loss= 0.76752 train_acc= 0.83569 val_loss= 0.74462 val_acc= 0.83002 time= 0.16900
Epoch: 0050 train_loss= 0.75288 train_acc= 0.83399 val_loss= 0.72474 val_acc= 0.83155 time= 0.18500
Epoch: 0051 train_loss= 0.73377 train_acc= 0.84147 val_loss= 0.70525 val_acc= 0.83461 time= 0.16600
Epoch: 0052 train_loss= 0.71256 train_acc= 0.84096 val_loss= 0.68610 val_acc= 0.83767 time= 0.16700
Epoch: 0053 train_loss= 0.68880 train_acc= 0.84844 val_loss= 0.66705 val_acc= 0.84380 time= 0.19133
Epoch: 0054 train_loss= 0.67724 train_acc= 0.84657 val_loss= 0.64836 val_acc= 0.85145 time= 0.16696
Epoch: 0055 train_loss= 0.65482 train_acc= 0.85151 val_loss= 0.62988 val_acc= 0.85758 time= 0.17203
Epoch: 0056 train_loss= 0.62893 train_acc= 0.86171 val_loss= 0.61187 val_acc= 0.86217 time= 0.17000
Epoch: 0057 train_loss= 0.60332 train_acc= 0.86545 val_loss= 0.59474 val_acc= 0.87136 time= 0.17000
Epoch: 0058 train_loss= 0.59937 train_acc= 0.86647 val_loss= 0.57817 val_acc= 0.87443 time= 0.17103
Epoch: 0059 train_loss= 0.57120 train_acc= 0.87549 val_loss= 0.56259 val_acc= 0.87902 time= 0.18900
Epoch: 0060 train_loss= 0.55136 train_acc= 0.88212 val_loss= 0.54745 val_acc= 0.87902 time= 0.16697
Epoch: 0061 train_loss= 0.54449 train_acc= 0.87974 val_loss= 0.53291 val_acc= 0.87902 time= 0.18803
Epoch: 0062 train_loss= 0.52863 train_acc= 0.88365 val_loss= 0.51842 val_acc= 0.88361 time= 0.16700
Epoch: 0063 train_loss= 0.50825 train_acc= 0.88808 val_loss= 0.50418 val_acc= 0.88515 time= 0.16597
Epoch: 0064 train_loss= 0.49097 train_acc= 0.88791 val_loss= 0.49046 val_acc= 0.88668 time= 0.17444
Epoch: 0065 train_loss= 0.47625 train_acc= 0.89284 val_loss= 0.47731 val_acc= 0.88821 time= 0.18000
Epoch: 0066 train_loss= 0.46174 train_acc= 0.89420 val_loss= 0.46472 val_acc= 0.88821 time= 0.17103
Epoch: 0067 train_loss= 0.45646 train_acc= 0.89556 val_loss= 0.45270 val_acc= 0.89280 time= 0.16600
Epoch: 0068 train_loss= 0.43424 train_acc= 0.90730 val_loss= 0.44112 val_acc= 0.89587 time= 0.16600
Epoch: 0069 train_loss= 0.42248 train_acc= 0.90577 val_loss= 0.43010 val_acc= 0.89587 time= 0.16800
Epoch: 0070 train_loss= 0.40806 train_acc= 0.91104 val_loss= 0.41980 val_acc= 0.89740 time= 0.19208
Epoch: 0071 train_loss= 0.39620 train_acc= 0.91359 val_loss= 0.41017 val_acc= 0.89893 time= 0.16900
Epoch: 0072 train_loss= 0.38872 train_acc= 0.91189 val_loss= 0.40097 val_acc= 0.89893 time= 0.16975
Epoch: 0073 train_loss= 0.37671 train_acc= 0.91801 val_loss= 0.39264 val_acc= 0.90046 time= 0.19000
Epoch: 0074 train_loss= 0.36613 train_acc= 0.92039 val_loss= 0.38423 val_acc= 0.90199 time= 0.17000
Epoch: 0075 train_loss= 0.35191 train_acc= 0.92499 val_loss= 0.37599 val_acc= 0.90046 time= 0.16700
Epoch: 0076 train_loss= 0.34411 train_acc= 0.92686 val_loss= 0.36809 val_acc= 0.90199 time= 0.19105
Epoch: 0077 train_loss= 0.32949 train_acc= 0.92975 val_loss= 0.36136 val_acc= 0.90352 time= 0.16713
Epoch: 0078 train_loss= 0.32604 train_acc= 0.93689 val_loss= 0.35459 val_acc= 0.90352 time= 0.18100
Epoch: 0079 train_loss= 0.31576 train_acc= 0.93264 val_loss= 0.34826 val_acc= 0.90505 time= 0.16725
Epoch: 0080 train_loss= 0.30404 train_acc= 0.93468 val_loss= 0.34226 val_acc= 0.90505 time= 0.17000
Epoch: 0081 train_loss= 0.29951 train_acc= 0.93349 val_loss= 0.33676 val_acc= 0.90505 time= 0.17222
Epoch: 0082 train_loss= 0.29313 train_acc= 0.93740 val_loss= 0.33266 val_acc= 0.90658 time= 0.19301
Epoch: 0083 train_loss= 0.27691 train_acc= 0.94132 val_loss= 0.32882 val_acc= 0.90965 time= 0.16700
Epoch: 0084 train_loss= 0.27620 train_acc= 0.94200 val_loss= 0.32543 val_acc= 0.90812 time= 0.17999
Epoch: 0085 train_loss= 0.27050 train_acc= 0.94217 val_loss= 0.32113 val_acc= 0.91118 time= 0.16800
Epoch: 0086 train_loss= 0.26267 train_acc= 0.94149 val_loss= 0.31643 val_acc= 0.91577 time= 0.16651
Epoch: 0087 train_loss= 0.25236 train_acc= 0.94676 val_loss= 0.31105 val_acc= 0.91424 time= 0.17060
Epoch: 0088 train_loss= 0.25064 train_acc= 0.94404 val_loss= 0.30646 val_acc= 0.91884 time= 0.18900
Epoch: 0089 train_loss= 0.24043 train_acc= 0.94863 val_loss= 0.30162 val_acc= 0.91884 time= 0.17041
Epoch: 0090 train_loss= 0.23669 train_acc= 0.94795 val_loss= 0.29793 val_acc= 0.91577 time= 0.18700
Epoch: 0091 train_loss= 0.22509 train_acc= 0.95220 val_loss= 0.29567 val_acc= 0.91730 time= 0.16800
Epoch: 0092 train_loss= 0.21957 train_acc= 0.95169 val_loss= 0.29283 val_acc= 0.91884 time= 0.16800
Epoch: 0093 train_loss= 0.21691 train_acc= 0.95254 val_loss= 0.28921 val_acc= 0.92037 time= 0.19101
Epoch: 0094 train_loss= 0.21075 train_acc= 0.95816 val_loss= 0.28628 val_acc= 0.91884 time= 0.16799
Epoch: 0095 train_loss= 0.20631 train_acc= 0.95884 val_loss= 0.28329 val_acc= 0.91884 time= 0.17000
Epoch: 0096 train_loss= 0.20123 train_acc= 0.95697 val_loss= 0.28069 val_acc= 0.92190 time= 0.17600
Epoch: 0097 train_loss= 0.20139 train_acc= 0.95731 val_loss= 0.27815 val_acc= 0.92190 time= 0.17126
Epoch: 0098 train_loss= 0.19345 train_acc= 0.95850 val_loss= 0.27571 val_acc= 0.92190 time= 0.17000
Epoch: 0099 train_loss= 0.18760 train_acc= 0.95816 val_loss= 0.27368 val_acc= 0.92343 time= 0.17700
Epoch: 0100 train_loss= 0.17895 train_acc= 0.96292 val_loss= 0.27159 val_acc= 0.92343 time= 0.16800
Epoch: 0101 train_loss= 0.17112 train_acc= 0.96258 val_loss= 0.27020 val_acc= 0.92190 time= 0.18800
Epoch: 0102 train_loss= 0.17336 train_acc= 0.96139 val_loss= 0.26782 val_acc= 0.92496 time= 0.16807
Epoch: 0103 train_loss= 0.17411 train_acc= 0.96156 val_loss= 0.26420 val_acc= 0.92649 time= 0.16597
Epoch: 0104 train_loss= 0.16362 train_acc= 0.96717 val_loss= 0.26056 val_acc= 0.92649 time= 0.17003
Epoch: 0105 train_loss= 0.16387 train_acc= 0.96411 val_loss= 0.25748 val_acc= 0.92802 time= 0.19214
Epoch: 0106 train_loss= 0.15928 train_acc= 0.96428 val_loss= 0.25487 val_acc= 0.92802 time= 0.17000
Epoch: 0107 train_loss= 0.15321 train_acc= 0.97040 val_loss= 0.25278 val_acc= 0.92956 time= 0.17103
Epoch: 0108 train_loss= 0.15027 train_acc= 0.96785 val_loss= 0.25171 val_acc= 0.92649 time= 0.16600
Epoch: 0109 train_loss= 0.14671 train_acc= 0.97074 val_loss= 0.25206 val_acc= 0.92343 time= 0.16697
Epoch: 0110 train_loss= 0.14425 train_acc= 0.97210 val_loss= 0.25233 val_acc= 0.92649 time= 0.19100
Epoch: 0111 train_loss= 0.14399 train_acc= 0.96802 val_loss= 0.25216 val_acc= 0.92649 time= 0.16712
Epoch: 0112 train_loss= 0.13428 train_acc= 0.97381 val_loss= 0.25087 val_acc= 0.92956 time= 0.16800
Epoch: 0113 train_loss= 0.13748 train_acc= 0.97346 val_loss= 0.24908 val_acc= 0.93109 time= 0.18800
Epoch: 0114 train_loss= 0.13423 train_acc= 0.97398 val_loss= 0.24652 val_acc= 0.93109 time= 0.16957
Epoch: 0115 train_loss= 0.12922 train_acc= 0.97381 val_loss= 0.24371 val_acc= 0.92802 time= 0.17000
Epoch: 0116 train_loss= 0.12715 train_acc= 0.97449 val_loss= 0.24173 val_acc= 0.92802 time= 0.17200
Epoch: 0117 train_loss= 0.12241 train_acc= 0.97517 val_loss= 0.24103 val_acc= 0.93109 time= 0.16704
Epoch: 0118 train_loss= 0.12094 train_acc= 0.97517 val_loss= 0.24138 val_acc= 0.93109 time= 0.16800
Epoch: 0119 train_loss= 0.12074 train_acc= 0.97500 val_loss= 0.24263 val_acc= 0.93109 time= 0.19100
Epoch: 0120 train_loss= 0.11470 train_acc= 0.97551 val_loss= 0.24268 val_acc= 0.93262 time= 0.16805
Epoch: 0121 train_loss= 0.11729 train_acc= 0.97619 val_loss= 0.24084 val_acc= 0.93262 time= 0.16814
Epoch: 0122 train_loss= 0.11118 train_acc= 0.97755 val_loss= 0.23774 val_acc= 0.92956 time= 0.19700
Epoch: 0123 train_loss= 0.10925 train_acc= 0.98061 val_loss= 0.23565 val_acc= 0.92956 time= 0.17200
Epoch: 0124 train_loss= 0.10810 train_acc= 0.97942 val_loss= 0.23501 val_acc= 0.92649 time= 0.18401
Epoch: 0125 train_loss= 0.10029 train_acc= 0.98231 val_loss= 0.23445 val_acc= 0.92649 time= 0.16600
Epoch: 0126 train_loss= 0.10881 train_acc= 0.97874 val_loss= 0.23376 val_acc= 0.92956 time= 0.16612
Epoch: 0127 train_loss= 0.10042 train_acc= 0.98095 val_loss= 0.23336 val_acc= 0.92956 time= 0.17010
Epoch: 0128 train_loss= 0.10099 train_acc= 0.98129 val_loss= 0.23384 val_acc= 0.93109 time= 0.18901
Epoch: 0129 train_loss= 0.09876 train_acc= 0.98027 val_loss= 0.23482 val_acc= 0.93109 time= 0.16800
Epoch: 0130 train_loss= 0.09470 train_acc= 0.98197 val_loss= 0.23640 val_acc= 0.92956 time= 0.17100
Early stopping...
Optimization Finished!
Test set results: cost= 0.26003 accuracy= 0.93847 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8023    0.9200    0.8571        75
           4     1.0000    1.0000    1.0000         9
           5     0.7905    0.9540    0.8646        87
           6     0.9200    0.9200    0.9200        25
           7     0.7059    0.9231    0.8000        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.2222    0.3636         9
          10     0.9286    0.7222    0.8125        36
          11     1.0000    0.9167    0.9565        12
          12     0.8623    0.9835    0.9189       121
          13     1.0000    0.7368    0.8485        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.8000    0.4444    0.5714         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9670    0.9670    0.9670       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8800    0.8148    0.8462        81
          36     1.0000    0.4167    0.5882        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7368    0.9333    0.8235        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9385      2568
   macro avg     0.7402    0.6619    0.6817      2568
weighted avg     0.9355    0.9385    0.9330      2568

Macro average Test Precision, Recall and F1-Score...
(0.7402197571737877, 0.661885058175163, 0.6817207662984303, None)
Micro average Test Precision, Recall and F1-Score...
(0.9384735202492211, 0.9384735202492211, 0.9384735202492211, None)
embeddings:
8892 6532 2568
[[-0.05767364  0.14616038 -0.17255841 ... -0.05749344 -0.19176927
   0.09139869]
 [-0.04164305  0.262264    0.17290723 ...  0.3755895   0.01142952
   0.4543532 ]
 [ 0.01907798 -0.00762398  0.03145559 ...  0.28344262  0.02553042
   0.924736  ]
 ...
 [-0.00866988  0.13124275  0.06573506 ...  0.25456315  0.07102039
   0.35490453]
 [ 0.06190901  0.02741096  0.02864996 ...  0.22011581  0.03971636
   0.42285222]
 [ 0.22851962  0.2625913   0.23782556 ...  0.21806408  0.22814
   0.24945563]]

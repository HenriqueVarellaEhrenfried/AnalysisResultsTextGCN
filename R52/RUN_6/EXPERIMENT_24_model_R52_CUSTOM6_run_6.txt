(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95127 train_acc= 0.00595 val_loss= 3.89628 val_acc= 0.66462 time= 0.45999
Epoch: 0002 train_loss= 3.89715 train_acc= 0.64212 val_loss= 3.79236 val_acc= 0.65391 time= 0.17901
Epoch: 0003 train_loss= 3.79390 train_acc= 0.63242 val_loss= 3.63271 val_acc= 0.64472 time= 0.17300
Epoch: 0004 train_loss= 3.63468 train_acc= 0.62426 val_loss= 3.41865 val_acc= 0.63553 time= 0.18892
Epoch: 0005 train_loss= 3.42707 train_acc= 0.61813 val_loss= 3.16230 val_acc= 0.63093 time= 0.18200
Epoch: 0006 train_loss= 3.16914 train_acc= 0.61507 val_loss= 2.88914 val_acc= 0.62940 time= 0.17000
Epoch: 0007 train_loss= 2.89155 train_acc= 0.61388 val_loss= 2.63102 val_acc= 0.62940 time= 0.16924
Epoch: 0008 train_loss= 2.63129 train_acc= 0.61422 val_loss= 2.42313 val_acc= 0.64012 time= 0.18400
Epoch: 0009 train_loss= 2.42214 train_acc= 0.62051 val_loss= 2.28952 val_acc= 0.66309 time= 0.16900
Epoch: 0010 train_loss= 2.29306 train_acc= 0.63701 val_loss= 2.21687 val_acc= 0.64319 time= 0.16800
Epoch: 0011 train_loss= 2.21831 train_acc= 0.63004 val_loss= 2.17167 val_acc= 0.48851 time= 0.16697
Epoch: 0012 train_loss= 2.17565 train_acc= 0.46913 val_loss= 2.12788 val_acc= 0.46248 time= 0.16704
Epoch: 0013 train_loss= 2.13629 train_acc= 0.43681 val_loss= 2.07060 val_acc= 0.45789 time= 0.16999
Epoch: 0014 train_loss= 2.08362 train_acc= 0.43358 val_loss= 1.99498 val_acc= 0.46095 time= 0.19900
Epoch: 0015 train_loss= 2.01525 train_acc= 0.43460 val_loss= 1.90631 val_acc= 0.46861 time= 0.17100
Epoch: 0016 train_loss= 1.92755 train_acc= 0.44123 val_loss= 1.81697 val_acc= 0.50230 time= 0.16700
Epoch: 0017 train_loss= 1.83712 train_acc= 0.48325 val_loss= 1.73851 val_acc= 0.58499 time= 0.18501
Epoch: 0018 train_loss= 1.76409 train_acc= 0.58377 val_loss= 1.67473 val_acc= 0.65544 time= 0.16701
Epoch: 0019 train_loss= 1.69851 train_acc= 0.64263 val_loss= 1.62085 val_acc= 0.67841 time= 0.16700
Epoch: 0020 train_loss= 1.64222 train_acc= 0.65657 val_loss= 1.56973 val_acc= 0.67841 time= 0.19199
Epoch: 0021 train_loss= 1.59230 train_acc= 0.65776 val_loss= 1.51769 val_acc= 0.67228 time= 0.16900
Epoch: 0022 train_loss= 1.53821 train_acc= 0.65742 val_loss= 1.46493 val_acc= 0.67381 time= 0.19000
Epoch: 0023 train_loss= 1.48330 train_acc= 0.66168 val_loss= 1.41361 val_acc= 0.67381 time= 0.17000
Epoch: 0024 train_loss= 1.43148 train_acc= 0.66491 val_loss= 1.36566 val_acc= 0.68453 time= 0.16700
Epoch: 0025 train_loss= 1.38588 train_acc= 0.66967 val_loss= 1.32188 val_acc= 0.69066 time= 0.17100
Epoch: 0026 train_loss= 1.34089 train_acc= 0.67903 val_loss= 1.28228 val_acc= 0.70291 time= 0.18800
Epoch: 0027 train_loss= 1.30116 train_acc= 0.68821 val_loss= 1.24635 val_acc= 0.71363 time= 0.17001
Epoch: 0028 train_loss= 1.26336 train_acc= 0.70029 val_loss= 1.21325 val_acc= 0.71822 time= 0.17311
Epoch: 0029 train_loss= 1.22969 train_acc= 0.71050 val_loss= 1.18209 val_acc= 0.72282 time= 0.16777
Epoch: 0030 train_loss= 1.19606 train_acc= 0.72529 val_loss= 1.15212 val_acc= 0.72894 time= 0.17100
Epoch: 0031 train_loss= 1.16594 train_acc= 0.73448 val_loss= 1.12270 val_acc= 0.73047 time= 0.17400
Epoch: 0032 train_loss= 1.13588 train_acc= 0.74332 val_loss= 1.09342 val_acc= 0.73966 time= 0.19300
Epoch: 0033 train_loss= 1.10503 train_acc= 0.75234 val_loss= 1.06423 val_acc= 0.75038 time= 0.16900
Epoch: 0034 train_loss= 1.07136 train_acc= 0.76254 val_loss= 1.03539 val_acc= 0.76723 time= 0.18300
Epoch: 0035 train_loss= 1.03980 train_acc= 0.77139 val_loss= 1.00725 val_acc= 0.77489 time= 0.16705
Epoch: 0036 train_loss= 1.01089 train_acc= 0.77887 val_loss= 0.98016 val_acc= 0.77642 time= 0.16795
Epoch: 0037 train_loss= 0.98395 train_acc= 0.78517 val_loss= 0.95417 val_acc= 0.78714 time= 0.16900
Epoch: 0038 train_loss= 0.95658 train_acc= 0.79248 val_loss= 0.92902 val_acc= 0.79786 time= 0.17152
Epoch: 0039 train_loss= 0.92984 train_acc= 0.80184 val_loss= 0.90448 val_acc= 0.81011 time= 0.17200
Epoch: 0040 train_loss= 0.90502 train_acc= 0.81034 val_loss= 0.88035 val_acc= 0.81317 time= 0.19200
Epoch: 0041 train_loss= 0.87871 train_acc= 0.81749 val_loss= 0.85650 val_acc= 0.81623 time= 0.16804
Epoch: 0042 train_loss= 0.85294 train_acc= 0.82412 val_loss= 0.83291 val_acc= 0.82236 time= 0.16729
Epoch: 0043 train_loss= 0.82838 train_acc= 0.82752 val_loss= 0.80954 val_acc= 0.82695 time= 0.19309
Epoch: 0044 train_loss= 0.80549 train_acc= 0.83296 val_loss= 0.78637 val_acc= 0.83155 time= 0.16807
Epoch: 0045 train_loss= 0.77806 train_acc= 0.83705 val_loss= 0.76355 val_acc= 0.83614 time= 0.18300
Epoch: 0046 train_loss= 0.75509 train_acc= 0.84164 val_loss= 0.74114 val_acc= 0.84227 time= 0.16897
Epoch: 0047 train_loss= 0.73123 train_acc= 0.84912 val_loss= 0.71916 val_acc= 0.84992 time= 0.17000
Epoch: 0048 train_loss= 0.70730 train_acc= 0.85355 val_loss= 0.69761 val_acc= 0.85605 time= 0.17300
Epoch: 0049 train_loss= 0.68470 train_acc= 0.85831 val_loss= 0.67659 val_acc= 0.85758 time= 0.18103
Epoch: 0050 train_loss= 0.66038 train_acc= 0.86171 val_loss= 0.65609 val_acc= 0.86064 time= 0.16700
Epoch: 0051 train_loss= 0.64068 train_acc= 0.86749 val_loss= 0.63625 val_acc= 0.86677 time= 0.17600
Epoch: 0052 train_loss= 0.61919 train_acc= 0.87107 val_loss= 0.61696 val_acc= 0.86830 time= 0.16700
Epoch: 0053 train_loss= 0.59783 train_acc= 0.87464 val_loss= 0.59833 val_acc= 0.86983 time= 0.16799
Epoch: 0054 train_loss= 0.57558 train_acc= 0.87838 val_loss= 0.58045 val_acc= 0.87136 time= 0.18297
Epoch: 0055 train_loss= 0.55812 train_acc= 0.87991 val_loss= 0.56334 val_acc= 0.87136 time= 0.17100
Epoch: 0056 train_loss= 0.53965 train_acc= 0.88416 val_loss= 0.54706 val_acc= 0.87136 time= 0.17100
Epoch: 0057 train_loss= 0.52251 train_acc= 0.88621 val_loss= 0.53153 val_acc= 0.87289 time= 0.17603
Epoch: 0058 train_loss= 0.50329 train_acc= 0.88859 val_loss= 0.51664 val_acc= 0.87289 time= 0.16800
Epoch: 0059 train_loss= 0.48591 train_acc= 0.89199 val_loss= 0.50212 val_acc= 0.87289 time= 0.16600
Epoch: 0060 train_loss= 0.46752 train_acc= 0.89369 val_loss= 0.48802 val_acc= 0.87596 time= 0.19125
Epoch: 0061 train_loss= 0.45152 train_acc= 0.89811 val_loss= 0.47416 val_acc= 0.87749 time= 0.16800
Epoch: 0062 train_loss= 0.43754 train_acc= 0.90270 val_loss= 0.46073 val_acc= 0.87902 time= 0.16807
Epoch: 0063 train_loss= 0.42146 train_acc= 0.90781 val_loss= 0.44781 val_acc= 0.88208 time= 0.19297
Epoch: 0064 train_loss= 0.40744 train_acc= 0.91257 val_loss= 0.43554 val_acc= 0.88515 time= 0.17000
Epoch: 0065 train_loss= 0.39309 train_acc= 0.91597 val_loss= 0.42398 val_acc= 0.89127 time= 0.17000
Epoch: 0066 train_loss= 0.38243 train_acc= 0.91954 val_loss= 0.41307 val_acc= 0.89433 time= 0.19103
Epoch: 0067 train_loss= 0.36840 train_acc= 0.92278 val_loss= 0.40282 val_acc= 0.89893 time= 0.16900
Epoch: 0068 train_loss= 0.35292 train_acc= 0.92635 val_loss= 0.39300 val_acc= 0.90046 time= 0.18103
Epoch: 0069 train_loss= 0.34233 train_acc= 0.92924 val_loss= 0.38359 val_acc= 0.89893 time= 0.16700
Epoch: 0070 train_loss= 0.32839 train_acc= 0.93247 val_loss= 0.37462 val_acc= 0.90505 time= 0.16822
Epoch: 0071 train_loss= 0.31869 train_acc= 0.93417 val_loss= 0.36603 val_acc= 0.90352 time= 0.17246
Epoch: 0072 train_loss= 0.30655 train_acc= 0.93774 val_loss= 0.35811 val_acc= 0.90352 time= 0.18400
Epoch: 0073 train_loss= 0.29527 train_acc= 0.93979 val_loss= 0.35056 val_acc= 0.90505 time= 0.17000
Epoch: 0074 train_loss= 0.28759 train_acc= 0.94234 val_loss= 0.34323 val_acc= 0.90505 time= 0.16903
Epoch: 0075 train_loss= 0.27564 train_acc= 0.94489 val_loss= 0.33657 val_acc= 0.90658 time= 0.16706
Epoch: 0076 train_loss= 0.26595 train_acc= 0.94710 val_loss= 0.33023 val_acc= 0.90812 time= 0.16712
Epoch: 0077 train_loss= 0.25643 train_acc= 0.94897 val_loss= 0.32437 val_acc= 0.91118 time= 0.19150
Epoch: 0078 train_loss= 0.24639 train_acc= 0.95169 val_loss= 0.31902 val_acc= 0.91424 time= 0.17000
Epoch: 0079 train_loss= 0.24040 train_acc= 0.95390 val_loss= 0.31378 val_acc= 0.91884 time= 0.16800
Epoch: 0080 train_loss= 0.22963 train_acc= 0.95509 val_loss= 0.30916 val_acc= 0.92037 time= 0.19500
Epoch: 0081 train_loss= 0.22170 train_acc= 0.95833 val_loss= 0.30500 val_acc= 0.92190 time= 0.17100
Epoch: 0082 train_loss= 0.21324 train_acc= 0.95850 val_loss= 0.30072 val_acc= 0.92343 time= 0.16900
Epoch: 0083 train_loss= 0.20618 train_acc= 0.96037 val_loss= 0.29638 val_acc= 0.92649 time= 0.19103
Epoch: 0084 train_loss= 0.19796 train_acc= 0.96088 val_loss= 0.29213 val_acc= 0.92496 time= 0.16997
Epoch: 0085 train_loss= 0.19084 train_acc= 0.96241 val_loss= 0.28821 val_acc= 0.92649 time= 0.16803
Epoch: 0086 train_loss= 0.18469 train_acc= 0.96360 val_loss= 0.28429 val_acc= 0.92802 time= 0.16610
Epoch: 0087 train_loss= 0.17743 train_acc= 0.96479 val_loss= 0.28043 val_acc= 0.92802 time= 0.16899
Epoch: 0088 train_loss= 0.17163 train_acc= 0.96666 val_loss= 0.27669 val_acc= 0.92802 time= 0.17496
Epoch: 0089 train_loss= 0.16570 train_acc= 0.96802 val_loss= 0.27306 val_acc= 0.92956 time= 0.19400
Epoch: 0090 train_loss= 0.15821 train_acc= 0.97040 val_loss= 0.27002 val_acc= 0.93109 time= 0.17000
Epoch: 0091 train_loss= 0.15336 train_acc= 0.97006 val_loss= 0.26798 val_acc= 0.93109 time= 0.18600
Epoch: 0092 train_loss= 0.14947 train_acc= 0.97176 val_loss= 0.26595 val_acc= 0.92956 time= 0.16804
Epoch: 0093 train_loss= 0.14267 train_acc= 0.97483 val_loss= 0.26333 val_acc= 0.92802 time= 0.16801
Epoch: 0094 train_loss= 0.13648 train_acc= 0.97687 val_loss= 0.26024 val_acc= 0.92956 time= 0.16999
Epoch: 0095 train_loss= 0.13276 train_acc= 0.97653 val_loss= 0.25681 val_acc= 0.93262 time= 0.18800
Epoch: 0096 train_loss= 0.12774 train_acc= 0.97823 val_loss= 0.25392 val_acc= 0.93262 time= 0.16796
Epoch: 0097 train_loss= 0.12252 train_acc= 0.97908 val_loss= 0.25150 val_acc= 0.93262 time= 0.17100
Epoch: 0098 train_loss= 0.11809 train_acc= 0.98129 val_loss= 0.24993 val_acc= 0.93262 time= 0.17200
Epoch: 0099 train_loss= 0.11530 train_acc= 0.98129 val_loss= 0.24908 val_acc= 0.93262 time= 0.16711
Epoch: 0100 train_loss= 0.11092 train_acc= 0.98231 val_loss= 0.24872 val_acc= 0.93415 time= 0.16900
Epoch: 0101 train_loss= 0.10692 train_acc= 0.98282 val_loss= 0.24822 val_acc= 0.93262 time= 0.16800
Epoch: 0102 train_loss= 0.10234 train_acc= 0.98418 val_loss= 0.24768 val_acc= 0.93262 time= 0.16700
Epoch: 0103 train_loss= 0.09873 train_acc= 0.98537 val_loss= 0.24667 val_acc= 0.93568 time= 0.18800
Epoch: 0104 train_loss= 0.09576 train_acc= 0.98503 val_loss= 0.24504 val_acc= 0.93568 time= 0.16805
Epoch: 0105 train_loss= 0.09161 train_acc= 0.98605 val_loss= 0.24324 val_acc= 0.93415 time= 0.17029
Epoch: 0106 train_loss= 0.09001 train_acc= 0.98622 val_loss= 0.24147 val_acc= 0.93262 time= 0.19700
Epoch: 0107 train_loss= 0.08558 train_acc= 0.98690 val_loss= 0.23988 val_acc= 0.93109 time= 0.16900
Epoch: 0108 train_loss= 0.08356 train_acc= 0.98741 val_loss= 0.23874 val_acc= 0.93262 time= 0.16774
Epoch: 0109 train_loss= 0.07995 train_acc= 0.98775 val_loss= 0.23836 val_acc= 0.93262 time= 0.16700
Epoch: 0110 train_loss= 0.07716 train_acc= 0.98979 val_loss= 0.23787 val_acc= 0.93568 time= 0.16900
Epoch: 0111 train_loss= 0.07545 train_acc= 0.98911 val_loss= 0.23807 val_acc= 0.93721 time= 0.17296
Epoch: 0112 train_loss= 0.07194 train_acc= 0.98996 val_loss= 0.23803 val_acc= 0.93721 time= 0.18810
Epoch: 0113 train_loss= 0.06894 train_acc= 0.98979 val_loss= 0.23724 val_acc= 0.93415 time= 0.17097
Epoch: 0114 train_loss= 0.06730 train_acc= 0.98979 val_loss= 0.23620 val_acc= 0.93721 time= 0.19200
Epoch: 0115 train_loss= 0.06567 train_acc= 0.99047 val_loss= 0.23490 val_acc= 0.93874 time= 0.16903
Epoch: 0116 train_loss= 0.06412 train_acc= 0.99064 val_loss= 0.23477 val_acc= 0.94028 time= 0.16699
Epoch: 0117 train_loss= 0.06062 train_acc= 0.99133 val_loss= 0.23486 val_acc= 0.93721 time= 0.17101
Epoch: 0118 train_loss= 0.05833 train_acc= 0.99252 val_loss= 0.23476 val_acc= 0.93568 time= 0.18900
Epoch: 0119 train_loss= 0.05810 train_acc= 0.99201 val_loss= 0.23463 val_acc= 0.93568 time= 0.16700
Epoch: 0120 train_loss= 0.05423 train_acc= 0.99252 val_loss= 0.23435 val_acc= 0.93874 time= 0.16704
Epoch: 0121 train_loss= 0.05364 train_acc= 0.99286 val_loss= 0.23357 val_acc= 0.93721 time= 0.16946
Epoch: 0122 train_loss= 0.05287 train_acc= 0.99320 val_loss= 0.23275 val_acc= 0.93721 time= 0.17100
Epoch: 0123 train_loss= 0.04953 train_acc= 0.99320 val_loss= 0.23109 val_acc= 0.94028 time= 0.18500
Epoch: 0124 train_loss= 0.04905 train_acc= 0.99422 val_loss= 0.22928 val_acc= 0.94181 time= 0.16916
Epoch: 0125 train_loss= 0.04736 train_acc= 0.99405 val_loss= 0.22847 val_acc= 0.94028 time= 0.16801
Epoch: 0126 train_loss= 0.04606 train_acc= 0.99490 val_loss= 0.22867 val_acc= 0.94028 time= 0.18896
Epoch: 0127 train_loss= 0.04430 train_acc= 0.99490 val_loss= 0.22953 val_acc= 0.94181 time= 0.16903
Epoch: 0128 train_loss= 0.04270 train_acc= 0.99507 val_loss= 0.23097 val_acc= 0.93874 time= 0.16800
Epoch: 0129 train_loss= 0.04194 train_acc= 0.99558 val_loss= 0.23276 val_acc= 0.93874 time= 0.19300
Early stopping...
Optimization Finished!
Test set results: cost= 0.24980 accuracy= 0.93731 time= 0.07700
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7931    0.9200    0.8519        75
           4     1.0000    1.0000    1.0000         9
           5     0.8478    0.8966    0.8715        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.5556    0.7143         9
          10     0.9200    0.6389    0.7541        36
          11     1.0000    0.9167    0.9565        12
          12     0.8696    0.9917    0.9266       121
          13     0.9375    0.7895    0.8571        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7778    0.8235    0.8000        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9654    0.9626    0.9640       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6000    0.9000    0.7200        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8118    0.8519    0.8313        81
          36     1.0000    0.4167    0.5882        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.7778    0.7778    0.7778         9
          43     1.0000    0.3333    0.5000         3
          44     0.9091    0.8333    0.8696        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7561    0.6895    0.7005      2568
weighted avg     0.9354    0.9373    0.9332      2568

Macro average Test Precision, Recall and F1-Score...
(0.7561465722058067, 0.6895128547154104, 0.700506766152417, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[ 1.5802388   0.02286724  0.02398437 ...  0.21313137 -0.03747803
   0.25287274]
 [ 0.7086596   0.59096724  0.07237881 ... -0.05302637  0.17618474
   0.4413538 ]
 [ 0.8644187   0.53939635  0.08093976 ...  0.67280793  0.10392781
   0.7681928 ]
 ...
 [ 0.19907083  0.27883518  0.07133646 ...  0.09111179  0.02600265
   0.12181266]
 [ 0.41647387  0.19502343  0.04675745 ...  0.29567546  0.07016599
   0.32301903]
 [ 0.20944516  0.3233769   0.09047875 ...  0.14613698  0.26832467
   0.22050498]]

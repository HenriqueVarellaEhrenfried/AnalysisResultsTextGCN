(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95137 train_acc= 0.00272 val_loss= 3.93082 val_acc= 0.67381 time= 0.44600
Epoch: 0002 train_loss= 3.93080 train_acc= 0.64807 val_loss= 3.89702 val_acc= 0.67075 time= 0.18811
Epoch: 0003 train_loss= 3.89891 train_acc= 0.64586 val_loss= 3.84923 val_acc= 0.66922 time= 0.17000
Epoch: 0004 train_loss= 3.84975 train_acc= 0.64433 val_loss= 3.78641 val_acc= 0.66922 time= 0.17144
Epoch: 0005 train_loss= 3.78797 train_acc= 0.64195 val_loss= 3.70769 val_acc= 0.66922 time= 0.18401
Epoch: 0006 train_loss= 3.70826 train_acc= 0.64331 val_loss= 3.61264 val_acc= 0.67075 time= 0.16700
Epoch: 0007 train_loss= 3.61572 train_acc= 0.64348 val_loss= 3.50164 val_acc= 0.67075 time= 0.18400
Epoch: 0008 train_loss= 3.50510 train_acc= 0.64501 val_loss= 3.37609 val_acc= 0.66922 time= 0.16699
Epoch: 0009 train_loss= 3.37342 train_acc= 0.64382 val_loss= 3.23877 val_acc= 0.66922 time= 0.16900
Epoch: 0010 train_loss= 3.23599 train_acc= 0.64518 val_loss= 3.09364 val_acc= 0.66922 time= 0.19097
Epoch: 0011 train_loss= 3.09095 train_acc= 0.64280 val_loss= 2.94538 val_acc= 0.66616 time= 0.17000
Epoch: 0012 train_loss= 2.95469 train_acc= 0.64007 val_loss= 2.79890 val_acc= 0.66462 time= 0.16904
Epoch: 0013 train_loss= 2.80568 train_acc= 0.64263 val_loss= 2.65969 val_acc= 0.66769 time= 0.16801
Epoch: 0014 train_loss= 2.65450 train_acc= 0.64195 val_loss= 2.53408 val_acc= 0.66769 time= 0.16698
Epoch: 0015 train_loss= 2.54661 train_acc= 0.64229 val_loss= 2.42798 val_acc= 0.67075 time= 0.16701
Epoch: 0016 train_loss= 2.43597 train_acc= 0.63650 val_loss= 2.34427 val_acc= 0.66616 time= 0.17799
Epoch: 0017 train_loss= 2.34917 train_acc= 0.64280 val_loss= 2.28146 val_acc= 0.64165 time= 0.16801
Epoch: 0018 train_loss= 2.28877 train_acc= 0.61898 val_loss= 2.23432 val_acc= 0.55436 time= 0.16917
Epoch: 0019 train_loss= 2.24163 train_acc= 0.54941 val_loss= 2.19634 val_acc= 0.48239 time= 0.17200
Epoch: 0020 train_loss= 2.19983 train_acc= 0.47525 val_loss= 2.16188 val_acc= 0.46248 time= 0.17004
Epoch: 0021 train_loss= 2.17268 train_acc= 0.43681 val_loss= 2.12683 val_acc= 0.45636 time= 0.17096
Epoch: 0022 train_loss= 2.13773 train_acc= 0.43443 val_loss= 2.08865 val_acc= 0.45636 time= 0.18303
Epoch: 0023 train_loss= 2.10272 train_acc= 0.43341 val_loss= 2.04646 val_acc= 0.45636 time= 0.16908
Epoch: 0024 train_loss= 2.07565 train_acc= 0.43239 val_loss= 2.00043 val_acc= 0.45636 time= 0.17100
Epoch: 0025 train_loss= 2.02620 train_acc= 0.43273 val_loss= 1.95166 val_acc= 0.45636 time= 0.18500
Epoch: 0026 train_loss= 1.97134 train_acc= 0.43239 val_loss= 1.90177 val_acc= 0.45636 time= 0.16900
Epoch: 0027 train_loss= 1.91457 train_acc= 0.43409 val_loss= 1.85270 val_acc= 0.46401 time= 0.17300
Epoch: 0028 train_loss= 1.86524 train_acc= 0.43647 val_loss= 1.80617 val_acc= 0.47626 time= 0.18800
Epoch: 0029 train_loss= 1.84654 train_acc= 0.44633 val_loss= 1.76312 val_acc= 0.50077 time= 0.16899
Epoch: 0030 train_loss= 1.78424 train_acc= 0.48120 val_loss= 1.72359 val_acc= 0.54364 time= 0.18900
Epoch: 0031 train_loss= 1.75776 train_acc= 0.53138 val_loss= 1.68681 val_acc= 0.60643 time= 0.16600
Epoch: 0032 train_loss= 1.71955 train_acc= 0.59228 val_loss= 1.65171 val_acc= 0.64931 time= 0.16803
Epoch: 0033 train_loss= 1.66473 train_acc= 0.63446 val_loss= 1.61745 val_acc= 0.66616 time= 0.18800
Epoch: 0034 train_loss= 1.64109 train_acc= 0.65470 val_loss= 1.58335 val_acc= 0.67841 time= 0.17100
Epoch: 0035 train_loss= 1.60187 train_acc= 0.66236 val_loss= 1.54956 val_acc= 0.68147 time= 0.16900
Epoch: 0036 train_loss= 1.58508 train_acc= 0.65913 val_loss= 1.51608 val_acc= 0.68453 time= 0.18004
Epoch: 0037 train_loss= 1.53255 train_acc= 0.66423 val_loss= 1.48366 val_acc= 0.68913 time= 0.16900
Epoch: 0038 train_loss= 1.50789 train_acc= 0.66746 val_loss= 1.45244 val_acc= 0.69219 time= 0.16700
Epoch: 0039 train_loss= 1.47258 train_acc= 0.67188 val_loss= 1.42270 val_acc= 0.69525 time= 0.18400
Epoch: 0040 train_loss= 1.44433 train_acc= 0.67750 val_loss= 1.39441 val_acc= 0.69985 time= 0.16800
Epoch: 0041 train_loss= 1.41386 train_acc= 0.67835 val_loss= 1.36748 val_acc= 0.69832 time= 0.17000
Epoch: 0042 train_loss= 1.38719 train_acc= 0.68600 val_loss= 1.34178 val_acc= 0.70291 time= 0.18800
Epoch: 0043 train_loss= 1.36061 train_acc= 0.69400 val_loss= 1.31716 val_acc= 0.70597 time= 0.17040
Epoch: 0044 train_loss= 1.33391 train_acc= 0.70301 val_loss= 1.29342 val_acc= 0.71057 time= 0.17200
Epoch: 0045 train_loss= 1.31439 train_acc= 0.70607 val_loss= 1.27039 val_acc= 0.71363 time= 0.18308
Epoch: 0046 train_loss= 1.29067 train_acc= 0.70794 val_loss= 1.24794 val_acc= 0.72282 time= 0.16901
Epoch: 0047 train_loss= 1.27013 train_acc= 0.71730 val_loss= 1.22603 val_acc= 0.72588 time= 0.16899
Epoch: 0048 train_loss= 1.24656 train_acc= 0.72274 val_loss= 1.20455 val_acc= 0.72894 time= 0.17101
Epoch: 0049 train_loss= 1.22097 train_acc= 0.73023 val_loss= 1.18350 val_acc= 0.73354 time= 0.16799
Epoch: 0050 train_loss= 1.20492 train_acc= 0.73397 val_loss= 1.16291 val_acc= 0.73966 time= 0.17000
Epoch: 0051 train_loss= 1.18037 train_acc= 0.73975 val_loss= 1.14278 val_acc= 0.74732 time= 0.19097
Epoch: 0052 train_loss= 1.15977 train_acc= 0.74843 val_loss= 1.12312 val_acc= 0.74732 time= 0.17049
Epoch: 0053 train_loss= 1.14517 train_acc= 0.75166 val_loss= 1.10393 val_acc= 0.75191 time= 0.18604
Epoch: 0054 train_loss= 1.12125 train_acc= 0.75404 val_loss= 1.08521 val_acc= 0.75957 time= 0.16600
Epoch: 0055 train_loss= 1.09995 train_acc= 0.76612 val_loss= 1.06686 val_acc= 0.77335 time= 0.16697
Epoch: 0056 train_loss= 1.08786 train_acc= 0.77088 val_loss= 1.04882 val_acc= 0.77948 time= 0.16803
Epoch: 0057 train_loss= 1.06354 train_acc= 0.77836 val_loss= 1.03109 val_acc= 0.78254 time= 0.16599
Epoch: 0058 train_loss= 1.04015 train_acc= 0.78721 val_loss= 1.01360 val_acc= 0.79326 time= 0.17000
Epoch: 0059 train_loss= 1.02348 train_acc= 0.79214 val_loss= 0.99628 val_acc= 0.79786 time= 0.18797
Epoch: 0060 train_loss= 1.00759 train_acc= 0.79673 val_loss= 0.97915 val_acc= 0.80398 time= 0.17000
Epoch: 0061 train_loss= 0.99126 train_acc= 0.80048 val_loss= 0.96222 val_acc= 0.80704 time= 0.17144
Epoch: 0062 train_loss= 0.97848 train_acc= 0.80439 val_loss= 0.94550 val_acc= 0.80704 time= 0.18501
Epoch: 0063 train_loss= 0.95356 train_acc= 0.81170 val_loss= 0.92891 val_acc= 0.80704 time= 0.16800
Epoch: 0064 train_loss= 0.94524 train_acc= 0.81034 val_loss= 0.91256 val_acc= 0.81011 time= 0.16999
Epoch: 0065 train_loss= 0.92212 train_acc= 0.81613 val_loss= 0.89639 val_acc= 0.81164 time= 0.18454
Epoch: 0066 train_loss= 0.90414 train_acc= 0.82582 val_loss= 0.88037 val_acc= 0.81470 time= 0.16707
Epoch: 0067 train_loss= 0.88892 train_acc= 0.82361 val_loss= 0.86454 val_acc= 0.81930 time= 0.17200
Epoch: 0068 train_loss= 0.87351 train_acc= 0.82548 val_loss= 0.84898 val_acc= 0.82695 time= 0.18800
Epoch: 0069 train_loss= 0.86219 train_acc= 0.82718 val_loss= 0.83365 val_acc= 0.83308 time= 0.17200
Epoch: 0070 train_loss= 0.84357 train_acc= 0.82973 val_loss= 0.81842 val_acc= 0.83767 time= 0.17500
Epoch: 0071 train_loss= 0.81745 train_acc= 0.83603 val_loss= 0.80344 val_acc= 0.83920 time= 0.16701
Epoch: 0072 train_loss= 0.80754 train_acc= 0.84028 val_loss= 0.78866 val_acc= 0.84227 time= 0.16899
Epoch: 0073 train_loss= 0.79231 train_acc= 0.84164 val_loss= 0.77411 val_acc= 0.84533 time= 0.18801
Epoch: 0074 train_loss= 0.77997 train_acc= 0.84334 val_loss= 0.75978 val_acc= 0.84533 time= 0.16600
Epoch: 0075 train_loss= 0.76229 train_acc= 0.85031 val_loss= 0.74571 val_acc= 0.84533 time= 0.16835
Epoch: 0076 train_loss= 0.74639 train_acc= 0.85117 val_loss= 0.73184 val_acc= 0.84992 time= 0.19199
Epoch: 0077 train_loss= 0.73178 train_acc= 0.85627 val_loss= 0.71817 val_acc= 0.85145 time= 0.17001
Epoch: 0078 train_loss= 0.71440 train_acc= 0.85491 val_loss= 0.70484 val_acc= 0.85605 time= 0.16600
Epoch: 0079 train_loss= 0.70418 train_acc= 0.86035 val_loss= 0.69186 val_acc= 0.86217 time= 0.18900
Epoch: 0080 train_loss= 0.69219 train_acc= 0.86069 val_loss= 0.67916 val_acc= 0.86524 time= 0.16600
Epoch: 0081 train_loss= 0.67619 train_acc= 0.86001 val_loss= 0.66678 val_acc= 0.86524 time= 0.16799
Epoch: 0082 train_loss= 0.66797 train_acc= 0.86001 val_loss= 0.65480 val_acc= 0.86677 time= 0.17102
Epoch: 0083 train_loss= 0.64797 train_acc= 0.87328 val_loss= 0.64311 val_acc= 0.86677 time= 0.16899
Epoch: 0084 train_loss= 0.64221 train_acc= 0.86886 val_loss= 0.63165 val_acc= 0.86830 time= 0.17086
Epoch: 0085 train_loss= 0.62928 train_acc= 0.87039 val_loss= 0.62031 val_acc= 0.86983 time= 0.19237
Epoch: 0086 train_loss= 0.62057 train_acc= 0.87753 val_loss= 0.60901 val_acc= 0.86983 time= 0.16900
Epoch: 0087 train_loss= 0.60379 train_acc= 0.87702 val_loss= 0.59801 val_acc= 0.87289 time= 0.16800
Epoch: 0088 train_loss= 0.59267 train_acc= 0.87770 val_loss= 0.58733 val_acc= 0.87596 time= 0.18500
Epoch: 0089 train_loss= 0.57713 train_acc= 0.88127 val_loss= 0.57688 val_acc= 0.87443 time= 0.16701
Epoch: 0090 train_loss= 0.56087 train_acc= 0.88621 val_loss= 0.56671 val_acc= 0.87596 time= 0.16899
Epoch: 0091 train_loss= 0.55783 train_acc= 0.88621 val_loss= 0.55681 val_acc= 0.87902 time= 0.18504
Epoch: 0092 train_loss= 0.54875 train_acc= 0.88757 val_loss= 0.54718 val_acc= 0.88208 time= 0.17158
Epoch: 0093 train_loss= 0.53105 train_acc= 0.89250 val_loss= 0.53784 val_acc= 0.88515 time= 0.17272
Epoch: 0094 train_loss= 0.51916 train_acc= 0.89386 val_loss= 0.52874 val_acc= 0.88668 time= 0.17303
Epoch: 0095 train_loss= 0.51383 train_acc= 0.89233 val_loss= 0.51986 val_acc= 0.88821 time= 0.16597
Epoch: 0096 train_loss= 0.50498 train_acc= 0.89590 val_loss= 0.51111 val_acc= 0.88821 time= 0.16900
Epoch: 0097 train_loss= 0.50256 train_acc= 0.89539 val_loss= 0.50261 val_acc= 0.88821 time= 0.18503
Epoch: 0098 train_loss= 0.48393 train_acc= 0.90134 val_loss= 0.49432 val_acc= 0.89127 time= 0.16800
Epoch: 0099 train_loss= 0.48143 train_acc= 0.90032 val_loss= 0.48630 val_acc= 0.89127 time= 0.18397
Epoch: 0100 train_loss= 0.46986 train_acc= 0.90373 val_loss= 0.47860 val_acc= 0.89433 time= 0.17100
Epoch: 0101 train_loss= 0.45978 train_acc= 0.90304 val_loss= 0.47127 val_acc= 0.89127 time= 0.17100
Epoch: 0102 train_loss= 0.45151 train_acc= 0.90560 val_loss= 0.46411 val_acc= 0.89280 time= 0.18803
Epoch: 0103 train_loss= 0.43979 train_acc= 0.90883 val_loss= 0.45705 val_acc= 0.89433 time= 0.16715
Epoch: 0104 train_loss= 0.43083 train_acc= 0.91121 val_loss= 0.45003 val_acc= 0.89740 time= 0.16900
Epoch: 0105 train_loss= 0.42814 train_acc= 0.91546 val_loss= 0.44328 val_acc= 0.90046 time= 0.16801
Epoch: 0106 train_loss= 0.41411 train_acc= 0.91631 val_loss= 0.43665 val_acc= 0.90046 time= 0.16699
Epoch: 0107 train_loss= 0.41208 train_acc= 0.91750 val_loss= 0.43001 val_acc= 0.90046 time= 0.17300
Epoch: 0108 train_loss= 0.41511 train_acc= 0.91546 val_loss= 0.42388 val_acc= 0.90046 time= 0.16937
Epoch: 0109 train_loss= 0.39467 train_acc= 0.91716 val_loss= 0.41795 val_acc= 0.90046 time= 0.16903
Epoch: 0110 train_loss= 0.39008 train_acc= 0.92125 val_loss= 0.41226 val_acc= 0.90046 time= 0.17400
Epoch: 0111 train_loss= 0.37977 train_acc= 0.92244 val_loss= 0.40669 val_acc= 0.90046 time= 0.17801
Epoch: 0112 train_loss= 0.37625 train_acc= 0.92618 val_loss= 0.40136 val_acc= 0.90046 time= 0.16706
Epoch: 0113 train_loss= 0.37228 train_acc= 0.92227 val_loss= 0.39645 val_acc= 0.90046 time= 0.18901
Epoch: 0114 train_loss= 0.36143 train_acc= 0.92975 val_loss= 0.39167 val_acc= 0.90046 time= 0.16616
Epoch: 0115 train_loss= 0.35781 train_acc= 0.93196 val_loss= 0.38696 val_acc= 0.89893 time= 0.16799
Epoch: 0116 train_loss= 0.35005 train_acc= 0.93213 val_loss= 0.38255 val_acc= 0.89893 time= 0.19197
Epoch: 0117 train_loss= 0.34682 train_acc= 0.93128 val_loss= 0.37803 val_acc= 0.90046 time= 0.17200
Epoch: 0118 train_loss= 0.33563 train_acc= 0.93553 val_loss= 0.37358 val_acc= 0.90046 time= 0.17004
Epoch: 0119 train_loss= 0.33327 train_acc= 0.93026 val_loss= 0.36920 val_acc= 0.90199 time= 0.18799
Epoch: 0120 train_loss= 0.33108 train_acc= 0.93315 val_loss= 0.36537 val_acc= 0.90352 time= 0.16797
Epoch: 0121 train_loss= 0.32585 train_acc= 0.93043 val_loss= 0.36180 val_acc= 0.90505 time= 0.16704
Epoch: 0122 train_loss= 0.31537 train_acc= 0.93740 val_loss= 0.35835 val_acc= 0.90658 time= 0.17114
Epoch: 0123 train_loss= 0.31211 train_acc= 0.93723 val_loss= 0.35505 val_acc= 0.90965 time= 0.16600
Epoch: 0124 train_loss= 0.30591 train_acc= 0.94098 val_loss= 0.35198 val_acc= 0.91118 time= 0.16995
Epoch: 0125 train_loss= 0.29790 train_acc= 0.93979 val_loss= 0.34906 val_acc= 0.91118 time= 0.17700
Epoch: 0126 train_loss= 0.29904 train_acc= 0.94115 val_loss= 0.34588 val_acc= 0.91424 time= 0.17200
Epoch: 0127 train_loss= 0.29781 train_acc= 0.94370 val_loss= 0.34244 val_acc= 0.91271 time= 0.17100
Epoch: 0128 train_loss= 0.28921 train_acc= 0.94251 val_loss= 0.33877 val_acc= 0.91577 time= 0.18404
Epoch: 0129 train_loss= 0.28827 train_acc= 0.94421 val_loss= 0.33537 val_acc= 0.91577 time= 0.16900
Epoch: 0130 train_loss= 0.27876 train_acc= 0.94455 val_loss= 0.33210 val_acc= 0.91730 time= 0.17115
Epoch: 0131 train_loss= 0.27845 train_acc= 0.94795 val_loss= 0.32898 val_acc= 0.91577 time= 0.18597
Epoch: 0132 train_loss= 0.26646 train_acc= 0.95152 val_loss= 0.32618 val_acc= 0.91730 time= 0.17200
Epoch: 0133 train_loss= 0.26370 train_acc= 0.94897 val_loss= 0.32335 val_acc= 0.91730 time= 0.19300
Epoch: 0134 train_loss= 0.26080 train_acc= 0.95033 val_loss= 0.32060 val_acc= 0.91730 time= 0.17361
Epoch: 0135 train_loss= 0.25603 train_acc= 0.95067 val_loss= 0.31781 val_acc= 0.91577 time= 0.16700
Epoch: 0136 train_loss= 0.26018 train_acc= 0.94846 val_loss= 0.31489 val_acc= 0.91577 time= 0.17128
Epoch: 0137 train_loss= 0.24837 train_acc= 0.95101 val_loss= 0.31238 val_acc= 0.91577 time= 0.18600
Epoch: 0138 train_loss= 0.24301 train_acc= 0.95322 val_loss= 0.31040 val_acc= 0.91730 time= 0.16700
Epoch: 0139 train_loss= 0.24082 train_acc= 0.95305 val_loss= 0.30827 val_acc= 0.91884 time= 0.17100
Epoch: 0140 train_loss= 0.23698 train_acc= 0.95203 val_loss= 0.30635 val_acc= 0.92037 time= 0.17418
Epoch: 0141 train_loss= 0.23807 train_acc= 0.95577 val_loss= 0.30473 val_acc= 0.91884 time= 0.17075
Epoch: 0142 train_loss= 0.22940 train_acc= 0.95612 val_loss= 0.30303 val_acc= 0.92190 time= 0.17300
Epoch: 0143 train_loss= 0.23064 train_acc= 0.95612 val_loss= 0.30096 val_acc= 0.92190 time= 0.18700
Epoch: 0144 train_loss= 0.22258 train_acc= 0.95901 val_loss= 0.29908 val_acc= 0.92190 time= 0.16805
Epoch: 0145 train_loss= 0.22429 train_acc= 0.95748 val_loss= 0.29689 val_acc= 0.92190 time= 0.18700
Epoch: 0146 train_loss= 0.21889 train_acc= 0.95629 val_loss= 0.29509 val_acc= 0.92190 time= 0.16595
Epoch: 0147 train_loss= 0.21598 train_acc= 0.95748 val_loss= 0.29338 val_acc= 0.92037 time= 0.16705
Epoch: 0148 train_loss= 0.21191 train_acc= 0.95612 val_loss= 0.29137 val_acc= 0.92343 time= 0.18995
Epoch: 0149 train_loss= 0.20592 train_acc= 0.96037 val_loss= 0.28972 val_acc= 0.92343 time= 0.17304
Epoch: 0150 train_loss= 0.20791 train_acc= 0.95748 val_loss= 0.28823 val_acc= 0.92649 time= 0.17500
Epoch: 0151 train_loss= 0.20884 train_acc= 0.95884 val_loss= 0.28658 val_acc= 0.92649 time= 0.18800
Epoch: 0152 train_loss= 0.19987 train_acc= 0.95731 val_loss= 0.28500 val_acc= 0.92649 time= 0.16700
Epoch: 0153 train_loss= 0.20028 train_acc= 0.96156 val_loss= 0.28354 val_acc= 0.92649 time= 0.17104
Epoch: 0154 train_loss= 0.19633 train_acc= 0.96190 val_loss= 0.28246 val_acc= 0.92496 time= 0.18301
Epoch: 0155 train_loss= 0.19033 train_acc= 0.96360 val_loss= 0.28141 val_acc= 0.92343 time= 0.16800
Epoch: 0156 train_loss= 0.18764 train_acc= 0.96462 val_loss= 0.27955 val_acc= 0.92496 time= 0.16900
Epoch: 0157 train_loss= 0.18641 train_acc= 0.96479 val_loss= 0.27815 val_acc= 0.92649 time= 0.19096
Epoch: 0158 train_loss= 0.19162 train_acc= 0.96343 val_loss= 0.27650 val_acc= 0.92649 time= 0.17100
Epoch: 0159 train_loss= 0.18193 train_acc= 0.96751 val_loss= 0.27506 val_acc= 0.92649 time= 0.17100
Epoch: 0160 train_loss= 0.17532 train_acc= 0.96700 val_loss= 0.27385 val_acc= 0.92802 time= 0.18500
Epoch: 0161 train_loss= 0.17851 train_acc= 0.96802 val_loss= 0.27277 val_acc= 0.92496 time= 0.16905
Epoch: 0162 train_loss= 0.17753 train_acc= 0.96479 val_loss= 0.27182 val_acc= 0.92343 time= 0.18399
Epoch: 0163 train_loss= 0.17313 train_acc= 0.96870 val_loss= 0.27077 val_acc= 0.92190 time= 0.16596
Epoch: 0164 train_loss= 0.17140 train_acc= 0.96632 val_loss= 0.26998 val_acc= 0.92343 time= 0.16731
Epoch: 0165 train_loss= 0.16908 train_acc= 0.96530 val_loss= 0.26964 val_acc= 0.92343 time= 0.19338
Epoch: 0166 train_loss= 0.16670 train_acc= 0.96615 val_loss= 0.26966 val_acc= 0.92343 time= 0.16962
Epoch: 0167 train_loss= 0.16434 train_acc= 0.97023 val_loss= 0.26923 val_acc= 0.92496 time= 0.16903
Epoch: 0168 train_loss= 0.15916 train_acc= 0.96938 val_loss= 0.26859 val_acc= 0.92802 time= 0.16696
Epoch: 0169 train_loss= 0.16091 train_acc= 0.96955 val_loss= 0.26777 val_acc= 0.92802 time= 0.16800
Epoch: 0170 train_loss= 0.15675 train_acc= 0.97193 val_loss= 0.26649 val_acc= 0.92956 time= 0.17100
Epoch: 0171 train_loss= 0.15566 train_acc= 0.97125 val_loss= 0.26499 val_acc= 0.92802 time= 0.18711
Epoch: 0172 train_loss= 0.15454 train_acc= 0.97142 val_loss= 0.26311 val_acc= 0.92802 time= 0.16796
Epoch: 0173 train_loss= 0.15389 train_acc= 0.97125 val_loss= 0.26135 val_acc= 0.92649 time= 0.17300
Epoch: 0174 train_loss= 0.15298 train_acc= 0.97023 val_loss= 0.25959 val_acc= 0.92496 time= 0.19000
Epoch: 0175 train_loss= 0.15211 train_acc= 0.97040 val_loss= 0.25852 val_acc= 0.92496 time= 0.16800
Epoch: 0176 train_loss= 0.14323 train_acc= 0.97210 val_loss= 0.25766 val_acc= 0.92649 time= 0.16900
Epoch: 0177 train_loss= 0.14457 train_acc= 0.97244 val_loss= 0.25680 val_acc= 0.92956 time= 0.18497
Epoch: 0178 train_loss= 0.14203 train_acc= 0.97346 val_loss= 0.25581 val_acc= 0.92956 time= 0.17000
Epoch: 0179 train_loss= 0.14219 train_acc= 0.97312 val_loss= 0.25429 val_acc= 0.92956 time= 0.17100
Epoch: 0180 train_loss= 0.14183 train_acc= 0.97125 val_loss= 0.25326 val_acc= 0.93109 time= 0.16900
Epoch: 0181 train_loss= 0.13764 train_acc= 0.97432 val_loss= 0.25227 val_acc= 0.93109 time= 0.17100
Epoch: 0182 train_loss= 0.13665 train_acc= 0.97398 val_loss= 0.25204 val_acc= 0.93262 time= 0.17306
Epoch: 0183 train_loss= 0.13534 train_acc= 0.97568 val_loss= 0.25153 val_acc= 0.93262 time= 0.18803
Epoch: 0184 train_loss= 0.13589 train_acc= 0.97500 val_loss= 0.25120 val_acc= 0.93262 time= 0.16800
Epoch: 0185 train_loss= 0.13512 train_acc= 0.97432 val_loss= 0.25132 val_acc= 0.92956 time= 0.18601
Epoch: 0186 train_loss= 0.13492 train_acc= 0.97517 val_loss= 0.25077 val_acc= 0.93109 time= 0.16600
Epoch: 0187 train_loss= 0.12850 train_acc= 0.97823 val_loss= 0.25051 val_acc= 0.93262 time= 0.16799
Epoch: 0188 train_loss= 0.13063 train_acc= 0.97551 val_loss= 0.25031 val_acc= 0.93262 time= 0.17001
Epoch: 0189 train_loss= 0.12311 train_acc= 0.97789 val_loss= 0.25012 val_acc= 0.92802 time= 0.16926
Epoch: 0190 train_loss= 0.12138 train_acc= 0.97823 val_loss= 0.25038 val_acc= 0.93109 time= 0.17300
Epoch: 0191 train_loss= 0.12397 train_acc= 0.97534 val_loss= 0.24935 val_acc= 0.92956 time= 0.19000
Epoch: 0192 train_loss= 0.11768 train_acc= 0.97738 val_loss= 0.24814 val_acc= 0.93109 time= 0.16800
Epoch: 0193 train_loss= 0.12213 train_acc= 0.97857 val_loss= 0.24650 val_acc= 0.92956 time= 0.17139
Epoch: 0194 train_loss= 0.11583 train_acc= 0.97993 val_loss= 0.24523 val_acc= 0.93109 time= 0.18702
Epoch: 0195 train_loss= 0.11673 train_acc= 0.97738 val_loss= 0.24411 val_acc= 0.93262 time= 0.16700
Epoch: 0196 train_loss= 0.11890 train_acc= 0.97908 val_loss= 0.24295 val_acc= 0.93415 time= 0.17097
Epoch: 0197 train_loss= 0.11884 train_acc= 0.97619 val_loss= 0.24204 val_acc= 0.93262 time= 0.18700
Epoch: 0198 train_loss= 0.11306 train_acc= 0.98010 val_loss= 0.24141 val_acc= 0.93262 time= 0.17000
Epoch: 0199 train_loss= 0.10864 train_acc= 0.98146 val_loss= 0.24112 val_acc= 0.93415 time= 0.17200
Epoch: 0200 train_loss= 0.11095 train_acc= 0.98146 val_loss= 0.24070 val_acc= 0.93415 time= 0.18700
Epoch: 0201 train_loss= 0.10793 train_acc= 0.97959 val_loss= 0.24045 val_acc= 0.93262 time= 0.16803
Epoch: 0202 train_loss= 0.10912 train_acc= 0.98044 val_loss= 0.24016 val_acc= 0.93109 time= 0.19008
Epoch: 0203 train_loss= 0.10686 train_acc= 0.98146 val_loss= 0.24035 val_acc= 0.93262 time= 0.16800
Epoch: 0204 train_loss= 0.10885 train_acc= 0.97942 val_loss= 0.24002 val_acc= 0.93262 time= 0.16900
Epoch: 0205 train_loss= 0.10711 train_acc= 0.97891 val_loss= 0.23975 val_acc= 0.93262 time= 0.19197
Epoch: 0206 train_loss= 0.10620 train_acc= 0.98010 val_loss= 0.23914 val_acc= 0.93415 time= 0.16907
Epoch: 0207 train_loss= 0.10478 train_acc= 0.98333 val_loss= 0.23835 val_acc= 0.93415 time= 0.17200
Epoch: 0208 train_loss= 0.10028 train_acc= 0.98350 val_loss= 0.23791 val_acc= 0.93262 time= 0.17103
Epoch: 0209 train_loss= 0.10175 train_acc= 0.98452 val_loss= 0.23756 val_acc= 0.93262 time= 0.16697
Epoch: 0210 train_loss= 0.09751 train_acc= 0.98469 val_loss= 0.23708 val_acc= 0.93415 time= 0.16800
Epoch: 0211 train_loss= 0.09876 train_acc= 0.98299 val_loss= 0.23644 val_acc= 0.93109 time= 0.18811
Epoch: 0212 train_loss= 0.09507 train_acc= 0.98316 val_loss= 0.23632 val_acc= 0.93415 time= 0.16800
Epoch: 0213 train_loss= 0.09747 train_acc= 0.98435 val_loss= 0.23656 val_acc= 0.93415 time= 0.16896
Epoch: 0214 train_loss= 0.09473 train_acc= 0.98452 val_loss= 0.23699 val_acc= 0.93568 time= 0.19500
Epoch: 0215 train_loss= 0.09735 train_acc= 0.98248 val_loss= 0.23716 val_acc= 0.93568 time= 0.17300
Epoch: 0216 train_loss= 0.09423 train_acc= 0.98350 val_loss= 0.23731 val_acc= 0.93721 time= 0.17200
Epoch: 0217 train_loss= 0.09317 train_acc= 0.98554 val_loss= 0.23731 val_acc= 0.93568 time= 0.18205
Early stopping...
Optimization Finished!
Test set results: cost= 0.26928 accuracy= 0.93458 time= 0.07501
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7912    0.9600    0.8675        75
           4     1.0000    1.0000    1.0000         9
           5     0.8061    0.9080    0.8541        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.8333    0.9091    0.8696        11
           9     1.0000    0.5556    0.7143         9
          10     0.8846    0.6389    0.7419        36
          11     1.0000    0.9167    0.9565        12
          12     0.8219    0.9917    0.8989       121
          13     0.9286    0.6842    0.7879        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.5556    0.7143         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7857    0.6471    0.7097        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9641    0.9641    0.9641       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6667    1.0000    0.8000        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8553    0.8025    0.8280        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9755    0.9917    0.9835      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.7778    0.8750         9
          43     1.0000    0.6667    0.8000         3
          44     0.7692    0.8333    0.8000        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9346      2568
   macro avg     0.7796    0.6778    0.6984      2568
weighted avg     0.9351    0.9346    0.9298      2568

Macro average Test Precision, Recall and F1-Score...
(0.7796452706675592, 0.6777820160708944, 0.6983545789440344, None)
Micro average Test Precision, Recall and F1-Score...
(0.9345794392523364, 0.9345794392523364, 0.9345794392523366, None)
embeddings:
8892 6532 2568
[[-0.01768198 -0.0647991  -0.07716174 ...  1.1960553   0.00140389
   1.2567267 ]
 [ 0.11485005 -0.04467958  0.43456203 ...  0.46725142  0.5958215
   0.7854823 ]
 [ 0.14004287 -0.05178577  0.1431445  ...  0.69609433  0.5324877
   0.5412311 ]
 ...
 [ 0.02397756 -0.02681797  0.17271602 ...  0.41802397  0.18409906
   0.14944646]
 [ 0.08002847 -0.02967547  0.09983937 ...  0.35681668  0.22205266
   0.17555843]
 [ 0.29778662 -0.04668498  0.30475637 ...  0.22450593  0.34150642
   0.4714388 ]]

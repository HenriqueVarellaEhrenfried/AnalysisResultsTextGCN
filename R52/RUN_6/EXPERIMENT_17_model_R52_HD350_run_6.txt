(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95118 train_acc= 0.00782 val_loss= 3.87461 val_acc= 0.66922 time= 2.33501
Epoch: 0002 train_loss= 3.87602 train_acc= 0.65283 val_loss= 3.71392 val_acc= 0.67381 time= 2.21300
Epoch: 0003 train_loss= 3.71549 train_acc= 0.65453 val_loss= 3.45986 val_acc= 0.67381 time= 2.20200
Epoch: 0004 train_loss= 3.46804 train_acc= 0.65164 val_loss= 3.12708 val_acc= 0.66922 time= 2.20618
Epoch: 0005 train_loss= 3.14088 train_acc= 0.64977 val_loss= 2.76784 val_acc= 0.65850 time= 2.20000
Epoch: 0006 train_loss= 2.76685 train_acc= 0.64535 val_loss= 2.45794 val_acc= 0.65084 time= 2.20146
Epoch: 0007 train_loss= 2.46257 train_acc= 0.63752 val_loss= 2.26891 val_acc= 0.63553 time= 2.20500
Epoch: 0008 train_loss= 2.26799 train_acc= 0.62341 val_loss= 2.19252 val_acc= 0.55590 time= 2.20400
Epoch: 0009 train_loss= 2.19896 train_acc= 0.55060 val_loss= 2.15241 val_acc= 0.48392 time= 2.19400
Epoch: 0010 train_loss= 2.15918 train_acc= 0.46419 val_loss= 2.09457 val_acc= 0.47167 time= 2.19000
Epoch: 0011 train_loss= 2.11417 train_acc= 0.44736 val_loss= 2.00338 val_acc= 0.47933 time= 2.20900
Epoch: 0012 train_loss= 2.01455 train_acc= 0.45433 val_loss= 1.88963 val_acc= 0.50995 time= 2.20300
Epoch: 0013 train_loss= 1.91576 train_acc= 0.48869 val_loss= 1.77878 val_acc= 0.59265 time= 2.19100
Epoch: 0014 train_loss= 1.80610 train_acc= 0.57969 val_loss= 1.69187 val_acc= 0.65544 time= 2.19798
Epoch: 0015 train_loss= 1.71535 train_acc= 0.64212 val_loss= 1.62725 val_acc= 0.67228 time= 2.20300
Epoch: 0016 train_loss= 1.65507 train_acc= 0.65062 val_loss= 1.56819 val_acc= 0.68147 time= 2.20107
Epoch: 0017 train_loss= 1.59812 train_acc= 0.65249 val_loss= 1.50572 val_acc= 0.67381 time= 2.17800
Epoch: 0018 train_loss= 1.53528 train_acc= 0.65725 val_loss= 1.44194 val_acc= 0.67534 time= 2.18774
Epoch: 0019 train_loss= 1.47291 train_acc= 0.66032 val_loss= 1.38194 val_acc= 0.67841 time= 2.18303
Epoch: 0020 train_loss= 1.41249 train_acc= 0.66423 val_loss= 1.32904 val_acc= 0.68760 time= 2.19200
Epoch: 0021 train_loss= 1.35873 train_acc= 0.67052 val_loss= 1.28362 val_acc= 0.69372 time= 2.21300
Epoch: 0022 train_loss= 1.31157 train_acc= 0.67733 val_loss= 1.24446 val_acc= 0.70750 time= 2.18600
Epoch: 0023 train_loss= 1.27422 train_acc= 0.68787 val_loss= 1.20967 val_acc= 0.71669 time= 2.19768
Epoch: 0024 train_loss= 1.23139 train_acc= 0.70386 val_loss= 1.17748 val_acc= 0.72282 time= 2.23181
Epoch: 0025 train_loss= 1.20350 train_acc= 0.71696 val_loss= 1.14653 val_acc= 0.72741 time= 2.22000
Epoch: 0026 train_loss= 1.16979 train_acc= 0.72767 val_loss= 1.11583 val_acc= 0.73201 time= 2.18900
Epoch: 0027 train_loss= 1.13197 train_acc= 0.73958 val_loss= 1.08492 val_acc= 0.73966 time= 2.19200
Epoch: 0028 train_loss= 1.10195 train_acc= 0.74758 val_loss= 1.05395 val_acc= 0.74579 time= 2.20025
Epoch: 0029 train_loss= 1.07001 train_acc= 0.75336 val_loss= 1.02333 val_acc= 0.75345 time= 2.18900
Epoch: 0030 train_loss= 1.03382 train_acc= 0.75829 val_loss= 0.99349 val_acc= 0.76110 time= 2.19939
Epoch: 0031 train_loss= 1.00697 train_acc= 0.76561 val_loss= 0.96477 val_acc= 0.77182 time= 2.18600
Epoch: 0032 train_loss= 0.97573 train_acc= 0.77343 val_loss= 0.93712 val_acc= 0.77948 time= 2.21600
Epoch: 0033 train_loss= 0.94748 train_acc= 0.78313 val_loss= 0.91025 val_acc= 0.79020 time= 2.21410
Epoch: 0034 train_loss= 0.92195 train_acc= 0.79367 val_loss= 0.88391 val_acc= 0.80551 time= 2.24401
Epoch: 0035 train_loss= 0.89383 train_acc= 0.80728 val_loss= 0.85824 val_acc= 0.81776 time= 2.20400
Epoch: 0036 train_loss= 0.86778 train_acc= 0.81612 val_loss= 0.83313 val_acc= 0.82848 time= 2.21882
Epoch: 0037 train_loss= 0.84539 train_acc= 0.82106 val_loss= 0.80865 val_acc= 0.83614 time= 2.22199
Epoch: 0038 train_loss= 0.82079 train_acc= 0.83262 val_loss= 0.78483 val_acc= 0.84686 time= 2.22800
Epoch: 0039 train_loss= 0.79239 train_acc= 0.83552 val_loss= 0.76137 val_acc= 0.85146 time= 2.23401
Epoch: 0040 train_loss= 0.77040 train_acc= 0.84198 val_loss= 0.73817 val_acc= 0.85146 time= 2.21900
Epoch: 0041 train_loss= 0.74697 train_acc= 0.84674 val_loss= 0.71551 val_acc= 0.84992 time= 2.20700
Epoch: 0042 train_loss= 0.72186 train_acc= 0.85099 val_loss= 0.69327 val_acc= 0.85146 time= 2.21717
Epoch: 0043 train_loss= 0.69844 train_acc= 0.84997 val_loss= 0.67147 val_acc= 0.85299 time= 2.22789
Epoch: 0044 train_loss= 0.67075 train_acc= 0.85542 val_loss= 0.65033 val_acc= 0.85299 time= 2.21701
Epoch: 0045 train_loss= 0.64788 train_acc= 0.85746 val_loss= 0.62964 val_acc= 0.85605 time= 2.20099
Epoch: 0046 train_loss= 0.62468 train_acc= 0.85814 val_loss= 0.60946 val_acc= 0.86064 time= 2.20360
Epoch: 0047 train_loss= 0.60981 train_acc= 0.86698 val_loss= 0.59000 val_acc= 0.86217 time= 2.27701
Epoch: 0048 train_loss= 0.58435 train_acc= 0.87124 val_loss= 0.57116 val_acc= 0.86677 time= 2.28933
Epoch: 0049 train_loss= 0.56598 train_acc= 0.87362 val_loss= 0.55317 val_acc= 0.87443 time= 2.25900
Epoch: 0050 train_loss= 0.54982 train_acc= 0.87515 val_loss= 0.53611 val_acc= 0.87443 time= 2.22232
Epoch: 0051 train_loss= 0.52747 train_acc= 0.87872 val_loss= 0.52001 val_acc= 0.87749 time= 2.21199
Epoch: 0052 train_loss= 0.50691 train_acc= 0.88093 val_loss= 0.50467 val_acc= 0.87902 time= 2.20500
Epoch: 0053 train_loss= 0.48456 train_acc= 0.88893 val_loss= 0.48991 val_acc= 0.88208 time= 2.21195
Epoch: 0054 train_loss= 0.46888 train_acc= 0.89131 val_loss= 0.47532 val_acc= 0.88515 time= 2.22000
Epoch: 0055 train_loss= 0.45151 train_acc= 0.89522 val_loss= 0.46110 val_acc= 0.88515 time= 2.20680
Epoch: 0056 train_loss= 0.43877 train_acc= 0.89879 val_loss= 0.44711 val_acc= 0.88821 time= 2.23867
Epoch: 0057 train_loss= 0.41805 train_acc= 0.90338 val_loss= 0.43360 val_acc= 0.89433 time= 2.23458
Epoch: 0058 train_loss= 0.40569 train_acc= 0.90798 val_loss= 0.42025 val_acc= 0.89740 time= 2.22100
Epoch: 0059 train_loss= 0.38917 train_acc= 0.91597 val_loss= 0.40757 val_acc= 0.90046 time= 2.22107
Epoch: 0060 train_loss= 0.37844 train_acc= 0.91801 val_loss= 0.39618 val_acc= 0.90046 time= 2.21300
Epoch: 0061 train_loss= 0.36788 train_acc= 0.92073 val_loss= 0.38580 val_acc= 0.90352 time= 2.21000
Epoch: 0062 train_loss= 0.34720 train_acc= 0.92567 val_loss= 0.37630 val_acc= 0.90352 time= 2.22013
Epoch: 0063 train_loss= 0.33310 train_acc= 0.93009 val_loss= 0.36767 val_acc= 0.90505 time= 2.27400
Epoch: 0064 train_loss= 0.32765 train_acc= 0.93179 val_loss= 0.35957 val_acc= 0.90505 time= 2.21646
Epoch: 0065 train_loss= 0.31362 train_acc= 0.93485 val_loss= 0.35167 val_acc= 0.90965 time= 2.23102
Epoch: 0066 train_loss= 0.29999 train_acc= 0.93757 val_loss= 0.34406 val_acc= 0.90965 time= 2.21700
Epoch: 0067 train_loss= 0.28741 train_acc= 0.94200 val_loss= 0.33698 val_acc= 0.90812 time= 2.21800
Epoch: 0068 train_loss= 0.27998 train_acc= 0.94336 val_loss= 0.32966 val_acc= 0.90812 time= 2.22199
Epoch: 0069 train_loss= 0.26698 train_acc= 0.94693 val_loss= 0.32225 val_acc= 0.90965 time= 2.21800
Epoch: 0070 train_loss= 0.26044 train_acc= 0.94727 val_loss= 0.31569 val_acc= 0.91424 time= 2.21402
Epoch: 0071 train_loss= 0.25073 train_acc= 0.95033 val_loss= 0.30981 val_acc= 0.91424 time= 2.20775
Epoch: 0072 train_loss= 0.23959 train_acc= 0.95220 val_loss= 0.30528 val_acc= 0.91731 time= 2.23834
Epoch: 0073 train_loss= 0.22956 train_acc= 0.95441 val_loss= 0.30100 val_acc= 0.92037 time= 2.22492
Epoch: 0074 train_loss= 0.21962 train_acc= 0.95543 val_loss= 0.29755 val_acc= 0.91731 time= 2.22799
Epoch: 0075 train_loss= 0.21450 train_acc= 0.95679 val_loss= 0.29353 val_acc= 0.91731 time= 2.21198
Epoch: 0076 train_loss= 0.20109 train_acc= 0.95969 val_loss= 0.28872 val_acc= 0.91884 time= 2.21200
Epoch: 0077 train_loss= 0.19804 train_acc= 0.95986 val_loss= 0.28421 val_acc= 0.92037 time= 2.22401
Epoch: 0078 train_loss= 0.19257 train_acc= 0.96343 val_loss= 0.27987 val_acc= 0.92649 time= 2.21600
Epoch: 0079 train_loss= 0.18395 train_acc= 0.96139 val_loss= 0.27567 val_acc= 0.92496 time= 2.22399
Epoch: 0080 train_loss= 0.17976 train_acc= 0.96513 val_loss= 0.27191 val_acc= 0.92496 time= 2.20355
Epoch: 0081 train_loss= 0.17525 train_acc= 0.96547 val_loss= 0.26798 val_acc= 0.92496 time= 2.21421
Epoch: 0082 train_loss= 0.16365 train_acc= 0.96819 val_loss= 0.26435 val_acc= 0.92496 time= 2.21500
Epoch: 0083 train_loss= 0.15615 train_acc= 0.97074 val_loss= 0.26104 val_acc= 0.92649 time= 2.23801
Epoch: 0084 train_loss= 0.15808 train_acc= 0.96819 val_loss= 0.25778 val_acc= 0.92802 time= 2.20677
Epoch: 0085 train_loss= 0.15013 train_acc= 0.97091 val_loss= 0.25442 val_acc= 0.92956 time= 2.20801
Epoch: 0086 train_loss= 0.14190 train_acc= 0.97312 val_loss= 0.25270 val_acc= 0.92956 time= 2.21100
Epoch: 0087 train_loss= 0.13979 train_acc= 0.97465 val_loss= 0.25202 val_acc= 0.92802 time= 2.20899
Epoch: 0088 train_loss= 0.13326 train_acc= 0.97312 val_loss= 0.25080 val_acc= 0.92956 time= 2.22001
Epoch: 0089 train_loss= 0.12767 train_acc= 0.97806 val_loss= 0.24891 val_acc= 0.92956 time= 2.21396
Epoch: 0090 train_loss= 0.12502 train_acc= 0.97755 val_loss= 0.24719 val_acc= 0.93109 time= 2.20600
Epoch: 0091 train_loss= 0.11896 train_acc= 0.97959 val_loss= 0.24513 val_acc= 0.92802 time= 2.23805
Epoch: 0092 train_loss= 0.11684 train_acc= 0.97772 val_loss= 0.24340 val_acc= 0.93109 time= 2.20999
Epoch: 0093 train_loss= 0.10740 train_acc= 0.98010 val_loss= 0.24288 val_acc= 0.92956 time= 2.50196
Epoch: 0094 train_loss= 0.11015 train_acc= 0.97857 val_loss= 0.24047 val_acc= 0.92956 time= 3.36595
Epoch: 0095 train_loss= 0.10301 train_acc= 0.98197 val_loss= 0.23746 val_acc= 0.92802 time= 2.32670
Epoch: 0096 train_loss= 0.10177 train_acc= 0.98231 val_loss= 0.23494 val_acc= 0.93109 time= 2.26190
Epoch: 0097 train_loss= 0.09697 train_acc= 0.98129 val_loss= 0.23268 val_acc= 0.93109 time= 2.25192
Epoch: 0098 train_loss= 0.09318 train_acc= 0.98520 val_loss= 0.23212 val_acc= 0.93415 time= 2.31354
Epoch: 0099 train_loss= 0.09319 train_acc= 0.98316 val_loss= 0.23195 val_acc= 0.93415 time= 2.22997
Epoch: 0100 train_loss= 0.08683 train_acc= 0.98350 val_loss= 0.23211 val_acc= 0.93262 time= 2.23904
Epoch: 0101 train_loss= 0.08488 train_acc= 0.98520 val_loss= 0.23440 val_acc= 0.93415 time= 2.29398
Epoch: 0102 train_loss= 0.08137 train_acc= 0.98673 val_loss= 0.23538 val_acc= 0.93109 time= 2.23503
Epoch: 0103 train_loss= 0.08235 train_acc= 0.98554 val_loss= 0.23364 val_acc= 0.93109 time= 2.21900
Epoch: 0104 train_loss= 0.07836 train_acc= 0.98673 val_loss= 0.23049 val_acc= 0.93721 time= 2.22362
Epoch: 0105 train_loss= 0.07706 train_acc= 0.98673 val_loss= 0.22803 val_acc= 0.93874 time= 2.24500
Epoch: 0106 train_loss= 0.07162 train_acc= 0.98775 val_loss= 0.22639 val_acc= 0.93874 time= 2.28299
Epoch: 0107 train_loss= 0.07257 train_acc= 0.98724 val_loss= 0.22479 val_acc= 0.93874 time= 2.26569
Epoch: 0108 train_loss= 0.07138 train_acc= 0.98809 val_loss= 0.22348 val_acc= 0.93721 time= 2.25579
Epoch: 0109 train_loss= 0.06995 train_acc= 0.98503 val_loss= 0.22371 val_acc= 0.93415 time= 2.22630
Epoch: 0110 train_loss= 0.06572 train_acc= 0.98928 val_loss= 0.22558 val_acc= 0.93415 time= 2.24842
Epoch: 0111 train_loss= 0.06207 train_acc= 0.99047 val_loss= 0.22814 val_acc= 0.93415 time= 2.24405
Epoch: 0112 train_loss= 0.06298 train_acc= 0.98860 val_loss= 0.22825 val_acc= 0.93415 time= 2.22201
Early stopping...
Optimization Finished!
Test set results: cost= 0.25174 accuracy= 0.94003 time= 0.75599
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.7941    0.9310    0.8571        87
           6     0.9200    0.9200    0.9200        25
           7     0.8000    0.9231    0.8571        13
           8     0.8333    0.9091    0.8696        11
           9     1.0000    0.5556    0.7143         9
          10     0.8966    0.7222    0.8000        36
          11     1.0000    0.9167    0.9565        12
          12     0.8832    1.0000    0.9380       121
          13     0.8824    0.7895    0.8333        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8750    0.8235    0.8485        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9670    0.9676       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6250    1.0000    0.7692        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8553    0.8025    0.8280        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9908    0.9858      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8333    0.8333    0.8333        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9400      2568
   macro avg     0.7363    0.6661    0.6820      2568
weighted avg     0.9367    0.9400    0.9351      2568

Macro average Test Precision, Recall and F1-Score...
(0.7363368335160595, 0.666128918493458, 0.6819634002308734, None)
Micro average Test Precision, Recall and F1-Score...
(0.9400311526479751, 0.9400311526479751, 0.9400311526479751, None)
embeddings:
8892 6532 2568
[[-1.0168264e-01 -1.4927475e-01 -8.1138059e-02 ... -4.3507457e-02
   6.6681258e-02  1.6014571e+00]
 [ 1.0122426e-01  5.6258027e-02 -6.0018200e-02 ...  2.9404704e-02
   2.2256395e-01  7.7658671e-01]
 [-4.5354553e-02 -3.2846361e-02 -5.4254621e-02 ... -4.4562474e-02
  -2.0190142e-04  5.3701061e-01]
 ...
 [ 5.2812543e-02 -1.0158466e-02  3.5778627e-02 ...  4.7565468e-02
   1.0417279e-01  2.3247637e-01]
 [ 3.9937548e-02  5.3778952e-03 -3.2816637e-02 ...  8.6379973e-03
   6.0289044e-02  3.2684171e-01]
 [ 2.0785064e-01  1.8284070e-01 -6.5058812e-02 ...  1.8187954e-01
   2.2730951e-01  4.2260805e-01]]

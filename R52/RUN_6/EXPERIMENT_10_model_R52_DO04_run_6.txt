(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95128 train_acc= 0.00680 val_loss= 3.90214 val_acc= 0.64319 time= 0.44465
Epoch: 0002 train_loss= 3.90213 train_acc= 0.63616 val_loss= 3.80779 val_acc= 0.64319 time= 0.16934
Epoch: 0003 train_loss= 3.80843 train_acc= 0.63089 val_loss= 3.66099 val_acc= 0.64165 time= 0.19200
Epoch: 0004 train_loss= 3.66747 train_acc= 0.62970 val_loss= 3.46057 val_acc= 0.64165 time= 0.17076
Epoch: 0005 train_loss= 3.46462 train_acc= 0.63463 val_loss= 3.21448 val_acc= 0.64778 time= 0.18300
Epoch: 0006 train_loss= 3.22792 train_acc= 0.63021 val_loss= 2.94356 val_acc= 0.65237 time= 0.17000
Epoch: 0007 train_loss= 2.95312 train_acc= 0.63412 val_loss= 2.67878 val_acc= 0.65237 time= 0.16705
Epoch: 0008 train_loss= 2.67548 train_acc= 0.63752 val_loss= 2.45555 val_acc= 0.65237 time= 0.19299
Epoch: 0009 train_loss= 2.45875 train_acc= 0.63055 val_loss= 2.30349 val_acc= 0.63247 time= 0.16596
Epoch: 0010 train_loss= 2.30776 train_acc= 0.62579 val_loss= 2.22077 val_acc= 0.56355 time= 0.18100
Epoch: 0011 train_loss= 2.22244 train_acc= 0.55741 val_loss= 2.17368 val_acc= 0.48698 time= 0.16700
Epoch: 0012 train_loss= 2.18467 train_acc= 0.47151 val_loss= 2.12997 val_acc= 0.46401 time= 0.17122
Epoch: 0013 train_loss= 2.14249 train_acc= 0.44276 val_loss= 2.07259 val_acc= 0.46095 time= 0.17400
Epoch: 0014 train_loss= 2.08329 train_acc= 0.43511 val_loss= 1.99712 val_acc= 0.46248 time= 0.19200
Epoch: 0015 train_loss= 2.01005 train_acc= 0.43800 val_loss= 1.90877 val_acc= 0.47320 time= 0.16700
Epoch: 0016 train_loss= 1.92647 train_acc= 0.44361 val_loss= 1.81894 val_acc= 0.50230 time= 0.17905
Epoch: 0017 train_loss= 1.83344 train_acc= 0.48818 val_loss= 1.73966 val_acc= 0.58040 time= 0.16803
Epoch: 0018 train_loss= 1.76717 train_acc= 0.57221 val_loss= 1.67547 val_acc= 0.65237 time= 0.16700
Epoch: 0019 train_loss= 1.69914 train_acc= 0.63378 val_loss= 1.62063 val_acc= 0.67688 time= 0.17097
Epoch: 0020 train_loss= 1.64560 train_acc= 0.66168 val_loss= 1.56740 val_acc= 0.68606 time= 0.19142
Epoch: 0021 train_loss= 1.59060 train_acc= 0.66661 val_loss= 1.51242 val_acc= 0.69372 time= 0.17200
Epoch: 0022 train_loss= 1.53362 train_acc= 0.67154 val_loss= 1.45664 val_acc= 0.68453 time= 0.19000
Epoch: 0023 train_loss= 1.47736 train_acc= 0.67460 val_loss= 1.40265 val_acc= 0.68606 time= 0.17000
Epoch: 0024 train_loss= 1.42284 train_acc= 0.67716 val_loss= 1.35264 val_acc= 0.69372 time= 0.16722
Epoch: 0025 train_loss= 1.36897 train_acc= 0.68532 val_loss= 1.30752 val_acc= 0.69985 time= 0.19300
Epoch: 0026 train_loss= 1.32381 train_acc= 0.68821 val_loss= 1.26696 val_acc= 0.71363 time= 0.16697
Epoch: 0027 train_loss= 1.28720 train_acc= 0.69740 val_loss= 1.23001 val_acc= 0.71975 time= 0.17016
Epoch: 0028 train_loss= 1.24912 train_acc= 0.71441 val_loss= 1.19582 val_acc= 0.72435 time= 0.17170
Epoch: 0029 train_loss= 1.21394 train_acc= 0.72036 val_loss= 1.16361 val_acc= 0.73201 time= 0.17200
Epoch: 0030 train_loss= 1.18019 train_acc= 0.73125 val_loss= 1.13259 val_acc= 0.73354 time= 0.17000
Epoch: 0031 train_loss= 1.14671 train_acc= 0.74434 val_loss= 1.10212 val_acc= 0.73813 time= 0.16803
Epoch: 0032 train_loss= 1.11778 train_acc= 0.74928 val_loss= 1.07191 val_acc= 0.74579 time= 0.16700
Epoch: 0033 train_loss= 1.08179 train_acc= 0.75965 val_loss= 1.04194 val_acc= 0.75345 time= 0.18897
Epoch: 0034 train_loss= 1.05093 train_acc= 0.76646 val_loss= 1.01245 val_acc= 0.76110 time= 0.16700
Epoch: 0035 train_loss= 1.02123 train_acc= 0.76867 val_loss= 0.98374 val_acc= 0.77182 time= 0.16803
Epoch: 0036 train_loss= 0.99258 train_acc= 0.77717 val_loss= 0.95617 val_acc= 0.78254 time= 0.16997
Epoch: 0037 train_loss= 0.96710 train_acc= 0.78483 val_loss= 0.92983 val_acc= 0.79326 time= 0.19532
Epoch: 0038 train_loss= 0.93023 train_acc= 0.79929 val_loss= 0.90459 val_acc= 0.80398 time= 0.17100
Epoch: 0039 train_loss= 0.91337 train_acc= 0.80507 val_loss= 0.88029 val_acc= 0.81317 time= 0.16803
Epoch: 0040 train_loss= 0.88827 train_acc= 0.81391 val_loss= 0.85670 val_acc= 0.82236 time= 0.16701
Epoch: 0041 train_loss= 0.86754 train_acc= 0.81936 val_loss= 0.83374 val_acc= 0.82848 time= 0.16900
Epoch: 0042 train_loss= 0.83853 train_acc= 0.82837 val_loss= 0.81138 val_acc= 0.83002 time= 0.18000
Epoch: 0043 train_loss= 0.81764 train_acc= 0.83262 val_loss= 0.78955 val_acc= 0.83767 time= 0.16600
Epoch: 0044 train_loss= 0.79592 train_acc= 0.83382 val_loss= 0.76827 val_acc= 0.83920 time= 0.16901
Epoch: 0045 train_loss= 0.76679 train_acc= 0.84045 val_loss= 0.74723 val_acc= 0.84074 time= 0.17096
Epoch: 0046 train_loss= 0.74485 train_acc= 0.84538 val_loss= 0.72640 val_acc= 0.84227 time= 0.17100
Epoch: 0047 train_loss= 0.71990 train_acc= 0.84997 val_loss= 0.70567 val_acc= 0.84227 time= 0.17003
Epoch: 0048 train_loss= 0.70350 train_acc= 0.85014 val_loss= 0.68504 val_acc= 0.84992 time= 0.19097
Epoch: 0049 train_loss= 0.67551 train_acc= 0.85355 val_loss= 0.66496 val_acc= 0.86064 time= 0.16761
Epoch: 0050 train_loss= 0.65443 train_acc= 0.86324 val_loss= 0.64526 val_acc= 0.86830 time= 0.16800
Epoch: 0051 train_loss= 0.64035 train_acc= 0.86545 val_loss= 0.62637 val_acc= 0.86830 time= 0.18801
Epoch: 0052 train_loss= 0.61991 train_acc= 0.87107 val_loss= 0.60821 val_acc= 0.87289 time= 0.16800
Epoch: 0053 train_loss= 0.59764 train_acc= 0.87413 val_loss= 0.59092 val_acc= 0.87289 time= 0.17000
Epoch: 0054 train_loss= 0.57429 train_acc= 0.87481 val_loss= 0.57426 val_acc= 0.87443 time= 0.19900
Epoch: 0055 train_loss= 0.56620 train_acc= 0.87923 val_loss= 0.55858 val_acc= 0.87749 time= 0.16899
Epoch: 0056 train_loss= 0.54321 train_acc= 0.88008 val_loss= 0.54394 val_acc= 0.87902 time= 0.18200
Epoch: 0057 train_loss= 0.53027 train_acc= 0.88314 val_loss= 0.53031 val_acc= 0.88208 time= 0.16801
Epoch: 0058 train_loss= 0.51210 train_acc= 0.88484 val_loss= 0.51705 val_acc= 0.88361 time= 0.16800
Epoch: 0059 train_loss= 0.49388 train_acc= 0.89080 val_loss= 0.50392 val_acc= 0.88668 time= 0.17100
Epoch: 0060 train_loss= 0.48034 train_acc= 0.89301 val_loss= 0.49074 val_acc= 0.88668 time= 0.17500
Epoch: 0061 train_loss= 0.46856 train_acc= 0.89641 val_loss= 0.47748 val_acc= 0.88668 time= 0.17059
Epoch: 0062 train_loss= 0.45315 train_acc= 0.89862 val_loss= 0.46465 val_acc= 0.88974 time= 0.17800
Epoch: 0063 train_loss= 0.44457 train_acc= 0.90032 val_loss= 0.45236 val_acc= 0.88974 time= 0.17200
Epoch: 0064 train_loss= 0.42476 train_acc= 0.90338 val_loss= 0.44085 val_acc= 0.89127 time= 0.16800
Epoch: 0065 train_loss= 0.41033 train_acc= 0.90492 val_loss= 0.43063 val_acc= 0.89280 time= 0.19095
Epoch: 0066 train_loss= 0.39771 train_acc= 0.91444 val_loss= 0.42110 val_acc= 0.89587 time= 0.16800
Epoch: 0067 train_loss= 0.38510 train_acc= 0.91903 val_loss= 0.41235 val_acc= 0.89587 time= 0.16800
Epoch: 0068 train_loss= 0.37378 train_acc= 0.92448 val_loss= 0.40371 val_acc= 0.89587 time= 0.18905
Epoch: 0069 train_loss= 0.36533 train_acc= 0.92090 val_loss= 0.39484 val_acc= 0.90199 time= 0.16904
Epoch: 0070 train_loss= 0.35825 train_acc= 0.92550 val_loss= 0.38639 val_acc= 0.90199 time= 0.17071
Epoch: 0071 train_loss= 0.33583 train_acc= 0.93094 val_loss= 0.37827 val_acc= 0.90505 time= 0.19700
Epoch: 0072 train_loss= 0.33152 train_acc= 0.93077 val_loss= 0.37042 val_acc= 0.90812 time= 0.16903
Epoch: 0073 train_loss= 0.32322 train_acc= 0.93111 val_loss= 0.36314 val_acc= 0.90812 time= 0.17897
Epoch: 0074 train_loss= 0.30865 train_acc= 0.93723 val_loss= 0.35561 val_acc= 0.90965 time= 0.16703
Epoch: 0075 train_loss= 0.30441 train_acc= 0.93468 val_loss= 0.34828 val_acc= 0.90965 time= 0.16900
Epoch: 0076 train_loss= 0.29229 train_acc= 0.93587 val_loss= 0.34162 val_acc= 0.90965 time= 0.17101
Epoch: 0077 train_loss= 0.28723 train_acc= 0.94166 val_loss= 0.33561 val_acc= 0.91271 time= 0.19010
Epoch: 0078 train_loss= 0.27459 train_acc= 0.94523 val_loss= 0.32993 val_acc= 0.91424 time= 0.17000
Epoch: 0079 train_loss= 0.26598 train_acc= 0.94557 val_loss= 0.32482 val_acc= 0.91577 time= 0.18833
Epoch: 0080 train_loss= 0.26337 train_acc= 0.95101 val_loss= 0.31942 val_acc= 0.91730 time= 0.17003
Epoch: 0081 train_loss= 0.25087 train_acc= 0.95033 val_loss= 0.31413 val_acc= 0.91730 time= 0.16820
Epoch: 0082 train_loss= 0.24469 train_acc= 0.94659 val_loss= 0.30931 val_acc= 0.92037 time= 0.17199
Epoch: 0083 train_loss= 0.23469 train_acc= 0.95424 val_loss= 0.30466 val_acc= 0.92496 time= 0.17108
Epoch: 0084 train_loss= 0.22940 train_acc= 0.95526 val_loss= 0.30034 val_acc= 0.92649 time= 0.16700
Epoch: 0085 train_loss= 0.22536 train_acc= 0.95390 val_loss= 0.29575 val_acc= 0.92802 time= 0.16700
Epoch: 0086 train_loss= 0.21625 train_acc= 0.95663 val_loss= 0.29204 val_acc= 0.92496 time= 0.16997
Epoch: 0087 train_loss= 0.20738 train_acc= 0.95816 val_loss= 0.28888 val_acc= 0.92802 time= 0.17000
Epoch: 0088 train_loss= 0.20485 train_acc= 0.95816 val_loss= 0.28571 val_acc= 0.92802 time= 0.17100
Epoch: 0089 train_loss= 0.19504 train_acc= 0.96088 val_loss= 0.28202 val_acc= 0.92956 time= 0.16703
Epoch: 0090 train_loss= 0.19101 train_acc= 0.96360 val_loss= 0.27841 val_acc= 0.93109 time= 0.16800
Epoch: 0091 train_loss= 0.18676 train_acc= 0.96258 val_loss= 0.27525 val_acc= 0.93262 time= 0.18000
Epoch: 0092 train_loss= 0.17897 train_acc= 0.96462 val_loss= 0.27250 val_acc= 0.93109 time= 0.16697
Epoch: 0093 train_loss= 0.17711 train_acc= 0.96411 val_loss= 0.26975 val_acc= 0.92956 time= 0.16700
Epoch: 0094 train_loss= 0.17227 train_acc= 0.96598 val_loss= 0.26707 val_acc= 0.92956 time= 0.19200
Epoch: 0095 train_loss= 0.16435 train_acc= 0.96751 val_loss= 0.26463 val_acc= 0.92956 time= 0.17000
Epoch: 0096 train_loss= 0.16223 train_acc= 0.96717 val_loss= 0.26202 val_acc= 0.93109 time= 0.17416
Epoch: 0097 train_loss= 0.15140 train_acc= 0.96972 val_loss= 0.25919 val_acc= 0.93109 time= 0.18504
Epoch: 0098 train_loss= 0.15409 train_acc= 0.97176 val_loss= 0.25672 val_acc= 0.93262 time= 0.16700
Epoch: 0099 train_loss= 0.14162 train_acc= 0.97346 val_loss= 0.25481 val_acc= 0.93568 time= 0.16803
Epoch: 0100 train_loss= 0.14553 train_acc= 0.97278 val_loss= 0.25383 val_acc= 0.93568 time= 0.19300
Epoch: 0101 train_loss= 0.13827 train_acc= 0.97483 val_loss= 0.25274 val_acc= 0.93262 time= 0.16700
Epoch: 0102 train_loss= 0.13542 train_acc= 0.97755 val_loss= 0.25104 val_acc= 0.93109 time= 0.18697
Epoch: 0103 train_loss= 0.13081 train_acc= 0.97653 val_loss= 0.24924 val_acc= 0.93262 time= 0.17100
Epoch: 0104 train_loss= 0.12685 train_acc= 0.97619 val_loss= 0.24785 val_acc= 0.93568 time= 0.17200
Epoch: 0105 train_loss= 0.12604 train_acc= 0.97806 val_loss= 0.24602 val_acc= 0.93721 time= 0.17204
Epoch: 0106 train_loss= 0.12152 train_acc= 0.97874 val_loss= 0.24371 val_acc= 0.93874 time= 0.18929
Epoch: 0107 train_loss= 0.11637 train_acc= 0.97993 val_loss= 0.24175 val_acc= 0.93721 time= 0.16900
Epoch: 0108 train_loss= 0.11650 train_acc= 0.97942 val_loss= 0.23975 val_acc= 0.93568 time= 0.16709
Epoch: 0109 train_loss= 0.11119 train_acc= 0.98214 val_loss= 0.23793 val_acc= 0.93568 time= 0.16601
Epoch: 0110 train_loss= 0.10857 train_acc= 0.98078 val_loss= 0.23705 val_acc= 0.93262 time= 0.16888
Epoch: 0111 train_loss= 0.10738 train_acc= 0.98163 val_loss= 0.23655 val_acc= 0.93262 time= 0.20100
Epoch: 0112 train_loss= 0.09961 train_acc= 0.98316 val_loss= 0.23630 val_acc= 0.93415 time= 0.17200
Epoch: 0113 train_loss= 0.10029 train_acc= 0.98384 val_loss= 0.23623 val_acc= 0.93568 time= 0.17000
Epoch: 0114 train_loss= 0.09689 train_acc= 0.98469 val_loss= 0.23728 val_acc= 0.93721 time= 0.18800
Epoch: 0115 train_loss= 0.09077 train_acc= 0.98605 val_loss= 0.23787 val_acc= 0.93721 time= 0.17000
Epoch: 0116 train_loss= 0.08901 train_acc= 0.98469 val_loss= 0.23764 val_acc= 0.93415 time= 0.16500
Epoch: 0117 train_loss= 0.08884 train_acc= 0.98452 val_loss= 0.23678 val_acc= 0.93415 time= 0.18900
Epoch: 0118 train_loss= 0.09040 train_acc= 0.98605 val_loss= 0.23410 val_acc= 0.93415 time= 0.16911
Epoch: 0119 train_loss= 0.08366 train_acc= 0.98843 val_loss= 0.23272 val_acc= 0.93415 time= 0.18512
Epoch: 0120 train_loss= 0.08146 train_acc= 0.98775 val_loss= 0.23146 val_acc= 0.93262 time= 0.17100
Epoch: 0121 train_loss= 0.07709 train_acc= 0.98826 val_loss= 0.23077 val_acc= 0.93568 time= 0.16901
Epoch: 0122 train_loss= 0.07815 train_acc= 0.98656 val_loss= 0.23041 val_acc= 0.93568 time= 0.17146
Epoch: 0123 train_loss= 0.07688 train_acc= 0.98622 val_loss= 0.22922 val_acc= 0.93568 time= 0.18800
Epoch: 0124 train_loss= 0.07416 train_acc= 0.98656 val_loss= 0.22708 val_acc= 0.93721 time= 0.16800
Epoch: 0125 train_loss= 0.07301 train_acc= 0.98860 val_loss= 0.22625 val_acc= 0.93568 time= 0.16704
Epoch: 0126 train_loss= 0.07223 train_acc= 0.98860 val_loss= 0.22694 val_acc= 0.93262 time= 0.16996
Epoch: 0127 train_loss= 0.07161 train_acc= 0.98792 val_loss= 0.22767 val_acc= 0.93415 time= 0.17108
Epoch: 0128 train_loss= 0.06614 train_acc= 0.98928 val_loss= 0.22788 val_acc= 0.93415 time= 0.19900
Epoch: 0129 train_loss= 0.06900 train_acc= 0.99047 val_loss= 0.22698 val_acc= 0.93415 time= 0.17003
Epoch: 0130 train_loss= 0.06351 train_acc= 0.99013 val_loss= 0.22744 val_acc= 0.93568 time= 0.16801
Epoch: 0131 train_loss= 0.06481 train_acc= 0.98979 val_loss= 0.22855 val_acc= 0.93262 time= 0.18406
Early stopping...
Optimization Finished!
Test set results: cost= 0.25242 accuracy= 0.93886 time= 0.07400
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8068    0.9467    0.8712        75
           4     1.0000    1.0000    1.0000         9
           5     0.7810    0.9425    0.8542        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.3333    0.5000         9
          10     0.9000    0.7500    0.8182        36
          11     1.0000    0.9167    0.9565        12
          12     0.8696    0.9917    0.9266       121
          13     0.9375    0.7895    0.8571        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.6667    0.8235    0.7368        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9000    0.7500    0.8182        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9655    0.9669       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8630    0.7778    0.8182        81
          36     1.0000    0.4167    0.5882        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8182    0.7500    0.7826        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9389      2568
   macro avg     0.7906    0.6920    0.7149      2568
weighted avg     0.9391    0.9389    0.9348      2568

Macro average Test Precision, Recall and F1-Score...
(0.7905503199038465, 0.6919978728900839, 0.7149173978472856, None)
Micro average Test Precision, Recall and F1-Score...
(0.9388629283489096, 0.9388629283489096, 0.9388629283489096, None)
embeddings:
8892 6532 2568
[[-0.05988506 -0.10068154 -0.04477773 ...  0.04535074  0.5517617
  -0.16864213]
 [ 0.14297402 -0.02885622  0.16340429 ...  0.2151548   0.19784196
   0.13530053]
 [ 0.09030631  0.01321469  0.26699716 ...  0.3353755   0.14334828
   0.09660313]
 ...
 [ 0.05431726  0.02409805  0.284387   ...  0.2045004   0.21731324
   0.12863411]
 [ 0.0646344   0.03070567  0.12747763 ...  0.2261836   0.10854781
   0.0820609 ]
 [ 0.23234904  0.23790827  0.18467528 ...  0.26302174  0.2268553
   0.24194059]]

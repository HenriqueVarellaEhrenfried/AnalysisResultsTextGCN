(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95126 train_acc= 0.01769 val_loss= 3.92820 val_acc= 0.47933 time= 0.44890
Epoch: 0002 train_loss= 3.92843 train_acc= 0.45467 val_loss= 3.89219 val_acc= 0.47779 time= 0.20400
Epoch: 0003 train_loss= 3.89277 train_acc= 0.44838 val_loss= 3.84242 val_acc= 0.46861 time= 0.16705
Epoch: 0004 train_loss= 3.84337 train_acc= 0.44446 val_loss= 3.77791 val_acc= 0.46248 time= 0.16700
Epoch: 0005 train_loss= 3.77723 train_acc= 0.43800 val_loss= 3.69758 val_acc= 0.46095 time= 0.16600
Epoch: 0006 train_loss= 3.70144 train_acc= 0.43596 val_loss= 3.60091 val_acc= 0.45942 time= 0.19400
Epoch: 0007 train_loss= 3.59762 train_acc= 0.43545 val_loss= 3.48814 val_acc= 0.45636 time= 0.16796
Epoch: 0008 train_loss= 3.49060 train_acc= 0.43426 val_loss= 3.36036 val_acc= 0.45636 time= 0.17000
Epoch: 0009 train_loss= 3.36370 train_acc= 0.43273 val_loss= 3.21999 val_acc= 0.45636 time= 0.17400
Epoch: 0010 train_loss= 3.21479 train_acc= 0.43256 val_loss= 3.07085 val_acc= 0.45636 time= 0.17004
Epoch: 0011 train_loss= 3.07328 train_acc= 0.43256 val_loss= 2.91815 val_acc= 0.45636 time= 0.16800
Epoch: 0012 train_loss= 2.91286 train_acc= 0.43239 val_loss= 2.76771 val_acc= 0.45636 time= 0.17712
Epoch: 0013 train_loss= 2.76164 train_acc= 0.43239 val_loss= 2.62601 val_acc= 0.45636 time= 0.16700
Epoch: 0014 train_loss= 2.63891 train_acc= 0.43239 val_loss= 2.49954 val_acc= 0.45636 time= 0.16600
Epoch: 0015 train_loss= 2.49524 train_acc= 0.43239 val_loss= 2.39356 val_acc= 0.45636 time= 0.18400
Epoch: 0016 train_loss= 2.40699 train_acc= 0.43239 val_loss= 2.31065 val_acc= 0.45636 time= 0.16900
Epoch: 0017 train_loss= 2.31327 train_acc= 0.43239 val_loss= 2.24918 val_acc= 0.45636 time= 0.16992
Epoch: 0018 train_loss= 2.25725 train_acc= 0.43239 val_loss= 2.20395 val_acc= 0.45636 time= 0.19300
Epoch: 0019 train_loss= 2.20819 train_acc= 0.43239 val_loss= 2.16794 val_acc= 0.45636 time= 0.16800
Epoch: 0020 train_loss= 2.18194 train_acc= 0.43256 val_loss= 2.13437 val_acc= 0.45636 time= 0.17100
Epoch: 0021 train_loss= 2.14925 train_acc= 0.43256 val_loss= 2.09848 val_acc= 0.45636 time= 0.16800
Epoch: 0022 train_loss= 2.11011 train_acc= 0.43375 val_loss= 2.05777 val_acc= 0.45789 time= 0.16705
Epoch: 0023 train_loss= 2.07655 train_acc= 0.43409 val_loss= 2.01203 val_acc= 0.46554 time= 0.16900
Epoch: 0024 train_loss= 2.02114 train_acc= 0.43681 val_loss= 1.96249 val_acc= 0.46861 time= 0.18800
Epoch: 0025 train_loss= 1.97978 train_acc= 0.45093 val_loss= 1.91129 val_acc= 0.48239 time= 0.17194
Epoch: 0026 train_loss= 1.93606 train_acc= 0.45484 val_loss= 1.86111 val_acc= 0.49311 time= 0.19200
Epoch: 0027 train_loss= 1.88093 train_acc= 0.47780 val_loss= 1.81401 val_acc= 0.51302 time= 0.16805
Epoch: 0028 train_loss= 1.84534 train_acc= 0.50638 val_loss= 1.77084 val_acc= 0.53293 time= 0.16817
Epoch: 0029 train_loss= 1.78917 train_acc= 0.53564 val_loss= 1.73134 val_acc= 0.56662 time= 0.17004
Epoch: 0030 train_loss= 1.76816 train_acc= 0.55282 val_loss= 1.69427 val_acc= 0.59877 time= 0.18827
Epoch: 0031 train_loss= 1.71074 train_acc= 0.58649 val_loss= 1.65844 val_acc= 0.62481 time= 0.16701
Epoch: 0032 train_loss= 1.67930 train_acc= 0.61048 val_loss= 1.62296 val_acc= 0.64165 time= 0.16599
Epoch: 0033 train_loss= 1.64620 train_acc= 0.63072 val_loss= 1.58755 val_acc= 0.66156 time= 0.17123
Epoch: 0034 train_loss= 1.60602 train_acc= 0.64195 val_loss= 1.55254 val_acc= 0.67381 time= 0.17000
Epoch: 0035 train_loss= 1.57735 train_acc= 0.65419 val_loss= 1.51832 val_acc= 0.67841 time= 0.19300
Epoch: 0036 train_loss= 1.54872 train_acc= 0.65436 val_loss= 1.48536 val_acc= 0.67994 time= 0.16600
Epoch: 0037 train_loss= 1.50478 train_acc= 0.66032 val_loss= 1.45416 val_acc= 0.68300 time= 0.16600
Epoch: 0038 train_loss= 1.47568 train_acc= 0.66100 val_loss= 1.42476 val_acc= 0.68760 time= 0.18300
Epoch: 0039 train_loss= 1.44862 train_acc= 0.66083 val_loss= 1.39700 val_acc= 0.69066 time= 0.16800
Epoch: 0040 train_loss= 1.42391 train_acc= 0.66372 val_loss= 1.37059 val_acc= 0.69219 time= 0.16601
Epoch: 0041 train_loss= 1.39395 train_acc= 0.67273 val_loss= 1.34525 val_acc= 0.69525 time= 0.19700
Epoch: 0042 train_loss= 1.37520 train_acc= 0.67409 val_loss= 1.32075 val_acc= 0.70597 time= 0.16970
Epoch: 0043 train_loss= 1.35097 train_acc= 0.68736 val_loss= 1.29695 val_acc= 0.70750 time= 0.18700
Epoch: 0044 train_loss= 1.32486 train_acc= 0.69366 val_loss= 1.27382 val_acc= 0.70904 time= 0.16800
Epoch: 0045 train_loss= 1.31095 train_acc= 0.70250 val_loss= 1.25125 val_acc= 0.71822 time= 0.16807
Epoch: 0046 train_loss= 1.27326 train_acc= 0.71220 val_loss= 1.22924 val_acc= 0.72129 time= 0.16900
Epoch: 0047 train_loss= 1.25855 train_acc= 0.72104 val_loss= 1.20778 val_acc= 0.72741 time= 0.18797
Epoch: 0048 train_loss= 1.23373 train_acc= 0.72870 val_loss= 1.18688 val_acc= 0.73354 time= 0.16717
Epoch: 0049 train_loss= 1.20758 train_acc= 0.73363 val_loss= 1.16651 val_acc= 0.73507 time= 0.18400
Epoch: 0050 train_loss= 1.19294 train_acc= 0.73941 val_loss= 1.14667 val_acc= 0.73966 time= 0.17100
Epoch: 0051 train_loss= 1.17483 train_acc= 0.74281 val_loss= 1.12735 val_acc= 0.74273 time= 0.16884
Epoch: 0052 train_loss= 1.14926 train_acc= 0.74571 val_loss= 1.10849 val_acc= 0.75038 time= 0.16997
Epoch: 0053 train_loss= 1.12909 train_acc= 0.75472 val_loss= 1.08999 val_acc= 0.76263 time= 0.19003
Epoch: 0054 train_loss= 1.11359 train_acc= 0.75506 val_loss= 1.07189 val_acc= 0.76417 time= 0.16600
Epoch: 0055 train_loss= 1.08622 train_acc= 0.76323 val_loss= 1.05406 val_acc= 0.76876 time= 0.18364
Epoch: 0056 train_loss= 1.07250 train_acc= 0.76476 val_loss= 1.03652 val_acc= 0.77489 time= 0.16698
Epoch: 0057 train_loss= 1.05504 train_acc= 0.77836 val_loss= 1.01913 val_acc= 0.78254 time= 0.16994
Epoch: 0058 train_loss= 1.03412 train_acc= 0.78313 val_loss= 1.00190 val_acc= 0.78714 time= 0.19248
Epoch: 0059 train_loss= 1.01927 train_acc= 0.79265 val_loss= 0.98486 val_acc= 0.79173 time= 0.16800
Epoch: 0060 train_loss= 1.00050 train_acc= 0.79775 val_loss= 0.96794 val_acc= 0.79786 time= 0.16900
Epoch: 0061 train_loss= 0.98501 train_acc= 0.80167 val_loss= 0.95123 val_acc= 0.80092 time= 0.17100
Epoch: 0062 train_loss= 0.96970 train_acc= 0.80269 val_loss= 0.93460 val_acc= 0.80398 time= 0.16800
Epoch: 0063 train_loss= 0.94795 train_acc= 0.81323 val_loss= 0.91807 val_acc= 0.80704 time= 0.16640
Epoch: 0064 train_loss= 0.93156 train_acc= 0.81170 val_loss= 0.90172 val_acc= 0.81011 time= 0.18905
Epoch: 0065 train_loss= 0.90824 train_acc= 0.81732 val_loss= 0.88555 val_acc= 0.82083 time= 0.16797
Epoch: 0066 train_loss= 0.89955 train_acc= 0.81783 val_loss= 0.86962 val_acc= 0.82389 time= 0.18600
Epoch: 0067 train_loss= 0.88370 train_acc= 0.81953 val_loss= 0.85391 val_acc= 0.82542 time= 0.16904
Epoch: 0068 train_loss= 0.86542 train_acc= 0.82786 val_loss= 0.83842 val_acc= 0.82695 time= 0.16803
Epoch: 0069 train_loss= 0.84665 train_acc= 0.82837 val_loss= 0.82314 val_acc= 0.82848 time= 0.16997
Epoch: 0070 train_loss= 0.83202 train_acc= 0.83126 val_loss= 0.80813 val_acc= 0.83002 time= 0.17503
Epoch: 0071 train_loss= 0.81235 train_acc= 0.83790 val_loss= 0.79339 val_acc= 0.82848 time= 0.16808
Epoch: 0072 train_loss= 0.79856 train_acc= 0.83756 val_loss= 0.77886 val_acc= 0.82848 time= 0.17700
Epoch: 0073 train_loss= 0.77882 train_acc= 0.84215 val_loss= 0.76454 val_acc= 0.83155 time= 0.16700
Epoch: 0074 train_loss= 0.77007 train_acc= 0.84453 val_loss= 0.75054 val_acc= 0.83614 time= 0.17300
Epoch: 0075 train_loss= 0.75550 train_acc= 0.84861 val_loss= 0.73681 val_acc= 0.84380 time= 0.18100
Epoch: 0076 train_loss= 0.74619 train_acc= 0.85253 val_loss= 0.72334 val_acc= 0.84839 time= 0.16801
Epoch: 0077 train_loss= 0.72003 train_acc= 0.85797 val_loss= 0.71018 val_acc= 0.85299 time= 0.16700
Epoch: 0078 train_loss= 0.70924 train_acc= 0.85474 val_loss= 0.69738 val_acc= 0.85452 time= 0.16800
Epoch: 0079 train_loss= 0.69507 train_acc= 0.86137 val_loss= 0.68492 val_acc= 0.85758 time= 0.16700
Epoch: 0080 train_loss= 0.68364 train_acc= 0.86392 val_loss= 0.67270 val_acc= 0.85758 time= 0.16800
Epoch: 0081 train_loss= 0.66672 train_acc= 0.86273 val_loss= 0.66065 val_acc= 0.86064 time= 0.16900
Epoch: 0082 train_loss= 0.66187 train_acc= 0.86766 val_loss= 0.64897 val_acc= 0.86524 time= 0.16899
Epoch: 0083 train_loss= 0.63773 train_acc= 0.86937 val_loss= 0.63750 val_acc= 0.86830 time= 0.17000
Epoch: 0084 train_loss= 0.63638 train_acc= 0.87039 val_loss= 0.62613 val_acc= 0.87443 time= 0.19300
Epoch: 0085 train_loss= 0.62643 train_acc= 0.87447 val_loss= 0.61497 val_acc= 0.87749 time= 0.16700
Epoch: 0086 train_loss= 0.60578 train_acc= 0.87702 val_loss= 0.60396 val_acc= 0.87749 time= 0.16701
Epoch: 0087 train_loss= 0.59677 train_acc= 0.87634 val_loss= 0.59317 val_acc= 0.87443 time= 0.19200
Epoch: 0088 train_loss= 0.58837 train_acc= 0.88076 val_loss= 0.58265 val_acc= 0.88055 time= 0.16800
Epoch: 0089 train_loss= 0.57580 train_acc= 0.88144 val_loss= 0.57255 val_acc= 0.88361 time= 0.16900
Epoch: 0090 train_loss= 0.56659 train_acc= 0.88604 val_loss= 0.56289 val_acc= 0.88361 time= 0.17477
Epoch: 0091 train_loss= 0.55039 train_acc= 0.88689 val_loss= 0.55354 val_acc= 0.88515 time= 0.17000
Epoch: 0092 train_loss= 0.53726 train_acc= 0.88655 val_loss= 0.54431 val_acc= 0.88515 time= 0.16904
Epoch: 0093 train_loss= 0.52884 train_acc= 0.88910 val_loss= 0.53531 val_acc= 0.88821 time= 0.19200
Epoch: 0094 train_loss= 0.52343 train_acc= 0.89709 val_loss= 0.52638 val_acc= 0.88974 time= 0.16700
Epoch: 0095 train_loss= 0.51404 train_acc= 0.89658 val_loss= 0.51741 val_acc= 0.88974 time= 0.18300
Epoch: 0096 train_loss= 0.50166 train_acc= 0.89556 val_loss= 0.50872 val_acc= 0.88668 time= 0.16700
Epoch: 0097 train_loss= 0.49243 train_acc= 0.89879 val_loss= 0.50039 val_acc= 0.88515 time= 0.16600
Epoch: 0098 train_loss= 0.48069 train_acc= 0.90373 val_loss= 0.49230 val_acc= 0.88668 time= 0.17100
Epoch: 0099 train_loss= 0.47052 train_acc= 0.90168 val_loss= 0.48467 val_acc= 0.88974 time= 0.19216
Epoch: 0100 train_loss= 0.46664 train_acc= 0.90356 val_loss= 0.47711 val_acc= 0.89127 time= 0.17001
Epoch: 0101 train_loss= 0.45557 train_acc= 0.90628 val_loss= 0.47001 val_acc= 0.89280 time= 0.16706
Epoch: 0102 train_loss= 0.45037 train_acc= 0.90730 val_loss= 0.46326 val_acc= 0.89280 time= 0.16697
Epoch: 0103 train_loss= 0.44218 train_acc= 0.90968 val_loss= 0.45666 val_acc= 0.89280 time= 0.16703
Epoch: 0104 train_loss= 0.42927 train_acc= 0.90985 val_loss= 0.45013 val_acc= 0.89587 time= 0.19200
Epoch: 0105 train_loss= 0.42841 train_acc= 0.91478 val_loss= 0.44363 val_acc= 0.89587 time= 0.16597
Epoch: 0106 train_loss= 0.42185 train_acc= 0.91784 val_loss= 0.43734 val_acc= 0.89587 time= 0.16800
Epoch: 0107 train_loss= 0.41500 train_acc= 0.91716 val_loss= 0.43100 val_acc= 0.89587 time= 0.19300
Epoch: 0108 train_loss= 0.40845 train_acc= 0.91733 val_loss= 0.42475 val_acc= 0.89740 time= 0.16897
Epoch: 0109 train_loss= 0.39405 train_acc= 0.91988 val_loss= 0.41890 val_acc= 0.89893 time= 0.16900
Epoch: 0110 train_loss= 0.38860 train_acc= 0.92261 val_loss= 0.41363 val_acc= 0.89740 time= 0.19500
Epoch: 0111 train_loss= 0.37784 train_acc= 0.92482 val_loss= 0.40862 val_acc= 0.89740 time= 0.16903
Epoch: 0112 train_loss= 0.37468 train_acc= 0.92669 val_loss= 0.40369 val_acc= 0.89740 time= 0.18401
Epoch: 0113 train_loss= 0.37598 train_acc= 0.92533 val_loss= 0.39863 val_acc= 0.89740 time= 0.16597
Epoch: 0114 train_loss= 0.36540 train_acc= 0.92363 val_loss= 0.39385 val_acc= 0.89893 time= 0.16803
Epoch: 0115 train_loss= 0.35209 train_acc= 0.93179 val_loss= 0.38917 val_acc= 0.90046 time= 0.17197
Epoch: 0116 train_loss= 0.34761 train_acc= 0.92771 val_loss= 0.38464 val_acc= 0.90352 time= 0.19495
Epoch: 0117 train_loss= 0.34209 train_acc= 0.93247 val_loss= 0.38026 val_acc= 0.90505 time= 0.16903
Epoch: 0118 train_loss= 0.34088 train_acc= 0.93179 val_loss= 0.37616 val_acc= 0.90505 time= 0.17697
Epoch: 0119 train_loss= 0.33298 train_acc= 0.93128 val_loss= 0.37216 val_acc= 0.90812 time= 0.16903
Epoch: 0120 train_loss= 0.32910 train_acc= 0.93213 val_loss= 0.36790 val_acc= 0.91118 time= 0.16705
Epoch: 0121 train_loss= 0.32025 train_acc= 0.93689 val_loss= 0.36373 val_acc= 0.91118 time= 0.17104
Epoch: 0122 train_loss= 0.31691 train_acc= 0.93877 val_loss= 0.35972 val_acc= 0.90965 time= 0.19253
Epoch: 0123 train_loss= 0.31088 train_acc= 0.93928 val_loss= 0.35567 val_acc= 0.90965 time= 0.16926
Epoch: 0124 train_loss= 0.30620 train_acc= 0.93842 val_loss= 0.35194 val_acc= 0.91118 time= 0.18700
Epoch: 0125 train_loss= 0.29946 train_acc= 0.94013 val_loss= 0.34826 val_acc= 0.91118 time= 0.17003
Epoch: 0126 train_loss= 0.29792 train_acc= 0.94115 val_loss= 0.34513 val_acc= 0.91271 time= 0.16800
Epoch: 0127 train_loss= 0.29224 train_acc= 0.94030 val_loss= 0.34217 val_acc= 0.91424 time= 0.19201
Epoch: 0128 train_loss= 0.29076 train_acc= 0.94013 val_loss= 0.33934 val_acc= 0.91424 time= 0.16700
Epoch: 0129 train_loss= 0.28236 train_acc= 0.94489 val_loss= 0.33639 val_acc= 0.91424 time= 0.17299
Epoch: 0130 train_loss= 0.27717 train_acc= 0.94370 val_loss= 0.33318 val_acc= 0.91424 time= 0.16800
Epoch: 0131 train_loss= 0.27219 train_acc= 0.94523 val_loss= 0.33013 val_acc= 0.91424 time= 0.16997
Epoch: 0132 train_loss= 0.27069 train_acc= 0.94506 val_loss= 0.32699 val_acc= 0.91271 time= 0.17000
Epoch: 0133 train_loss= 0.26915 train_acc= 0.94965 val_loss= 0.32433 val_acc= 0.91730 time= 0.19800
Epoch: 0134 train_loss= 0.26461 train_acc= 0.94370 val_loss= 0.32185 val_acc= 0.91577 time= 0.16803
Epoch: 0135 train_loss= 0.25895 train_acc= 0.94863 val_loss= 0.31907 val_acc= 0.91730 time= 0.18303
Epoch: 0136 train_loss= 0.25258 train_acc= 0.95152 val_loss= 0.31650 val_acc= 0.91730 time= 0.16700
Epoch: 0137 train_loss= 0.24791 train_acc= 0.95237 val_loss= 0.31383 val_acc= 0.91884 time= 0.16800
Epoch: 0138 train_loss= 0.25038 train_acc= 0.95203 val_loss= 0.31107 val_acc= 0.91884 time= 0.17004
Epoch: 0139 train_loss= 0.24226 train_acc= 0.95152 val_loss= 0.30857 val_acc= 0.91730 time= 0.18996
Epoch: 0140 train_loss= 0.23725 train_acc= 0.95203 val_loss= 0.30591 val_acc= 0.91730 time= 0.17100
Epoch: 0141 train_loss= 0.23401 train_acc= 0.95646 val_loss= 0.30334 val_acc= 0.91884 time= 0.17000
Epoch: 0142 train_loss= 0.23176 train_acc= 0.95373 val_loss= 0.30136 val_acc= 0.92190 time= 0.16700
Epoch: 0143 train_loss= 0.22674 train_acc= 0.95543 val_loss= 0.29976 val_acc= 0.92190 time= 0.16600
Epoch: 0144 train_loss= 0.22908 train_acc= 0.95288 val_loss= 0.29834 val_acc= 0.92343 time= 0.19204
Epoch: 0145 train_loss= 0.22479 train_acc= 0.95714 val_loss= 0.29727 val_acc= 0.92343 time= 0.16901
Epoch: 0146 train_loss= 0.22149 train_acc= 0.95543 val_loss= 0.29604 val_acc= 0.92190 time= 0.16900
Epoch: 0147 train_loss= 0.21583 train_acc= 0.95577 val_loss= 0.29509 val_acc= 0.92190 time= 0.18646
Epoch: 0148 train_loss= 0.21597 train_acc= 0.96071 val_loss= 0.29408 val_acc= 0.92037 time= 0.17043
Epoch: 0149 train_loss= 0.20645 train_acc= 0.95918 val_loss= 0.29260 val_acc= 0.92037 time= 0.17000
Epoch: 0150 train_loss= 0.20866 train_acc= 0.96071 val_loss= 0.29084 val_acc= 0.92190 time= 0.19200
Epoch: 0151 train_loss= 0.20269 train_acc= 0.96037 val_loss= 0.28855 val_acc= 0.92190 time= 0.16505
Epoch: 0152 train_loss= 0.20227 train_acc= 0.96003 val_loss= 0.28663 val_acc= 0.92037 time= 0.17600
Epoch: 0153 train_loss= 0.19838 train_acc= 0.96139 val_loss= 0.28483 val_acc= 0.92343 time= 0.16700
Epoch: 0154 train_loss= 0.19395 train_acc= 0.96241 val_loss= 0.28310 val_acc= 0.92343 time= 0.16699
Epoch: 0155 train_loss= 0.19011 train_acc= 0.96547 val_loss= 0.28146 val_acc= 0.92343 time= 0.17156
Epoch: 0156 train_loss= 0.18893 train_acc= 0.96275 val_loss= 0.27986 val_acc= 0.92037 time= 0.19401
Epoch: 0157 train_loss= 0.18696 train_acc= 0.96292 val_loss= 0.27818 val_acc= 0.92190 time= 0.17000
Epoch: 0158 train_loss= 0.18682 train_acc= 0.96139 val_loss= 0.27673 val_acc= 0.92190 time= 0.18400
Epoch: 0159 train_loss= 0.18256 train_acc= 0.96411 val_loss= 0.27523 val_acc= 0.92190 time= 0.16748
Epoch: 0160 train_loss= 0.18122 train_acc= 0.96411 val_loss= 0.27398 val_acc= 0.92496 time= 0.16900
Epoch: 0161 train_loss= 0.17905 train_acc= 0.96683 val_loss= 0.27279 val_acc= 0.92496 time= 0.19100
Epoch: 0162 train_loss= 0.17574 train_acc= 0.96530 val_loss= 0.27161 val_acc= 0.92496 time= 0.16600
Epoch: 0163 train_loss= 0.17233 train_acc= 0.97074 val_loss= 0.27022 val_acc= 0.92649 time= 0.16795
Epoch: 0164 train_loss= 0.17156 train_acc= 0.96717 val_loss= 0.26863 val_acc= 0.92802 time= 0.17610
Epoch: 0165 train_loss= 0.17106 train_acc= 0.96802 val_loss= 0.26686 val_acc= 0.92956 time= 0.16900
Epoch: 0166 train_loss= 0.16885 train_acc= 0.96887 val_loss= 0.26502 val_acc= 0.93109 time= 0.16903
Epoch: 0167 train_loss= 0.16505 train_acc= 0.96955 val_loss= 0.26342 val_acc= 0.92956 time= 0.19101
Epoch: 0168 train_loss= 0.16894 train_acc= 0.96683 val_loss= 0.26227 val_acc= 0.93109 time= 0.16699
Epoch: 0169 train_loss= 0.16189 train_acc= 0.96768 val_loss= 0.26172 val_acc= 0.92956 time= 0.16700
Epoch: 0170 train_loss= 0.16019 train_acc= 0.97108 val_loss= 0.26171 val_acc= 0.93109 time= 0.18799
Epoch: 0171 train_loss= 0.15530 train_acc= 0.97006 val_loss= 0.26199 val_acc= 0.93262 time= 0.16600
Epoch: 0172 train_loss= 0.15582 train_acc= 0.97210 val_loss= 0.26172 val_acc= 0.93262 time= 0.17174
Epoch: 0173 train_loss= 0.15134 train_acc= 0.97329 val_loss= 0.26150 val_acc= 0.93262 time= 0.19600
Epoch: 0174 train_loss= 0.14968 train_acc= 0.97091 val_loss= 0.26083 val_acc= 0.93262 time= 0.17000
Epoch: 0175 train_loss= 0.15059 train_acc= 0.97006 val_loss= 0.25936 val_acc= 0.92802 time= 0.18100
Epoch: 0176 train_loss= 0.14960 train_acc= 0.97329 val_loss= 0.25788 val_acc= 0.92802 time= 0.16700
Epoch: 0177 train_loss= 0.14323 train_acc= 0.97278 val_loss= 0.25676 val_acc= 0.92956 time= 0.16908
Epoch: 0178 train_loss= 0.14423 train_acc= 0.97295 val_loss= 0.25548 val_acc= 0.93109 time= 0.16999
Epoch: 0179 train_loss= 0.13896 train_acc= 0.97534 val_loss= 0.25459 val_acc= 0.93109 time= 0.19000
Epoch: 0180 train_loss= 0.14199 train_acc= 0.97602 val_loss= 0.25385 val_acc= 0.93109 time= 0.17100
Epoch: 0181 train_loss= 0.13874 train_acc= 0.97755 val_loss= 0.25349 val_acc= 0.93262 time= 0.17200
Epoch: 0182 train_loss= 0.13835 train_acc= 0.97500 val_loss= 0.25284 val_acc= 0.93415 time= 0.16800
Epoch: 0183 train_loss= 0.13549 train_acc= 0.97619 val_loss= 0.25267 val_acc= 0.93415 time= 0.16806
Epoch: 0184 train_loss= 0.13263 train_acc= 0.97312 val_loss= 0.25310 val_acc= 0.92956 time= 0.19300
Epoch: 0185 train_loss= 0.13221 train_acc= 0.97755 val_loss= 0.25265 val_acc= 0.92956 time= 0.16800
Epoch: 0186 train_loss= 0.13118 train_acc= 0.97738 val_loss= 0.25200 val_acc= 0.93109 time= 0.16601
Epoch: 0187 train_loss= 0.13178 train_acc= 0.97636 val_loss= 0.25110 val_acc= 0.92956 time= 0.18800
Epoch: 0188 train_loss= 0.12452 train_acc= 0.97755 val_loss= 0.24973 val_acc= 0.92956 time= 0.17200
Epoch: 0189 train_loss= 0.12868 train_acc= 0.97466 val_loss= 0.24799 val_acc= 0.92956 time= 0.17099
Epoch: 0190 train_loss= 0.12684 train_acc= 0.97670 val_loss= 0.24704 val_acc= 0.93109 time= 0.19500
Epoch: 0191 train_loss= 0.12259 train_acc= 0.97874 val_loss= 0.24618 val_acc= 0.92956 time= 0.16800
Epoch: 0192 train_loss= 0.12505 train_acc= 0.97840 val_loss= 0.24558 val_acc= 0.92956 time= 0.18200
Epoch: 0193 train_loss= 0.11752 train_acc= 0.97891 val_loss= 0.24505 val_acc= 0.92956 time= 0.16801
Epoch: 0194 train_loss= 0.12420 train_acc= 0.98129 val_loss= 0.24484 val_acc= 0.93109 time= 0.16800
Epoch: 0195 train_loss= 0.11834 train_acc= 0.97942 val_loss= 0.24371 val_acc= 0.93109 time= 0.17008
Epoch: 0196 train_loss= 0.11315 train_acc= 0.97908 val_loss= 0.24264 val_acc= 0.93415 time= 0.18500
Epoch: 0197 train_loss= 0.11484 train_acc= 0.98044 val_loss= 0.24214 val_acc= 0.93415 time= 0.16900
Epoch: 0198 train_loss= 0.11554 train_acc= 0.98027 val_loss= 0.24120 val_acc= 0.93415 time= 0.18600
Epoch: 0199 train_loss= 0.11066 train_acc= 0.98061 val_loss= 0.24053 val_acc= 0.93415 time= 0.16600
Epoch: 0200 train_loss= 0.11282 train_acc= 0.98129 val_loss= 0.24028 val_acc= 0.93415 time= 0.16800
Epoch: 0201 train_loss= 0.10981 train_acc= 0.98265 val_loss= 0.23997 val_acc= 0.93568 time= 0.17000
Epoch: 0202 train_loss= 0.10768 train_acc= 0.98180 val_loss= 0.24010 val_acc= 0.93415 time= 0.18900
Epoch: 0203 train_loss= 0.10809 train_acc= 0.97993 val_loss= 0.24009 val_acc= 0.93262 time= 0.16900
Epoch: 0204 train_loss= 0.10835 train_acc= 0.98112 val_loss= 0.24064 val_acc= 0.93262 time= 0.17900
Epoch: 0205 train_loss= 0.10461 train_acc= 0.98299 val_loss= 0.24028 val_acc= 0.93109 time= 0.17000
Epoch: 0206 train_loss= 0.10483 train_acc= 0.98163 val_loss= 0.23998 val_acc= 0.92956 time= 0.16700
Epoch: 0207 train_loss= 0.10551 train_acc= 0.98469 val_loss= 0.23926 val_acc= 0.92956 time= 0.19301
Epoch: 0208 train_loss= 0.10285 train_acc= 0.98282 val_loss= 0.23887 val_acc= 0.92802 time= 0.16799
Epoch: 0209 train_loss= 0.10081 train_acc= 0.98214 val_loss= 0.23875 val_acc= 0.93109 time= 0.16600
Epoch: 0210 train_loss= 0.10502 train_acc= 0.98197 val_loss= 0.23873 val_acc= 0.92956 time= 0.18503
Epoch: 0211 train_loss= 0.09863 train_acc= 0.98316 val_loss= 0.23795 val_acc= 0.93109 time= 0.16900
Epoch: 0212 train_loss= 0.10022 train_acc= 0.98231 val_loss= 0.23716 val_acc= 0.93415 time= 0.17030
Epoch: 0213 train_loss= 0.09678 train_acc= 0.98435 val_loss= 0.23646 val_acc= 0.93415 time= 0.19600
Epoch: 0214 train_loss= 0.09609 train_acc= 0.98384 val_loss= 0.23573 val_acc= 0.93568 time= 0.16900
Epoch: 0215 train_loss= 0.09348 train_acc= 0.98469 val_loss= 0.23494 val_acc= 0.93568 time= 0.18200
Epoch: 0216 train_loss= 0.09289 train_acc= 0.98469 val_loss= 0.23434 val_acc= 0.93415 time= 0.16600
Epoch: 0217 train_loss= 0.09255 train_acc= 0.98639 val_loss= 0.23372 val_acc= 0.93568 time= 0.16900
Epoch: 0218 train_loss= 0.09117 train_acc= 0.98367 val_loss= 0.23311 val_acc= 0.93262 time= 0.17100
Epoch: 0219 train_loss= 0.09184 train_acc= 0.98435 val_loss= 0.23336 val_acc= 0.93109 time= 0.18800
Epoch: 0220 train_loss= 0.08893 train_acc= 0.98707 val_loss= 0.23378 val_acc= 0.93415 time= 0.17075
Epoch: 0221 train_loss= 0.08788 train_acc= 0.98707 val_loss= 0.23389 val_acc= 0.93262 time= 0.17099
Epoch: 0222 train_loss= 0.08828 train_acc= 0.98469 val_loss= 0.23293 val_acc= 0.92956 time= 0.16800
Epoch: 0223 train_loss= 0.08809 train_acc= 0.98690 val_loss= 0.23212 val_acc= 0.93415 time= 0.16801
Epoch: 0224 train_loss= 0.08794 train_acc= 0.98622 val_loss= 0.23185 val_acc= 0.93262 time= 0.19100
Epoch: 0225 train_loss= 0.08797 train_acc= 0.98537 val_loss= 0.23219 val_acc= 0.93262 time= 0.16800
Epoch: 0226 train_loss= 0.08493 train_acc= 0.98520 val_loss= 0.23287 val_acc= 0.93262 time= 0.16800
Epoch: 0227 train_loss= 0.08475 train_acc= 0.98690 val_loss= 0.23316 val_acc= 0.92956 time= 0.18600
Early stopping...
Optimization Finished!
Test set results: cost= 0.25970 accuracy= 0.93692 time= 0.07501
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.8081    0.9195    0.8602        87
           6     0.9200    0.9200    0.9200        25
           7     0.6667    0.9231    0.7742        13
           8     0.9091    0.9091    0.9091        11
           9     1.0000    0.2222    0.3636         9
          10     0.8667    0.7222    0.7879        36
          11     1.0000    0.9167    0.9565        12
          12     0.8511    0.9917    0.9160       121
          13     1.0000    0.7368    0.8485        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.8000    0.4444    0.5714         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.6842    0.7647    0.7222        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9698    0.9684    0.9691       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6667    1.0000    0.8000        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8553    0.8025    0.8280        81
          36     1.0000    0.3333    0.5000        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7670    0.6589    0.6833      2568
weighted avg     0.9371    0.9369    0.9318      2568

Macro average Test Precision, Recall and F1-Score...
(0.7669894718947278, 0.6589043208352918, 0.6832601578486821, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[ 0.04044395  0.34067246  0.7767837  ... -0.07617133  1.1786342
   0.0940458 ]
 [ 0.0179987   0.18290617  0.34642997 ...  0.19793625  0.6719301
   0.01642239]
 [ 0.11639862  0.18874383  0.7744764  ...  0.13827801  0.89495236
   0.25362378]
 ...
 [ 0.00837702  0.10823339  0.26569128 ...  0.03683305  0.1889765
   0.10846513]
 [ 0.07491658  0.12041849  0.36257145 ...  0.10427211  0.3078206
   0.13794829]
 [ 0.2681189   0.24873482  0.30103552 ...  0.23992075  0.27404395
   0.18548574]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95121 train_acc= 0.00527 val_loss= 3.36268 val_acc= 0.62481 time= 0.44129
Epoch: 0002 train_loss= 3.36282 train_acc= 0.61099 val_loss= 2.34569 val_acc= 0.56815 time= 0.17803
Epoch: 0003 train_loss= 2.35121 train_acc= 0.56778 val_loss= 2.20788 val_acc= 0.55436 time= 0.19900
Epoch: 0004 train_loss= 2.22273 train_acc= 0.54295 val_loss= 1.97956 val_acc= 0.64012 time= 0.16701
Epoch: 0005 train_loss= 2.00030 train_acc= 0.62783 val_loss= 1.61865 val_acc= 0.66769 time= 0.16715
Epoch: 0006 train_loss= 1.65357 train_acc= 0.65130 val_loss= 1.48593 val_acc= 0.68300 time= 0.19404
Epoch: 0007 train_loss= 1.52107 train_acc= 0.66066 val_loss= 1.38897 val_acc= 0.70291 time= 0.16701
Epoch: 0008 train_loss= 1.42762 train_acc= 0.68957 val_loss= 1.26766 val_acc= 0.69525 time= 0.16895
Epoch: 0009 train_loss= 1.29742 train_acc= 0.68770 val_loss= 1.18176 val_acc= 0.69832 time= 0.18700
Epoch: 0010 train_loss= 1.20882 train_acc= 0.69008 val_loss= 1.12124 val_acc= 0.71975 time= 0.16989
Epoch: 0011 train_loss= 1.13709 train_acc= 0.71390 val_loss= 1.07762 val_acc= 0.73047 time= 0.16900
Epoch: 0012 train_loss= 1.08327 train_acc= 0.73567 val_loss= 1.02609 val_acc= 0.73507 time= 0.16912
Epoch: 0013 train_loss= 1.02557 train_acc= 0.75234 val_loss= 0.95955 val_acc= 0.75038 time= 0.17600
Epoch: 0014 train_loss= 0.95357 train_acc= 0.76237 val_loss= 0.89628 val_acc= 0.76876 time= 0.16700
Epoch: 0015 train_loss= 0.89471 train_acc= 0.77836 val_loss= 0.84085 val_acc= 0.79786 time= 0.17500
Epoch: 0016 train_loss= 0.84926 train_acc= 0.79758 val_loss= 0.78931 val_acc= 0.82695 time= 0.16704
Epoch: 0017 train_loss= 0.79657 train_acc= 0.82735 val_loss= 0.74570 val_acc= 0.83920 time= 0.16911
Epoch: 0018 train_loss= 0.75608 train_acc= 0.84351 val_loss= 0.70485 val_acc= 0.84992 time= 0.18300
Epoch: 0019 train_loss= 0.70655 train_acc= 0.85014 val_loss= 0.66129 val_acc= 0.85299 time= 0.17083
Epoch: 0020 train_loss= 0.65720 train_acc= 0.85610 val_loss= 0.61910 val_acc= 0.85452 time= 0.16700
Epoch: 0021 train_loss= 0.61685 train_acc= 0.86154 val_loss= 0.58405 val_acc= 0.85911 time= 0.18808
Epoch: 0022 train_loss= 0.56201 train_acc= 0.87515 val_loss= 0.55527 val_acc= 0.86371 time= 0.16684
Epoch: 0023 train_loss= 0.52921 train_acc= 0.88246 val_loss= 0.52925 val_acc= 0.86830 time= 0.16610
Epoch: 0024 train_loss= 0.49376 train_acc= 0.88484 val_loss= 0.50207 val_acc= 0.86983 time= 0.19100
Epoch: 0025 train_loss= 0.45750 train_acc= 0.88995 val_loss= 0.47362 val_acc= 0.87596 time= 0.16795
Epoch: 0026 train_loss= 0.41897 train_acc= 0.90015 val_loss= 0.44831 val_acc= 0.88208 time= 0.18900
Epoch: 0027 train_loss= 0.39020 train_acc= 0.90866 val_loss= 0.42916 val_acc= 0.88515 time= 0.16978
Epoch: 0028 train_loss= 0.35715 train_acc= 0.91223 val_loss= 0.40885 val_acc= 0.89433 time= 0.16705
Epoch: 0029 train_loss= 0.32915 train_acc= 0.92244 val_loss= 0.38956 val_acc= 0.90046 time= 0.19099
Epoch: 0030 train_loss= 0.30327 train_acc= 0.92652 val_loss= 0.37178 val_acc= 0.90199 time= 0.16696
Epoch: 0031 train_loss= 0.28468 train_acc= 0.93111 val_loss= 0.35643 val_acc= 0.90199 time= 0.16911
Epoch: 0032 train_loss= 0.25966 train_acc= 0.93945 val_loss= 0.34513 val_acc= 0.90658 time= 0.17700
Epoch: 0033 train_loss= 0.24258 train_acc= 0.94387 val_loss= 0.33404 val_acc= 0.91118 time= 0.16991
Epoch: 0034 train_loss= 0.22820 train_acc= 0.94438 val_loss= 0.32587 val_acc= 0.91577 time= 0.17022
Epoch: 0035 train_loss= 0.20133 train_acc= 0.94608 val_loss= 0.32148 val_acc= 0.91424 time= 0.17300
Epoch: 0036 train_loss= 0.18891 train_acc= 0.95288 val_loss= 0.32007 val_acc= 0.91730 time= 0.19000
Epoch: 0037 train_loss= 0.17068 train_acc= 0.95799 val_loss= 0.31585 val_acc= 0.91884 time= 0.16601
Epoch: 0038 train_loss= 0.15528 train_acc= 0.95901 val_loss= 0.30993 val_acc= 0.92037 time= 0.18199
Epoch: 0039 train_loss= 0.14425 train_acc= 0.96105 val_loss= 0.30558 val_acc= 0.92496 time= 0.16600
Epoch: 0040 train_loss= 0.14004 train_acc= 0.96275 val_loss= 0.30130 val_acc= 0.92496 time= 0.16700
Epoch: 0041 train_loss= 0.12577 train_acc= 0.96887 val_loss= 0.29744 val_acc= 0.92496 time= 0.17900
Epoch: 0042 train_loss= 0.12049 train_acc= 0.97057 val_loss= 0.29196 val_acc= 0.92496 time= 0.17000
Epoch: 0043 train_loss= 0.10402 train_acc= 0.97108 val_loss= 0.28524 val_acc= 0.92956 time= 0.17100
Epoch: 0044 train_loss= 0.09854 train_acc= 0.97415 val_loss= 0.28167 val_acc= 0.93415 time= 0.18103
Epoch: 0045 train_loss= 0.09434 train_acc= 0.97704 val_loss= 0.28889 val_acc= 0.93109 time= 0.16707
Epoch: 0046 train_loss= 0.08813 train_acc= 0.97959 val_loss= 0.29340 val_acc= 0.92649 time= 0.16700
Epoch: 0047 train_loss= 0.07482 train_acc= 0.98265 val_loss= 0.29836 val_acc= 0.92956 time= 0.19200
Early stopping...
Optimization Finished!
Test set results: cost= 0.29116 accuracy= 0.92874 time= 0.07297
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.5000    0.6667         8
           1     0.5000    0.3333    0.4000         6
           2     1.0000    1.0000    1.0000         1
           3     0.8214    0.9200    0.8679        75
           4     0.9000    1.0000    0.9474         9
           5     0.8333    0.8621    0.8475        87
           6     0.9583    0.9200    0.9388        25
           7     0.7500    0.9231    0.8276        13
           8     1.0000    1.0000    1.0000        11
           9     1.0000    0.1111    0.2000         9
          10     0.8750    0.7778    0.8235        36
          11     1.0000    0.9167    0.9565        12
          12     0.8676    0.9752    0.9183       121
          13     0.8000    0.6316    0.7059        19
          14     0.7812    0.8929    0.8333        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.7500    0.3333    0.4615         9
          21     0.9048    0.9500    0.9268        20
          22     0.4000    0.4000    0.4000         5
          23     0.0000    0.0000    0.0000         1
          24     0.5385    0.8235    0.6512        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9000    0.7500    0.8182        12
          28     1.0000    0.8182    0.9000        11
          29     0.9653    0.9598    0.9625       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.6667    0.8000         3
          32     0.6667    0.8000    0.7273        10
          33     1.0000    0.6667    0.8000         3
          34     1.0000    1.0000    1.0000         1
          35     0.7816    0.8395    0.8095        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9791    0.9926    0.9858      1083
          40     0.8000    0.8000    0.8000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.5333    0.6667    0.5926        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7500    0.8000    0.7742        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9287      2568
   macro avg     0.7334    0.6409    0.6653      2568
weighted avg     0.9270    0.9287    0.9239      2568

Macro average Test Precision, Recall and F1-Score...
(0.7333642238324506, 0.6409122469017855, 0.6653235768177775, None)
Micro average Test Precision, Recall and F1-Score...
(0.9287383177570093, 0.9287383177570093, 0.9287383177570093, None)
embeddings:
8892 6532 2568
[[ 0.19185202 -0.0319742   0.13454512 ... -0.57325584 -0.01281799
  -0.6974319 ]
 [ 0.4199056  -0.03492921 -0.14050934 ... -0.3313067  -0.05440143
  -0.41232243]
 [ 0.13801102  0.65221345 -0.32885468 ... -0.4325598   0.1566645
  -0.17772219]
 ...
 [ 0.1838774   0.39134645 -0.15602231 ... -0.17948876  0.09853707
   0.1594022 ]
 [-0.03284623  0.15675747 -0.11672662 ... -0.18160132  0.10027549
  -0.06006315]
 [-0.02733672  0.16777639  0.07580419 ...  0.06270169  0.4273998
   0.10420243]]

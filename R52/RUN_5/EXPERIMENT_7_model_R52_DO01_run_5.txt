(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95131 train_acc= 0.00034 val_loss= 3.89878 val_acc= 0.67075 time= 0.45100
Epoch: 0002 train_loss= 3.89887 train_acc= 0.64416 val_loss= 3.79968 val_acc= 0.66769 time= 0.19200
Epoch: 0003 train_loss= 3.80037 train_acc= 0.64229 val_loss= 3.64655 val_acc= 0.66922 time= 0.16714
Epoch: 0004 train_loss= 3.64917 train_acc= 0.64348 val_loss= 3.43891 val_acc= 0.67075 time= 0.16698
Epoch: 0005 train_loss= 3.43950 train_acc= 0.64518 val_loss= 3.18604 val_acc= 0.66922 time= 0.19201
Epoch: 0006 train_loss= 3.18922 train_acc= 0.64739 val_loss= 2.91012 val_acc= 0.66922 time= 0.16499
Epoch: 0007 train_loss= 2.90986 train_acc= 0.64892 val_loss= 2.64374 val_acc= 0.67075 time= 0.17297
Epoch: 0008 train_loss= 2.64473 train_acc= 0.64892 val_loss= 2.42355 val_acc= 0.65850 time= 0.17800
Epoch: 0009 train_loss= 2.41896 train_acc= 0.64807 val_loss= 2.27900 val_acc= 0.64778 time= 0.16903
Epoch: 0010 train_loss= 2.27540 train_acc= 0.63429 val_loss= 2.20252 val_acc= 0.58652 time= 0.16709
Epoch: 0011 train_loss= 2.20501 train_acc= 0.58717 val_loss= 2.15748 val_acc= 0.51149 time= 0.19100
Epoch: 0012 train_loss= 2.16082 train_acc= 0.49430 val_loss= 2.11166 val_acc= 0.48392 time= 0.16700
Epoch: 0013 train_loss= 2.11609 train_acc= 0.45807 val_loss= 2.04913 val_acc= 0.47933 time= 0.18399
Epoch: 0014 train_loss= 2.06180 train_acc= 0.45178 val_loss= 1.96752 val_acc= 0.48545 time= 0.16700
Epoch: 0015 train_loss= 1.98650 train_acc= 0.46130 val_loss= 1.87494 val_acc= 0.50842 time= 0.17000
Epoch: 0016 train_loss= 1.89209 train_acc= 0.49770 val_loss= 1.78484 val_acc= 0.56202 time= 0.17400
Epoch: 0017 train_loss= 1.80842 train_acc= 0.55809 val_loss= 1.70904 val_acc= 0.62481 time= 0.18411
Epoch: 0018 train_loss= 1.73865 train_acc= 0.61932 val_loss= 1.64943 val_acc= 0.65697 time= 0.16700
Epoch: 0019 train_loss= 1.67942 train_acc= 0.64518 val_loss= 1.59785 val_acc= 0.67381 time= 0.16900
Epoch: 0020 train_loss= 1.62990 train_acc= 0.64994 val_loss= 1.54598 val_acc= 0.67994 time= 0.16900
Epoch: 0021 train_loss= 1.57425 train_acc= 0.65504 val_loss= 1.49203 val_acc= 0.68453 time= 0.16600
Epoch: 0022 train_loss= 1.52587 train_acc= 0.65759 val_loss= 1.43813 val_acc= 0.67841 time= 0.19438
Epoch: 0023 train_loss= 1.46872 train_acc= 0.66066 val_loss= 1.38735 val_acc= 0.67994 time= 0.16900
Epoch: 0024 train_loss= 1.42321 train_acc= 0.66134 val_loss= 1.34114 val_acc= 0.68453 time= 0.17000
Epoch: 0025 train_loss= 1.37354 train_acc= 0.66882 val_loss= 1.29970 val_acc= 0.69678 time= 0.18800
Epoch: 0026 train_loss= 1.33023 train_acc= 0.67528 val_loss= 1.26252 val_acc= 0.70904 time= 0.16706
Epoch: 0027 train_loss= 1.29375 train_acc= 0.68923 val_loss= 1.22870 val_acc= 0.71516 time= 0.16700
Epoch: 0028 train_loss= 1.25794 train_acc= 0.70148 val_loss= 1.19736 val_acc= 0.72435 time= 0.18900
Epoch: 0029 train_loss= 1.22523 train_acc= 0.71339 val_loss= 1.16763 val_acc= 0.72741 time= 0.16600
Epoch: 0030 train_loss= 1.19351 train_acc= 0.72836 val_loss= 1.13879 val_acc= 0.73201 time= 0.17100
Epoch: 0031 train_loss= 1.16200 train_acc= 0.73822 val_loss= 1.11037 val_acc= 0.73507 time= 0.17397
Epoch: 0032 train_loss= 1.12949 train_acc= 0.74707 val_loss= 1.08222 val_acc= 0.74273 time= 0.17200
Epoch: 0033 train_loss= 1.10079 train_acc= 0.75336 val_loss= 1.05438 val_acc= 0.74579 time= 0.16903
Epoch: 0034 train_loss= 1.07006 train_acc= 0.75965 val_loss= 1.02707 val_acc= 0.75498 time= 0.19134
Epoch: 0035 train_loss= 1.03840 train_acc= 0.76612 val_loss= 1.00038 val_acc= 0.76417 time= 0.16700
Epoch: 0036 train_loss= 1.00994 train_acc= 0.77394 val_loss= 0.97437 val_acc= 0.76876 time= 0.18197
Epoch: 0037 train_loss= 0.98470 train_acc= 0.78143 val_loss= 0.94898 val_acc= 0.77948 time= 0.16800
Epoch: 0038 train_loss= 0.95544 train_acc= 0.78840 val_loss= 0.92407 val_acc= 0.78867 time= 0.16700
Epoch: 0039 train_loss= 0.93413 train_acc= 0.79486 val_loss= 0.89955 val_acc= 0.80704 time= 0.17309
Epoch: 0040 train_loss= 0.90688 train_acc= 0.80745 val_loss= 0.87525 val_acc= 0.81011 time= 0.17700
Epoch: 0041 train_loss= 0.87899 train_acc= 0.81613 val_loss= 0.85119 val_acc= 0.81776 time= 0.17003
Epoch: 0042 train_loss= 0.85659 train_acc= 0.82055 val_loss= 0.82744 val_acc= 0.82389 time= 0.16700
Epoch: 0043 train_loss= 0.83062 train_acc= 0.82701 val_loss= 0.80415 val_acc= 0.83308 time= 0.16801
Epoch: 0044 train_loss= 0.80427 train_acc= 0.83569 val_loss= 0.78129 val_acc= 0.83767 time= 0.16709
Epoch: 0045 train_loss= 0.77981 train_acc= 0.83960 val_loss= 0.75887 val_acc= 0.84074 time= 0.18899
Epoch: 0046 train_loss= 0.75604 train_acc= 0.84266 val_loss= 0.73674 val_acc= 0.84533 time= 0.16700
Epoch: 0047 train_loss= 0.73118 train_acc= 0.85048 val_loss= 0.71484 val_acc= 0.84686 time= 0.16816
Epoch: 0048 train_loss= 0.70876 train_acc= 0.85219 val_loss= 0.69321 val_acc= 0.84992 time= 0.19800
Epoch: 0049 train_loss= 0.68744 train_acc= 0.85678 val_loss= 0.67199 val_acc= 0.85145 time= 0.17000
Epoch: 0050 train_loss= 0.66261 train_acc= 0.85967 val_loss= 0.65135 val_acc= 0.85758 time= 0.16700
Epoch: 0051 train_loss= 0.64097 train_acc= 0.86324 val_loss= 0.63142 val_acc= 0.86371 time= 0.19301
Epoch: 0052 train_loss= 0.62233 train_acc= 0.86886 val_loss= 0.61233 val_acc= 0.86524 time= 0.16699
Epoch: 0053 train_loss= 0.59962 train_acc= 0.87311 val_loss= 0.59398 val_acc= 0.86983 time= 0.16900
Epoch: 0054 train_loss= 0.58066 train_acc= 0.87566 val_loss= 0.57637 val_acc= 0.87289 time= 0.16500
Epoch: 0055 train_loss= 0.56243 train_acc= 0.87974 val_loss= 0.55950 val_acc= 0.87443 time= 0.16900
Epoch: 0056 train_loss= 0.54635 train_acc= 0.88212 val_loss= 0.54349 val_acc= 0.87749 time= 0.17500
Epoch: 0057 train_loss= 0.52490 train_acc= 0.88587 val_loss= 0.52822 val_acc= 0.87749 time= 0.19535
Epoch: 0058 train_loss= 0.50667 train_acc= 0.88978 val_loss= 0.51357 val_acc= 0.87902 time= 0.16900
Epoch: 0059 train_loss= 0.49084 train_acc= 0.89114 val_loss= 0.49934 val_acc= 0.88208 time= 0.18600
Epoch: 0060 train_loss= 0.47454 train_acc= 0.89675 val_loss= 0.48530 val_acc= 0.88668 time= 0.16600
Epoch: 0061 train_loss= 0.45795 train_acc= 0.90032 val_loss= 0.47151 val_acc= 0.88668 time= 0.16700
Epoch: 0062 train_loss= 0.44264 train_acc= 0.90424 val_loss= 0.45806 val_acc= 0.89127 time= 0.17199
Epoch: 0063 train_loss= 0.42875 train_acc= 0.90917 val_loss= 0.44514 val_acc= 0.89280 time= 0.18901
Epoch: 0064 train_loss= 0.41264 train_acc= 0.91393 val_loss= 0.43278 val_acc= 0.89893 time= 0.17000
Epoch: 0065 train_loss= 0.40063 train_acc= 0.91818 val_loss= 0.42118 val_acc= 0.89893 time= 0.16900
Epoch: 0066 train_loss= 0.38596 train_acc= 0.92193 val_loss= 0.41030 val_acc= 0.90352 time= 0.16800
Epoch: 0067 train_loss= 0.37468 train_acc= 0.92635 val_loss= 0.40029 val_acc= 0.90352 time= 0.16613
Epoch: 0068 train_loss= 0.36089 train_acc= 0.92737 val_loss= 0.39099 val_acc= 0.90352 time= 0.18314
Epoch: 0069 train_loss= 0.34868 train_acc= 0.92958 val_loss= 0.38233 val_acc= 0.90505 time= 0.16615
Epoch: 0070 train_loss= 0.33831 train_acc= 0.93281 val_loss= 0.37423 val_acc= 0.90658 time= 0.16703
Epoch: 0071 train_loss= 0.32650 train_acc= 0.93502 val_loss= 0.36658 val_acc= 0.90812 time= 0.18797
Epoch: 0072 train_loss= 0.31817 train_acc= 0.93740 val_loss= 0.35914 val_acc= 0.90965 time= 0.17000
Epoch: 0073 train_loss= 0.30601 train_acc= 0.93962 val_loss= 0.35167 val_acc= 0.91118 time= 0.17000
Epoch: 0074 train_loss= 0.29687 train_acc= 0.94047 val_loss= 0.34429 val_acc= 0.91118 time= 0.16903
Epoch: 0075 train_loss= 0.28663 train_acc= 0.94387 val_loss= 0.33722 val_acc= 0.91271 time= 0.16900
Epoch: 0076 train_loss= 0.27629 train_acc= 0.94540 val_loss= 0.33062 val_acc= 0.91424 time= 0.16702
Epoch: 0077 train_loss= 0.26792 train_acc= 0.94727 val_loss= 0.32444 val_acc= 0.91730 time= 0.18700
Epoch: 0078 train_loss= 0.25900 train_acc= 0.94931 val_loss= 0.31877 val_acc= 0.91730 time= 0.16800
Epoch: 0079 train_loss= 0.24961 train_acc= 0.95084 val_loss= 0.31348 val_acc= 0.91424 time= 0.16700
Epoch: 0080 train_loss= 0.24066 train_acc= 0.95407 val_loss= 0.30863 val_acc= 0.91884 time= 0.19667
Epoch: 0081 train_loss= 0.23118 train_acc= 0.95697 val_loss= 0.30376 val_acc= 0.92037 time= 0.17000
Epoch: 0082 train_loss= 0.22471 train_acc= 0.95816 val_loss= 0.29878 val_acc= 0.92037 time= 0.18600
Epoch: 0083 train_loss= 0.21703 train_acc= 0.95952 val_loss= 0.29416 val_acc= 0.92190 time= 0.16714
Epoch: 0084 train_loss= 0.20982 train_acc= 0.96020 val_loss= 0.28969 val_acc= 0.92343 time= 0.16718
Epoch: 0085 train_loss= 0.20314 train_acc= 0.96105 val_loss= 0.28565 val_acc= 0.92649 time= 0.17100
Epoch: 0086 train_loss= 0.19641 train_acc= 0.96275 val_loss= 0.28211 val_acc= 0.92649 time= 0.18800
Epoch: 0087 train_loss= 0.18876 train_acc= 0.96326 val_loss= 0.27892 val_acc= 0.92649 time= 0.16700
Epoch: 0088 train_loss= 0.18229 train_acc= 0.96530 val_loss= 0.27613 val_acc= 0.92802 time= 0.16900
Epoch: 0089 train_loss= 0.17658 train_acc= 0.96751 val_loss= 0.27366 val_acc= 0.92956 time= 0.16900
Epoch: 0090 train_loss= 0.16959 train_acc= 0.96819 val_loss= 0.27087 val_acc= 0.92802 time= 0.16800
Epoch: 0091 train_loss= 0.16314 train_acc= 0.96972 val_loss= 0.26786 val_acc= 0.92649 time= 0.19402
Epoch: 0092 train_loss= 0.15782 train_acc= 0.97125 val_loss= 0.26402 val_acc= 0.92649 time= 0.16798
Epoch: 0093 train_loss= 0.15275 train_acc= 0.97108 val_loss= 0.26040 val_acc= 0.92649 time= 0.16599
Epoch: 0094 train_loss= 0.14717 train_acc= 0.97483 val_loss= 0.25752 val_acc= 0.92802 time= 0.18800
Epoch: 0095 train_loss= 0.14271 train_acc= 0.97346 val_loss= 0.25505 val_acc= 0.92956 time= 0.16807
Epoch: 0096 train_loss= 0.13813 train_acc= 0.97653 val_loss= 0.25266 val_acc= 0.92956 time= 0.16897
Epoch: 0097 train_loss= 0.13232 train_acc= 0.97806 val_loss= 0.25100 val_acc= 0.93109 time= 0.19600
Epoch: 0098 train_loss= 0.12639 train_acc= 0.97942 val_loss= 0.24952 val_acc= 0.93109 time= 0.16800
Epoch: 0099 train_loss= 0.12241 train_acc= 0.97959 val_loss= 0.24810 val_acc= 0.93262 time= 0.18000
Epoch: 0100 train_loss= 0.11851 train_acc= 0.98078 val_loss= 0.24664 val_acc= 0.93109 time= 0.16700
Epoch: 0101 train_loss= 0.11528 train_acc= 0.98146 val_loss= 0.24590 val_acc= 0.93109 time= 0.16708
Epoch: 0102 train_loss= 0.11003 train_acc= 0.98367 val_loss= 0.24408 val_acc= 0.93109 time= 0.16900
Epoch: 0103 train_loss= 0.10655 train_acc= 0.98282 val_loss= 0.24205 val_acc= 0.93415 time= 0.18900
Epoch: 0104 train_loss= 0.10368 train_acc= 0.98367 val_loss= 0.24049 val_acc= 0.93415 time= 0.16995
Epoch: 0105 train_loss= 0.09961 train_acc= 0.98520 val_loss= 0.23914 val_acc= 0.93415 time= 0.18699
Epoch: 0106 train_loss= 0.09572 train_acc= 0.98588 val_loss= 0.23846 val_acc= 0.93109 time= 0.16800
Epoch: 0107 train_loss= 0.09282 train_acc= 0.98605 val_loss= 0.23815 val_acc= 0.93109 time= 0.16842
Epoch: 0108 train_loss= 0.09014 train_acc= 0.98741 val_loss= 0.23809 val_acc= 0.93109 time= 0.18896
Epoch: 0109 train_loss= 0.08702 train_acc= 0.98741 val_loss= 0.23698 val_acc= 0.93109 time= 0.16804
Epoch: 0110 train_loss= 0.08366 train_acc= 0.98724 val_loss= 0.23525 val_acc= 0.93109 time= 0.16701
Epoch: 0111 train_loss= 0.08089 train_acc= 0.98826 val_loss= 0.23337 val_acc= 0.93262 time= 0.16696
Epoch: 0112 train_loss= 0.07879 train_acc= 0.98792 val_loss= 0.23127 val_acc= 0.93415 time= 0.16908
Epoch: 0113 train_loss= 0.07636 train_acc= 0.98911 val_loss= 0.23029 val_acc= 0.93109 time= 0.17000
Epoch: 0114 train_loss= 0.07313 train_acc= 0.98979 val_loss= 0.23012 val_acc= 0.93109 time= 0.19600
Epoch: 0115 train_loss= 0.07187 train_acc= 0.98979 val_loss= 0.23032 val_acc= 0.93262 time= 0.16700
Epoch: 0116 train_loss= 0.06862 train_acc= 0.99013 val_loss= 0.23116 val_acc= 0.92956 time= 0.16910
Epoch: 0117 train_loss= 0.06706 train_acc= 0.99030 val_loss= 0.23136 val_acc= 0.93109 time= 0.18400
Epoch: 0118 train_loss= 0.06418 train_acc= 0.99167 val_loss= 0.23093 val_acc= 0.93109 time= 0.16597
Epoch: 0119 train_loss= 0.06253 train_acc= 0.99115 val_loss= 0.22951 val_acc= 0.93109 time= 0.16803
Epoch: 0120 train_loss= 0.06024 train_acc= 0.99184 val_loss= 0.22796 val_acc= 0.93721 time= 0.19297
Epoch: 0121 train_loss= 0.05877 train_acc= 0.99133 val_loss= 0.22705 val_acc= 0.93568 time= 0.17000
Epoch: 0122 train_loss= 0.05599 train_acc= 0.99286 val_loss= 0.22704 val_acc= 0.93568 time= 0.17500
Epoch: 0123 train_loss= 0.05476 train_acc= 0.99269 val_loss= 0.22755 val_acc= 0.93568 time= 0.16700
Epoch: 0124 train_loss= 0.05338 train_acc= 0.99286 val_loss= 0.22784 val_acc= 0.93721 time= 0.16704
Epoch: 0125 train_loss= 0.05328 train_acc= 0.99337 val_loss= 0.22841 val_acc= 0.93721 time= 0.18699
Epoch: 0126 train_loss= 0.05067 train_acc= 0.99371 val_loss= 0.22918 val_acc= 0.93568 time= 0.16800
Early stopping...
Optimization Finished!
Test set results: cost= 0.25299 accuracy= 0.93731 time= 0.07332
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7742    0.9600    0.8571        75
           4     1.0000    1.0000    1.0000         9
           5     0.8652    0.8851    0.8750        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.4444    0.6154         9
          10     0.9259    0.6944    0.7937        36
          11     1.0000    0.9167    0.9565        12
          12     0.8633    0.9917    0.9231       121
          13     0.9333    0.7368    0.8235        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9167    0.9167    0.9167        12
          28     1.0000    0.8182    0.9000        11
          29     0.9627    0.9641    0.9634       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8000    0.8395    0.8193        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9781    0.9908    0.9844      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.3333    0.5000         3
          44     0.8182    0.7500    0.7826        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9231    0.8000    0.8571        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9373      2568
   macro avg     0.7692    0.6835    0.6999      2568
weighted avg     0.9360    0.9373    0.9327      2568

Macro average Test Precision, Recall and F1-Score...
(0.7692430558217752, 0.6834794726377946, 0.6998579102670553, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[-0.16082722 -0.22306499 -0.00257835 ... -0.10184731 -0.12544794
  -0.00500474]
 [-0.04806319 -0.05429716  0.2677625  ... -0.00294804  0.28248116
   0.19641283]
 [-0.02897337  0.1593658   0.7757319  ...  0.00169072 -0.00216379
   0.21242952]
 ...
 [ 0.04741401  0.26534462  0.12603913 ... -0.02291514  0.0866745
   0.12336776]
 [ 0.03387195  0.12673013  0.4451606  ...  0.03527581  0.06390694
   0.09335577]
 [ 0.25674808  0.22969818  0.33113503 ...  0.2550506   0.27902088
   0.25541386]]

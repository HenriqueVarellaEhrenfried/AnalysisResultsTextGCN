(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95113 train_acc= 0.01735 val_loss= 3.87158 val_acc= 0.67534 time= 2.35399
Epoch: 0002 train_loss= 3.87332 train_acc= 0.65436 val_loss= 3.70742 val_acc= 0.67381 time= 2.20400
Epoch: 0003 train_loss= 3.70853 train_acc= 0.64705 val_loss= 3.45226 val_acc= 0.67075 time= 2.23300
Epoch: 0004 train_loss= 3.45722 train_acc= 0.64790 val_loss= 3.12213 val_acc= 0.67228 time= 2.19800
Epoch: 0005 train_loss= 3.12583 train_acc= 0.64382 val_loss= 2.76905 val_acc= 0.67075 time= 2.20100
Epoch: 0006 train_loss= 2.77777 train_acc= 0.65079 val_loss= 2.46721 val_acc= 0.66922 time= 2.19729
Epoch: 0007 train_loss= 2.46218 train_acc= 0.65096 val_loss= 2.28322 val_acc= 0.65544 time= 2.18700
Epoch: 0008 train_loss= 2.28388 train_acc= 0.64110 val_loss= 2.20575 val_acc= 0.52374 time= 2.19300
Epoch: 0009 train_loss= 2.21566 train_acc= 0.50876 val_loss= 2.16620 val_acc= 0.46401 time= 2.19900
Epoch: 0010 train_loss= 2.17483 train_acc= 0.43868 val_loss= 2.11283 val_acc= 0.45942 time= 2.21400
Epoch: 0011 train_loss= 2.13132 train_acc= 0.43477 val_loss= 2.02616 val_acc= 0.46248 time= 2.18200
Epoch: 0012 train_loss= 2.04154 train_acc= 0.43494 val_loss= 1.91465 val_acc= 0.47779 time= 2.22300
Epoch: 0013 train_loss= 1.93766 train_acc= 0.45314 val_loss= 1.80228 val_acc= 0.54058 time= 2.20000
Epoch: 0014 train_loss= 1.82529 train_acc= 0.53751 val_loss= 1.71091 val_acc= 0.64319 time= 2.18100
Epoch: 0015 train_loss= 1.73218 train_acc= 0.63106 val_loss= 1.64238 val_acc= 0.67075 time= 2.17700
Epoch: 0016 train_loss= 1.66551 train_acc= 0.65062 val_loss= 1.58192 val_acc= 0.67994 time= 2.20500
Epoch: 0017 train_loss= 1.60524 train_acc= 0.65351 val_loss= 1.51959 val_acc= 0.67534 time= 2.21200
Epoch: 0018 train_loss= 1.53859 train_acc= 0.65691 val_loss= 1.45538 val_acc= 0.67688 time= 2.19900
Epoch: 0019 train_loss= 1.47831 train_acc= 0.66236 val_loss= 1.39341 val_acc= 0.68300 time= 2.19300
Epoch: 0020 train_loss= 1.41941 train_acc= 0.66729 val_loss= 1.33704 val_acc= 0.69678 time= 2.19500
Epoch: 0021 train_loss= 1.36424 train_acc= 0.67563 val_loss= 1.28748 val_acc= 0.70904 time= 2.21552
Epoch: 0022 train_loss= 1.30715 train_acc= 0.69059 val_loss= 1.24431 val_acc= 0.72282 time= 2.20700
Epoch: 0023 train_loss= 1.26479 train_acc= 0.70947 val_loss= 1.20599 val_acc= 0.72588 time= 2.21600
Epoch: 0024 train_loss= 1.22861 train_acc= 0.72070 val_loss= 1.17073 val_acc= 0.73201 time= 2.36800
Epoch: 0025 train_loss= 1.18723 train_acc= 0.73295 val_loss= 1.13715 val_acc= 0.73507 time= 2.23700
Epoch: 0026 train_loss= 1.15346 train_acc= 0.74366 val_loss= 1.10427 val_acc= 0.74273 time= 2.19500
Epoch: 0027 train_loss= 1.12065 train_acc= 0.74741 val_loss= 1.07168 val_acc= 0.74732 time= 2.23155
Epoch: 0028 train_loss= 1.08163 train_acc= 0.75761 val_loss= 1.03934 val_acc= 0.75345 time= 2.22571
Epoch: 0029 train_loss= 1.04529 train_acc= 0.76459 val_loss= 1.00744 val_acc= 0.75498 time= 2.20897
Epoch: 0030 train_loss= 1.01363 train_acc= 0.76935 val_loss= 0.97631 val_acc= 0.76570 time= 2.22601
Epoch: 0031 train_loss= 0.98241 train_acc= 0.77785 val_loss= 0.94614 val_acc= 0.77489 time= 2.22200
Epoch: 0032 train_loss= 0.94974 train_acc= 0.78602 val_loss= 0.91687 val_acc= 0.78867 time= 2.24481
Epoch: 0033 train_loss= 0.92477 train_acc= 0.79520 val_loss= 0.88851 val_acc= 0.79786 time= 2.21401
Epoch: 0034 train_loss= 0.89384 train_acc= 0.80711 val_loss= 0.86090 val_acc= 0.80704 time= 2.21700
Epoch: 0035 train_loss= 0.86895 train_acc= 0.81408 val_loss= 0.83424 val_acc= 0.82695 time= 2.24700
Epoch: 0036 train_loss= 0.84073 train_acc= 0.82106 val_loss= 0.80845 val_acc= 0.83308 time= 2.21500
Epoch: 0037 train_loss= 0.81347 train_acc= 0.83075 val_loss= 0.78351 val_acc= 0.83920 time= 2.20300
Epoch: 0038 train_loss= 0.79051 train_acc= 0.83688 val_loss= 0.75943 val_acc= 0.84380 time= 2.21159
Epoch: 0039 train_loss= 0.76932 train_acc= 0.83858 val_loss= 0.73619 val_acc= 0.84686 time= 2.22701
Epoch: 0040 train_loss= 0.74093 train_acc= 0.84402 val_loss= 0.71364 val_acc= 0.84992 time= 2.23399
Epoch: 0041 train_loss= 0.71621 train_acc= 0.84606 val_loss= 0.69181 val_acc= 0.85146 time= 2.20200
Epoch: 0042 train_loss= 0.68588 train_acc= 0.85218 val_loss= 0.67040 val_acc= 0.85146 time= 2.21101
Epoch: 0043 train_loss= 0.65956 train_acc= 0.85661 val_loss= 0.64972 val_acc= 0.85452 time= 2.23200
Epoch: 0044 train_loss= 0.64364 train_acc= 0.85865 val_loss= 0.62964 val_acc= 0.85605 time= 2.22500
Epoch: 0045 train_loss= 0.61567 train_acc= 0.86409 val_loss= 0.60997 val_acc= 0.85911 time= 2.20800
Epoch: 0046 train_loss= 0.59112 train_acc= 0.87090 val_loss= 0.59062 val_acc= 0.86217 time= 2.21899
Epoch: 0047 train_loss= 0.57797 train_acc= 0.87175 val_loss= 0.57222 val_acc= 0.86371 time= 2.20799
Epoch: 0048 train_loss= 0.55303 train_acc= 0.87532 val_loss= 0.55468 val_acc= 0.87136 time= 2.21498
Epoch: 0049 train_loss= 0.53733 train_acc= 0.88246 val_loss= 0.53788 val_acc= 0.87443 time= 2.20503
Epoch: 0050 train_loss= 0.51182 train_acc= 0.88416 val_loss= 0.52238 val_acc= 0.87902 time= 2.21199
Epoch: 0051 train_loss= 0.49761 train_acc= 0.88842 val_loss= 0.50765 val_acc= 0.87902 time= 2.22901
Epoch: 0052 train_loss= 0.48298 train_acc= 0.88842 val_loss= 0.49354 val_acc= 0.88361 time= 2.22504
Epoch: 0053 train_loss= 0.46311 train_acc= 0.89675 val_loss= 0.47981 val_acc= 0.88515 time= 2.20901
Epoch: 0054 train_loss= 0.44070 train_acc= 0.90202 val_loss= 0.46598 val_acc= 0.88668 time= 2.21898
Epoch: 0055 train_loss= 0.42952 train_acc= 0.90577 val_loss= 0.45251 val_acc= 0.88821 time= 2.20898
Epoch: 0056 train_loss= 0.41006 train_acc= 0.90713 val_loss= 0.43968 val_acc= 0.89433 time= 2.19701
Epoch: 0057 train_loss= 0.39489 train_acc= 0.91019 val_loss= 0.42729 val_acc= 0.89893 time= 2.21399
Epoch: 0058 train_loss= 0.38700 train_acc= 0.91495 val_loss= 0.41526 val_acc= 0.89740 time= 2.21101
Epoch: 0059 train_loss= 0.37238 train_acc= 0.91784 val_loss= 0.40379 val_acc= 0.89433 time= 2.21500
Epoch: 0060 train_loss= 0.35453 train_acc= 0.92448 val_loss= 0.39332 val_acc= 0.89740 time= 2.22699
Epoch: 0061 train_loss= 0.34048 train_acc= 0.92839 val_loss= 0.38361 val_acc= 0.90046 time= 2.23700
Epoch: 0062 train_loss= 0.33151 train_acc= 0.93128 val_loss= 0.37497 val_acc= 0.90352 time= 2.22900
Epoch: 0063 train_loss= 0.32078 train_acc= 0.93434 val_loss= 0.36698 val_acc= 0.90352 time= 2.21099
Epoch: 0064 train_loss= 0.30422 train_acc= 0.94080 val_loss= 0.35968 val_acc= 0.90505 time= 2.22399
Epoch: 0065 train_loss= 0.29722 train_acc= 0.94200 val_loss= 0.35223 val_acc= 0.90505 time= 2.21601
Epoch: 0066 train_loss= 0.28846 train_acc= 0.93995 val_loss= 0.34542 val_acc= 0.90352 time= 2.21099
Epoch: 0067 train_loss= 0.27423 train_acc= 0.94268 val_loss= 0.33971 val_acc= 0.90505 time= 2.20600
Epoch: 0068 train_loss= 0.26447 train_acc= 0.94795 val_loss= 0.33424 val_acc= 0.90505 time= 2.21400
Epoch: 0069 train_loss= 0.25811 train_acc= 0.94829 val_loss= 0.32854 val_acc= 0.90812 time= 2.25501
Epoch: 0070 train_loss= 0.24827 train_acc= 0.94863 val_loss= 0.32333 val_acc= 0.90965 time= 2.22888
Epoch: 0071 train_loss= 0.24202 train_acc= 0.95050 val_loss= 0.31808 val_acc= 0.91271 time= 2.23398
Epoch: 0072 train_loss= 0.22806 train_acc= 0.95407 val_loss= 0.31335 val_acc= 0.91424 time= 2.26102
Epoch: 0073 train_loss= 0.21853 train_acc= 0.95288 val_loss= 0.30777 val_acc= 0.91731 time= 2.21000
Epoch: 0074 train_loss= 0.21605 train_acc= 0.95594 val_loss= 0.30143 val_acc= 0.91884 time= 2.23701
Epoch: 0075 train_loss= 0.20586 train_acc= 0.95815 val_loss= 0.29591 val_acc= 0.92037 time= 2.22299
Epoch: 0076 train_loss= 0.20316 train_acc= 0.95662 val_loss= 0.29147 val_acc= 0.92190 time= 2.22001
Epoch: 0077 train_loss= 0.19251 train_acc= 0.96003 val_loss= 0.28733 val_acc= 0.92496 time= 2.21399
Epoch: 0078 train_loss= 0.18407 train_acc= 0.96122 val_loss= 0.28438 val_acc= 0.92343 time= 2.20691
Epoch: 0079 train_loss= 0.17829 train_acc= 0.96173 val_loss= 0.28202 val_acc= 0.92190 time= 2.21800
Epoch: 0080 train_loss= 0.17831 train_acc= 0.96071 val_loss= 0.28074 val_acc= 0.92190 time= 2.21003
Epoch: 0081 train_loss= 0.16502 train_acc= 0.96615 val_loss= 0.27886 val_acc= 0.92190 time= 2.21438
Epoch: 0082 train_loss= 0.16066 train_acc= 0.96802 val_loss= 0.27590 val_acc= 0.92190 time= 2.23199
Epoch: 0083 train_loss= 0.15707 train_acc= 0.96700 val_loss= 0.27231 val_acc= 0.92496 time= 2.24801
Epoch: 0084 train_loss= 0.14946 train_acc= 0.96938 val_loss= 0.26830 val_acc= 0.92496 time= 2.22700
Epoch: 0085 train_loss= 0.14295 train_acc= 0.96836 val_loss= 0.26494 val_acc= 0.92649 time= 2.22199
Epoch: 0086 train_loss= 0.14168 train_acc= 0.97210 val_loss= 0.26196 val_acc= 0.92956 time= 2.19814
Epoch: 0087 train_loss= 0.13282 train_acc= 0.97244 val_loss= 0.25985 val_acc= 0.92802 time= 2.21700
Epoch: 0088 train_loss= 0.13117 train_acc= 0.97295 val_loss= 0.25727 val_acc= 0.92956 time= 2.23901
Epoch: 0089 train_loss= 0.12757 train_acc= 0.97516 val_loss= 0.25547 val_acc= 0.93109 time= 2.21864
Epoch: 0090 train_loss= 0.12056 train_acc= 0.97687 val_loss= 0.25410 val_acc= 0.93415 time= 2.23099
Epoch: 0091 train_loss= 0.11770 train_acc= 0.97857 val_loss= 0.25287 val_acc= 0.92956 time= 2.21301
Epoch: 0092 train_loss= 0.11545 train_acc= 0.97925 val_loss= 0.25208 val_acc= 0.93109 time= 2.20702
Epoch: 0093 train_loss= 0.10947 train_acc= 0.97840 val_loss= 0.25004 val_acc= 0.93262 time= 2.23199
Epoch: 0094 train_loss= 0.10715 train_acc= 0.98010 val_loss= 0.24830 val_acc= 0.93415 time= 2.20700
Epoch: 0095 train_loss= 0.10143 train_acc= 0.98095 val_loss= 0.24750 val_acc= 0.93721 time= 2.23099
Epoch: 0096 train_loss= 0.09649 train_acc= 0.98214 val_loss= 0.24612 val_acc= 0.93874 time= 2.23201
Epoch: 0097 train_loss= 0.09509 train_acc= 0.98010 val_loss= 0.24613 val_acc= 0.93721 time= 2.21997
Epoch: 0098 train_loss= 0.09283 train_acc= 0.98367 val_loss= 0.24705 val_acc= 0.93415 time= 2.22324
Epoch: 0099 train_loss= 0.09171 train_acc= 0.98401 val_loss= 0.24662 val_acc= 0.93262 time= 2.22500
Epoch: 0100 train_loss= 0.08707 train_acc= 0.98673 val_loss= 0.24579 val_acc= 0.93109 time= 2.21900
Epoch: 0101 train_loss= 0.08651 train_acc= 0.98537 val_loss= 0.24337 val_acc= 0.92956 time= 2.20400
Epoch: 0102 train_loss= 0.08283 train_acc= 0.98435 val_loss= 0.24273 val_acc= 0.93262 time= 2.22200
Epoch: 0103 train_loss= 0.08160 train_acc= 0.98537 val_loss= 0.24079 val_acc= 0.93415 time= 2.20200
Epoch: 0104 train_loss= 0.07772 train_acc= 0.98588 val_loss= 0.23975 val_acc= 0.93415 time= 2.21100
Epoch: 0105 train_loss= 0.07342 train_acc= 0.98775 val_loss= 0.23786 val_acc= 0.93568 time= 2.22899
Epoch: 0106 train_loss= 0.07287 train_acc= 0.98690 val_loss= 0.23610 val_acc= 0.93874 time= 2.22410
Epoch: 0107 train_loss= 0.06799 train_acc= 0.99013 val_loss= 0.23532 val_acc= 0.93721 time= 2.24001
Epoch: 0108 train_loss= 0.06688 train_acc= 0.98962 val_loss= 0.23456 val_acc= 0.93721 time= 2.22332
Epoch: 0109 train_loss= 0.06661 train_acc= 0.98945 val_loss= 0.23473 val_acc= 0.94181 time= 2.21596
Epoch: 0110 train_loss= 0.06414 train_acc= 0.98962 val_loss= 0.23571 val_acc= 0.94334 time= 2.21504
Epoch: 0111 train_loss= 0.06224 train_acc= 0.98996 val_loss= 0.23733 val_acc= 0.94334 time= 2.22199
Epoch: 0112 train_loss= 0.05986 train_acc= 0.99047 val_loss= 0.23917 val_acc= 0.93721 time= 2.22642
Early stopping...
Optimization Finished!
Test set results: cost= 0.25168 accuracy= 0.93964 time= 0.75634
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.8163    0.9195    0.8649        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.7333    1.0000    0.8462        11
           9     1.0000    0.5556    0.7143         9
          10     0.8571    0.6667    0.7500        36
          11     1.0000    0.9167    0.9565        12
          12     0.8582    1.0000    0.9237       121
          13     0.9375    0.7895    0.8571        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.5000    0.2500    0.3333         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.8667    0.7647    0.8125        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9641    0.9662       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7143    1.0000    0.8333        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8333    0.8025    0.8176        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9926    0.9867      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9091    0.8333    0.8696        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9396      2568
   macro avg     0.7675    0.6893    0.7078      2568
weighted avg     0.9372    0.9396    0.9350      2568

Macro average Test Precision, Recall and F1-Score...
(0.7674865838936172, 0.689278005063876, 0.7078059230950687, None)
Micro average Test Precision, Recall and F1-Score...
(0.9396417445482866, 0.9396417445482866, 0.9396417445482866, None)
embeddings:
8892 6532 2568
[[ 1.6505435e-02 -1.2677455e-01  7.3556826e-02 ... -5.5858310e-02
   1.5480884e+00 -1.9483529e-01]
 [ 1.5040174e-01  9.6435718e-02  4.6464182e-02 ...  1.8534863e-01
   8.1135589e-01 -1.8748477e-02]
 [ 6.0745710e-01  6.1219934e-02  2.5717866e-02 ... -4.3605585e-02
   2.5922245e-01  4.9568713e-03]
 ...
 [ 2.6848090e-01  1.0804695e-01  4.2762402e-02 ...  9.2198243e-03
   3.0214465e-01  9.6203789e-02]
 [ 2.6922554e-01  3.0807469e-02  1.4031979e-02 ...  1.3632567e-02
   2.4189548e-01  2.9046526e-02]
 [ 2.4419507e-01  1.8725251e-01  8.8705483e-04 ...  2.0332575e-01
   2.6767746e-01  2.0064759e-01]]

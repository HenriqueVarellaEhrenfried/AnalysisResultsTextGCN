(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95128 train_acc= 0.00953 val_loss= 3.91215 val_acc= 0.66616 time= 0.45813
Epoch: 0002 train_loss= 3.91404 train_acc= 0.63599 val_loss= 3.82826 val_acc= 0.64778 time= 0.17608
Epoch: 0003 train_loss= 3.83019 train_acc= 0.61592 val_loss= 3.69704 val_acc= 0.63400 time= 0.17060
Epoch: 0004 train_loss= 3.69324 train_acc= 0.61048 val_loss= 3.51799 val_acc= 0.62175 time= 0.18899
Epoch: 0005 train_loss= 3.51542 train_acc= 0.59806 val_loss= 3.29699 val_acc= 0.61103 time= 0.17701
Epoch: 0006 train_loss= 3.29970 train_acc= 0.60282 val_loss= 3.05096 val_acc= 0.60643 time= 0.16699
Epoch: 0007 train_loss= 3.05001 train_acc= 0.59772 val_loss= 2.80516 val_acc= 0.60031 time= 0.16800
Epoch: 0008 train_loss= 2.81688 train_acc= 0.58564 val_loss= 2.58713 val_acc= 0.59571 time= 0.17500
Epoch: 0009 train_loss= 2.58757 train_acc= 0.58632 val_loss= 2.42399 val_acc= 0.59418 time= 0.19300
Epoch: 0010 train_loss= 2.45816 train_acc= 0.54975 val_loss= 2.32565 val_acc= 0.61256 time= 0.16900
Epoch: 0011 train_loss= 2.29894 train_acc= 0.58871 val_loss= 2.27152 val_acc= 0.66616 time= 0.17500
Epoch: 0012 train_loss= 2.30553 train_acc= 0.61099 val_loss= 2.23368 val_acc= 0.48392 time= 0.16800
Epoch: 0013 train_loss= 2.28159 train_acc= 0.50417 val_loss= 2.19335 val_acc= 0.45636 time= 0.16797
Epoch: 0014 train_loss= 2.19175 train_acc= 0.44242 val_loss= 2.14071 val_acc= 0.45636 time= 0.19303
Epoch: 0015 train_loss= 2.17573 train_acc= 0.43664 val_loss= 2.07144 val_acc= 0.45636 time= 0.16706
Epoch: 0016 train_loss= 2.09884 train_acc= 0.43409 val_loss= 1.98872 val_acc= 0.45636 time= 0.16803
Epoch: 0017 train_loss= 2.02437 train_acc= 0.43749 val_loss= 1.90267 val_acc= 0.46708 time= 0.19003
Epoch: 0018 train_loss= 1.93311 train_acc= 0.46675 val_loss= 1.82342 val_acc= 0.50842 time= 0.17097
Epoch: 0019 train_loss= 1.87484 train_acc= 0.52866 val_loss= 1.75673 val_acc= 0.60490 time= 0.16803
Epoch: 0020 train_loss= 1.79007 train_acc= 0.56404 val_loss= 1.70106 val_acc= 0.65850 time= 0.17300
Epoch: 0021 train_loss= 1.73029 train_acc= 0.62766 val_loss= 1.65144 val_acc= 0.67228 time= 0.16507
Epoch: 0022 train_loss= 1.66890 train_acc= 0.63531 val_loss= 1.60350 val_acc= 0.67228 time= 0.18000
Epoch: 0023 train_loss= 1.66015 train_acc= 0.64110 val_loss= 1.55396 val_acc= 0.66922 time= 0.16800
Epoch: 0024 train_loss= 1.58123 train_acc= 0.64229 val_loss= 1.50457 val_acc= 0.66769 time= 0.16900
Epoch: 0025 train_loss= 1.56306 train_acc= 0.63752 val_loss= 1.45685 val_acc= 0.67075 time= 0.17311
Epoch: 0026 train_loss= 1.51351 train_acc= 0.64756 val_loss= 1.41256 val_acc= 0.67841 time= 0.19400
Epoch: 0027 train_loss= 1.47180 train_acc= 0.65402 val_loss= 1.37241 val_acc= 0.68606 time= 0.16902
Epoch: 0028 train_loss= 1.41708 train_acc= 0.65742 val_loss= 1.33632 val_acc= 0.68760 time= 0.17805
Epoch: 0029 train_loss= 1.36728 train_acc= 0.67103 val_loss= 1.30370 val_acc= 0.69372 time= 0.16756
Epoch: 0030 train_loss= 1.34748 train_acc= 0.66712 val_loss= 1.27362 val_acc= 0.70138 time= 0.16896
Epoch: 0031 train_loss= 1.32207 train_acc= 0.67460 val_loss= 1.24518 val_acc= 0.70597 time= 0.19200
Epoch: 0032 train_loss= 1.29117 train_acc= 0.69110 val_loss= 1.21803 val_acc= 0.72129 time= 0.16700
Epoch: 0033 train_loss= 1.26479 train_acc= 0.68940 val_loss= 1.19192 val_acc= 0.72588 time= 0.17200
Epoch: 0034 train_loss= 1.23065 train_acc= 0.71169 val_loss= 1.16661 val_acc= 0.73047 time= 0.19243
Epoch: 0035 train_loss= 1.19907 train_acc= 0.71492 val_loss= 1.14163 val_acc= 0.73507 time= 0.16900
Epoch: 0036 train_loss= 1.17079 train_acc= 0.72563 val_loss= 1.11715 val_acc= 0.73813 time= 0.16663
Epoch: 0037 train_loss= 1.13449 train_acc= 0.73414 val_loss= 1.09295 val_acc= 0.73660 time= 0.19201
Epoch: 0038 train_loss= 1.14047 train_acc= 0.73465 val_loss= 1.06971 val_acc= 0.73813 time= 0.16800
Epoch: 0039 train_loss= 1.10707 train_acc= 0.74162 val_loss= 1.04714 val_acc= 0.74579 time= 0.17099
Epoch: 0040 train_loss= 1.07822 train_acc= 0.74264 val_loss= 1.02547 val_acc= 0.75191 time= 0.16900
Epoch: 0041 train_loss= 1.06037 train_acc= 0.74741 val_loss= 1.00459 val_acc= 0.75651 time= 0.17100
Epoch: 0042 train_loss= 1.04523 train_acc= 0.74860 val_loss= 0.98413 val_acc= 0.76417 time= 0.16959
Epoch: 0043 train_loss= 1.03582 train_acc= 0.75659 val_loss= 0.96396 val_acc= 0.77029 time= 0.19401
Epoch: 0044 train_loss= 1.01034 train_acc= 0.76271 val_loss= 0.94395 val_acc= 0.77642 time= 0.16699
Epoch: 0045 train_loss= 0.98883 train_acc= 0.77496 val_loss= 0.92436 val_acc= 0.78254 time= 0.18342
Epoch: 0046 train_loss= 0.97288 train_acc= 0.77479 val_loss= 0.90545 val_acc= 0.78407 time= 0.16811
Epoch: 0047 train_loss= 0.96614 train_acc= 0.78075 val_loss= 0.88777 val_acc= 0.80704 time= 0.17013
Epoch: 0048 train_loss= 0.93594 train_acc= 0.78891 val_loss= 0.87134 val_acc= 0.81470 time= 0.16997
Epoch: 0049 train_loss= 0.90666 train_acc= 0.80065 val_loss= 0.85511 val_acc= 0.81623 time= 0.19000
Epoch: 0050 train_loss= 0.88374 train_acc= 0.80966 val_loss= 0.83809 val_acc= 0.82083 time= 0.17060
Epoch: 0051 train_loss= 0.87387 train_acc= 0.80728 val_loss= 0.82074 val_acc= 0.82236 time= 0.17039
Epoch: 0052 train_loss= 0.86787 train_acc= 0.80524 val_loss= 0.80279 val_acc= 0.82542 time= 0.16806
Epoch: 0053 train_loss= 0.85960 train_acc= 0.80541 val_loss= 0.78497 val_acc= 0.82848 time= 0.16704
Epoch: 0054 train_loss= 0.83796 train_acc= 0.81357 val_loss= 0.76730 val_acc= 0.83461 time= 0.19199
Epoch: 0055 train_loss= 0.81465 train_acc= 0.82259 val_loss= 0.75071 val_acc= 0.83767 time= 0.16601
Epoch: 0056 train_loss= 0.80169 train_acc= 0.81715 val_loss= 0.73483 val_acc= 0.84074 time= 0.16896
Epoch: 0057 train_loss= 0.78352 train_acc= 0.82361 val_loss= 0.71974 val_acc= 0.84074 time= 0.18640
Epoch: 0058 train_loss= 0.75524 train_acc= 0.82514 val_loss= 0.70495 val_acc= 0.84227 time= 0.17200
Epoch: 0059 train_loss= 0.74826 train_acc= 0.82565 val_loss= 0.69009 val_acc= 0.84839 time= 0.17000
Epoch: 0060 train_loss= 0.74710 train_acc= 0.82157 val_loss= 0.67528 val_acc= 0.85145 time= 0.19200
Epoch: 0061 train_loss= 0.73681 train_acc= 0.82735 val_loss= 0.66110 val_acc= 0.85299 time= 0.16600
Epoch: 0062 train_loss= 0.70641 train_acc= 0.83569 val_loss= 0.64798 val_acc= 0.85758 time= 0.18504
Epoch: 0063 train_loss= 0.69162 train_acc= 0.84096 val_loss= 0.63559 val_acc= 0.86217 time= 0.16716
Epoch: 0064 train_loss= 0.68897 train_acc= 0.83926 val_loss= 0.62403 val_acc= 0.86217 time= 0.16699
Epoch: 0065 train_loss= 0.66122 train_acc= 0.83739 val_loss= 0.61328 val_acc= 0.86677 time= 0.17329
Epoch: 0066 train_loss= 0.66235 train_acc= 0.84300 val_loss= 0.60251 val_acc= 0.86677 time= 0.19400
Epoch: 0067 train_loss= 0.65415 train_acc= 0.84776 val_loss= 0.59083 val_acc= 0.86524 time= 0.17100
Epoch: 0068 train_loss= 0.64243 train_acc= 0.84980 val_loss= 0.57758 val_acc= 0.86371 time= 0.18111
Epoch: 0069 train_loss= 0.62527 train_acc= 0.85031 val_loss= 0.56438 val_acc= 0.86677 time= 0.16800
Epoch: 0070 train_loss= 0.60891 train_acc= 0.85933 val_loss= 0.55189 val_acc= 0.87749 time= 0.16700
Epoch: 0071 train_loss= 0.60315 train_acc= 0.85525 val_loss= 0.54069 val_acc= 0.87749 time= 0.18499
Epoch: 0072 train_loss= 0.57646 train_acc= 0.86358 val_loss= 0.53039 val_acc= 0.88055 time= 0.16696
Epoch: 0073 train_loss= 0.57590 train_acc= 0.86035 val_loss= 0.52103 val_acc= 0.88055 time= 0.17100
Epoch: 0074 train_loss= 0.55310 train_acc= 0.86783 val_loss= 0.51249 val_acc= 0.88055 time= 0.17200
Epoch: 0075 train_loss= 0.54763 train_acc= 0.86852 val_loss= 0.50403 val_acc= 0.88668 time= 0.17000
Epoch: 0076 train_loss= 0.54749 train_acc= 0.85967 val_loss= 0.49596 val_acc= 0.88668 time= 0.16704
Epoch: 0077 train_loss= 0.53797 train_acc= 0.87107 val_loss= 0.48760 val_acc= 0.89127 time= 0.19100
Epoch: 0078 train_loss= 0.51942 train_acc= 0.87107 val_loss= 0.48038 val_acc= 0.89127 time= 0.16597
Epoch: 0079 train_loss= 0.53080 train_acc= 0.87362 val_loss= 0.47307 val_acc= 0.88821 time= 0.16705
Epoch: 0080 train_loss= 0.49444 train_acc= 0.88569 val_loss= 0.46508 val_acc= 0.88668 time= 0.18695
Epoch: 0081 train_loss= 0.51126 train_acc= 0.87039 val_loss= 0.45702 val_acc= 0.88668 time= 0.17000
Epoch: 0082 train_loss= 0.48087 train_acc= 0.88859 val_loss= 0.44831 val_acc= 0.88974 time= 0.17300
Epoch: 0083 train_loss= 0.47868 train_acc= 0.88229 val_loss= 0.44038 val_acc= 0.89433 time= 0.19576
Epoch: 0084 train_loss= 0.47446 train_acc= 0.88365 val_loss= 0.43272 val_acc= 0.89587 time= 0.17000
Epoch: 0085 train_loss= 0.46403 train_acc= 0.88978 val_loss= 0.42560 val_acc= 0.89587 time= 0.18200
Epoch: 0086 train_loss= 0.44764 train_acc= 0.89386 val_loss= 0.41835 val_acc= 0.89893 time= 0.16610
Epoch: 0087 train_loss= 0.45860 train_acc= 0.88331 val_loss= 0.41111 val_acc= 0.89893 time= 0.16796
Epoch: 0088 train_loss= 0.43540 train_acc= 0.88876 val_loss= 0.40423 val_acc= 0.89893 time= 0.17000
Epoch: 0089 train_loss= 0.43248 train_acc= 0.89675 val_loss= 0.39759 val_acc= 0.90046 time= 0.18900
Epoch: 0090 train_loss= 0.42462 train_acc= 0.88961 val_loss= 0.39151 val_acc= 0.90046 time= 0.17200
Epoch: 0091 train_loss= 0.43008 train_acc= 0.89726 val_loss= 0.38634 val_acc= 0.90352 time= 0.17200
Epoch: 0092 train_loss= 0.41923 train_acc= 0.90202 val_loss= 0.38162 val_acc= 0.90352 time= 0.16901
Epoch: 0093 train_loss= 0.41570 train_acc= 0.90083 val_loss= 0.37759 val_acc= 0.90658 time= 0.16800
Epoch: 0094 train_loss= 0.39756 train_acc= 0.90730 val_loss= 0.37456 val_acc= 0.90658 time= 0.17898
Epoch: 0095 train_loss= 0.39103 train_acc= 0.90151 val_loss= 0.37199 val_acc= 0.90658 time= 0.16500
Epoch: 0096 train_loss= 0.38841 train_acc= 0.90270 val_loss= 0.36980 val_acc= 0.90658 time= 0.16897
Epoch: 0097 train_loss= 0.39176 train_acc= 0.90849 val_loss= 0.36642 val_acc= 0.90352 time= 0.16903
Epoch: 0098 train_loss= 0.38651 train_acc= 0.90696 val_loss= 0.36135 val_acc= 0.90658 time= 0.16897
Epoch: 0099 train_loss= 0.41007 train_acc= 0.89777 val_loss= 0.35558 val_acc= 0.91118 time= 0.17100
Epoch: 0100 train_loss= 0.37406 train_acc= 0.90832 val_loss= 0.35039 val_acc= 0.91118 time= 0.19600
Epoch: 0101 train_loss= 0.35709 train_acc= 0.91410 val_loss= 0.34578 val_acc= 0.91118 time= 0.16703
Epoch: 0102 train_loss= 0.36254 train_acc= 0.91121 val_loss= 0.34134 val_acc= 0.91271 time= 0.16961
Epoch: 0103 train_loss= 0.36335 train_acc= 0.90611 val_loss= 0.33788 val_acc= 0.91424 time= 0.18504
Epoch: 0104 train_loss= 0.34436 train_acc= 0.92142 val_loss= 0.33458 val_acc= 0.91271 time= 0.16704
Epoch: 0105 train_loss= 0.33682 train_acc= 0.91155 val_loss= 0.33102 val_acc= 0.91271 time= 0.16899
Epoch: 0106 train_loss= 0.35679 train_acc= 0.91019 val_loss= 0.32801 val_acc= 0.91424 time= 0.19305
Epoch: 0107 train_loss= 0.32903 train_acc= 0.92108 val_loss= 0.32609 val_acc= 0.91271 time= 0.17100
Epoch: 0108 train_loss= 0.34184 train_acc= 0.91614 val_loss= 0.32472 val_acc= 0.91118 time= 0.18600
Epoch: 0109 train_loss= 0.32134 train_acc= 0.91750 val_loss= 0.32305 val_acc= 0.91271 time= 0.16800
Epoch: 0110 train_loss= 0.32151 train_acc= 0.92414 val_loss= 0.32040 val_acc= 0.91118 time= 0.17110
Epoch: 0111 train_loss= 0.30250 train_acc= 0.93043 val_loss= 0.31674 val_acc= 0.91271 time= 0.17300
Epoch: 0112 train_loss= 0.31620 train_acc= 0.92244 val_loss= 0.31148 val_acc= 0.91424 time= 0.18807
Epoch: 0113 train_loss= 0.32482 train_acc= 0.92090 val_loss= 0.30542 val_acc= 0.91577 time= 0.16696
Epoch: 0114 train_loss= 0.31358 train_acc= 0.91801 val_loss= 0.30022 val_acc= 0.91424 time= 0.17000
Epoch: 0115 train_loss= 0.30071 train_acc= 0.92380 val_loss= 0.29608 val_acc= 0.91577 time= 0.17100
Epoch: 0116 train_loss= 0.30417 train_acc= 0.91937 val_loss= 0.29331 val_acc= 0.91884 time= 0.17002
Epoch: 0117 train_loss= 0.29576 train_acc= 0.92312 val_loss= 0.29158 val_acc= 0.91730 time= 0.19300
Epoch: 0118 train_loss= 0.28447 train_acc= 0.92669 val_loss= 0.28970 val_acc= 0.91884 time= 0.16700
Epoch: 0119 train_loss= 0.28587 train_acc= 0.92771 val_loss= 0.28801 val_acc= 0.92037 time= 0.16800
Epoch: 0120 train_loss= 0.27815 train_acc= 0.93247 val_loss= 0.28752 val_acc= 0.92649 time= 0.18500
Epoch: 0121 train_loss= 0.28129 train_acc= 0.92924 val_loss= 0.28887 val_acc= 0.92343 time= 0.16605
Epoch: 0122 train_loss= 0.29514 train_acc= 0.92652 val_loss= 0.29158 val_acc= 0.92190 time= 0.16699
Epoch: 0123 train_loss= 0.27530 train_acc= 0.93060 val_loss= 0.29281 val_acc= 0.92037 time= 0.18496
Epoch: 0124 train_loss= 0.27712 train_acc= 0.92720 val_loss= 0.29214 val_acc= 0.91730 time= 0.17000
Early stopping...
Optimization Finished!
Test set results: cost= 0.34499 accuracy= 0.92095 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.7500    0.8571         8
           1     1.0000    0.3333    0.5000         6
           2     0.0000    0.0000    0.0000         1
           3     0.7447    0.9333    0.8284        75
           4     1.0000    1.0000    1.0000         9
           5     0.7788    0.9310    0.8482        87
           6     0.9200    0.9200    0.9200        25
           7     0.6471    0.8462    0.7333        13
           8     0.8750    0.6364    0.7368        11
           9     0.0000    0.0000    0.0000         9
          10     0.9167    0.6111    0.7333        36
          11     1.0000    0.9167    0.9565        12
          12     0.8207    0.9835    0.8947       121
          13     0.8667    0.6842    0.7647        19
          14     0.7059    0.8571    0.7742        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.8333    0.5000    0.6250        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.8636    0.9500    0.9048        20
          22     0.5000    0.8000    0.6154         5
          23     0.0000    0.0000    0.0000         1
          24     0.4615    0.7059    0.5581        17
          25     0.8571    0.8000    0.8276        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.3333    0.5000        12
          28     1.0000    0.7273    0.8421        11
          29     0.9643    0.9713    0.9678       696
          30     0.9167    1.0000    0.9565        22
          31     0.0000    0.0000    0.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.9028    0.8025    0.8497        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9755    0.9926    0.9840      1083
          40     1.0000    0.2000    0.3333         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.6154    0.6667    0.6400        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.7222    0.8667    0.7879        15
          48     0.8889    0.8889    0.8889         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9210      2568
   macro avg     0.6110    0.5211    0.5345      2568
weighted avg     0.9128    0.9210    0.9106      2568

Macro average Test Precision, Recall and F1-Score...
(0.6110182664803806, 0.5210611481918395, 0.5344675331491732, None)
Micro average Test Precision, Recall and F1-Score...
(0.9209501557632399, 0.9209501557632399, 0.92095015576324, None)
embeddings:
8892 6532 2568
[[-0.01741488  0.01905844  0.03803618 ...  0.06279172 -0.22762565
   0.00254925]
 [ 0.18357302  0.37938744 -0.0042257  ... -0.01388434 -0.07536858
   0.09854819]
 [-0.04216318  0.02794508 -0.00454052 ... -0.08151879 -0.01786074
   0.0657814 ]
 ...
 [ 0.03414537  0.11535239 -0.00099748 ...  0.03095358  0.03660239
   0.04608212]
 [ 0.0091496   0.04130262  0.03316063 ...  0.03407906  0.03017338
   0.10541649]
 [ 0.2352525   0.26963827  0.20560451 ...  0.2719491   0.20871064
   0.1272039 ]]

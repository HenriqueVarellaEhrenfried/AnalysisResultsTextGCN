(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95118 train_acc= 0.02824 val_loss= 3.90865 val_acc= 0.55896 time= 0.46195
Epoch: 0002 train_loss= 3.90694 train_acc= 0.56132 val_loss= 3.81512 val_acc= 0.49311 time= 0.17000
Epoch: 0003 train_loss= 3.82312 train_acc= 0.49974 val_loss= 3.67089 val_acc= 0.47933 time= 0.17000
Epoch: 0004 train_loss= 3.67627 train_acc= 0.46368 val_loss= 3.47599 val_acc= 0.46708 time= 0.19200
Epoch: 0005 train_loss= 3.46657 train_acc= 0.45263 val_loss= 3.24011 val_acc= 0.46248 time= 0.16604
Epoch: 0006 train_loss= 3.23628 train_acc= 0.45790 val_loss= 2.98460 val_acc= 0.46248 time= 0.16601
Epoch: 0007 train_loss= 3.01889 train_acc= 0.46590 val_loss= 2.73760 val_acc= 0.45942 time= 0.18695
Epoch: 0008 train_loss= 2.73188 train_acc= 0.46011 val_loss= 2.52628 val_acc= 0.45942 time= 0.16800
Epoch: 0009 train_loss= 2.51372 train_acc= 0.46488 val_loss= 2.37703 val_acc= 0.46095 time= 0.17100
Epoch: 0010 train_loss= 2.38967 train_acc= 0.48410 val_loss= 2.29404 val_acc= 0.46248 time= 0.18500
Epoch: 0011 train_loss= 2.33760 train_acc= 0.48716 val_loss= 2.25484 val_acc= 0.45942 time= 0.16948
Epoch: 0012 train_loss= 2.24075 train_acc= 0.45773 val_loss= 2.22548 val_acc= 0.45636 time= 0.18599
Epoch: 0013 train_loss= 2.27162 train_acc= 0.44684 val_loss= 2.18525 val_acc= 0.45636 time= 0.16696
Epoch: 0014 train_loss= 2.20388 train_acc= 0.45552 val_loss= 2.12551 val_acc= 0.45636 time= 0.16645
Epoch: 0015 train_loss= 2.17884 train_acc= 0.43800 val_loss= 2.04850 val_acc= 0.45636 time= 0.18604
Epoch: 0016 train_loss= 2.11897 train_acc= 0.43732 val_loss= 1.96347 val_acc= 0.46248 time= 0.16699
Epoch: 0017 train_loss= 1.99542 train_acc= 0.44787 val_loss= 1.88017 val_acc= 0.47626 time= 0.17096
Epoch: 0018 train_loss= 1.93873 train_acc= 0.48648 val_loss= 1.80762 val_acc= 0.50995 time= 0.18800
Epoch: 0019 train_loss= 1.84039 train_acc= 0.52611 val_loss= 1.74709 val_acc= 0.57734 time= 0.16900
Epoch: 0020 train_loss= 1.80887 train_acc= 0.53768 val_loss= 1.69477 val_acc= 0.63247 time= 0.16805
Epoch: 0021 train_loss= 1.70245 train_acc= 0.58207 val_loss= 1.64656 val_acc= 0.66003 time= 0.17001
Epoch: 0022 train_loss= 1.66889 train_acc= 0.60555 val_loss= 1.59905 val_acc= 0.67688 time= 0.18503
Epoch: 0023 train_loss= 1.62944 train_acc= 0.63327 val_loss= 1.55081 val_acc= 0.67841 time= 0.16708
Epoch: 0024 train_loss= 1.59994 train_acc= 0.63293 val_loss= 1.50250 val_acc= 0.67688 time= 0.18600
Epoch: 0025 train_loss= 1.53135 train_acc= 0.63922 val_loss= 1.45672 val_acc= 0.67841 time= 0.16996
Epoch: 0026 train_loss= 1.50846 train_acc= 0.64093 val_loss= 1.41415 val_acc= 0.68300 time= 0.17100
Epoch: 0027 train_loss= 1.44865 train_acc= 0.65011 val_loss= 1.37538 val_acc= 0.68913 time= 0.19700
Epoch: 0028 train_loss= 1.41641 train_acc= 0.65385 val_loss= 1.33996 val_acc= 0.69372 time= 0.16804
Epoch: 0029 train_loss= 1.40012 train_acc= 0.66661 val_loss= 1.30700 val_acc= 0.70597 time= 0.17099
Epoch: 0030 train_loss= 1.35049 train_acc= 0.67818 val_loss= 1.27610 val_acc= 0.70750 time= 0.16600
Epoch: 0031 train_loss= 1.32678 train_acc= 0.68447 val_loss= 1.24665 val_acc= 0.71210 time= 0.16700
Epoch: 0032 train_loss= 1.29850 train_acc= 0.69093 val_loss= 1.21835 val_acc= 0.71975 time= 0.17000
Epoch: 0033 train_loss= 1.26792 train_acc= 0.70505 val_loss= 1.19105 val_acc= 0.72435 time= 0.18797
Epoch: 0034 train_loss= 1.23325 train_acc= 0.72138 val_loss= 1.16440 val_acc= 0.73047 time= 0.17100
Epoch: 0035 train_loss= 1.20564 train_acc= 0.73125 val_loss= 1.13851 val_acc= 0.73966 time= 0.19100
Epoch: 0036 train_loss= 1.17424 train_acc= 0.72716 val_loss= 1.11336 val_acc= 0.74426 time= 0.16713
Epoch: 0037 train_loss= 1.15057 train_acc= 0.73754 val_loss= 1.08872 val_acc= 0.74885 time= 0.16800
Epoch: 0038 train_loss= 1.12370 train_acc= 0.74400 val_loss= 1.06511 val_acc= 0.75345 time= 0.17095
Epoch: 0039 train_loss= 1.10490 train_acc= 0.74060 val_loss= 1.04232 val_acc= 0.75957 time= 0.18805
Epoch: 0040 train_loss= 1.09349 train_acc= 0.74809 val_loss= 1.01992 val_acc= 0.76110 time= 0.16795
Epoch: 0041 train_loss= 1.06015 train_acc= 0.75302 val_loss= 0.99817 val_acc= 0.76417 time= 0.16999
Epoch: 0042 train_loss= 1.04117 train_acc= 0.76067 val_loss= 0.97696 val_acc= 0.78101 time= 0.17100
Epoch: 0043 train_loss= 1.03005 train_acc= 0.76186 val_loss= 0.95617 val_acc= 0.78560 time= 0.17100
Epoch: 0044 train_loss= 0.99290 train_acc= 0.77836 val_loss= 0.93538 val_acc= 0.79173 time= 0.17900
Epoch: 0045 train_loss= 0.97400 train_acc= 0.78330 val_loss= 0.91496 val_acc= 0.80092 time= 0.16700
Epoch: 0046 train_loss= 0.93957 train_acc= 0.79163 val_loss= 0.89512 val_acc= 0.81011 time= 0.16800
Epoch: 0047 train_loss= 0.93865 train_acc= 0.78993 val_loss= 0.87581 val_acc= 0.81470 time= 0.18804
Epoch: 0048 train_loss= 0.93184 train_acc= 0.79537 val_loss= 0.85706 val_acc= 0.82236 time= 0.16801
Epoch: 0049 train_loss= 0.90936 train_acc= 0.79316 val_loss= 0.83928 val_acc= 0.83155 time= 0.16895
Epoch: 0050 train_loss= 0.88586 train_acc= 0.80031 val_loss= 0.82234 val_acc= 0.82695 time= 0.19800
Epoch: 0051 train_loss= 0.86824 train_acc= 0.81306 val_loss= 0.80616 val_acc= 0.82848 time= 0.17012
Epoch: 0052 train_loss= 0.86437 train_acc= 0.81068 val_loss= 0.79003 val_acc= 0.83461 time= 0.18305
Epoch: 0053 train_loss= 0.83798 train_acc= 0.80898 val_loss= 0.77329 val_acc= 0.83767 time= 0.16600
Epoch: 0054 train_loss= 0.82824 train_acc= 0.80745 val_loss= 0.75660 val_acc= 0.83920 time= 0.16798
Epoch: 0055 train_loss= 0.83332 train_acc= 0.80762 val_loss= 0.74031 val_acc= 0.83920 time= 0.16900
Epoch: 0056 train_loss= 0.79537 train_acc= 0.81647 val_loss= 0.72412 val_acc= 0.84380 time= 0.18803
Epoch: 0057 train_loss= 0.76511 train_acc= 0.82293 val_loss= 0.70835 val_acc= 0.84380 time= 0.16756
Epoch: 0058 train_loss= 0.75651 train_acc= 0.82174 val_loss= 0.69267 val_acc= 0.84686 time= 0.19123
Epoch: 0059 train_loss= 0.75892 train_acc= 0.82412 val_loss= 0.67764 val_acc= 0.84992 time= 0.17000
Epoch: 0060 train_loss= 0.73124 train_acc= 0.83126 val_loss= 0.66317 val_acc= 0.85605 time= 0.16901
Epoch: 0061 train_loss= 0.72266 train_acc= 0.83126 val_loss= 0.64947 val_acc= 0.85911 time= 0.17300
Epoch: 0062 train_loss= 0.67980 train_acc= 0.83909 val_loss= 0.63636 val_acc= 0.85911 time= 0.18907
Epoch: 0063 train_loss= 0.69791 train_acc= 0.83160 val_loss= 0.62341 val_acc= 0.85911 time= 0.16800
Epoch: 0064 train_loss= 0.68350 train_acc= 0.83416 val_loss= 0.61165 val_acc= 0.86371 time= 0.18500
Epoch: 0065 train_loss= 0.67388 train_acc= 0.84164 val_loss= 0.60030 val_acc= 0.86677 time= 0.16800
Epoch: 0066 train_loss= 0.65453 train_acc= 0.84181 val_loss= 0.58937 val_acc= 0.86371 time= 0.17297
Epoch: 0067 train_loss= 0.63638 train_acc= 0.85082 val_loss= 0.57954 val_acc= 0.86677 time= 0.19700
Epoch: 0068 train_loss= 0.64349 train_acc= 0.84997 val_loss= 0.56984 val_acc= 0.86371 time= 0.17003
Epoch: 0069 train_loss= 0.61463 train_acc= 0.85593 val_loss= 0.55966 val_acc= 0.86371 time= 0.16800
Epoch: 0070 train_loss= 0.61347 train_acc= 0.85644 val_loss= 0.54910 val_acc= 0.86830 time= 0.16800
Epoch: 0071 train_loss= 0.60111 train_acc= 0.85576 val_loss= 0.53816 val_acc= 0.87136 time= 0.16701
Epoch: 0072 train_loss= 0.58611 train_acc= 0.85967 val_loss= 0.52733 val_acc= 0.87902 time= 0.16797
Epoch: 0073 train_loss= 0.58404 train_acc= 0.85933 val_loss= 0.51708 val_acc= 0.88208 time= 0.18803
Epoch: 0074 train_loss= 0.55758 train_acc= 0.86188 val_loss= 0.50767 val_acc= 0.88361 time= 0.16997
Epoch: 0075 train_loss= 0.55705 train_acc= 0.86205 val_loss= 0.49807 val_acc= 0.88668 time= 0.19000
Epoch: 0076 train_loss= 0.55017 train_acc= 0.86783 val_loss= 0.48889 val_acc= 0.88515 time= 0.16903
Epoch: 0077 train_loss= 0.52850 train_acc= 0.87090 val_loss= 0.48026 val_acc= 0.88668 time= 0.16700
Epoch: 0078 train_loss= 0.53997 train_acc= 0.87243 val_loss= 0.47244 val_acc= 0.88668 time= 0.17099
Epoch: 0079 train_loss= 0.50505 train_acc= 0.87940 val_loss= 0.46575 val_acc= 0.88821 time= 0.18900
Epoch: 0080 train_loss= 0.50331 train_acc= 0.87039 val_loss= 0.45889 val_acc= 0.88974 time= 0.16907
Epoch: 0081 train_loss= 0.51139 train_acc= 0.87107 val_loss= 0.45190 val_acc= 0.89127 time= 0.16812
Epoch: 0082 train_loss= 0.49230 train_acc= 0.88144 val_loss= 0.44465 val_acc= 0.89433 time= 0.16999
Epoch: 0083 train_loss= 0.49306 train_acc= 0.87974 val_loss= 0.43845 val_acc= 0.89893 time= 0.16955
Epoch: 0084 train_loss= 0.46576 train_acc= 0.88331 val_loss= 0.43170 val_acc= 0.89893 time= 0.17000
Epoch: 0085 train_loss= 0.46714 train_acc= 0.88655 val_loss= 0.42539 val_acc= 0.89433 time= 0.16700
Epoch: 0086 train_loss= 0.48280 train_acc= 0.88246 val_loss= 0.41964 val_acc= 0.89740 time= 0.16900
Epoch: 0087 train_loss= 0.45628 train_acc= 0.88842 val_loss= 0.41455 val_acc= 0.89587 time= 0.17800
Epoch: 0088 train_loss= 0.44624 train_acc= 0.88893 val_loss= 0.40896 val_acc= 0.89740 time= 0.16710
Epoch: 0089 train_loss= 0.42569 train_acc= 0.89539 val_loss= 0.40398 val_acc= 0.89893 time= 0.16795
Epoch: 0090 train_loss= 0.44255 train_acc= 0.89675 val_loss= 0.39940 val_acc= 0.89587 time= 0.19406
Epoch: 0091 train_loss= 0.42297 train_acc= 0.89743 val_loss= 0.39521 val_acc= 0.89587 time= 0.17100
Epoch: 0092 train_loss= 0.42933 train_acc= 0.89403 val_loss= 0.39031 val_acc= 0.89893 time= 0.17400
Epoch: 0093 train_loss= 0.40651 train_acc= 0.90032 val_loss= 0.38538 val_acc= 0.90046 time= 0.18024
Epoch: 0094 train_loss= 0.39283 train_acc= 0.90287 val_loss= 0.37947 val_acc= 0.89893 time= 0.16754
Epoch: 0095 train_loss= 0.39674 train_acc= 0.90543 val_loss= 0.37402 val_acc= 0.89740 time= 0.16801
Epoch: 0096 train_loss= 0.40562 train_acc= 0.90338 val_loss= 0.36937 val_acc= 0.90046 time= 0.19099
Epoch: 0097 train_loss= 0.40733 train_acc= 0.89896 val_loss= 0.36451 val_acc= 0.90199 time= 0.16699
Epoch: 0098 train_loss= 0.39378 train_acc= 0.90100 val_loss= 0.36061 val_acc= 0.90199 time= 0.18197
Epoch: 0099 train_loss= 0.37604 train_acc= 0.90849 val_loss= 0.35684 val_acc= 0.90199 time= 0.17100
Epoch: 0100 train_loss= 0.37656 train_acc= 0.90815 val_loss= 0.35357 val_acc= 0.90199 time= 0.17170
Epoch: 0101 train_loss= 0.37883 train_acc= 0.91257 val_loss= 0.35045 val_acc= 0.90199 time= 0.19304
Epoch: 0102 train_loss= 0.38305 train_acc= 0.90747 val_loss= 0.34656 val_acc= 0.90199 time= 0.16699
Epoch: 0103 train_loss= 0.35836 train_acc= 0.90798 val_loss= 0.34281 val_acc= 0.90199 time= 0.17101
Epoch: 0104 train_loss= 0.35482 train_acc= 0.91376 val_loss= 0.33941 val_acc= 0.90658 time= 0.16899
Epoch: 0105 train_loss= 0.35844 train_acc= 0.90832 val_loss= 0.33743 val_acc= 0.90965 time= 0.16701
Epoch: 0106 train_loss= 0.36115 train_acc= 0.91087 val_loss= 0.33502 val_acc= 0.90965 time= 0.16806
Epoch: 0107 train_loss= 0.34622 train_acc= 0.91903 val_loss= 0.33255 val_acc= 0.90965 time= 0.20003
Epoch: 0108 train_loss= 0.33188 train_acc= 0.91563 val_loss= 0.32801 val_acc= 0.91118 time= 0.16999
Epoch: 0109 train_loss= 0.33386 train_acc= 0.91461 val_loss= 0.32357 val_acc= 0.91118 time= 0.16700
Epoch: 0110 train_loss= 0.33134 train_acc= 0.91512 val_loss= 0.31975 val_acc= 0.90965 time= 0.18401
Epoch: 0111 train_loss= 0.34425 train_acc= 0.90917 val_loss= 0.31701 val_acc= 0.91271 time= 0.16799
Epoch: 0112 train_loss= 0.32041 train_acc= 0.91597 val_loss= 0.31531 val_acc= 0.91730 time= 0.16759
Epoch: 0113 train_loss= 0.32566 train_acc= 0.91529 val_loss= 0.31400 val_acc= 0.91577 time= 0.19200
Epoch: 0114 train_loss= 0.30990 train_acc= 0.92125 val_loss= 0.31204 val_acc= 0.91577 time= 0.16773
Epoch: 0115 train_loss= 0.30360 train_acc= 0.92193 val_loss= 0.30991 val_acc= 0.91271 time= 0.18497
Epoch: 0116 train_loss= 0.30487 train_acc= 0.92635 val_loss= 0.30898 val_acc= 0.91424 time= 0.17003
Epoch: 0117 train_loss= 0.28592 train_acc= 0.92584 val_loss= 0.30893 val_acc= 0.91424 time= 0.16800
Epoch: 0118 train_loss= 0.30648 train_acc= 0.92482 val_loss= 0.30902 val_acc= 0.91577 time= 0.17313
Epoch: 0119 train_loss= 0.30120 train_acc= 0.92873 val_loss= 0.30682 val_acc= 0.91577 time= 0.18900
Epoch: 0120 train_loss= 0.29417 train_acc= 0.92754 val_loss= 0.30439 val_acc= 0.91577 time= 0.16912
Epoch: 0121 train_loss= 0.30784 train_acc= 0.91529 val_loss= 0.30008 val_acc= 0.91577 time= 0.17898
Epoch: 0122 train_loss= 0.27417 train_acc= 0.93264 val_loss= 0.29631 val_acc= 0.91577 time= 0.16800
Epoch: 0123 train_loss= 0.27991 train_acc= 0.92907 val_loss= 0.29309 val_acc= 0.91730 time= 0.17101
Epoch: 0124 train_loss= 0.28254 train_acc= 0.92907 val_loss= 0.29045 val_acc= 0.91884 time= 0.17497
Epoch: 0125 train_loss= 0.28989 train_acc= 0.92805 val_loss= 0.28786 val_acc= 0.92037 time= 0.19403
Epoch: 0126 train_loss= 0.27359 train_acc= 0.93111 val_loss= 0.28527 val_acc= 0.92190 time= 0.16600
Epoch: 0127 train_loss= 0.26745 train_acc= 0.93230 val_loss= 0.28273 val_acc= 0.91884 time= 0.18300
Epoch: 0128 train_loss= 0.26837 train_acc= 0.93043 val_loss= 0.28025 val_acc= 0.91884 time= 0.16700
Epoch: 0129 train_loss= 0.25183 train_acc= 0.93655 val_loss= 0.27824 val_acc= 0.92037 time= 0.16800
Epoch: 0130 train_loss= 0.25285 train_acc= 0.94030 val_loss= 0.27687 val_acc= 0.91730 time= 0.19200
Epoch: 0131 train_loss= 0.27602 train_acc= 0.93111 val_loss= 0.27737 val_acc= 0.91577 time= 0.16900
Epoch: 0132 train_loss= 0.25382 train_acc= 0.93774 val_loss= 0.27884 val_acc= 0.92037 time= 0.17401
Epoch: 0133 train_loss= 0.25393 train_acc= 0.93587 val_loss= 0.28046 val_acc= 0.92037 time= 0.16799
Epoch: 0134 train_loss= 0.24908 train_acc= 0.93706 val_loss= 0.28330 val_acc= 0.92037 time= 0.16800
Early stopping...
Optimization Finished!
Test set results: cost= 0.32949 accuracy= 0.92212 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.6250    0.7692         8
           1     0.6667    0.3333    0.4444         6
           2     0.0000    0.0000    0.0000         1
           3     0.8161    0.9467    0.8765        75
           4     1.0000    1.0000    1.0000         9
           5     0.7477    0.9540    0.8384        87
           6     0.8846    0.9200    0.9020        25
           7     0.5909    1.0000    0.7429        13
           8     0.8000    0.3636    0.5000        11
           9     0.0000    0.0000    0.0000         9
          10     0.8788    0.8056    0.8406        36
          11     1.0000    0.9167    0.9565        12
          12     0.8731    0.9669    0.9176       121
          13     0.7778    0.7368    0.7568        19
          14     0.7419    0.8214    0.7797        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.8696    1.0000    0.9302        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.4615    0.7059    0.5581        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.4167    0.5882        12
          28     1.0000    0.6364    0.7778        11
          29     0.9697    0.9641    0.9669       696
          30     0.9167    1.0000    0.9565        22
          31     0.0000    0.0000    0.0000         3
          32     0.4348    1.0000    0.6061        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.9130    0.7778    0.8400        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.5000    0.6667         4
          38     0.0000    0.0000    0.0000         1
          39     0.9693    0.9926    0.9808      1083
          40     0.0000    0.0000    0.0000         5
          41     0.0000    0.0000    0.0000         2
          42     1.0000    0.8889    0.9412         9
          43     0.0000    0.0000    0.0000         3
          44     0.8182    0.7500    0.7826        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.6842    0.8667    0.7647        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.0000    0.0000    0.0000         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9221      2568
   macro avg     0.5928    0.5250    0.5348      2568
weighted avg     0.9133    0.9221    0.9124      2568

Macro average Test Precision, Recall and F1-Score...
(0.5928183611582021, 0.5249722648104196, 0.5347874008489761, None)
Micro average Test Precision, Recall and F1-Score...
(0.9221183800623053, 0.9221183800623053, 0.9221183800623053, None)
embeddings:
8892 6532 2568
[[ 0.1405964  -0.1921801  -0.20694286 ... -0.11322718 -0.05128555
  -0.01618004]
 [-0.07010064  0.00461924  0.29388165 ... -0.02784994 -0.0618967
   0.00121271]
 [ 0.19572802 -0.02353215  0.10368071 ...  0.03855824  0.12872551
   0.35991517]
 ...
 [ 0.2483997  -0.01136579  0.32925245 ...  0.00898893  0.14303955
   0.20460752]
 [ 0.10342038  0.02735822  0.0211703  ...  0.04864968  0.0897298
   0.41491666]
 [ 0.29927334  0.19906053  0.17591146 ...  0.18740137  0.2341478
   0.19504857]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.01310 val_loss= 3.93204 val_acc= 0.66922 time= 0.37053
Epoch: 0002 train_loss= 3.93262 train_acc= 0.65096 val_loss= 3.90049 val_acc= 0.67228 time= 0.12900
Epoch: 0003 train_loss= 3.90179 train_acc= 0.64960 val_loss= 3.85534 val_acc= 0.67534 time= 0.15508
Epoch: 0004 train_loss= 3.85792 train_acc= 0.65504 val_loss= 3.79503 val_acc= 0.67534 time= 0.13600
Epoch: 0005 train_loss= 3.80277 train_acc= 0.65453 val_loss= 3.71861 val_acc= 0.66922 time= 0.13101
Epoch: 0006 train_loss= 3.72550 train_acc= 0.62647 val_loss= 3.62572 val_acc= 0.66769 time= 0.13110
Epoch: 0007 train_loss= 3.63102 train_acc= 0.63922 val_loss= 3.51669 val_acc= 0.65544 time= 0.15497
Epoch: 0008 train_loss= 3.53186 train_acc= 0.63429 val_loss= 3.39261 val_acc= 0.64319 time= 0.13300
Epoch: 0009 train_loss= 3.39562 train_acc= 0.62613 val_loss= 3.25595 val_acc= 0.62787 time= 0.13200
Epoch: 0010 train_loss= 3.25821 train_acc= 0.59381 val_loss= 3.11050 val_acc= 0.61103 time= 0.15600
Epoch: 0011 train_loss= 3.12577 train_acc= 0.58871 val_loss= 2.96068 val_acc= 0.59265 time= 0.13403
Epoch: 0012 train_loss= 2.98233 train_acc= 0.57391 val_loss= 2.81183 val_acc= 0.56815 time= 0.13000
Epoch: 0013 train_loss= 2.84218 train_acc= 0.56540 val_loss= 2.66998 val_acc= 0.53905 time= 0.13000
Epoch: 0014 train_loss= 2.67972 train_acc= 0.54431 val_loss= 2.54126 val_acc= 0.51149 time= 0.15300
Epoch: 0015 train_loss= 2.56982 train_acc= 0.52373 val_loss= 2.43167 val_acc= 0.49464 time= 0.13114
Epoch: 0016 train_loss= 2.45407 train_acc= 0.50723 val_loss= 2.34455 val_acc= 0.48086 time= 0.13100
Epoch: 0017 train_loss= 2.34723 train_acc= 0.47083 val_loss= 2.27927 val_acc= 0.46708 time= 0.13300
Epoch: 0018 train_loss= 2.28621 train_acc= 0.47083 val_loss= 2.23133 val_acc= 0.46554 time= 0.16100
Epoch: 0019 train_loss= 2.25879 train_acc= 0.45688 val_loss= 2.19378 val_acc= 0.45636 time= 0.13300
Epoch: 0020 train_loss= 2.18344 train_acc= 0.44259 val_loss= 2.15981 val_acc= 0.45636 time= 0.13400
Epoch: 0021 train_loss= 2.16665 train_acc= 0.43494 val_loss= 2.12449 val_acc= 0.45636 time= 0.14600
Epoch: 0022 train_loss= 2.12953 train_acc= 0.43664 val_loss= 2.08556 val_acc= 0.45636 time= 0.13401
Epoch: 0023 train_loss= 2.11161 train_acc= 0.43358 val_loss= 2.04260 val_acc= 0.45636 time= 0.12899
Epoch: 0024 train_loss= 2.06785 train_acc= 0.43324 val_loss= 1.99637 val_acc= 0.45636 time= 0.13200
Epoch: 0025 train_loss= 2.01821 train_acc= 0.43511 val_loss= 1.94832 val_acc= 0.45636 time= 0.15600
Epoch: 0026 train_loss= 1.97608 train_acc= 0.43579 val_loss= 1.90038 val_acc= 0.45636 time= 0.13000
Epoch: 0027 train_loss= 1.91698 train_acc= 0.43749 val_loss= 1.85417 val_acc= 0.46401 time= 0.13006
Epoch: 0028 train_loss= 1.88120 train_acc= 0.44259 val_loss= 1.81081 val_acc= 0.47320 time= 0.13497
Epoch: 0029 train_loss= 1.84169 train_acc= 0.45637 val_loss= 1.77066 val_acc= 0.50077 time= 0.15600
Epoch: 0030 train_loss= 1.80618 train_acc= 0.49345 val_loss= 1.73337 val_acc= 0.52986 time= 0.13600
Epoch: 0031 train_loss= 1.75074 train_acc= 0.52543 val_loss= 1.69817 val_acc= 0.57427 time= 0.13103
Epoch: 0032 train_loss= 1.73229 train_acc= 0.56744 val_loss= 1.66426 val_acc= 0.61562 time= 0.15497
Epoch: 0033 train_loss= 1.69776 train_acc= 0.59687 val_loss= 1.63102 val_acc= 0.63859 time= 0.13003
Epoch: 0034 train_loss= 1.64288 train_acc= 0.61405 val_loss= 1.59847 val_acc= 0.65391 time= 0.12897
Epoch: 0035 train_loss= 1.63539 train_acc= 0.63293 val_loss= 1.56636 val_acc= 0.66309 time= 0.15500
Epoch: 0036 train_loss= 1.60631 train_acc= 0.63055 val_loss= 1.53500 val_acc= 0.66309 time= 0.13100
Epoch: 0037 train_loss= 1.55759 train_acc= 0.62970 val_loss= 1.50480 val_acc= 0.66462 time= 0.13103
Epoch: 0038 train_loss= 1.53710 train_acc= 0.63854 val_loss= 1.47567 val_acc= 0.66616 time= 0.13400
Epoch: 0039 train_loss= 1.49929 train_acc= 0.63582 val_loss= 1.44797 val_acc= 0.66769 time= 0.15704
Epoch: 0040 train_loss= 1.48717 train_acc= 0.63429 val_loss= 1.42148 val_acc= 0.67228 time= 0.14200
Epoch: 0041 train_loss= 1.45591 train_acc= 0.64093 val_loss= 1.39603 val_acc= 0.67381 time= 0.13200
Epoch: 0042 train_loss= 1.43332 train_acc= 0.64127 val_loss= 1.37146 val_acc= 0.67534 time= 0.13004
Epoch: 0043 train_loss= 1.41511 train_acc= 0.64382 val_loss= 1.34760 val_acc= 0.67841 time= 0.15496
Epoch: 0044 train_loss= 1.37805 train_acc= 0.64994 val_loss= 1.32438 val_acc= 0.68300 time= 0.13005
Epoch: 0045 train_loss= 1.36150 train_acc= 0.65504 val_loss= 1.30163 val_acc= 0.68760 time= 0.12999
Epoch: 0046 train_loss= 1.34964 train_acc= 0.65623 val_loss= 1.27931 val_acc= 0.68760 time= 0.13396
Epoch: 0047 train_loss= 1.30462 train_acc= 0.67222 val_loss= 1.25737 val_acc= 0.69372 time= 0.15185
Epoch: 0048 train_loss= 1.29047 train_acc= 0.67563 val_loss= 1.23584 val_acc= 0.70444 time= 0.13000
Epoch: 0049 train_loss= 1.28100 train_acc= 0.68855 val_loss= 1.21476 val_acc= 0.70750 time= 0.13496
Epoch: 0050 train_loss= 1.25132 train_acc= 0.70080 val_loss= 1.19417 val_acc= 0.71210 time= 0.15900
Epoch: 0051 train_loss= 1.24218 train_acc= 0.70335 val_loss= 1.17400 val_acc= 0.71822 time= 0.13329
Epoch: 0052 train_loss= 1.21386 train_acc= 0.72087 val_loss= 1.15435 val_acc= 0.72894 time= 0.13000
Epoch: 0053 train_loss= 1.19112 train_acc= 0.72189 val_loss= 1.13521 val_acc= 0.73813 time= 0.13196
Epoch: 0054 train_loss= 1.16976 train_acc= 0.72989 val_loss= 1.11653 val_acc= 0.74426 time= 0.15704
Epoch: 0055 train_loss= 1.15221 train_acc= 0.74043 val_loss= 1.09825 val_acc= 0.75345 time= 0.13000
Epoch: 0056 train_loss= 1.12459 train_acc= 0.75149 val_loss= 1.08040 val_acc= 0.75804 time= 0.12900
Epoch: 0057 train_loss= 1.11101 train_acc= 0.75030 val_loss= 1.06288 val_acc= 0.76417 time= 0.13397
Epoch: 0058 train_loss= 1.10075 train_acc= 0.76425 val_loss= 1.04580 val_acc= 0.77029 time= 0.15104
Epoch: 0059 train_loss= 1.07754 train_acc= 0.75914 val_loss= 1.02898 val_acc= 0.77335 time= 0.13340
Epoch: 0060 train_loss= 1.05803 train_acc= 0.76084 val_loss= 1.01255 val_acc= 0.77335 time= 0.13300
Epoch: 0061 train_loss= 1.04715 train_acc= 0.76799 val_loss= 0.99644 val_acc= 0.77642 time= 0.16000
Epoch: 0062 train_loss= 1.02536 train_acc= 0.77717 val_loss= 0.98066 val_acc= 0.78101 time= 0.13200
Epoch: 0063 train_loss= 1.00332 train_acc= 0.77836 val_loss= 0.96514 val_acc= 0.78714 time= 0.12901
Epoch: 0064 train_loss= 0.98760 train_acc= 0.78721 val_loss= 0.94995 val_acc= 0.79173 time= 0.13302
Epoch: 0065 train_loss= 0.97196 train_acc= 0.78925 val_loss= 0.93488 val_acc= 0.79479 time= 0.15297
Epoch: 0066 train_loss= 0.95457 train_acc= 0.79469 val_loss= 0.92004 val_acc= 0.80245 time= 0.13200
Epoch: 0067 train_loss= 0.93681 train_acc= 0.79452 val_loss= 0.90531 val_acc= 0.80551 time= 0.13100
Epoch: 0068 train_loss= 0.92095 train_acc= 0.80592 val_loss= 0.89055 val_acc= 0.80858 time= 0.14500
Epoch: 0069 train_loss= 0.91871 train_acc= 0.79384 val_loss= 0.87598 val_acc= 0.80551 time= 0.13796
Epoch: 0070 train_loss= 0.89444 train_acc= 0.80218 val_loss= 0.86160 val_acc= 0.80704 time= 0.13384
Epoch: 0071 train_loss= 0.88275 train_acc= 0.80558 val_loss= 0.84759 val_acc= 0.80704 time= 0.13300
Epoch: 0072 train_loss= 0.86748 train_acc= 0.80728 val_loss= 0.83359 val_acc= 0.80858 time= 0.16000
Epoch: 0073 train_loss= 0.86281 train_acc= 0.80269 val_loss= 0.81988 val_acc= 0.81930 time= 0.13220
Epoch: 0074 train_loss= 0.83501 train_acc= 0.81664 val_loss= 0.80645 val_acc= 0.82083 time= 0.13106
Epoch: 0075 train_loss= 0.82436 train_acc= 0.81596 val_loss= 0.79332 val_acc= 0.82695 time= 0.13300
Epoch: 0076 train_loss= 0.80805 train_acc= 0.81459 val_loss= 0.78048 val_acc= 0.82695 time= 0.15300
Epoch: 0077 train_loss= 0.80771 train_acc= 0.82429 val_loss= 0.76782 val_acc= 0.82695 time= 0.13105
Epoch: 0078 train_loss= 0.77822 train_acc= 0.83313 val_loss= 0.75515 val_acc= 0.82848 time= 0.13095
Epoch: 0079 train_loss= 0.78618 train_acc= 0.82259 val_loss= 0.74275 val_acc= 0.83002 time= 0.15000
Epoch: 0080 train_loss= 0.76853 train_acc= 0.82820 val_loss= 0.73068 val_acc= 0.83308 time= 0.13300
Epoch: 0081 train_loss= 0.75143 train_acc= 0.84113 val_loss= 0.71905 val_acc= 0.83614 time= 0.13300
Epoch: 0082 train_loss= 0.72779 train_acc= 0.84232 val_loss= 0.70751 val_acc= 0.83767 time= 0.16100
Epoch: 0083 train_loss= 0.72071 train_acc= 0.83552 val_loss= 0.69616 val_acc= 0.83920 time= 0.13700
Epoch: 0084 train_loss= 0.70319 train_acc= 0.84810 val_loss= 0.68507 val_acc= 0.84074 time= 0.13004
Epoch: 0085 train_loss= 0.69204 train_acc= 0.84640 val_loss= 0.67398 val_acc= 0.84074 time= 0.13101
Epoch: 0086 train_loss= 0.70008 train_acc= 0.84810 val_loss= 0.66312 val_acc= 0.84686 time= 0.15600
Epoch: 0087 train_loss= 0.67572 train_acc= 0.85134 val_loss= 0.65244 val_acc= 0.84839 time= 0.13095
Epoch: 0088 train_loss= 0.67075 train_acc= 0.85031 val_loss= 0.64195 val_acc= 0.85145 time= 0.12900
Epoch: 0089 train_loss= 0.66259 train_acc= 0.85542 val_loss= 0.63130 val_acc= 0.85299 time= 0.13400
Epoch: 0090 train_loss= 0.64588 train_acc= 0.85610 val_loss= 0.62087 val_acc= 0.85452 time= 0.15900
Epoch: 0091 train_loss= 0.62058 train_acc= 0.86647 val_loss= 0.61083 val_acc= 0.85452 time= 0.13400
Epoch: 0092 train_loss= 0.63248 train_acc= 0.85712 val_loss= 0.60119 val_acc= 0.85911 time= 0.13300
Epoch: 0093 train_loss= 0.60929 train_acc= 0.86545 val_loss= 0.59178 val_acc= 0.85911 time= 0.13700
Epoch: 0094 train_loss= 0.62070 train_acc= 0.86324 val_loss= 0.58266 val_acc= 0.86217 time= 0.15100
Epoch: 0095 train_loss= 0.59280 train_acc= 0.87073 val_loss= 0.57396 val_acc= 0.86217 time= 0.13046
Epoch: 0096 train_loss= 0.58069 train_acc= 0.86988 val_loss= 0.56538 val_acc= 0.86217 time= 0.13100
Epoch: 0097 train_loss= 0.57854 train_acc= 0.87753 val_loss= 0.55708 val_acc= 0.86217 time= 0.15400
Epoch: 0098 train_loss= 0.56327 train_acc= 0.87311 val_loss= 0.54879 val_acc= 0.86677 time= 0.13201
Epoch: 0099 train_loss= 0.54979 train_acc= 0.87872 val_loss= 0.54096 val_acc= 0.86830 time= 0.13100
Epoch: 0100 train_loss= 0.54520 train_acc= 0.88178 val_loss= 0.53348 val_acc= 0.86830 time= 0.13400
Epoch: 0101 train_loss= 0.53167 train_acc= 0.87974 val_loss= 0.52606 val_acc= 0.87136 time= 0.14997
Epoch: 0102 train_loss= 0.54470 train_acc= 0.87583 val_loss= 0.51895 val_acc= 0.87596 time= 0.13504
Epoch: 0103 train_loss= 0.53302 train_acc= 0.87600 val_loss= 0.51184 val_acc= 0.87749 time= 0.13299
Epoch: 0104 train_loss= 0.52840 train_acc= 0.88348 val_loss= 0.50476 val_acc= 0.87902 time= 0.15500
Epoch: 0105 train_loss= 0.51041 train_acc= 0.88042 val_loss= 0.49751 val_acc= 0.87902 time= 0.13407
Epoch: 0106 train_loss= 0.50184 train_acc= 0.89046 val_loss= 0.49037 val_acc= 0.87902 time= 0.12999
Epoch: 0107 train_loss= 0.50101 train_acc= 0.88501 val_loss= 0.48352 val_acc= 0.87902 time= 0.13100
Epoch: 0108 train_loss= 0.48125 train_acc= 0.89573 val_loss= 0.47665 val_acc= 0.87902 time= 0.14200
Epoch: 0109 train_loss= 0.48064 train_acc= 0.89165 val_loss= 0.46974 val_acc= 0.88055 time= 0.13100
Epoch: 0110 train_loss= 0.47476 train_acc= 0.89114 val_loss= 0.46316 val_acc= 0.88361 time= 0.13100
Epoch: 0111 train_loss= 0.45679 train_acc= 0.89964 val_loss= 0.45691 val_acc= 0.88821 time= 0.13697
Epoch: 0112 train_loss= 0.46493 train_acc= 0.89216 val_loss= 0.45082 val_acc= 0.88821 time= 0.16100
Epoch: 0113 train_loss= 0.46099 train_acc= 0.89743 val_loss= 0.44512 val_acc= 0.88974 time= 0.13300
Epoch: 0114 train_loss= 0.45170 train_acc= 0.89879 val_loss= 0.44007 val_acc= 0.89280 time= 0.13200
Epoch: 0115 train_loss= 0.43932 train_acc= 0.90390 val_loss= 0.43495 val_acc= 0.89433 time= 0.14200
Epoch: 0116 train_loss= 0.43965 train_acc= 0.90424 val_loss= 0.43054 val_acc= 0.89280 time= 0.13000
Epoch: 0117 train_loss= 0.43510 train_acc= 0.90134 val_loss= 0.42628 val_acc= 0.89127 time= 0.13100
Epoch: 0118 train_loss= 0.42581 train_acc= 0.90611 val_loss= 0.42163 val_acc= 0.89433 time= 0.14300
Epoch: 0119 train_loss= 0.42360 train_acc= 0.90713 val_loss= 0.41673 val_acc= 0.89587 time= 0.13901
Epoch: 0120 train_loss= 0.41167 train_acc= 0.90934 val_loss= 0.41209 val_acc= 0.89587 time= 0.12906
Epoch: 0121 train_loss= 0.41828 train_acc= 0.90832 val_loss= 0.40723 val_acc= 0.89740 time= 0.13000
Epoch: 0122 train_loss= 0.39346 train_acc= 0.91138 val_loss= 0.40282 val_acc= 0.89740 time= 0.13760
Epoch: 0123 train_loss= 0.40616 train_acc= 0.90492 val_loss= 0.39874 val_acc= 0.89740 time= 0.15700
Epoch: 0124 train_loss= 0.40275 train_acc= 0.90611 val_loss= 0.39484 val_acc= 0.89587 time= 0.13400
Epoch: 0125 train_loss= 0.39527 train_acc= 0.91189 val_loss= 0.39167 val_acc= 0.89433 time= 0.13103
Epoch: 0126 train_loss= 0.37809 train_acc= 0.91036 val_loss= 0.38848 val_acc= 0.89587 time= 0.15697
Epoch: 0127 train_loss= 0.37713 train_acc= 0.91342 val_loss= 0.38520 val_acc= 0.89587 time= 0.13110
Epoch: 0128 train_loss= 0.36513 train_acc= 0.91495 val_loss= 0.38164 val_acc= 0.89893 time= 0.12900
Epoch: 0129 train_loss= 0.36835 train_acc= 0.91580 val_loss= 0.37787 val_acc= 0.90046 time= 0.13100
Epoch: 0130 train_loss= 0.37233 train_acc= 0.91019 val_loss= 0.37404 val_acc= 0.90046 time= 0.15507
Epoch: 0131 train_loss= 0.35902 train_acc= 0.91869 val_loss= 0.37048 val_acc= 0.90199 time= 0.13000
Epoch: 0132 train_loss= 0.35408 train_acc= 0.91869 val_loss= 0.36744 val_acc= 0.90199 time= 0.13164
Epoch: 0133 train_loss= 0.35528 train_acc= 0.91427 val_loss= 0.36405 val_acc= 0.90505 time= 0.16103
Epoch: 0134 train_loss= 0.35457 train_acc= 0.92312 val_loss= 0.36022 val_acc= 0.90505 time= 0.13597
Epoch: 0135 train_loss= 0.34304 train_acc= 0.91954 val_loss= 0.35641 val_acc= 0.90505 time= 0.13203
Epoch: 0136 train_loss= 0.34448 train_acc= 0.91971 val_loss= 0.35303 val_acc= 0.90505 time= 0.13100
Epoch: 0137 train_loss= 0.34027 train_acc= 0.91937 val_loss= 0.34968 val_acc= 0.90505 time= 0.15500
Epoch: 0138 train_loss= 0.34339 train_acc= 0.91835 val_loss= 0.34678 val_acc= 0.90505 time= 0.12997
Epoch: 0139 train_loss= 0.32912 train_acc= 0.92805 val_loss= 0.34413 val_acc= 0.90505 time= 0.13103
Epoch: 0140 train_loss= 0.31865 train_acc= 0.92720 val_loss= 0.34191 val_acc= 0.90658 time= 0.13200
Epoch: 0141 train_loss= 0.31068 train_acc= 0.92788 val_loss= 0.33975 val_acc= 0.90658 time= 0.15101
Epoch: 0142 train_loss= 0.31577 train_acc= 0.92635 val_loss= 0.33775 val_acc= 0.90812 time= 0.13205
Epoch: 0143 train_loss= 0.31036 train_acc= 0.92601 val_loss= 0.33589 val_acc= 0.90812 time= 0.13348
Epoch: 0144 train_loss= 0.29802 train_acc= 0.92907 val_loss= 0.33393 val_acc= 0.91118 time= 0.16036
Epoch: 0145 train_loss= 0.30160 train_acc= 0.93094 val_loss= 0.33222 val_acc= 0.91118 time= 0.13297
Epoch: 0146 train_loss= 0.30151 train_acc= 0.93502 val_loss= 0.33017 val_acc= 0.91118 time= 0.13003
Epoch: 0147 train_loss= 0.30568 train_acc= 0.92958 val_loss= 0.32803 val_acc= 0.91118 time= 0.13305
Epoch: 0148 train_loss= 0.29053 train_acc= 0.93536 val_loss= 0.32600 val_acc= 0.91118 time= 0.15700
Epoch: 0149 train_loss= 0.28517 train_acc= 0.93366 val_loss= 0.32464 val_acc= 0.90965 time= 0.13000
Epoch: 0150 train_loss= 0.28740 train_acc= 0.93298 val_loss= 0.32376 val_acc= 0.90812 time= 0.13000
Epoch: 0151 train_loss= 0.28837 train_acc= 0.93247 val_loss= 0.32218 val_acc= 0.90812 time= 0.13300
Epoch: 0152 train_loss= 0.27982 train_acc= 0.93315 val_loss= 0.32051 val_acc= 0.91271 time= 0.15100
Epoch: 0153 train_loss= 0.27480 train_acc= 0.93740 val_loss= 0.31819 val_acc= 0.91424 time= 0.13500
Epoch: 0154 train_loss= 0.27997 train_acc= 0.93570 val_loss= 0.31596 val_acc= 0.91271 time= 0.13400
Epoch: 0155 train_loss= 0.28379 train_acc= 0.93536 val_loss= 0.31374 val_acc= 0.91271 time= 0.16000
Epoch: 0156 train_loss= 0.27867 train_acc= 0.93349 val_loss= 0.31157 val_acc= 0.91118 time= 0.13104
Epoch: 0157 train_loss= 0.26635 train_acc= 0.93638 val_loss= 0.30905 val_acc= 0.91424 time= 0.13031
Epoch: 0158 train_loss= 0.27211 train_acc= 0.93723 val_loss= 0.30624 val_acc= 0.91577 time= 0.13313
Epoch: 0159 train_loss= 0.27301 train_acc= 0.93723 val_loss= 0.30384 val_acc= 0.92037 time= 0.15300
Epoch: 0160 train_loss= 0.26861 train_acc= 0.93774 val_loss= 0.30260 val_acc= 0.92190 time= 0.13200
Epoch: 0161 train_loss= 0.26211 train_acc= 0.94149 val_loss= 0.30130 val_acc= 0.92190 time= 0.13100
Epoch: 0162 train_loss= 0.25255 train_acc= 0.93928 val_loss= 0.29980 val_acc= 0.92037 time= 0.15511
Epoch: 0163 train_loss= 0.25682 train_acc= 0.93723 val_loss= 0.29813 val_acc= 0.91884 time= 0.14100
Epoch: 0164 train_loss= 0.25316 train_acc= 0.93979 val_loss= 0.29733 val_acc= 0.91271 time= 0.13500
Epoch: 0165 train_loss= 0.25789 train_acc= 0.94115 val_loss= 0.29818 val_acc= 0.90965 time= 0.13400
Epoch: 0166 train_loss= 0.24739 train_acc= 0.94336 val_loss= 0.29862 val_acc= 0.90965 time= 0.14900
Epoch: 0167 train_loss= 0.24999 train_acc= 0.94234 val_loss= 0.29749 val_acc= 0.91424 time= 0.12896
Epoch: 0168 train_loss= 0.24641 train_acc= 0.94387 val_loss= 0.29595 val_acc= 0.91424 time= 0.12900
Epoch: 0169 train_loss= 0.24644 train_acc= 0.94285 val_loss= 0.29294 val_acc= 0.91730 time= 0.13800
Epoch: 0170 train_loss= 0.23584 train_acc= 0.94268 val_loss= 0.29026 val_acc= 0.91730 time= 0.14205
Epoch: 0171 train_loss= 0.24322 train_acc= 0.94217 val_loss= 0.28863 val_acc= 0.91730 time= 0.13100
Epoch: 0172 train_loss= 0.25078 train_acc= 0.93945 val_loss= 0.28781 val_acc= 0.91884 time= 0.12900
Epoch: 0173 train_loss= 0.24032 train_acc= 0.94081 val_loss= 0.28812 val_acc= 0.91730 time= 0.15800
Epoch: 0174 train_loss= 0.23764 train_acc= 0.94642 val_loss= 0.28787 val_acc= 0.91884 time= 0.13400
Epoch: 0175 train_loss= 0.22196 train_acc= 0.94778 val_loss= 0.28764 val_acc= 0.91730 time= 0.13300
Epoch: 0176 train_loss= 0.23379 train_acc= 0.94710 val_loss= 0.28692 val_acc= 0.91884 time= 0.13500
Epoch: 0177 train_loss= 0.22134 train_acc= 0.95016 val_loss= 0.28542 val_acc= 0.91730 time= 0.15000
Epoch: 0178 train_loss= 0.22657 train_acc= 0.94557 val_loss= 0.28313 val_acc= 0.92343 time= 0.13100
Epoch: 0179 train_loss= 0.22344 train_acc= 0.94523 val_loss= 0.28126 val_acc= 0.92343 time= 0.12900
Epoch: 0180 train_loss= 0.21720 train_acc= 0.94982 val_loss= 0.27897 val_acc= 0.91884 time= 0.15500
Epoch: 0181 train_loss= 0.21342 train_acc= 0.94897 val_loss= 0.27695 val_acc= 0.91424 time= 0.12906
Epoch: 0182 train_loss= 0.21231 train_acc= 0.94693 val_loss= 0.27498 val_acc= 0.91424 time= 0.13000
Epoch: 0183 train_loss= 0.21420 train_acc= 0.94778 val_loss= 0.27306 val_acc= 0.91577 time= 0.15100
Epoch: 0184 train_loss= 0.21861 train_acc= 0.94965 val_loss= 0.27199 val_acc= 0.91424 time= 0.13900
Epoch: 0185 train_loss= 0.21705 train_acc= 0.94693 val_loss= 0.27090 val_acc= 0.91577 time= 0.13300
Epoch: 0186 train_loss= 0.20782 train_acc= 0.95237 val_loss= 0.26997 val_acc= 0.91884 time= 0.13400
Epoch: 0187 train_loss= 0.20701 train_acc= 0.95441 val_loss= 0.27003 val_acc= 0.91730 time= 0.13300
Epoch: 0188 train_loss= 0.20722 train_acc= 0.95322 val_loss= 0.27037 val_acc= 0.91884 time= 0.15311
Epoch: 0189 train_loss= 0.20811 train_acc= 0.95118 val_loss= 0.27118 val_acc= 0.91884 time= 0.13100
Epoch: 0190 train_loss= 0.19582 train_acc= 0.95509 val_loss= 0.27195 val_acc= 0.92037 time= 0.13099
Epoch: 0191 train_loss= 0.20461 train_acc= 0.95356 val_loss= 0.27269 val_acc= 0.92037 time= 0.15497
Early stopping...
Optimization Finished!
Test set results: cost= 0.32180 accuracy= 0.91511 time= 0.06303
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.3750    0.5455         8
           1     0.2000    0.1667    0.1818         6
           2     0.0000    0.0000    0.0000         1
           3     0.7527    0.9333    0.8333        75
           4     0.9000    1.0000    0.9474         9
           5     0.8163    0.9195    0.8649        87
           6     0.9231    0.9600    0.9412        25
           7     0.7500    0.9231    0.8276        13
           8     0.8000    0.3636    0.5000        11
           9     0.0000    0.0000    0.0000         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    0.9167    0.9565        12
          12     0.8252    0.9752    0.8939       121
          13     0.7222    0.6842    0.7027        19
          14     0.7353    0.8929    0.8065        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     0.5000    0.1111    0.1818         9
          21     0.7917    0.9500    0.8636        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.4138    0.7059    0.5217        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.2500    0.4000        12
          28     0.7778    0.6364    0.7000        11
          29     0.9682    0.9626    0.9654       696
          30     0.9565    1.0000    0.9778        22
          31     0.0000    0.0000    0.0000         3
          32     0.5000    0.7000    0.5833        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8462    0.8148    0.8302        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.2500    0.4000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9693    0.9917    0.9804      1083
          40     0.6667    0.4000    0.5000         5
          41     0.0000    0.0000    0.0000         2
          42     0.6154    0.8889    0.7273         9
          43     0.0000    0.0000    0.0000         3
          44     0.7778    0.5833    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    0.6667    0.8000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     0.4286    0.7500    0.5455         4

    accuracy                         0.9151      2568
   macro avg     0.5917    0.5000    0.5124      2568
weighted avg     0.9081    0.9151    0.9052      2568

Macro average Test Precision, Recall and F1-Score...
(0.5917412914555823, 0.5000336043680768, 0.5123709241164774, None)
Micro average Test Precision, Recall and F1-Score...
(0.9151090342679128, 0.9151090342679128, 0.9151090342679129, None)
embeddings:
8892 6532 2568
[[-0.01240635 -0.29066938  1.8238484  ...  2.6057894  -0.14640917
   1.6073681 ]
 [-0.03337678  0.6981895   1.1894183  ...  0.63258183  0.55472004
   0.9838727 ]
 [ 0.27218354  0.6088442   0.489829   ...  0.5369718   0.1087732
   0.30545264]
 ...
 [ 0.2568569   0.3693578   0.5996411  ...  0.49805474  0.12070956
   0.35195556]
 [ 0.14311832  0.20755577  0.24604855 ...  0.58126223  0.07272219
   0.26177225]
 [ 0.3487918   0.39419162  0.46582338 ...  0.44050768  0.5152009
   0.42086804]]

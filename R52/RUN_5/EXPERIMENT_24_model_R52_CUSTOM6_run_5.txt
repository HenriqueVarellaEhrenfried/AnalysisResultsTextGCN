(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.01140 val_loss= 3.89809 val_acc= 0.57427 time= 0.44563
Epoch: 0002 train_loss= 3.89832 train_acc= 0.57765 val_loss= 3.80014 val_acc= 0.55896 time= 0.19700
Epoch: 0003 train_loss= 3.80111 train_acc= 0.55656 val_loss= 3.64916 val_acc= 0.54364 time= 0.17403
Epoch: 0004 train_loss= 3.65390 train_acc= 0.51897 val_loss= 3.44405 val_acc= 0.49923 time= 0.16700
Epoch: 0005 train_loss= 3.44736 train_acc= 0.48614 val_loss= 3.19347 val_acc= 0.48545 time= 0.16801
Epoch: 0006 train_loss= 3.20162 train_acc= 0.45705 val_loss= 2.91919 val_acc= 0.46708 time= 0.18999
Epoch: 0007 train_loss= 2.92342 train_acc= 0.44378 val_loss= 2.65210 val_acc= 0.46248 time= 0.16600
Epoch: 0008 train_loss= 2.65421 train_acc= 0.43800 val_loss= 2.42800 val_acc= 0.45942 time= 0.16900
Epoch: 0009 train_loss= 2.43582 train_acc= 0.43528 val_loss= 2.27780 val_acc= 0.45942 time= 0.18897
Epoch: 0010 train_loss= 2.28089 train_acc= 0.43443 val_loss= 2.19926 val_acc= 0.45942 time= 0.17100
Epoch: 0011 train_loss= 2.20178 train_acc= 0.43528 val_loss= 2.15840 val_acc= 0.46248 time= 0.16903
Epoch: 0012 train_loss= 2.17052 train_acc= 0.43545 val_loss= 2.11862 val_acc= 0.46401 time= 0.18500
Epoch: 0013 train_loss= 2.13001 train_acc= 0.43766 val_loss= 2.06011 val_acc= 0.47320 time= 0.16701
Epoch: 0014 train_loss= 2.07407 train_acc= 0.44480 val_loss= 1.98097 val_acc= 0.49158 time= 0.18500
Epoch: 0015 train_loss= 1.99791 train_acc= 0.46777 val_loss= 1.88981 val_acc= 0.51914 time= 0.16699
Epoch: 0016 train_loss= 1.90940 train_acc= 0.51199 val_loss= 1.80000 val_acc= 0.57580 time= 0.16900
Epoch: 0017 train_loss= 1.82301 train_acc= 0.56030 val_loss= 1.72307 val_acc= 0.62021 time= 0.17139
Epoch: 0018 train_loss= 1.74969 train_acc= 0.61439 val_loss= 1.66169 val_acc= 0.64625 time= 0.19303
Epoch: 0019 train_loss= 1.69078 train_acc= 0.63514 val_loss= 1.60857 val_acc= 0.67075 time= 0.17100
Epoch: 0020 train_loss= 1.63676 train_acc= 0.65334 val_loss= 1.55499 val_acc= 0.68913 time= 0.17204
Epoch: 0021 train_loss= 1.58042 train_acc= 0.67375 val_loss= 1.49864 val_acc= 0.70444 time= 0.16819
Epoch: 0022 train_loss= 1.52613 train_acc= 0.68974 val_loss= 1.44201 val_acc= 0.70904 time= 0.16700
Epoch: 0023 train_loss= 1.47152 train_acc= 0.69212 val_loss= 1.38856 val_acc= 0.71057 time= 0.16975
Epoch: 0024 train_loss= 1.41383 train_acc= 0.69280 val_loss= 1.34059 val_acc= 0.71057 time= 0.18947
Epoch: 0025 train_loss= 1.36566 train_acc= 0.69383 val_loss= 1.29809 val_acc= 0.71210 time= 0.16900
Epoch: 0026 train_loss= 1.32592 train_acc= 0.69570 val_loss= 1.25994 val_acc= 0.71669 time= 0.19100
Epoch: 0027 train_loss= 1.28771 train_acc= 0.70318 val_loss= 1.22469 val_acc= 0.72129 time= 0.17100
Epoch: 0028 train_loss= 1.25001 train_acc= 0.71407 val_loss= 1.19117 val_acc= 0.72588 time= 0.16904
Epoch: 0029 train_loss= 1.21480 train_acc= 0.72546 val_loss= 1.15865 val_acc= 0.73507 time= 0.18344
Epoch: 0030 train_loss= 1.18175 train_acc= 0.73720 val_loss= 1.12683 val_acc= 0.74732 time= 0.16700
Epoch: 0031 train_loss= 1.14718 train_acc= 0.75030 val_loss= 1.09555 val_acc= 0.75498 time= 0.16997
Epoch: 0032 train_loss= 1.11625 train_acc= 0.76033 val_loss= 1.06488 val_acc= 0.76417 time= 0.18003
Epoch: 0033 train_loss= 1.08156 train_acc= 0.77037 val_loss= 1.03509 val_acc= 0.77029 time= 0.16797
Epoch: 0034 train_loss= 1.05107 train_acc= 0.77785 val_loss= 1.00632 val_acc= 0.77795 time= 0.17200
Epoch: 0035 train_loss= 1.01774 train_acc= 0.78687 val_loss= 0.97849 val_acc= 0.78560 time= 0.19703
Epoch: 0036 train_loss= 0.98735 train_acc= 0.79333 val_loss= 0.95153 val_acc= 0.79173 time= 0.16800
Epoch: 0037 train_loss= 0.96220 train_acc= 0.80031 val_loss= 0.92525 val_acc= 0.80858 time= 0.18400
Epoch: 0038 train_loss= 0.92990 train_acc= 0.80847 val_loss= 0.89945 val_acc= 0.81776 time= 0.16600
Epoch: 0039 train_loss= 0.90229 train_acc= 0.81613 val_loss= 0.87403 val_acc= 0.81776 time= 0.16700
Epoch: 0040 train_loss= 0.87744 train_acc= 0.82259 val_loss= 0.84885 val_acc= 0.81930 time= 0.17100
Epoch: 0041 train_loss= 0.85071 train_acc= 0.82684 val_loss= 0.82388 val_acc= 0.82695 time= 0.18997
Epoch: 0042 train_loss= 0.82780 train_acc= 0.83058 val_loss= 0.79926 val_acc= 0.83308 time= 0.17200
Epoch: 0043 train_loss= 0.79732 train_acc= 0.83569 val_loss= 0.77509 val_acc= 0.83767 time= 0.16900
Epoch: 0044 train_loss= 0.77454 train_acc= 0.84079 val_loss= 0.75148 val_acc= 0.84074 time= 0.16803
Epoch: 0045 train_loss= 0.74618 train_acc= 0.84487 val_loss= 0.72847 val_acc= 0.84074 time= 0.16597
Epoch: 0046 train_loss= 0.72242 train_acc= 0.84691 val_loss= 0.70596 val_acc= 0.84992 time= 0.19200
Epoch: 0047 train_loss= 0.69773 train_acc= 0.85253 val_loss= 0.68391 val_acc= 0.85758 time= 0.16700
Epoch: 0048 train_loss= 0.67704 train_acc= 0.85814 val_loss= 0.66254 val_acc= 0.86371 time= 0.16703
Epoch: 0049 train_loss= 0.64916 train_acc= 0.86494 val_loss= 0.64191 val_acc= 0.86983 time= 0.18497
Epoch: 0050 train_loss= 0.63055 train_acc= 0.86766 val_loss= 0.62217 val_acc= 0.87136 time= 0.17049
Epoch: 0051 train_loss= 0.60976 train_acc= 0.87039 val_loss= 0.60321 val_acc= 0.87136 time= 0.17000
Epoch: 0052 train_loss= 0.58804 train_acc= 0.87345 val_loss= 0.58504 val_acc= 0.86830 time= 0.17103
Epoch: 0053 train_loss= 0.56681 train_acc= 0.87668 val_loss= 0.56766 val_acc= 0.87136 time= 0.16600
Epoch: 0054 train_loss= 0.54784 train_acc= 0.87821 val_loss= 0.55123 val_acc= 0.87136 time= 0.17300
Epoch: 0055 train_loss= 0.53038 train_acc= 0.88263 val_loss= 0.53563 val_acc= 0.87443 time= 0.17501
Epoch: 0056 train_loss= 0.51093 train_acc= 0.88621 val_loss= 0.52092 val_acc= 0.87443 time= 0.16707
Epoch: 0057 train_loss= 0.49383 train_acc= 0.88910 val_loss= 0.50685 val_acc= 0.87902 time= 0.16721
Epoch: 0058 train_loss= 0.47841 train_acc= 0.89199 val_loss= 0.49310 val_acc= 0.88208 time= 0.19497
Epoch: 0059 train_loss= 0.46126 train_acc= 0.89760 val_loss= 0.47959 val_acc= 0.88668 time= 0.17103
Epoch: 0060 train_loss= 0.44764 train_acc= 0.89981 val_loss= 0.46624 val_acc= 0.88821 time= 0.18300
Epoch: 0061 train_loss= 0.43024 train_acc= 0.90219 val_loss= 0.45332 val_acc= 0.89127 time= 0.16800
Epoch: 0062 train_loss= 0.41733 train_acc= 0.90628 val_loss= 0.44083 val_acc= 0.89280 time= 0.16700
Epoch: 0063 train_loss= 0.40320 train_acc= 0.91019 val_loss= 0.42910 val_acc= 0.89433 time= 0.17108
Epoch: 0064 train_loss= 0.38935 train_acc= 0.91631 val_loss= 0.41818 val_acc= 0.89740 time= 0.18900
Epoch: 0065 train_loss= 0.37506 train_acc= 0.91852 val_loss= 0.40811 val_acc= 0.89893 time= 0.16700
Epoch: 0066 train_loss= 0.36335 train_acc= 0.92244 val_loss= 0.39887 val_acc= 0.89893 time= 0.16900
Epoch: 0067 train_loss= 0.35229 train_acc= 0.92448 val_loss= 0.39018 val_acc= 0.89740 time= 0.16997
Epoch: 0068 train_loss= 0.33972 train_acc= 0.92720 val_loss= 0.38185 val_acc= 0.90199 time= 0.17103
Epoch: 0069 train_loss= 0.32839 train_acc= 0.93128 val_loss= 0.37372 val_acc= 0.90352 time= 0.19100
Epoch: 0070 train_loss= 0.31735 train_acc= 0.93298 val_loss= 0.36588 val_acc= 0.90352 time= 0.16604
Epoch: 0071 train_loss= 0.30615 train_acc= 0.93706 val_loss= 0.35811 val_acc= 0.90505 time= 0.16700
Epoch: 0072 train_loss= 0.29634 train_acc= 0.93825 val_loss= 0.35047 val_acc= 0.90965 time= 0.18705
Epoch: 0073 train_loss= 0.28566 train_acc= 0.94132 val_loss= 0.34285 val_acc= 0.91271 time= 0.16603
Epoch: 0074 train_loss= 0.27704 train_acc= 0.94251 val_loss= 0.33542 val_acc= 0.91424 time= 0.16897
Epoch: 0075 train_loss= 0.26799 train_acc= 0.94472 val_loss= 0.32867 val_acc= 0.91730 time= 0.19677
Epoch: 0076 train_loss= 0.25780 train_acc= 0.94863 val_loss= 0.32240 val_acc= 0.91730 time= 0.17000
Epoch: 0077 train_loss= 0.24908 train_acc= 0.95152 val_loss= 0.31690 val_acc= 0.91884 time= 0.17204
Epoch: 0078 train_loss= 0.24106 train_acc= 0.95271 val_loss= 0.31206 val_acc= 0.91577 time= 0.16636
Epoch: 0079 train_loss= 0.23390 train_acc= 0.95271 val_loss= 0.30775 val_acc= 0.91730 time= 0.16729
Epoch: 0080 train_loss= 0.22507 train_acc= 0.95492 val_loss= 0.30378 val_acc= 0.91884 time= 0.17000
Epoch: 0081 train_loss= 0.21678 train_acc= 0.95799 val_loss= 0.29937 val_acc= 0.92343 time= 0.18801
Epoch: 0082 train_loss= 0.20942 train_acc= 0.95850 val_loss= 0.29426 val_acc= 0.92496 time= 0.16899
Epoch: 0083 train_loss= 0.20287 train_acc= 0.96139 val_loss= 0.28917 val_acc= 0.92496 time= 0.19100
Epoch: 0084 train_loss= 0.19523 train_acc= 0.96326 val_loss= 0.28465 val_acc= 0.92802 time= 0.17100
Epoch: 0085 train_loss= 0.18785 train_acc= 0.96377 val_loss= 0.28035 val_acc= 0.92802 time= 0.16800
Epoch: 0086 train_loss= 0.18186 train_acc= 0.96462 val_loss= 0.27628 val_acc= 0.92802 time= 0.17104
Epoch: 0087 train_loss= 0.17574 train_acc= 0.96394 val_loss= 0.27270 val_acc= 0.92956 time= 0.18801
Epoch: 0088 train_loss= 0.17091 train_acc= 0.96547 val_loss= 0.26983 val_acc= 0.92956 time= 0.16700
Epoch: 0089 train_loss= 0.16348 train_acc= 0.96836 val_loss= 0.26731 val_acc= 0.93109 time= 0.16600
Epoch: 0090 train_loss= 0.15709 train_acc= 0.96989 val_loss= 0.26545 val_acc= 0.93262 time= 0.16696
Epoch: 0091 train_loss= 0.15327 train_acc= 0.97108 val_loss= 0.26373 val_acc= 0.93262 time= 0.17061
Epoch: 0092 train_loss= 0.14694 train_acc= 0.97227 val_loss= 0.26146 val_acc= 0.93415 time= 0.19700
Epoch: 0093 train_loss= 0.14281 train_acc= 0.97312 val_loss= 0.25849 val_acc= 0.93262 time= 0.16801
Epoch: 0094 train_loss= 0.13871 train_acc= 0.97415 val_loss= 0.25515 val_acc= 0.93415 time= 0.16609
Epoch: 0095 train_loss= 0.13261 train_acc= 0.97670 val_loss= 0.25228 val_acc= 0.93415 time= 0.18499
Epoch: 0096 train_loss= 0.12845 train_acc= 0.97806 val_loss= 0.24981 val_acc= 0.93262 time= 0.16602
Epoch: 0097 train_loss= 0.12410 train_acc= 0.97891 val_loss= 0.24750 val_acc= 0.93262 time= 0.16594
Epoch: 0098 train_loss= 0.12067 train_acc= 0.97772 val_loss= 0.24553 val_acc= 0.93568 time= 0.19078
Epoch: 0099 train_loss= 0.11544 train_acc= 0.98027 val_loss= 0.24479 val_acc= 0.93262 time= 0.17196
Epoch: 0100 train_loss= 0.11288 train_acc= 0.98044 val_loss= 0.24423 val_acc= 0.93415 time= 0.18900
Epoch: 0101 train_loss= 0.10883 train_acc= 0.98180 val_loss= 0.24317 val_acc= 0.93568 time= 0.16900
Epoch: 0102 train_loss= 0.10466 train_acc= 0.98384 val_loss= 0.24212 val_acc= 0.93415 time= 0.17000
Epoch: 0103 train_loss= 0.10194 train_acc= 0.98452 val_loss= 0.24114 val_acc= 0.93415 time= 0.18500
Epoch: 0104 train_loss= 0.09955 train_acc= 0.98435 val_loss= 0.24015 val_acc= 0.93415 time= 0.16804
Epoch: 0105 train_loss= 0.09436 train_acc= 0.98588 val_loss= 0.23912 val_acc= 0.93721 time= 0.16899
Epoch: 0106 train_loss= 0.09179 train_acc= 0.98605 val_loss= 0.23806 val_acc= 0.93874 time= 0.18300
Epoch: 0107 train_loss= 0.08979 train_acc= 0.98605 val_loss= 0.23684 val_acc= 0.93568 time= 0.16897
Epoch: 0108 train_loss= 0.08624 train_acc= 0.98673 val_loss= 0.23555 val_acc= 0.93721 time= 0.17200
Epoch: 0109 train_loss= 0.08352 train_acc= 0.98690 val_loss= 0.23417 val_acc= 0.93568 time= 0.17200
Epoch: 0110 train_loss= 0.07960 train_acc= 0.98877 val_loss= 0.23374 val_acc= 0.93721 time= 0.19000
Epoch: 0111 train_loss= 0.07813 train_acc= 0.98826 val_loss= 0.23314 val_acc= 0.93721 time= 0.16700
Epoch: 0112 train_loss= 0.07595 train_acc= 0.98996 val_loss= 0.23297 val_acc= 0.93874 time= 0.18200
Epoch: 0113 train_loss= 0.07332 train_acc= 0.99064 val_loss= 0.23283 val_acc= 0.93874 time= 0.16700
Epoch: 0114 train_loss= 0.07179 train_acc= 0.99047 val_loss= 0.23264 val_acc= 0.93874 time= 0.16805
Epoch: 0115 train_loss= 0.06820 train_acc= 0.99098 val_loss= 0.23260 val_acc= 0.93568 time= 0.19595
Epoch: 0116 train_loss= 0.06616 train_acc= 0.99115 val_loss= 0.23261 val_acc= 0.93415 time= 0.17103
Epoch: 0117 train_loss= 0.06408 train_acc= 0.99047 val_loss= 0.23234 val_acc= 0.93568 time= 0.16901
Epoch: 0118 train_loss= 0.06231 train_acc= 0.99150 val_loss= 0.23098 val_acc= 0.93721 time= 0.16695
Epoch: 0119 train_loss= 0.06145 train_acc= 0.99116 val_loss= 0.23001 val_acc= 0.93721 time= 0.16803
Epoch: 0120 train_loss= 0.05952 train_acc= 0.99133 val_loss= 0.22877 val_acc= 0.93721 time= 0.17200
Epoch: 0121 train_loss= 0.05708 train_acc= 0.99269 val_loss= 0.22829 val_acc= 0.93568 time= 0.18800
Epoch: 0122 train_loss= 0.05558 train_acc= 0.99252 val_loss= 0.22794 val_acc= 0.93721 time= 0.16699
Epoch: 0123 train_loss= 0.05393 train_acc= 0.99269 val_loss= 0.22822 val_acc= 0.93721 time= 0.19183
Epoch: 0124 train_loss= 0.05240 train_acc= 0.99269 val_loss= 0.22853 val_acc= 0.93721 time= 0.17100
Epoch: 0125 train_loss= 0.05206 train_acc= 0.99337 val_loss= 0.22915 val_acc= 0.93874 time= 0.16900
Epoch: 0126 train_loss= 0.04982 train_acc= 0.99388 val_loss= 0.22922 val_acc= 0.93874 time= 0.17004
Epoch: 0127 train_loss= 0.04781 train_acc= 0.99422 val_loss= 0.22909 val_acc= 0.93874 time= 0.18901
Epoch: 0128 train_loss= 0.04688 train_acc= 0.99456 val_loss= 0.22853 val_acc= 0.93568 time= 0.16836
Epoch: 0129 train_loss= 0.04580 train_acc= 0.99439 val_loss= 0.22877 val_acc= 0.93568 time= 0.16500
Epoch: 0130 train_loss= 0.04432 train_acc= 0.99456 val_loss= 0.22837 val_acc= 0.93568 time= 0.16796
Epoch: 0131 train_loss= 0.04275 train_acc= 0.99456 val_loss= 0.22697 val_acc= 0.93568 time= 0.17000
Epoch: 0132 train_loss= 0.04209 train_acc= 0.99456 val_loss= 0.22540 val_acc= 0.94028 time= 0.19700
Epoch: 0133 train_loss= 0.04142 train_acc= 0.99524 val_loss= 0.22451 val_acc= 0.94028 time= 0.16900
Epoch: 0134 train_loss= 0.03996 train_acc= 0.99507 val_loss= 0.22477 val_acc= 0.94181 time= 0.16906
Epoch: 0135 train_loss= 0.03917 train_acc= 0.99490 val_loss= 0.22539 val_acc= 0.94487 time= 0.18498
Epoch: 0136 train_loss= 0.03746 train_acc= 0.99592 val_loss= 0.22615 val_acc= 0.94487 time= 0.16796
Epoch: 0137 train_loss= 0.03729 train_acc= 0.99507 val_loss= 0.22827 val_acc= 0.93874 time= 0.16805
Early stopping...
Optimization Finished!
Test set results: cost= 0.25196 accuracy= 0.93692 time= 0.07795
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7553    0.9467    0.8402        75
           4     1.0000    1.0000    1.0000         9
           5     0.8061    0.9080    0.8541        87
           6     0.9200    0.9200    0.9200        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.6667    0.8000         9
          10     0.8800    0.6111    0.7213        36
          11     1.0000    0.9167    0.9565        12
          12     0.8440    0.9835    0.9084       121
          13     0.9333    0.7368    0.8235        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    1.0000    1.0000         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.7273    0.8421        11
          29     0.9655    0.9641    0.9648       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8421    0.7901    0.8153        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    1.0000    1.0000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9000    0.7500    0.8182        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7879    0.6880    0.7120      2568
weighted avg     0.9379    0.9369    0.9331      2568

Macro average Test Precision, Recall and F1-Score...
(0.7879099678043681, 0.687984174289752, 0.7120395100984502, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[ 0.59814316  1.1812085   0.29629853 ... -0.0775364   0.0329866
   0.19877231]
 [ 0.33912036  0.37447405  0.2742476  ...  0.11042992  0.29914245
   0.07714394]
 [ 0.12000428  0.47762963  0.4414637  ... -0.00721585  0.20499174
   0.54081637]
 ...
 [ 0.13988711  0.194869    0.17344029 ...  0.07664732  0.09835876
   0.13109839]
 [ 0.0867983   0.34103972  0.391537   ...  0.03033569  0.08906509
   0.32344767]
 [ 0.27276754  0.5452355   0.36760214 ...  0.25945875  0.22430134
   0.55106884]]

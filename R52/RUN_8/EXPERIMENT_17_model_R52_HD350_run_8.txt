(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95141 train_acc= 0.00442 val_loss= 3.88753 val_acc= 0.67381 time= 0.54303
Epoch: 0002 train_loss= 3.88745 train_acc= 0.65674 val_loss= 3.74864 val_acc= 0.67381 time= 0.20897
Epoch: 0003 train_loss= 3.75183 train_acc= 0.65215 val_loss= 3.52425 val_acc= 0.67228 time= 0.21000
Epoch: 0004 train_loss= 3.52712 train_acc= 0.65130 val_loss= 3.22205 val_acc= 0.67228 time= 0.21103
Epoch: 0005 train_loss= 3.23036 train_acc= 0.65317 val_loss= 2.87986 val_acc= 0.66769 time= 0.23397
Epoch: 0006 train_loss= 2.88669 train_acc= 0.64756 val_loss= 2.55869 val_acc= 0.66462 time= 0.21400
Epoch: 0007 train_loss= 2.55849 train_acc= 0.64467 val_loss= 2.32827 val_acc= 0.65544 time= 0.22300
Epoch: 0008 train_loss= 2.33234 train_acc= 0.63956 val_loss= 2.21601 val_acc= 0.60949 time= 0.21100
Epoch: 0009 train_loss= 2.22152 train_acc= 0.59670 val_loss= 2.16742 val_acc= 0.51149 time= 0.20903
Epoch: 0010 train_loss= 2.17012 train_acc= 0.49736 val_loss= 2.11914 val_acc= 0.47779 time= 0.23397
Epoch: 0011 train_loss= 2.13041 train_acc= 0.45008 val_loss= 2.04178 val_acc= 0.47320 time= 0.21500
Epoch: 0012 train_loss= 2.06019 train_acc= 0.44548 val_loss= 1.93603 val_acc= 0.48698 time= 0.21003
Epoch: 0013 train_loss= 1.95168 train_acc= 0.46998 val_loss= 1.82096 val_acc= 0.53139 time= 0.21197
Epoch: 0014 train_loss= 1.84935 train_acc= 0.51863 val_loss= 1.72037 val_acc= 0.60949 time= 0.21500
Epoch: 0015 train_loss= 1.74834 train_acc= 0.60895 val_loss= 1.64560 val_acc= 0.65850 time= 0.22300
Epoch: 0016 train_loss= 1.67754 train_acc= 0.64161 val_loss= 1.58526 val_acc= 0.67841 time= 0.22104
Epoch: 0017 train_loss= 1.61933 train_acc= 0.65640 val_loss= 1.52441 val_acc= 0.68913 time= 0.20900
Epoch: 0018 train_loss= 1.54962 train_acc= 0.66984 val_loss= 1.45982 val_acc= 0.69066 time= 0.20896
Epoch: 0019 train_loss= 1.48926 train_acc= 0.67699 val_loss= 1.39527 val_acc= 0.69525 time= 0.23403
Epoch: 0020 train_loss= 1.41842 train_acc= 0.68719 val_loss= 1.33558 val_acc= 0.70138 time= 0.21397
Epoch: 0021 train_loss= 1.36429 train_acc= 0.68974 val_loss= 1.28281 val_acc= 0.71210 time= 0.21265
Epoch: 0022 train_loss= 1.31054 train_acc= 0.69774 val_loss= 1.23662 val_acc= 0.72129 time= 0.21048
Epoch: 0023 train_loss= 1.26187 train_acc= 0.70607 val_loss= 1.19559 val_acc= 0.72282 time= 0.21200
Epoch: 0024 train_loss= 1.22379 train_acc= 0.71900 val_loss= 1.15809 val_acc= 0.73354 time= 0.20900
Epoch: 0025 train_loss= 1.17852 train_acc= 0.73244 val_loss= 1.12268 val_acc= 0.73507 time= 0.21800
Epoch: 0026 train_loss= 1.14181 train_acc= 0.74826 val_loss= 1.08824 val_acc= 0.74273 time= 0.20903
Epoch: 0027 train_loss= 1.10488 train_acc= 0.75421 val_loss= 1.05417 val_acc= 0.75498 time= 0.21097
Epoch: 0028 train_loss= 1.06540 train_acc= 0.76357 val_loss= 1.02021 val_acc= 0.76876 time= 0.21600
Epoch: 0029 train_loss= 1.03481 train_acc= 0.76969 val_loss= 0.98654 val_acc= 0.77642 time= 0.23283
Epoch: 0030 train_loss= 0.99896 train_acc= 0.77921 val_loss= 0.95347 val_acc= 0.78407 time= 0.21600
Epoch: 0031 train_loss= 0.96419 train_acc= 0.78330 val_loss= 0.92141 val_acc= 0.79173 time= 0.21100
Epoch: 0032 train_loss= 0.93343 train_acc= 0.79622 val_loss= 0.89058 val_acc= 0.80092 time= 0.21000
Epoch: 0033 train_loss= 0.90132 train_acc= 0.80592 val_loss= 0.86124 val_acc= 0.81011 time= 0.21301
Epoch: 0034 train_loss= 0.87044 train_acc= 0.81953 val_loss= 0.83344 val_acc= 0.82542 time= 0.21409
Epoch: 0035 train_loss= 0.83974 train_acc= 0.82463 val_loss= 0.80676 val_acc= 0.83155 time= 0.21373
Epoch: 0036 train_loss= 0.81247 train_acc= 0.83450 val_loss= 0.78099 val_acc= 0.83767 time= 0.21200
Epoch: 0037 train_loss= 0.78571 train_acc= 0.83858 val_loss= 0.75594 val_acc= 0.84074 time= 0.21303
Epoch: 0038 train_loss= 0.75797 train_acc= 0.84419 val_loss= 0.73147 val_acc= 0.84533 time= 0.20901
Epoch: 0039 train_loss= 0.73210 train_acc= 0.84657 val_loss= 0.70748 val_acc= 0.84686 time= 0.22096
Epoch: 0040 train_loss= 0.70549 train_acc= 0.85236 val_loss= 0.68413 val_acc= 0.84839 time= 0.20903
Epoch: 0041 train_loss= 0.68126 train_acc= 0.85661 val_loss= 0.66150 val_acc= 0.85605 time= 0.21397
Epoch: 0042 train_loss= 0.65452 train_acc= 0.86052 val_loss= 0.63922 val_acc= 0.85911 time= 0.21610
Epoch: 0043 train_loss= 0.63133 train_acc= 0.86528 val_loss= 0.61744 val_acc= 0.86064 time= 0.23497
Epoch: 0044 train_loss= 0.61462 train_acc= 0.86817 val_loss= 0.59600 val_acc= 0.86524 time= 0.21697
Epoch: 0045 train_loss= 0.58528 train_acc= 0.87158 val_loss= 0.57551 val_acc= 0.87136 time= 0.20804
Epoch: 0046 train_loss= 0.56631 train_acc= 0.87515 val_loss= 0.55600 val_acc= 0.86983 time= 0.21238
Epoch: 0047 train_loss= 0.54262 train_acc= 0.87957 val_loss= 0.53761 val_acc= 0.87289 time= 0.20900
Epoch: 0048 train_loss= 0.52289 train_acc= 0.88450 val_loss= 0.52074 val_acc= 0.87443 time= 0.21301
Epoch: 0049 train_loss= 0.50393 train_acc= 0.88518 val_loss= 0.50551 val_acc= 0.87902 time= 0.21167
Epoch: 0050 train_loss= 0.48756 train_acc= 0.88859 val_loss= 0.49142 val_acc= 0.88055 time= 0.21300
Epoch: 0051 train_loss= 0.46896 train_acc= 0.89080 val_loss= 0.47813 val_acc= 0.88208 time= 0.21210
Epoch: 0052 train_loss= 0.45062 train_acc= 0.89471 val_loss= 0.46528 val_acc= 0.88208 time= 0.22899
Epoch: 0053 train_loss= 0.43395 train_acc= 0.89573 val_loss= 0.45212 val_acc= 0.88821 time= 0.20997
Epoch: 0054 train_loss= 0.41978 train_acc= 0.90117 val_loss= 0.43871 val_acc= 0.88974 time= 0.21011
Epoch: 0055 train_loss= 0.40278 train_acc= 0.90883 val_loss= 0.42526 val_acc= 0.88974 time= 0.21300
Epoch: 0056 train_loss= 0.39120 train_acc= 0.90747 val_loss= 0.41233 val_acc= 0.89740 time= 0.24000
Epoch: 0057 train_loss= 0.37368 train_acc= 0.91614 val_loss= 0.40058 val_acc= 0.89893 time= 0.21200
Epoch: 0058 train_loss= 0.36012 train_acc= 0.92210 val_loss= 0.39012 val_acc= 0.90046 time= 0.22400
Epoch: 0059 train_loss= 0.34762 train_acc= 0.92073 val_loss= 0.38044 val_acc= 0.90199 time= 0.21100
Epoch: 0060 train_loss= 0.33577 train_acc= 0.92465 val_loss= 0.37178 val_acc= 0.90505 time= 0.21200
Epoch: 0061 train_loss= 0.32683 train_acc= 0.92958 val_loss= 0.36410 val_acc= 0.90352 time= 0.23600
Epoch: 0062 train_loss= 0.31517 train_acc= 0.93213 val_loss= 0.35728 val_acc= 0.90352 time= 0.22021
Epoch: 0063 train_loss= 0.30528 train_acc= 0.93145 val_loss= 0.35012 val_acc= 0.90812 time= 0.21300
Epoch: 0064 train_loss= 0.29040 train_acc= 0.94268 val_loss= 0.34254 val_acc= 0.90812 time= 0.21300
Epoch: 0065 train_loss= 0.27878 train_acc= 0.93945 val_loss= 0.33504 val_acc= 0.91118 time= 0.21300
Epoch: 0066 train_loss= 0.27259 train_acc= 0.93979 val_loss= 0.32792 val_acc= 0.91271 time= 0.23200
Epoch: 0067 train_loss= 0.25614 train_acc= 0.94472 val_loss= 0.32161 val_acc= 0.91577 time= 0.21600
Epoch: 0068 train_loss= 0.25023 train_acc= 0.94642 val_loss= 0.31520 val_acc= 0.91730 time= 0.20900
Epoch: 0069 train_loss= 0.24137 train_acc= 0.95135 val_loss= 0.31024 val_acc= 0.91730 time= 0.21331
Epoch: 0070 train_loss= 0.23204 train_acc= 0.95390 val_loss= 0.30502 val_acc= 0.91424 time= 0.24100
Epoch: 0071 train_loss= 0.22425 train_acc= 0.95424 val_loss= 0.29985 val_acc= 0.91577 time= 0.22204
Epoch: 0072 train_loss= 0.21425 train_acc= 0.95458 val_loss= 0.29580 val_acc= 0.91577 time= 0.20899
Epoch: 0073 train_loss= 0.20603 train_acc= 0.95731 val_loss= 0.29234 val_acc= 0.91730 time= 0.21196
Epoch: 0074 train_loss= 0.20086 train_acc= 0.96003 val_loss= 0.28822 val_acc= 0.91884 time= 0.21200
Epoch: 0075 train_loss= 0.18999 train_acc= 0.96190 val_loss= 0.28394 val_acc= 0.91884 time= 0.22400
Epoch: 0076 train_loss= 0.18556 train_acc= 0.96139 val_loss= 0.27932 val_acc= 0.92190 time= 0.21184
Epoch: 0077 train_loss= 0.17883 train_acc= 0.96462 val_loss= 0.27507 val_acc= 0.92037 time= 0.21400
Epoch: 0078 train_loss= 0.17146 train_acc= 0.96581 val_loss= 0.27197 val_acc= 0.92190 time= 0.21200
Epoch: 0079 train_loss= 0.16316 train_acc= 0.96547 val_loss= 0.26839 val_acc= 0.92190 time= 0.20900
Epoch: 0080 train_loss= 0.16169 train_acc= 0.96547 val_loss= 0.26486 val_acc= 0.92343 time= 0.21000
Epoch: 0081 train_loss= 0.15287 train_acc= 0.96819 val_loss= 0.26118 val_acc= 0.92649 time= 0.22700
Epoch: 0082 train_loss= 0.14655 train_acc= 0.97006 val_loss= 0.25681 val_acc= 0.92649 time= 0.21005
Epoch: 0083 train_loss= 0.14364 train_acc= 0.97227 val_loss= 0.25328 val_acc= 0.92802 time= 0.21695
Epoch: 0084 train_loss= 0.14130 train_acc= 0.97006 val_loss= 0.25054 val_acc= 0.92956 time= 0.24300
Epoch: 0085 train_loss= 0.13582 train_acc= 0.97295 val_loss= 0.24919 val_acc= 0.92956 time= 0.21500
Epoch: 0086 train_loss= 0.12825 train_acc= 0.97551 val_loss= 0.24967 val_acc= 0.92956 time= 0.21104
Epoch: 0087 train_loss= 0.12580 train_acc= 0.97585 val_loss= 0.24982 val_acc= 0.92956 time= 0.20996
Epoch: 0088 train_loss= 0.11890 train_acc= 0.97823 val_loss= 0.24811 val_acc= 0.92956 time= 0.21400
Epoch: 0089 train_loss= 0.11245 train_acc= 0.98027 val_loss= 0.24527 val_acc= 0.93109 time= 0.21700
Epoch: 0090 train_loss= 0.10894 train_acc= 0.97942 val_loss= 0.24217 val_acc= 0.93109 time= 0.21700
Epoch: 0091 train_loss= 0.10694 train_acc= 0.97942 val_loss= 0.23938 val_acc= 0.93415 time= 0.21300
Epoch: 0092 train_loss= 0.10097 train_acc= 0.98010 val_loss= 0.23718 val_acc= 0.93721 time= 0.20900
Epoch: 0093 train_loss= 0.09864 train_acc= 0.98129 val_loss= 0.23670 val_acc= 0.93874 time= 0.23400
Epoch: 0094 train_loss= 0.09644 train_acc= 0.98214 val_loss= 0.23698 val_acc= 0.93721 time= 0.21000
Epoch: 0095 train_loss= 0.09225 train_acc= 0.98282 val_loss= 0.23749 val_acc= 0.93415 time= 0.21105
Epoch: 0096 train_loss= 0.09124 train_acc= 0.98452 val_loss= 0.23730 val_acc= 0.93262 time= 0.21267
Epoch: 0097 train_loss= 0.08622 train_acc= 0.98656 val_loss= 0.23579 val_acc= 0.93415 time= 0.21400
Epoch: 0098 train_loss= 0.08325 train_acc= 0.98588 val_loss= 0.23434 val_acc= 0.93415 time= 0.21264
Epoch: 0099 train_loss= 0.07882 train_acc= 0.98758 val_loss= 0.23232 val_acc= 0.93415 time= 0.22000
Epoch: 0100 train_loss= 0.07638 train_acc= 0.98707 val_loss= 0.23020 val_acc= 0.93721 time= 0.20900
Epoch: 0101 train_loss= 0.07652 train_acc= 0.98554 val_loss= 0.23026 val_acc= 0.94028 time= 0.21000
Epoch: 0102 train_loss= 0.07339 train_acc= 0.98826 val_loss= 0.23109 val_acc= 0.93874 time= 0.21404
Epoch: 0103 train_loss= 0.07059 train_acc= 0.98809 val_loss= 0.22892 val_acc= 0.93874 time= 0.23296
Epoch: 0104 train_loss= 0.06965 train_acc= 0.98809 val_loss= 0.22720 val_acc= 0.93415 time= 0.22100
Epoch: 0105 train_loss= 0.06683 train_acc= 0.98843 val_loss= 0.22717 val_acc= 0.93568 time= 0.21551
Epoch: 0106 train_loss= 0.06559 train_acc= 0.98979 val_loss= 0.22833 val_acc= 0.93721 time= 0.21100
Epoch: 0107 train_loss= 0.06385 train_acc= 0.98962 val_loss= 0.22978 val_acc= 0.93721 time= 0.23505
Epoch: 0108 train_loss= 0.05794 train_acc= 0.99030 val_loss= 0.23017 val_acc= 0.94028 time= 0.21695
Early stopping...
Optimization Finished!
Test set results: cost= 0.25198 accuracy= 0.93808 time= 0.09000
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8068    0.9467    0.8712        75
           4     1.0000    1.0000    1.0000         9
           5     0.8404    0.9080    0.8729        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.4444    0.6154         9
          10     0.9130    0.5833    0.7119        36
          11     1.0000    0.9167    0.9565        12
          12     0.8176    1.0000    0.8996       121
          13     0.9375    0.7895    0.8571        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    0.7500    0.8571         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9655    0.9669       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8214    0.8519    0.8364        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9791    0.9926    0.9858      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8000    0.6667    0.7273        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9231    0.8000    0.8571        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9381      2568
   macro avg     0.7857    0.6888    0.7146      2568
weighted avg     0.9372    0.9381    0.9335      2568

Macro average Test Precision, Recall and F1-Score...
(0.785744307755723, 0.6888150541403097, 0.7146337293020925, None)
Micro average Test Precision, Recall and F1-Score...
(0.9380841121495327, 0.9380841121495327, 0.9380841121495327, None)
embeddings:
8892 6532 2568
[[-0.08192904 -0.04173969 -0.0530569  ... -0.08030749  0.12293231
   0.8065602 ]
 [ 0.1425857  -0.05434069  0.12530655 ...  0.14826034  0.1930428
   0.2736964 ]
 [ 0.02406226  0.02552639  0.04746734 ...  0.31588653  0.20757301
   0.09003999]
 ...
 [ 0.05135735 -0.0040678   0.00119465 ...  0.23824377  0.19135089
   0.01707135]
 [ 0.0182228   0.02965721  0.0518696  ...  0.13236442  0.20294419
   0.1293463 ]
 [ 0.167773    0.19127566  0.19597784 ...  0.2018671   0.28915113
   0.18262868]]

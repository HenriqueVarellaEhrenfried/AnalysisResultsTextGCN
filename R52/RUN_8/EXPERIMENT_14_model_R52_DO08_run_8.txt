(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.00731 val_loss= 3.90195 val_acc= 0.61562 time= 0.46942
Epoch: 0002 train_loss= 3.90346 train_acc= 0.60231 val_loss= 3.80427 val_acc= 0.60337 time= 0.17399
Epoch: 0003 train_loss= 3.80422 train_acc= 0.59823 val_loss= 3.65499 val_acc= 0.59724 time= 0.17309
Epoch: 0004 train_loss= 3.66021 train_acc= 0.59585 val_loss= 3.45344 val_acc= 0.58193 time= 0.18000
Epoch: 0005 train_loss= 3.45520 train_acc= 0.56999 val_loss= 3.20781 val_acc= 0.56202 time= 0.16700
Epoch: 0006 train_loss= 3.23782 train_acc= 0.54941 val_loss= 2.94083 val_acc= 0.54364 time= 0.18300
Epoch: 0007 train_loss= 2.95133 train_acc= 0.54907 val_loss= 2.68438 val_acc= 0.51149 time= 0.17000
Epoch: 0008 train_loss= 2.69327 train_acc= 0.51420 val_loss= 2.47018 val_acc= 0.49923 time= 0.17204
Epoch: 0009 train_loss= 2.47741 train_acc= 0.48546 val_loss= 2.32569 val_acc= 0.49158 time= 0.18993
Epoch: 0010 train_loss= 2.31869 train_acc= 0.48767 val_loss= 2.24822 val_acc= 0.48086 time= 0.16800
Epoch: 0011 train_loss= 2.28335 train_acc= 0.47134 val_loss= 2.20792 val_acc= 0.46861 time= 0.19100
Epoch: 0012 train_loss= 2.20969 train_acc= 0.46079 val_loss= 2.17170 val_acc= 0.46248 time= 0.16530
Epoch: 0013 train_loss= 2.18543 train_acc= 0.45076 val_loss= 2.12087 val_acc= 0.46248 time= 0.16604
Epoch: 0014 train_loss= 2.13302 train_acc= 0.44174 val_loss= 2.05060 val_acc= 0.46554 time= 0.18601
Epoch: 0015 train_loss= 2.07960 train_acc= 0.44736 val_loss= 1.96653 val_acc= 0.47473 time= 0.16695
Epoch: 0016 train_loss= 1.98462 train_acc= 0.46470 val_loss= 1.87848 val_acc= 0.50230 time= 0.17000
Epoch: 0017 train_loss= 1.91855 train_acc= 0.49311 val_loss= 1.79648 val_acc= 0.55130 time= 0.18300
Epoch: 0018 train_loss= 1.81518 train_acc= 0.53904 val_loss= 1.72758 val_acc= 0.61868 time= 0.16951
Epoch: 0019 train_loss= 1.77673 train_acc= 0.60112 val_loss= 1.67102 val_acc= 0.65697 time= 0.16600
Epoch: 0020 train_loss= 1.69575 train_acc= 0.63820 val_loss= 1.62068 val_acc= 0.67534 time= 0.16700
Epoch: 0021 train_loss= 1.63546 train_acc= 0.64722 val_loss= 1.57181 val_acc= 0.67688 time= 0.16705
Epoch: 0022 train_loss= 1.58110 train_acc= 0.64773 val_loss= 1.52267 val_acc= 0.68147 time= 0.16995
Epoch: 0023 train_loss= 1.53605 train_acc= 0.65776 val_loss= 1.47382 val_acc= 0.67688 time= 0.18400
Epoch: 0024 train_loss= 1.50188 train_acc= 0.65385 val_loss= 1.42652 val_acc= 0.67994 time= 0.16853
Epoch: 0025 train_loss= 1.45555 train_acc= 0.65862 val_loss= 1.38236 val_acc= 0.68760 time= 0.17187
Epoch: 0026 train_loss= 1.41834 train_acc= 0.66916 val_loss= 1.34166 val_acc= 0.69678 time= 0.18897
Epoch: 0027 train_loss= 1.36599 train_acc= 0.67631 val_loss= 1.30467 val_acc= 0.70750 time= 0.17003
Epoch: 0028 train_loss= 1.33506 train_acc= 0.68056 val_loss= 1.27070 val_acc= 0.71516 time= 0.17000
Epoch: 0029 train_loss= 1.29937 train_acc= 0.69774 val_loss= 1.23918 val_acc= 0.71669 time= 0.18607
Epoch: 0030 train_loss= 1.27507 train_acc= 0.70556 val_loss= 1.20930 val_acc= 0.72435 time= 0.17196
Epoch: 0031 train_loss= 1.23009 train_acc= 0.72172 val_loss= 1.18052 val_acc= 0.72588 time= 0.17003
Epoch: 0032 train_loss= 1.21623 train_acc= 0.72359 val_loss= 1.15242 val_acc= 0.73201 time= 0.18508
Epoch: 0033 train_loss= 1.16950 train_acc= 0.73890 val_loss= 1.12451 val_acc= 0.73813 time= 0.16997
Epoch: 0034 train_loss= 1.16143 train_acc= 0.73788 val_loss= 1.09700 val_acc= 0.74579 time= 0.18900
Epoch: 0035 train_loss= 1.11909 train_acc= 0.74962 val_loss= 1.06986 val_acc= 0.75345 time= 0.17000
Epoch: 0036 train_loss= 1.09543 train_acc= 0.75234 val_loss= 1.04318 val_acc= 0.75191 time= 0.16838
Epoch: 0037 train_loss= 1.07039 train_acc= 0.75693 val_loss= 1.01721 val_acc= 0.76110 time= 0.18697
Epoch: 0038 train_loss= 1.05069 train_acc= 0.75880 val_loss= 0.99212 val_acc= 0.76723 time= 0.16800
Epoch: 0039 train_loss= 1.01938 train_acc= 0.76714 val_loss= 0.96755 val_acc= 0.77335 time= 0.16700
Epoch: 0040 train_loss= 0.99940 train_acc= 0.77105 val_loss= 0.94377 val_acc= 0.78714 time= 0.17300
Epoch: 0041 train_loss= 0.96475 train_acc= 0.77972 val_loss= 0.92068 val_acc= 0.79326 time= 0.16700
Epoch: 0042 train_loss= 0.94577 train_acc= 0.79316 val_loss= 0.89838 val_acc= 0.79786 time= 0.17173
Epoch: 0043 train_loss= 0.92605 train_acc= 0.79895 val_loss= 0.87670 val_acc= 0.81470 time= 0.19179
Epoch: 0044 train_loss= 0.90415 train_acc= 0.81034 val_loss= 0.85560 val_acc= 0.82848 time= 0.17000
Epoch: 0045 train_loss= 0.88273 train_acc= 0.81272 val_loss= 0.83517 val_acc= 0.83002 time= 0.16900
Epoch: 0046 train_loss= 0.87639 train_acc= 0.80813 val_loss= 0.81543 val_acc= 0.83461 time= 0.18603
Epoch: 0047 train_loss= 0.84530 train_acc= 0.82310 val_loss= 0.79602 val_acc= 0.83308 time= 0.16800
Epoch: 0048 train_loss= 0.82659 train_acc= 0.82344 val_loss= 0.77678 val_acc= 0.83614 time= 0.16800
Epoch: 0049 train_loss= 0.79219 train_acc= 0.82684 val_loss= 0.75779 val_acc= 0.83614 time= 0.18500
Epoch: 0050 train_loss= 0.76828 train_acc= 0.83790 val_loss= 0.73873 val_acc= 0.83767 time= 0.16897
Epoch: 0051 train_loss= 0.76356 train_acc= 0.83177 val_loss= 0.72031 val_acc= 0.83767 time= 0.19300
Epoch: 0052 train_loss= 0.74372 train_acc= 0.83007 val_loss= 0.70203 val_acc= 0.84227 time= 0.17000
Epoch: 0053 train_loss= 0.71391 train_acc= 0.83875 val_loss= 0.68414 val_acc= 0.84839 time= 0.16800
Epoch: 0054 train_loss= 0.71957 train_acc= 0.83841 val_loss= 0.66668 val_acc= 0.84839 time= 0.18803
Epoch: 0055 train_loss= 0.70122 train_acc= 0.84555 val_loss= 0.64986 val_acc= 0.84992 time= 0.16600
Epoch: 0056 train_loss= 0.65512 train_acc= 0.84504 val_loss= 0.63383 val_acc= 0.85605 time= 0.16897
Epoch: 0057 train_loss= 0.66316 train_acc= 0.85389 val_loss= 0.61852 val_acc= 0.86217 time= 0.18703
Epoch: 0058 train_loss= 0.62607 train_acc= 0.85882 val_loss= 0.60391 val_acc= 0.86830 time= 0.16716
Epoch: 0059 train_loss= 0.62195 train_acc= 0.85559 val_loss= 0.59007 val_acc= 0.87289 time= 0.17200
Epoch: 0060 train_loss= 0.60662 train_acc= 0.85576 val_loss= 0.57681 val_acc= 0.87443 time= 0.17800
Epoch: 0061 train_loss= 0.58172 train_acc= 0.86886 val_loss= 0.56377 val_acc= 0.87289 time= 0.16900
Epoch: 0062 train_loss= 0.58030 train_acc= 0.86545 val_loss= 0.55102 val_acc= 0.87443 time= 0.16900
Epoch: 0063 train_loss= 0.55909 train_acc= 0.86869 val_loss= 0.53854 val_acc= 0.87749 time= 0.16797
Epoch: 0064 train_loss= 0.53000 train_acc= 0.87515 val_loss= 0.52580 val_acc= 0.87749 time= 0.16803
Epoch: 0065 train_loss= 0.53638 train_acc= 0.87532 val_loss= 0.51257 val_acc= 0.87902 time= 0.17003
Epoch: 0066 train_loss= 0.52504 train_acc= 0.87872 val_loss= 0.49956 val_acc= 0.88361 time= 0.18407
Epoch: 0067 train_loss= 0.52336 train_acc= 0.87702 val_loss= 0.48777 val_acc= 0.88668 time= 0.16900
Epoch: 0068 train_loss= 0.48973 train_acc= 0.88621 val_loss= 0.47671 val_acc= 0.88821 time= 0.17230
Epoch: 0069 train_loss= 0.48274 train_acc= 0.88586 val_loss= 0.46682 val_acc= 0.88974 time= 0.18900
Epoch: 0070 train_loss= 0.48248 train_acc= 0.88535 val_loss= 0.45748 val_acc= 0.89127 time= 0.16800
Epoch: 0071 train_loss= 0.45912 train_acc= 0.89284 val_loss= 0.44851 val_acc= 0.89280 time= 0.17000
Epoch: 0072 train_loss= 0.44742 train_acc= 0.89675 val_loss= 0.44043 val_acc= 0.89433 time= 0.18700
Epoch: 0073 train_loss= 0.44101 train_acc= 0.89896 val_loss= 0.43282 val_acc= 0.89433 time= 0.16711
Epoch: 0074 train_loss= 0.42284 train_acc= 0.90509 val_loss= 0.42556 val_acc= 0.89740 time= 0.16997
Epoch: 0075 train_loss= 0.42053 train_acc= 0.90441 val_loss= 0.41784 val_acc= 0.90046 time= 0.16900
Epoch: 0076 train_loss= 0.40266 train_acc= 0.91087 val_loss= 0.40989 val_acc= 0.89893 time= 0.17023
Epoch: 0077 train_loss= 0.39691 train_acc= 0.90679 val_loss= 0.40109 val_acc= 0.89893 time= 0.19200
Epoch: 0078 train_loss= 0.38912 train_acc= 0.91461 val_loss= 0.39245 val_acc= 0.90199 time= 0.17000
Epoch: 0079 train_loss= 0.38530 train_acc= 0.90781 val_loss= 0.38483 val_acc= 0.90046 time= 0.16803
Epoch: 0080 train_loss= 0.36876 train_acc= 0.91801 val_loss= 0.37761 val_acc= 0.90352 time= 0.18811
Epoch: 0081 train_loss= 0.36960 train_acc= 0.91614 val_loss= 0.37104 val_acc= 0.90199 time= 0.16706
Epoch: 0082 train_loss= 0.36125 train_acc= 0.91886 val_loss= 0.36469 val_acc= 0.90352 time= 0.16704
Epoch: 0083 train_loss= 0.35263 train_acc= 0.92193 val_loss= 0.35961 val_acc= 0.90352 time= 0.18696
Epoch: 0084 train_loss= 0.34459 train_acc= 0.92005 val_loss= 0.35534 val_acc= 0.90199 time= 0.16923
Epoch: 0085 train_loss= 0.34319 train_acc= 0.91801 val_loss= 0.35141 val_acc= 0.90505 time= 0.17000
Epoch: 0086 train_loss= 0.32423 train_acc= 0.92244 val_loss= 0.34765 val_acc= 0.90505 time= 0.19300
Epoch: 0087 train_loss= 0.33488 train_acc= 0.92210 val_loss= 0.34386 val_acc= 0.90352 time= 0.16880
Epoch: 0088 train_loss= 0.31658 train_acc= 0.92669 val_loss= 0.34042 val_acc= 0.90658 time= 0.17200
Epoch: 0089 train_loss= 0.31239 train_acc= 0.93026 val_loss= 0.33637 val_acc= 0.90812 time= 0.18406
Epoch: 0090 train_loss= 0.30022 train_acc= 0.92805 val_loss= 0.33120 val_acc= 0.91271 time= 0.16795
Epoch: 0091 train_loss= 0.31263 train_acc= 0.93196 val_loss= 0.32520 val_acc= 0.91271 time= 0.17016
Epoch: 0092 train_loss= 0.28856 train_acc= 0.93179 val_loss= 0.32076 val_acc= 0.90965 time= 0.18500
Epoch: 0093 train_loss= 0.28389 train_acc= 0.93451 val_loss= 0.31751 val_acc= 0.91271 time= 0.17100
Epoch: 0094 train_loss= 0.27499 train_acc= 0.93247 val_loss= 0.31455 val_acc= 0.91730 time= 0.17300
Epoch: 0095 train_loss= 0.27959 train_acc= 0.93757 val_loss= 0.31127 val_acc= 0.92037 time= 0.18804
Epoch: 0096 train_loss= 0.26798 train_acc= 0.93842 val_loss= 0.30632 val_acc= 0.92343 time= 0.16796
Epoch: 0097 train_loss= 0.27535 train_acc= 0.93911 val_loss= 0.30166 val_acc= 0.92343 time= 0.18600
Epoch: 0098 train_loss= 0.26947 train_acc= 0.93689 val_loss= 0.29781 val_acc= 0.92190 time= 0.16904
Epoch: 0099 train_loss= 0.25634 train_acc= 0.93791 val_loss= 0.29464 val_acc= 0.92343 time= 0.16900
Epoch: 0100 train_loss= 0.23960 train_acc= 0.94353 val_loss= 0.29184 val_acc= 0.92190 time= 0.18700
Epoch: 0101 train_loss= 0.24498 train_acc= 0.94387 val_loss= 0.28923 val_acc= 0.92037 time= 0.16796
Epoch: 0102 train_loss= 0.23953 train_acc= 0.94285 val_loss= 0.28676 val_acc= 0.92190 time= 0.16900
Epoch: 0103 train_loss= 0.24917 train_acc= 0.93860 val_loss= 0.28434 val_acc= 0.92343 time= 0.17000
Epoch: 0104 train_loss= 0.22424 train_acc= 0.94591 val_loss= 0.28247 val_acc= 0.92649 time= 0.16904
Epoch: 0105 train_loss= 0.23274 train_acc= 0.94778 val_loss= 0.28085 val_acc= 0.92649 time= 0.16899
Epoch: 0106 train_loss= 0.22533 train_acc= 0.94761 val_loss= 0.27968 val_acc= 0.92802 time= 0.18497
Epoch: 0107 train_loss= 0.21397 train_acc= 0.95169 val_loss= 0.27881 val_acc= 0.92956 time= 0.16804
Epoch: 0108 train_loss= 0.21316 train_acc= 0.95118 val_loss= 0.27760 val_acc= 0.92956 time= 0.17099
Epoch: 0109 train_loss= 0.21651 train_acc= 0.95118 val_loss= 0.27639 val_acc= 0.92956 time= 0.18609
Epoch: 0110 train_loss= 0.21600 train_acc= 0.94472 val_loss= 0.27449 val_acc= 0.92956 time= 0.17200
Epoch: 0111 train_loss= 0.21100 train_acc= 0.94931 val_loss= 0.27241 val_acc= 0.92496 time= 0.19300
Epoch: 0112 train_loss= 0.19825 train_acc= 0.95254 val_loss= 0.27130 val_acc= 0.92649 time= 0.17000
Epoch: 0113 train_loss= 0.19471 train_acc= 0.95748 val_loss= 0.27033 val_acc= 0.92956 time= 0.16800
Epoch: 0114 train_loss= 0.19861 train_acc= 0.95509 val_loss= 0.26881 val_acc= 0.92956 time= 0.17400
Epoch: 0115 train_loss= 0.18836 train_acc= 0.95833 val_loss= 0.26586 val_acc= 0.92802 time= 0.16904
Epoch: 0116 train_loss= 0.18796 train_acc= 0.95714 val_loss= 0.26365 val_acc= 0.93262 time= 0.16799
Epoch: 0117 train_loss= 0.19484 train_acc= 0.95492 val_loss= 0.26143 val_acc= 0.92956 time= 0.18701
Epoch: 0118 train_loss= 0.18979 train_acc= 0.94897 val_loss= 0.26142 val_acc= 0.92802 time= 0.17064
Epoch: 0119 train_loss= 0.17680 train_acc= 0.95935 val_loss= 0.26237 val_acc= 0.92802 time= 0.17100
Epoch: 0120 train_loss= 0.17643 train_acc= 0.95697 val_loss= 0.26228 val_acc= 0.92956 time= 0.18300
Epoch: 0121 train_loss= 0.18341 train_acc= 0.95629 val_loss= 0.25822 val_acc= 0.93262 time= 0.16900
Epoch: 0122 train_loss= 0.17359 train_acc= 0.96207 val_loss= 0.25400 val_acc= 0.93109 time= 0.16704
Epoch: 0123 train_loss= 0.16530 train_acc= 0.96071 val_loss= 0.25053 val_acc= 0.93262 time= 0.18897
Epoch: 0124 train_loss= 0.17196 train_acc= 0.95969 val_loss= 0.24920 val_acc= 0.93262 time= 0.16700
Epoch: 0125 train_loss= 0.16086 train_acc= 0.96207 val_loss= 0.24946 val_acc= 0.93109 time= 0.16800
Epoch: 0126 train_loss= 0.15713 train_acc= 0.96309 val_loss= 0.24997 val_acc= 0.93109 time= 0.16703
Epoch: 0127 train_loss= 0.15138 train_acc= 0.96343 val_loss= 0.24908 val_acc= 0.92956 time= 0.16846
Epoch: 0128 train_loss= 0.15559 train_acc= 0.96547 val_loss= 0.24890 val_acc= 0.92802 time= 0.17400
Epoch: 0129 train_loss= 0.15107 train_acc= 0.96819 val_loss= 0.24911 val_acc= 0.92802 time= 0.19000
Epoch: 0130 train_loss= 0.15425 train_acc= 0.96394 val_loss= 0.24839 val_acc= 0.92956 time= 0.16806
Epoch: 0131 train_loss= 0.15208 train_acc= 0.96496 val_loss= 0.24656 val_acc= 0.92956 time= 0.17056
Epoch: 0132 train_loss= 0.15602 train_acc= 0.96411 val_loss= 0.24489 val_acc= 0.93109 time= 0.18601
Epoch: 0133 train_loss= 0.14540 train_acc= 0.96615 val_loss= 0.24266 val_acc= 0.93109 time= 0.16800
Epoch: 0134 train_loss= 0.15094 train_acc= 0.96241 val_loss= 0.24126 val_acc= 0.93262 time= 0.17100
Epoch: 0135 train_loss= 0.14330 train_acc= 0.96581 val_loss= 0.23944 val_acc= 0.93109 time= 0.18600
Epoch: 0136 train_loss= 0.14269 train_acc= 0.96717 val_loss= 0.23892 val_acc= 0.93721 time= 0.17000
Epoch: 0137 train_loss= 0.13675 train_acc= 0.96768 val_loss= 0.23940 val_acc= 0.93721 time= 0.19200
Epoch: 0138 train_loss= 0.13385 train_acc= 0.96989 val_loss= 0.24061 val_acc= 0.93874 time= 0.16800
Epoch: 0139 train_loss= 0.13403 train_acc= 0.96989 val_loss= 0.24124 val_acc= 0.93721 time= 0.17101
Epoch: 0140 train_loss= 0.13791 train_acc= 0.96921 val_loss= 0.23890 val_acc= 0.93721 time= 0.18600
Epoch: 0141 train_loss= 0.13730 train_acc= 0.96751 val_loss= 0.23844 val_acc= 0.93568 time= 0.16708
Epoch: 0142 train_loss= 0.12196 train_acc= 0.97057 val_loss= 0.24004 val_acc= 0.93262 time= 0.16633
Epoch: 0143 train_loss= 0.12878 train_acc= 0.97040 val_loss= 0.24261 val_acc= 0.93721 time= 0.17996
Early stopping...
Optimization Finished!
Test set results: cost= 0.27322 accuracy= 0.93302 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7228    0.9733    0.8295        75
           4     1.0000    1.0000    1.0000         9
           5     0.7387    0.9425    0.8283        87
           6     0.8846    0.9200    0.9020        25
           7     0.7857    0.8462    0.8148        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.2222    0.3636         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    0.9167    0.9565        12
          12     0.8815    0.9835    0.9297       121
          13     0.8750    0.7368    0.8000        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.3333    0.5000         9
          21     0.8696    1.0000    0.9302        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.6500    0.7647    0.7027        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9684    0.9684    0.9684       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.6667    0.8000         3
          32     0.8000    0.8000    0.8000        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8615    0.6914    0.7671        81
          36     0.8333    0.4167    0.5556        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9908    0.9858      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.8462    0.9167    0.8800        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9330      2568
   macro avg     0.7194    0.6355    0.6567      2568
weighted avg     0.9300    0.9330    0.9268      2568

Macro average Test Precision, Recall and F1-Score...
(0.7194442550762029, 0.6354926264255659, 0.6567460239174382, None)
Micro average Test Precision, Recall and F1-Score...
(0.9330218068535826, 0.9330218068535826, 0.9330218068535826, None)
embeddings:
8892 6532 2568
[[-0.03194659 -0.0707977   0.33705404 ...  0.00531804 -0.04201786
   0.9630979 ]
 [ 0.01899756  0.11016096  0.05859961 ... -0.01527213  0.2852625
   0.28922984]
 [ 0.09857927 -0.02106781  0.06731447 ...  0.17202154  0.7319114
   0.2438708 ]
 ...
 [ 0.1938607   0.01513106  0.15695968 ...  0.24319099  0.0674371
   0.04091989]
 [ 0.07222729  0.04795211  0.10877445 ...  0.10721206  0.3158817
   0.17451508]
 [ 0.22288541  0.26836038  0.24887711 ...  0.25206774  0.05010686
   0.29650158]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95136 train_acc= 0.00493 val_loss= 3.90426 val_acc= 0.57734 time= 0.44510
Epoch: 0002 train_loss= 3.90437 train_acc= 0.57153 val_loss= 3.81344 val_acc= 0.61868 time= 0.19300
Epoch: 0003 train_loss= 3.81355 train_acc= 0.60912 val_loss= 3.67231 val_acc= 0.63706 time= 0.17709
Epoch: 0004 train_loss= 3.67441 train_acc= 0.63089 val_loss= 3.47967 val_acc= 0.64625 time= 0.16796
Epoch: 0005 train_loss= 3.47653 train_acc= 0.64059 val_loss= 3.24273 val_acc= 0.65084 time= 0.17300
Epoch: 0006 train_loss= 3.25053 train_acc= 0.63820 val_loss= 2.98111 val_acc= 0.65084 time= 0.18900
Epoch: 0007 train_loss= 2.97886 train_acc= 0.64161 val_loss= 2.72194 val_acc= 0.65850 time= 0.16996
Epoch: 0008 train_loss= 2.72133 train_acc= 0.64348 val_loss= 2.49610 val_acc= 0.66462 time= 0.16803
Epoch: 0009 train_loss= 2.50232 train_acc= 0.64501 val_loss= 2.33485 val_acc= 0.65697 time= 0.17796
Epoch: 0010 train_loss= 2.33133 train_acc= 0.63565 val_loss= 2.24306 val_acc= 0.63553 time= 0.16700
Epoch: 0011 train_loss= 2.24164 train_acc= 0.62477 val_loss= 2.19138 val_acc= 0.54824 time= 0.17000
Epoch: 0012 train_loss= 2.18318 train_acc= 0.53938 val_loss= 2.14623 val_acc= 0.48545 time= 0.18910
Epoch: 0013 train_loss= 2.15244 train_acc= 0.47015 val_loss= 2.08991 val_acc= 0.47167 time= 0.17118
Epoch: 0014 train_loss= 2.10875 train_acc= 0.44548 val_loss= 2.01664 val_acc= 0.46861 time= 0.19000
Epoch: 0015 train_loss= 2.03550 train_acc= 0.44701 val_loss= 1.92979 val_acc= 0.47779 time= 0.16900
Epoch: 0016 train_loss= 1.95412 train_acc= 0.45348 val_loss= 1.83873 val_acc= 0.50689 time= 0.17003
Epoch: 0017 train_loss= 1.86950 train_acc= 0.49158 val_loss= 1.75496 val_acc= 0.57274 time= 0.18701
Epoch: 0018 train_loss= 1.78559 train_acc= 0.55928 val_loss= 1.68496 val_acc= 0.63859 time= 0.16702
Epoch: 0019 train_loss= 1.71917 train_acc= 0.62460 val_loss= 1.62668 val_acc= 0.67381 time= 0.16801
Epoch: 0020 train_loss= 1.65244 train_acc= 0.65470 val_loss= 1.57313 val_acc= 0.67994 time= 0.16600
Epoch: 0021 train_loss= 1.60373 train_acc= 0.66253 val_loss= 1.51929 val_acc= 0.69832 time= 0.16600
Epoch: 0022 train_loss= 1.54706 train_acc= 0.67086 val_loss= 1.46448 val_acc= 0.69525 time= 0.17211
Epoch: 0023 train_loss= 1.49176 train_acc= 0.68192 val_loss= 1.41056 val_acc= 0.70138 time= 0.18900
Epoch: 0024 train_loss= 1.44200 train_acc= 0.68889 val_loss= 1.36008 val_acc= 0.70444 time= 0.16900
Epoch: 0025 train_loss= 1.38612 train_acc= 0.69502 val_loss= 1.31443 val_acc= 0.70904 time= 0.17115
Epoch: 0026 train_loss= 1.33907 train_acc= 0.69774 val_loss= 1.27363 val_acc= 0.71516 time= 0.18600
Epoch: 0027 train_loss= 1.29960 train_acc= 0.70675 val_loss= 1.23683 val_acc= 0.71975 time= 0.16900
Epoch: 0028 train_loss= 1.26294 train_acc= 0.71288 val_loss= 1.20299 val_acc= 0.72435 time= 0.17060
Epoch: 0029 train_loss= 1.22837 train_acc= 0.72376 val_loss= 1.17109 val_acc= 0.73507 time= 0.18506
Epoch: 0030 train_loss= 1.19481 train_acc= 0.73244 val_loss= 1.14038 val_acc= 0.73813 time= 0.16994
Epoch: 0031 train_loss= 1.16721 train_acc= 0.74298 val_loss= 1.11039 val_acc= 0.74579 time= 0.17531
Epoch: 0032 train_loss= 1.13381 train_acc= 0.75268 val_loss= 1.08081 val_acc= 0.75957 time= 0.17000
Epoch: 0033 train_loss= 1.10618 train_acc= 0.75744 val_loss= 1.05151 val_acc= 0.76723 time= 0.16900
Epoch: 0034 train_loss= 1.07341 train_acc= 0.76578 val_loss= 1.02267 val_acc= 0.77182 time= 0.17000
Epoch: 0035 train_loss= 1.04292 train_acc= 0.77173 val_loss= 0.99459 val_acc= 0.77795 time= 0.18403
Epoch: 0036 train_loss= 1.01027 train_acc= 0.78006 val_loss= 0.96743 val_acc= 0.78560 time= 0.16704
Epoch: 0037 train_loss= 0.98717 train_acc= 0.78313 val_loss= 0.94113 val_acc= 0.79020 time= 0.18601
Epoch: 0038 train_loss= 0.95474 train_acc= 0.79520 val_loss= 0.91554 val_acc= 0.80551 time= 0.16707
Epoch: 0039 train_loss= 0.93736 train_acc= 0.80235 val_loss= 0.89069 val_acc= 0.81623 time= 0.16839
Epoch: 0040 train_loss= 0.91020 train_acc= 0.81306 val_loss= 0.86651 val_acc= 0.82389 time= 0.19400
Epoch: 0041 train_loss= 0.88091 train_acc= 0.81834 val_loss= 0.84280 val_acc= 0.83155 time= 0.17000
Epoch: 0042 train_loss= 0.86144 train_acc= 0.82446 val_loss= 0.81940 val_acc= 0.83920 time= 0.16800
Epoch: 0043 train_loss= 0.83225 train_acc= 0.83296 val_loss= 0.79642 val_acc= 0.84227 time= 0.17000
Epoch: 0044 train_loss= 0.81048 train_acc= 0.83296 val_loss= 0.77394 val_acc= 0.84533 time= 0.16825
Epoch: 0045 train_loss= 0.78322 train_acc= 0.83841 val_loss= 0.75194 val_acc= 0.84533 time= 0.17104
Epoch: 0046 train_loss= 0.75802 train_acc= 0.84096 val_loss= 0.73055 val_acc= 0.84533 time= 0.18496
Epoch: 0047 train_loss= 0.73428 train_acc= 0.84249 val_loss= 0.70986 val_acc= 0.84839 time= 0.16700
Epoch: 0048 train_loss= 0.70945 train_acc= 0.84640 val_loss= 0.68957 val_acc= 0.85911 time= 0.17333
Epoch: 0049 train_loss= 0.69329 train_acc= 0.85525 val_loss= 0.66997 val_acc= 0.86524 time= 0.18900
Epoch: 0050 train_loss= 0.66849 train_acc= 0.85882 val_loss= 0.65069 val_acc= 0.86830 time= 0.17000
Epoch: 0051 train_loss= 0.65602 train_acc= 0.86052 val_loss= 0.63182 val_acc= 0.86983 time= 0.18800
Epoch: 0052 train_loss= 0.63219 train_acc= 0.86647 val_loss= 0.61362 val_acc= 0.87136 time= 0.16805
Epoch: 0053 train_loss= 0.61148 train_acc= 0.86937 val_loss= 0.59627 val_acc= 0.87596 time= 0.16803
Epoch: 0054 train_loss= 0.58842 train_acc= 0.87362 val_loss= 0.57955 val_acc= 0.87902 time= 0.18601
Epoch: 0055 train_loss= 0.57784 train_acc= 0.87192 val_loss= 0.56364 val_acc= 0.88055 time= 0.16899
Epoch: 0056 train_loss= 0.55329 train_acc= 0.87617 val_loss= 0.54834 val_acc= 0.88208 time= 0.16997
Epoch: 0057 train_loss= 0.54117 train_acc= 0.88161 val_loss= 0.53380 val_acc= 0.88515 time= 0.19222
Epoch: 0058 train_loss= 0.52965 train_acc= 0.87923 val_loss= 0.51955 val_acc= 0.88668 time= 0.16940
Epoch: 0059 train_loss= 0.50582 train_acc= 0.88621 val_loss= 0.50598 val_acc= 0.88668 time= 0.16903
Epoch: 0060 train_loss= 0.49283 train_acc= 0.89012 val_loss= 0.49268 val_acc= 0.88668 time= 0.17697
Epoch: 0061 train_loss= 0.47112 train_acc= 0.89471 val_loss= 0.47956 val_acc= 0.88668 time= 0.16703
Epoch: 0062 train_loss= 0.46046 train_acc= 0.90100 val_loss= 0.46654 val_acc= 0.88668 time= 0.16787
Epoch: 0063 train_loss= 0.44842 train_acc= 0.89777 val_loss= 0.45405 val_acc= 0.89280 time= 0.18995
Epoch: 0064 train_loss= 0.43331 train_acc= 0.90441 val_loss= 0.44217 val_acc= 0.89893 time= 0.16705
Epoch: 0065 train_loss= 0.41913 train_acc= 0.90951 val_loss= 0.43108 val_acc= 0.90046 time= 0.17091
Epoch: 0066 train_loss= 0.40560 train_acc= 0.91444 val_loss= 0.42053 val_acc= 0.90505 time= 0.19300
Epoch: 0067 train_loss= 0.39385 train_acc= 0.91325 val_loss= 0.41110 val_acc= 0.90352 time= 0.17000
Epoch: 0068 train_loss= 0.38204 train_acc= 0.91716 val_loss= 0.40228 val_acc= 0.90505 time= 0.17000
Epoch: 0069 train_loss= 0.36812 train_acc= 0.92295 val_loss= 0.39413 val_acc= 0.90505 time= 0.18500
Epoch: 0070 train_loss= 0.35891 train_acc= 0.92295 val_loss= 0.38580 val_acc= 0.90352 time= 0.16800
Epoch: 0071 train_loss= 0.34868 train_acc= 0.92720 val_loss= 0.37785 val_acc= 0.90352 time= 0.17004
Epoch: 0072 train_loss= 0.33912 train_acc= 0.92924 val_loss= 0.37017 val_acc= 0.90505 time= 0.17196
Epoch: 0073 train_loss= 0.32459 train_acc= 0.93213 val_loss= 0.36269 val_acc= 0.90658 time= 0.17109
Epoch: 0074 train_loss= 0.31783 train_acc= 0.93570 val_loss= 0.35569 val_acc= 0.90658 time= 0.17300
Epoch: 0075 train_loss= 0.30382 train_acc= 0.93672 val_loss= 0.34876 val_acc= 0.90812 time= 0.19300
Epoch: 0076 train_loss= 0.29987 train_acc= 0.94115 val_loss= 0.34181 val_acc= 0.90965 time= 0.16801
Epoch: 0077 train_loss= 0.29073 train_acc= 0.94013 val_loss= 0.33518 val_acc= 0.91118 time= 0.18699
Epoch: 0078 train_loss= 0.28145 train_acc= 0.94285 val_loss= 0.32926 val_acc= 0.90965 time= 0.16700
Epoch: 0079 train_loss= 0.27466 train_acc= 0.94268 val_loss= 0.32432 val_acc= 0.91118 time= 0.16708
Epoch: 0080 train_loss= 0.26919 train_acc= 0.94642 val_loss= 0.31956 val_acc= 0.91424 time= 0.18597
Epoch: 0081 train_loss= 0.25914 train_acc= 0.94489 val_loss= 0.31445 val_acc= 0.91577 time= 0.16904
Epoch: 0082 train_loss= 0.24931 train_acc= 0.94999 val_loss= 0.30944 val_acc= 0.91884 time= 0.17200
Epoch: 0083 train_loss= 0.24520 train_acc= 0.95118 val_loss= 0.30442 val_acc= 0.92496 time= 0.17200
Epoch: 0084 train_loss= 0.23409 train_acc= 0.95373 val_loss= 0.29994 val_acc= 0.92496 time= 0.16900
Epoch: 0085 train_loss= 0.22522 train_acc= 0.95697 val_loss= 0.29550 val_acc= 0.92496 time= 0.17009
Epoch: 0086 train_loss= 0.21971 train_acc= 0.95663 val_loss= 0.29020 val_acc= 0.92496 time= 0.18595
Epoch: 0087 train_loss= 0.21446 train_acc= 0.95782 val_loss= 0.28547 val_acc= 0.92802 time= 0.16807
Epoch: 0088 train_loss= 0.20461 train_acc= 0.95901 val_loss= 0.28201 val_acc= 0.92802 time= 0.17097
Epoch: 0089 train_loss= 0.20238 train_acc= 0.96003 val_loss= 0.28006 val_acc= 0.92649 time= 0.17403
Epoch: 0090 train_loss= 0.19093 train_acc= 0.96224 val_loss= 0.27891 val_acc= 0.92802 time= 0.16897
Epoch: 0091 train_loss= 0.19276 train_acc= 0.96190 val_loss= 0.27752 val_acc= 0.92496 time= 0.17285
Epoch: 0092 train_loss= 0.18778 train_acc= 0.96343 val_loss= 0.27438 val_acc= 0.92956 time= 0.19100
Epoch: 0093 train_loss= 0.17937 train_acc= 0.96581 val_loss= 0.27090 val_acc= 0.92956 time= 0.16800
Epoch: 0094 train_loss= 0.17043 train_acc= 0.96547 val_loss= 0.26688 val_acc= 0.93262 time= 0.17803
Epoch: 0095 train_loss= 0.17296 train_acc= 0.96683 val_loss= 0.26309 val_acc= 0.93262 time= 0.16700
Epoch: 0096 train_loss= 0.16275 train_acc= 0.96836 val_loss= 0.25955 val_acc= 0.93262 time= 0.16700
Epoch: 0097 train_loss= 0.15956 train_acc= 0.96887 val_loss= 0.25713 val_acc= 0.93109 time= 0.18809
Epoch: 0098 train_loss= 0.16207 train_acc= 0.96955 val_loss= 0.25522 val_acc= 0.93262 time= 0.16713
Epoch: 0099 train_loss= 0.15268 train_acc= 0.97364 val_loss= 0.25408 val_acc= 0.92956 time= 0.17100
Epoch: 0100 train_loss= 0.14736 train_acc= 0.97176 val_loss= 0.25304 val_acc= 0.93262 time= 0.19200
Epoch: 0101 train_loss= 0.14506 train_acc= 0.97159 val_loss= 0.25063 val_acc= 0.93262 time= 0.17203
Epoch: 0102 train_loss= 0.14316 train_acc= 0.97261 val_loss= 0.24705 val_acc= 0.93109 time= 0.16797
Epoch: 0103 train_loss= 0.13523 train_acc= 0.97415 val_loss= 0.24405 val_acc= 0.93415 time= 0.16900
Epoch: 0104 train_loss= 0.13242 train_acc= 0.97483 val_loss= 0.24221 val_acc= 0.93262 time= 0.16803
Epoch: 0105 train_loss= 0.12626 train_acc= 0.97670 val_loss= 0.24157 val_acc= 0.93262 time= 0.17100
Epoch: 0106 train_loss= 0.12453 train_acc= 0.97687 val_loss= 0.24216 val_acc= 0.93109 time= 0.18300
Epoch: 0107 train_loss= 0.12255 train_acc= 0.97823 val_loss= 0.24363 val_acc= 0.93262 time= 0.16897
Epoch: 0108 train_loss= 0.11683 train_acc= 0.97857 val_loss= 0.24558 val_acc= 0.93262 time= 0.17400
Epoch: 0109 train_loss= 0.11723 train_acc= 0.97823 val_loss= 0.24645 val_acc= 0.92802 time= 0.18900
Early stopping...
Optimization Finished!
Test set results: cost= 0.27583 accuracy= 0.93302 time= 0.07603
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     0.3333    1.0000    0.5000         1
           3     0.8235    0.9333    0.8750        75
           4     1.0000    1.0000    1.0000         9
           5     0.8211    0.8966    0.8571        87
           6     0.9200    0.9200    0.9200        25
           7     0.8000    0.9231    0.8571        13
           8     0.7692    0.9091    0.8333        11
           9     1.0000    0.4444    0.6154         9
          10     0.9200    0.6389    0.7541        36
          11     1.0000    0.9167    0.9565        12
          12     0.8440    0.9835    0.9084       121
          13     0.8824    0.7895    0.8333        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.5000    0.6667         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.9000    0.9474        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.7857    0.6471    0.7097        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9615    0.9684    0.9649       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6000    0.9000    0.7200        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8193    0.8395    0.8293        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9764    0.9917    0.9840      1083
          40     0.6667    0.8000    0.7273         5
          41     0.0000    0.0000    0.0000         2
          42     0.8750    0.7778    0.8235         9
          43     1.0000    0.3333    0.5000         3
          44     0.8000    0.6667    0.7273        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9330      2568
   macro avg     0.7305    0.6366    0.6553      2568
weighted avg     0.9307    0.9330    0.9276      2568

Macro average Test Precision, Recall and F1-Score...
(0.7305429759617401, 0.6366366616029258, 0.6552628111656909, None)
Micro average Test Precision, Recall and F1-Score...
(0.9330218068535826, 0.9330218068535826, 0.9330218068535826, None)
embeddings:
8892 6532 2568
[[ 0.02279546  1.3249726  -0.14534508 ...  0.0466433  -0.02410391
  -0.14600772]
 [ 0.24421489  0.64305276 -0.00912246 ...  0.18477586  0.04339052
  -0.04656537]
 [ 0.14736108  0.17281187  0.01151681 ...  0.11466295  0.09973322
   0.87812465]
 ...
 [ 0.03642198  0.07821316  0.04213539 ...  0.04314169  0.29503992
   0.2985194 ]
 [ 0.06242843  0.1668732   0.04932843 ...  0.06336307  0.12266768
   0.4101069 ]
 [ 0.2310701   0.29585716  0.24787775 ...  0.2285412   0.26288375
   0.2967979 ]]

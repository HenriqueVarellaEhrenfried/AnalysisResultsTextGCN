(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.00850 val_loss= 3.90205 val_acc= 0.63553 time= 0.44901
Epoch: 0002 train_loss= 3.90194 train_acc= 0.62085 val_loss= 3.80693 val_acc= 0.60949 time= 0.18100
Epoch: 0003 train_loss= 3.80822 train_acc= 0.60231 val_loss= 3.65881 val_acc= 0.59877 time= 0.18900
Epoch: 0004 train_loss= 3.65993 train_acc= 0.59449 val_loss= 3.45757 val_acc= 0.58959 time= 0.16704
Epoch: 0005 train_loss= 3.46200 train_acc= 0.58139 val_loss= 3.21245 val_acc= 0.58193 time= 0.18396
Epoch: 0006 train_loss= 3.21856 train_acc= 0.57595 val_loss= 2.94659 val_acc= 0.57734 time= 0.16600
Epoch: 0007 train_loss= 2.94429 train_acc= 0.57051 val_loss= 2.68992 val_acc= 0.56202 time= 0.16903
Epoch: 0008 train_loss= 2.69406 train_acc= 0.56268 val_loss= 2.47541 val_acc= 0.55896 time= 0.18729
Epoch: 0009 train_loss= 2.47193 train_acc= 0.56370 val_loss= 2.32821 val_acc= 0.57734 time= 0.17019
Epoch: 0010 train_loss= 2.33407 train_acc= 0.56880 val_loss= 2.23900 val_acc= 0.61562 time= 0.18981
Epoch: 0011 train_loss= 2.24254 train_acc= 0.60674 val_loss= 2.17778 val_acc= 0.64012 time= 0.17000
Epoch: 0012 train_loss= 2.18728 train_acc= 0.63072 val_loss= 2.12534 val_acc= 0.47473 time= 0.16813
Epoch: 0013 train_loss= 2.13802 train_acc= 0.44923 val_loss= 2.06780 val_acc= 0.45789 time= 0.18503
Epoch: 0014 train_loss= 2.08927 train_acc= 0.43443 val_loss= 1.99649 val_acc= 0.45636 time= 0.16701
Epoch: 0015 train_loss= 2.01900 train_acc= 0.43375 val_loss= 1.91172 val_acc= 0.46248 time= 0.16996
Epoch: 0016 train_loss= 1.93603 train_acc= 0.43698 val_loss= 1.82301 val_acc= 0.48392 time= 0.16904
Epoch: 0017 train_loss= 1.84910 train_acc= 0.45535 val_loss= 1.74215 val_acc= 0.54824 time= 0.16833
Epoch: 0018 train_loss= 1.77491 train_acc= 0.54601 val_loss= 1.67476 val_acc= 0.64165 time= 0.17000
Epoch: 0019 train_loss= 1.70296 train_acc= 0.63140 val_loss= 1.61797 val_acc= 0.67381 time= 0.19500
Epoch: 0020 train_loss= 1.64183 train_acc= 0.65266 val_loss= 1.56555 val_acc= 0.67994 time= 0.16803
Epoch: 0021 train_loss= 1.58764 train_acc= 0.65164 val_loss= 1.51339 val_acc= 0.66922 time= 0.16700
Epoch: 0022 train_loss= 1.53445 train_acc= 0.65198 val_loss= 1.46093 val_acc= 0.66922 time= 0.17700
Epoch: 0023 train_loss= 1.48513 train_acc= 0.65640 val_loss= 1.40943 val_acc= 0.67688 time= 0.16714
Epoch: 0024 train_loss= 1.43051 train_acc= 0.66032 val_loss= 1.36061 val_acc= 0.68913 time= 0.16897
Epoch: 0025 train_loss= 1.38606 train_acc= 0.67222 val_loss= 1.31546 val_acc= 0.69678 time= 0.18800
Epoch: 0026 train_loss= 1.33726 train_acc= 0.68243 val_loss= 1.27435 val_acc= 0.71516 time= 0.17000
Epoch: 0027 train_loss= 1.29956 train_acc= 0.69315 val_loss= 1.23701 val_acc= 0.72129 time= 0.17300
Epoch: 0028 train_loss= 1.25992 train_acc= 0.71186 val_loss= 1.20264 val_acc= 0.73201 time= 0.18703
Epoch: 0029 train_loss= 1.22864 train_acc= 0.72444 val_loss= 1.17027 val_acc= 0.73813 time= 0.16900
Epoch: 0030 train_loss= 1.19205 train_acc= 0.73975 val_loss= 1.13891 val_acc= 0.73813 time= 0.17082
Epoch: 0031 train_loss= 1.15860 train_acc= 0.74860 val_loss= 1.10792 val_acc= 0.74579 time= 0.18500
Epoch: 0032 train_loss= 1.12338 train_acc= 0.76084 val_loss= 1.07700 val_acc= 0.75038 time= 0.16901
Epoch: 0033 train_loss= 1.09280 train_acc= 0.76561 val_loss= 1.04630 val_acc= 0.76417 time= 0.18499
Epoch: 0034 train_loss= 1.05923 train_acc= 0.77224 val_loss= 1.01612 val_acc= 0.77642 time= 0.16947
Epoch: 0035 train_loss= 1.02869 train_acc= 0.77887 val_loss= 0.98687 val_acc= 0.77948 time= 0.17000
Epoch: 0036 train_loss= 1.00297 train_acc= 0.78466 val_loss= 0.95891 val_acc= 0.78867 time= 0.19300
Epoch: 0037 train_loss= 0.97304 train_acc= 0.79401 val_loss= 0.93222 val_acc= 0.79939 time= 0.16703
Epoch: 0038 train_loss= 0.94514 train_acc= 0.79946 val_loss= 0.90653 val_acc= 0.80551 time= 0.16819
Epoch: 0039 train_loss= 0.92012 train_acc= 0.81153 val_loss= 0.88171 val_acc= 0.81011 time= 0.19500
Epoch: 0040 train_loss= 0.89051 train_acc= 0.81664 val_loss= 0.85748 val_acc= 0.81317 time= 0.16734
Epoch: 0041 train_loss= 0.86827 train_acc= 0.82361 val_loss= 0.83380 val_acc= 0.82389 time= 0.17203
Epoch: 0042 train_loss= 0.84140 train_acc= 0.83160 val_loss= 0.81052 val_acc= 0.83308 time= 0.18597
Epoch: 0043 train_loss= 0.81774 train_acc= 0.83262 val_loss= 0.78762 val_acc= 0.83614 time= 0.17333
Epoch: 0044 train_loss= 0.79241 train_acc= 0.83790 val_loss= 0.76514 val_acc= 0.84074 time= 0.17337
Epoch: 0045 train_loss= 0.77021 train_acc= 0.84402 val_loss= 0.74286 val_acc= 0.84533 time= 0.18804
Epoch: 0046 train_loss= 0.74878 train_acc= 0.84623 val_loss= 0.72125 val_acc= 0.84839 time= 0.16802
Epoch: 0047 train_loss= 0.71677 train_acc= 0.85031 val_loss= 0.70012 val_acc= 0.85299 time= 0.16895
Epoch: 0048 train_loss= 0.70078 train_acc= 0.85491 val_loss= 0.67945 val_acc= 0.85605 time= 0.17804
Epoch: 0049 train_loss= 0.67756 train_acc= 0.85661 val_loss= 0.65947 val_acc= 0.85758 time= 0.16711
Epoch: 0050 train_loss= 0.65281 train_acc= 0.86086 val_loss= 0.63990 val_acc= 0.86371 time= 0.18724
Epoch: 0051 train_loss= 0.63512 train_acc= 0.86886 val_loss= 0.62101 val_acc= 0.86830 time= 0.16900
Epoch: 0052 train_loss= 0.61706 train_acc= 0.87022 val_loss= 0.60299 val_acc= 0.86983 time= 0.17000
Epoch: 0053 train_loss= 0.59336 train_acc= 0.87481 val_loss= 0.58552 val_acc= 0.87443 time= 0.19300
Epoch: 0054 train_loss= 0.57126 train_acc= 0.87770 val_loss= 0.56881 val_acc= 0.87443 time= 0.16700
Epoch: 0055 train_loss= 0.55366 train_acc= 0.88178 val_loss= 0.55269 val_acc= 0.87749 time= 0.16799
Epoch: 0056 train_loss= 0.54005 train_acc= 0.88450 val_loss= 0.53730 val_acc= 0.87749 time= 0.17700
Epoch: 0057 train_loss= 0.52412 train_acc= 0.88638 val_loss= 0.52241 val_acc= 0.88055 time= 0.16700
Epoch: 0058 train_loss= 0.50098 train_acc= 0.88995 val_loss= 0.50801 val_acc= 0.88208 time= 0.16900
Epoch: 0059 train_loss= 0.48848 train_acc= 0.89403 val_loss= 0.49430 val_acc= 0.88055 time= 0.18400
Epoch: 0060 train_loss= 0.46490 train_acc= 0.89658 val_loss= 0.48116 val_acc= 0.88208 time= 0.17000
Epoch: 0061 train_loss= 0.46171 train_acc= 0.89998 val_loss= 0.46832 val_acc= 0.88208 time= 0.17100
Epoch: 0062 train_loss= 0.44629 train_acc= 0.90066 val_loss= 0.45584 val_acc= 0.88821 time= 0.19200
Epoch: 0063 train_loss= 0.42444 train_acc= 0.90900 val_loss= 0.44342 val_acc= 0.88974 time= 0.16700
Epoch: 0064 train_loss= 0.41299 train_acc= 0.91257 val_loss= 0.43099 val_acc= 0.89280 time= 0.17106
Epoch: 0065 train_loss= 0.40009 train_acc= 0.91563 val_loss= 0.41905 val_acc= 0.89740 time= 0.18453
Epoch: 0066 train_loss= 0.38570 train_acc= 0.91427 val_loss= 0.40790 val_acc= 0.89740 time= 0.16801
Epoch: 0067 train_loss= 0.37104 train_acc= 0.92159 val_loss= 0.39736 val_acc= 0.89893 time= 0.17095
Epoch: 0068 train_loss= 0.36193 train_acc= 0.92414 val_loss= 0.38759 val_acc= 0.90046 time= 0.17700
Epoch: 0069 train_loss= 0.35214 train_acc= 0.92533 val_loss= 0.37878 val_acc= 0.89893 time= 0.16900
Epoch: 0070 train_loss= 0.33893 train_acc= 0.92890 val_loss= 0.37074 val_acc= 0.90046 time= 0.17200
Epoch: 0071 train_loss= 0.32858 train_acc= 0.93094 val_loss= 0.36323 val_acc= 0.90199 time= 0.18605
Epoch: 0072 train_loss= 0.31741 train_acc= 0.93570 val_loss= 0.35602 val_acc= 0.90505 time= 0.16895
Epoch: 0073 train_loss= 0.30669 train_acc= 0.93638 val_loss= 0.34886 val_acc= 0.90505 time= 0.18803
Epoch: 0074 train_loss= 0.29362 train_acc= 0.94047 val_loss= 0.34191 val_acc= 0.90199 time= 0.16701
Epoch: 0075 train_loss= 0.28725 train_acc= 0.94234 val_loss= 0.33453 val_acc= 0.90505 time= 0.16800
Epoch: 0076 train_loss= 0.28017 train_acc= 0.94506 val_loss= 0.32726 val_acc= 0.90812 time= 0.18995
Epoch: 0077 train_loss= 0.26951 train_acc= 0.94693 val_loss= 0.32035 val_acc= 0.90965 time= 0.17000
Epoch: 0078 train_loss= 0.25540 train_acc= 0.95067 val_loss= 0.31390 val_acc= 0.91271 time= 0.17000
Epoch: 0079 train_loss= 0.24962 train_acc= 0.94965 val_loss= 0.30855 val_acc= 0.91577 time= 0.16904
Epoch: 0080 train_loss= 0.24600 train_acc= 0.95220 val_loss= 0.30368 val_acc= 0.91884 time= 0.16701
Epoch: 0081 train_loss= 0.23429 train_acc= 0.95577 val_loss= 0.29928 val_acc= 0.91730 time= 0.16940
Epoch: 0082 train_loss= 0.22972 train_acc= 0.95475 val_loss= 0.29544 val_acc= 0.91730 time= 0.18500
Epoch: 0083 train_loss= 0.21779 train_acc= 0.95782 val_loss= 0.29146 val_acc= 0.92037 time= 0.16800
Epoch: 0084 train_loss= 0.21110 train_acc= 0.95901 val_loss= 0.28715 val_acc= 0.92037 time= 0.17000
Epoch: 0085 train_loss= 0.20732 train_acc= 0.95918 val_loss= 0.28314 val_acc= 0.92037 time= 0.18700
Epoch: 0086 train_loss= 0.20227 train_acc= 0.95969 val_loss= 0.27879 val_acc= 0.92190 time= 0.17101
Epoch: 0087 train_loss= 0.19277 train_acc= 0.96496 val_loss= 0.27447 val_acc= 0.92649 time= 0.17200
Epoch: 0088 train_loss= 0.18764 train_acc= 0.96360 val_loss= 0.27061 val_acc= 0.92496 time= 0.18800
Epoch: 0089 train_loss= 0.17967 train_acc= 0.96513 val_loss= 0.26769 val_acc= 0.92496 time= 0.16708
Epoch: 0090 train_loss= 0.17596 train_acc= 0.96530 val_loss= 0.26444 val_acc= 0.92649 time= 0.16800
Epoch: 0091 train_loss= 0.17029 train_acc= 0.96768 val_loss= 0.26090 val_acc= 0.92802 time= 0.17097
Epoch: 0092 train_loss= 0.16635 train_acc= 0.96632 val_loss= 0.25802 val_acc= 0.93109 time= 0.16600
Epoch: 0093 train_loss= 0.15850 train_acc= 0.97108 val_loss= 0.25495 val_acc= 0.93262 time= 0.17000
Epoch: 0094 train_loss= 0.15552 train_acc= 0.97108 val_loss= 0.25320 val_acc= 0.93262 time= 0.19000
Epoch: 0095 train_loss= 0.14677 train_acc= 0.97329 val_loss= 0.25070 val_acc= 0.93262 time= 0.17000
Epoch: 0096 train_loss= 0.14350 train_acc= 0.97227 val_loss= 0.24796 val_acc= 0.93109 time= 0.19100
Epoch: 0097 train_loss= 0.14020 train_acc= 0.97415 val_loss= 0.24572 val_acc= 0.93109 time= 0.16703
Epoch: 0098 train_loss= 0.13481 train_acc= 0.97517 val_loss= 0.24335 val_acc= 0.93415 time= 0.16997
Epoch: 0099 train_loss= 0.12881 train_acc= 0.97976 val_loss= 0.24126 val_acc= 0.93415 time= 0.18900
Epoch: 0100 train_loss= 0.13010 train_acc= 0.97585 val_loss= 0.23934 val_acc= 0.93415 time= 0.16803
Epoch: 0101 train_loss= 0.12127 train_acc= 0.97993 val_loss= 0.23790 val_acc= 0.93415 time= 0.16797
Epoch: 0102 train_loss= 0.12034 train_acc= 0.97959 val_loss= 0.23670 val_acc= 0.93415 time= 0.17100
Epoch: 0103 train_loss= 0.11512 train_acc= 0.97993 val_loss= 0.23449 val_acc= 0.93721 time= 0.17100
Epoch: 0104 train_loss= 0.11444 train_acc= 0.97891 val_loss= 0.23317 val_acc= 0.93721 time= 0.18600
Epoch: 0105 train_loss= 0.10745 train_acc= 0.98197 val_loss= 0.23159 val_acc= 0.93874 time= 0.16800
Epoch: 0106 train_loss= 0.10596 train_acc= 0.98112 val_loss= 0.23047 val_acc= 0.93721 time= 0.16684
Epoch: 0107 train_loss= 0.10168 train_acc= 0.98503 val_loss= 0.22913 val_acc= 0.93874 time= 0.18800
Epoch: 0108 train_loss= 0.09656 train_acc= 0.98469 val_loss= 0.22826 val_acc= 0.93874 time= 0.16911
Epoch: 0109 train_loss= 0.09885 train_acc= 0.98435 val_loss= 0.22702 val_acc= 0.93568 time= 0.16904
Epoch: 0110 train_loss= 0.09346 train_acc= 0.98605 val_loss= 0.22568 val_acc= 0.93568 time= 0.16996
Epoch: 0111 train_loss= 0.09137 train_acc= 0.98418 val_loss= 0.22522 val_acc= 0.93568 time= 0.18900
Epoch: 0112 train_loss= 0.08907 train_acc= 0.98469 val_loss= 0.22505 val_acc= 0.93568 time= 0.17000
Epoch: 0113 train_loss= 0.08464 train_acc= 0.98707 val_loss= 0.22581 val_acc= 0.93721 time= 0.18903
Epoch: 0114 train_loss= 0.08354 train_acc= 0.98622 val_loss= 0.22589 val_acc= 0.93568 time= 0.17197
Epoch: 0115 train_loss= 0.08092 train_acc= 0.98758 val_loss= 0.22541 val_acc= 0.93721 time= 0.16803
Epoch: 0116 train_loss= 0.07864 train_acc= 0.98792 val_loss= 0.22376 val_acc= 0.93874 time= 0.19068
Epoch: 0117 train_loss= 0.07759 train_acc= 0.98792 val_loss= 0.22251 val_acc= 0.93721 time= 0.16700
Epoch: 0118 train_loss= 0.07310 train_acc= 0.99064 val_loss= 0.22087 val_acc= 0.93874 time= 0.16700
Epoch: 0119 train_loss= 0.07297 train_acc= 0.98979 val_loss= 0.21930 val_acc= 0.93721 time= 0.18200
Epoch: 0120 train_loss= 0.06969 train_acc= 0.99047 val_loss= 0.21815 val_acc= 0.94028 time= 0.17000
Epoch: 0121 train_loss= 0.06812 train_acc= 0.98996 val_loss= 0.21732 val_acc= 0.94181 time= 0.17000
Epoch: 0122 train_loss= 0.06671 train_acc= 0.98809 val_loss= 0.21757 val_acc= 0.94487 time= 0.18800
Epoch: 0123 train_loss= 0.06345 train_acc= 0.98996 val_loss= 0.21798 val_acc= 0.94181 time= 0.16700
Epoch: 0124 train_loss= 0.06208 train_acc= 0.99013 val_loss= 0.21815 val_acc= 0.93874 time= 0.16800
Epoch: 0125 train_loss= 0.06154 train_acc= 0.99098 val_loss= 0.21936 val_acc= 0.93721 time= 0.19000
Epoch: 0126 train_loss= 0.06159 train_acc= 0.99133 val_loss= 0.21983 val_acc= 0.93568 time= 0.16800
Early stopping...
Optimization Finished!
Test set results: cost= 0.25325 accuracy= 0.93653 time= 0.07445
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7802    0.9467    0.8554        75
           4     1.0000    1.0000    1.0000         9
           5     0.8020    0.9310    0.8617        87
           6     0.9200    0.9200    0.9200        25
           7     0.8000    0.9231    0.8571        13
           8     0.9091    0.9091    0.9091        11
           9     1.0000    0.5556    0.7143         9
          10     0.8889    0.6667    0.7619        36
          11     1.0000    0.9167    0.9565        12
          12     0.8623    0.9835    0.9189       121
          13     0.9333    0.7368    0.8235        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.7273    0.8421        11
          29     0.9640    0.9612    0.9626       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    0.6667    0.8000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8462    0.8148    0.8302        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    1.0000    1.0000         4
          38     0.0000    0.0000    0.0000         1
          39     0.9763    0.9908    0.9835      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9000    0.7500    0.8182        12
          45     0.5000    0.1667    0.2500         6
          46     1.0000    0.2857    0.4444         7
          47     0.8125    0.8667    0.8387        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9365      2568
   macro avg     0.7550    0.6792    0.6951      2568
weighted avg     0.9340    0.9365    0.9319      2568

Macro average Test Precision, Recall and F1-Score...
(0.75504027512404, 0.679210824567101, 0.6951404769678196, None)
Micro average Test Precision, Recall and F1-Score...
(0.9365264797507789, 0.9365264797507789, 0.9365264797507789, None)
embeddings:
8892 6532 2568
[[-3.2995917e-02  2.0623907e-01  1.1648153e-02 ...  2.3812500e-03
  -1.3201539e-01  1.5332805e+00]
 [ 4.1750133e-02  4.0541500e-02 -3.0123653e-02 ...  6.8751678e-02
   1.5689564e-01  8.9487487e-01]
 [-1.7961716e-02  1.9565237e-01  3.7225905e-01 ...  1.8788403e-01
  -1.1105530e-03  7.9089218e-01]
 ...
 [ 4.6498515e-02  2.6036955e-02  2.0129909e-01 ...  3.8528776e-01
   2.1394718e-02  6.4915627e-02]
 [ 6.2498700e-02  1.2287793e-01  1.5845978e-01 ...  1.5178141e-01
   3.3719990e-02  3.4802404e-01]
 [ 3.5727790e-01  2.4372166e-01  2.4307500e-01 ...  2.0683609e-01
   2.5661045e-01  2.4587367e-01]]

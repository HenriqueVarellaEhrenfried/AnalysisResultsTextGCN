(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95119 train_acc= 0.11720 val_loss= 3.92627 val_acc= 0.45636 time= 0.37203
Epoch: 0002 train_loss= 3.92769 train_acc= 0.43273 val_loss= 3.88644 val_acc= 0.45636 time= 0.13197
Epoch: 0003 train_loss= 3.89073 train_acc= 0.43222 val_loss= 3.82989 val_acc= 0.45636 time= 0.13203
Epoch: 0004 train_loss= 3.82949 train_acc= 0.43239 val_loss= 3.75547 val_acc= 0.45636 time= 0.15497
Epoch: 0005 train_loss= 3.75641 train_acc= 0.43239 val_loss= 3.66224 val_acc= 0.45636 time= 0.14200
Epoch: 0006 train_loss= 3.67085 train_acc= 0.43239 val_loss= 3.55026 val_acc= 0.45636 time= 0.13000
Epoch: 0007 train_loss= 3.55881 train_acc= 0.43239 val_loss= 3.42080 val_acc= 0.45636 time= 0.13200
Epoch: 0008 train_loss= 3.43502 train_acc= 0.43239 val_loss= 3.27672 val_acc= 0.45636 time= 0.13400
Epoch: 0009 train_loss= 3.27796 train_acc= 0.43239 val_loss= 3.12218 val_acc= 0.45636 time= 0.15700
Epoch: 0010 train_loss= 3.12875 train_acc= 0.43239 val_loss= 2.96302 val_acc= 0.45636 time= 0.13400
Epoch: 0011 train_loss= 2.98731 train_acc= 0.43239 val_loss= 2.80579 val_acc= 0.45636 time= 0.13400
Epoch: 0012 train_loss= 2.80740 train_acc= 0.43239 val_loss= 2.65688 val_acc= 0.45636 time= 0.15703
Epoch: 0013 train_loss= 2.67705 train_acc= 0.43239 val_loss= 2.52437 val_acc= 0.45636 time= 0.12997
Epoch: 0014 train_loss= 2.55995 train_acc= 0.43239 val_loss= 2.41553 val_acc= 0.45636 time= 0.13205
Epoch: 0015 train_loss= 2.43525 train_acc= 0.43239 val_loss= 2.33365 val_acc= 0.45636 time= 0.15603
Epoch: 0016 train_loss= 2.35178 train_acc= 0.43239 val_loss= 2.27675 val_acc= 0.45636 time= 0.12997
Epoch: 0017 train_loss= 2.28355 train_acc= 0.43239 val_loss= 2.23775 val_acc= 0.45636 time= 0.13103
Epoch: 0018 train_loss= 2.24527 train_acc= 0.43239 val_loss= 2.20832 val_acc= 0.45636 time= 0.13405
Epoch: 0019 train_loss= 2.21961 train_acc= 0.43222 val_loss= 2.18100 val_acc= 0.45636 time= 0.15502
Epoch: 0020 train_loss= 2.18763 train_acc= 0.43239 val_loss= 2.15104 val_acc= 0.45636 time= 0.14289
Epoch: 0021 train_loss= 2.18109 train_acc= 0.43239 val_loss= 2.11643 val_acc= 0.45636 time= 0.13297
Epoch: 0022 train_loss= 2.12236 train_acc= 0.43256 val_loss= 2.07703 val_acc= 0.45636 time= 0.13403
Epoch: 0023 train_loss= 2.13605 train_acc= 0.43375 val_loss= 2.03362 val_acc= 0.45636 time= 0.15800
Epoch: 0024 train_loss= 2.06876 train_acc= 0.43715 val_loss= 1.98820 val_acc= 0.46708 time= 0.13101
Epoch: 0025 train_loss= 2.01193 train_acc= 0.45790 val_loss= 1.94253 val_acc= 0.48392 time= 0.13000
Epoch: 0026 train_loss= 1.96454 train_acc= 0.48444 val_loss= 1.89801 val_acc= 0.50077 time= 0.15400
Epoch: 0027 train_loss= 1.91453 train_acc= 0.49906 val_loss= 1.85570 val_acc= 0.51455 time= 0.13701
Epoch: 0028 train_loss= 1.88032 train_acc= 0.50383 val_loss= 1.81623 val_acc= 0.52680 time= 0.13099
Epoch: 0029 train_loss= 1.84862 train_acc= 0.54006 val_loss= 1.77925 val_acc= 0.53752 time= 0.13000
Epoch: 0030 train_loss= 1.80614 train_acc= 0.52849 val_loss= 1.74447 val_acc= 0.54211 time= 0.15697
Epoch: 0031 train_loss= 1.77700 train_acc= 0.55707 val_loss= 1.71099 val_acc= 0.55283 time= 0.13393
Epoch: 0032 train_loss= 1.72781 train_acc= 0.53717 val_loss= 1.67829 val_acc= 0.56968 time= 0.13400
Epoch: 0033 train_loss= 1.69862 train_acc= 0.55452 val_loss= 1.64607 val_acc= 0.58959 time= 0.13600
Epoch: 0034 train_loss= 1.67044 train_acc= 0.56846 val_loss= 1.61433 val_acc= 0.61868 time= 0.15497
Epoch: 0035 train_loss= 1.66829 train_acc= 0.61507 val_loss= 1.58279 val_acc= 0.62940 time= 0.13103
Epoch: 0036 train_loss= 1.61916 train_acc= 0.62307 val_loss= 1.55180 val_acc= 0.63706 time= 0.13101
Epoch: 0037 train_loss= 1.58102 train_acc= 0.61609 val_loss= 1.52179 val_acc= 0.64778 time= 0.15500
Epoch: 0038 train_loss= 1.55159 train_acc= 0.62443 val_loss= 1.49300 val_acc= 0.65697 time= 0.13000
Epoch: 0039 train_loss= 1.52788 train_acc= 0.64501 val_loss= 1.46539 val_acc= 0.66769 time= 0.13200
Epoch: 0040 train_loss= 1.50420 train_acc= 0.65198 val_loss= 1.43892 val_acc= 0.67228 time= 0.13304
Epoch: 0041 train_loss= 1.48188 train_acc= 0.64841 val_loss= 1.41357 val_acc= 0.67994 time= 0.15500
Epoch: 0042 train_loss= 1.43837 train_acc= 0.66134 val_loss= 1.38923 val_acc= 0.68913 time= 0.13300
Epoch: 0043 train_loss= 1.42248 train_acc= 0.65947 val_loss= 1.36575 val_acc= 0.68606 time= 0.13300
Epoch: 0044 train_loss= 1.38662 train_acc= 0.67001 val_loss= 1.34297 val_acc= 0.68913 time= 0.13600
Epoch: 0045 train_loss= 1.38147 train_acc= 0.66678 val_loss= 1.32073 val_acc= 0.69372 time= 0.15600
Epoch: 0046 train_loss= 1.34359 train_acc= 0.67869 val_loss= 1.29901 val_acc= 0.69985 time= 0.13107
Epoch: 0047 train_loss= 1.33192 train_acc= 0.68260 val_loss= 1.27772 val_acc= 0.70904 time= 0.13097
Epoch: 0048 train_loss= 1.31302 train_acc= 0.69127 val_loss= 1.25679 val_acc= 0.71210 time= 0.15500
Epoch: 0049 train_loss= 1.28120 train_acc= 0.69706 val_loss= 1.23627 val_acc= 0.72129 time= 0.13000
Epoch: 0050 train_loss= 1.26393 train_acc= 0.70947 val_loss= 1.21623 val_acc= 0.72435 time= 0.13103
Epoch: 0051 train_loss= 1.24383 train_acc= 0.71169 val_loss= 1.19666 val_acc= 0.72741 time= 0.13497
Epoch: 0052 train_loss= 1.22071 train_acc= 0.72019 val_loss= 1.17764 val_acc= 0.73201 time= 0.15800
Epoch: 0053 train_loss= 1.21050 train_acc= 0.72478 val_loss= 1.15917 val_acc= 0.73966 time= 0.13400
Epoch: 0054 train_loss= 1.18404 train_acc= 0.73108 val_loss= 1.14106 val_acc= 0.73966 time= 0.13400
Epoch: 0055 train_loss= 1.16165 train_acc= 0.73924 val_loss= 1.12332 val_acc= 0.73966 time= 0.15900
Epoch: 0056 train_loss= 1.14341 train_acc= 0.73652 val_loss= 1.10585 val_acc= 0.74579 time= 0.13265
Epoch: 0057 train_loss= 1.12852 train_acc= 0.74366 val_loss= 1.08861 val_acc= 0.74732 time= 0.13201
Epoch: 0058 train_loss= 1.10519 train_acc= 0.74979 val_loss= 1.07153 val_acc= 0.74885 time= 0.13299
Epoch: 0059 train_loss= 1.09724 train_acc= 0.74741 val_loss= 1.05469 val_acc= 0.75498 time= 0.15197
Epoch: 0060 train_loss= 1.08081 train_acc= 0.75132 val_loss= 1.03812 val_acc= 0.75651 time= 0.13100
Epoch: 0061 train_loss= 1.05718 train_acc= 0.76118 val_loss= 1.02166 val_acc= 0.76570 time= 0.13203
Epoch: 0062 train_loss= 1.04495 train_acc= 0.75829 val_loss= 1.00537 val_acc= 0.76723 time= 0.15400
Epoch: 0063 train_loss= 1.02216 train_acc= 0.76561 val_loss= 0.98935 val_acc= 0.76876 time= 0.13896
Epoch: 0064 train_loss= 1.01478 train_acc= 0.76680 val_loss= 0.97353 val_acc= 0.77335 time= 0.13400
Epoch: 0065 train_loss= 0.98791 train_acc= 0.77530 val_loss= 0.95792 val_acc= 0.78101 time= 0.13400
Epoch: 0066 train_loss= 0.97692 train_acc= 0.77683 val_loss= 0.94243 val_acc= 0.78101 time= 0.15900
Epoch: 0067 train_loss= 0.96492 train_acc= 0.78449 val_loss= 0.92721 val_acc= 0.78407 time= 0.13403
Epoch: 0068 train_loss= 0.95123 train_acc= 0.77836 val_loss= 0.91221 val_acc= 0.78867 time= 0.12997
Epoch: 0069 train_loss= 0.92361 train_acc= 0.78330 val_loss= 0.89743 val_acc= 0.79479 time= 0.15510
Epoch: 0070 train_loss= 0.91645 train_acc= 0.79350 val_loss= 0.88287 val_acc= 0.79632 time= 0.13500
Epoch: 0071 train_loss= 0.90622 train_acc= 0.79146 val_loss= 0.86850 val_acc= 0.80092 time= 0.13100
Epoch: 0072 train_loss= 0.88357 train_acc= 0.80898 val_loss= 0.85427 val_acc= 0.80858 time= 0.13100
Epoch: 0073 train_loss= 0.86457 train_acc= 0.80915 val_loss= 0.84019 val_acc= 0.81776 time= 0.15400
Epoch: 0074 train_loss= 0.85627 train_acc= 0.81187 val_loss= 0.82625 val_acc= 0.81930 time= 0.13505
Epoch: 0075 train_loss= 0.83983 train_acc= 0.81970 val_loss= 0.81264 val_acc= 0.82542 time= 0.13400
Epoch: 0076 train_loss= 0.82990 train_acc= 0.81936 val_loss= 0.79924 val_acc= 0.82542 time= 0.13650
Epoch: 0077 train_loss= 0.81825 train_acc= 0.82174 val_loss= 0.78610 val_acc= 0.83002 time= 0.15634
Epoch: 0078 train_loss= 0.81277 train_acc= 0.82276 val_loss= 0.77281 val_acc= 0.83308 time= 0.13198
Epoch: 0079 train_loss= 0.78318 train_acc= 0.83007 val_loss= 0.75973 val_acc= 0.83308 time= 0.13113
Epoch: 0080 train_loss= 0.77595 train_acc= 0.82956 val_loss= 0.74697 val_acc= 0.83461 time= 0.15200
Epoch: 0081 train_loss= 0.76391 train_acc= 0.83382 val_loss= 0.73421 val_acc= 0.83614 time= 0.13101
Epoch: 0082 train_loss= 0.73805 train_acc= 0.84045 val_loss= 0.72159 val_acc= 0.83920 time= 0.13199
Epoch: 0083 train_loss= 0.72953 train_acc= 0.83279 val_loss= 0.70925 val_acc= 0.84074 time= 0.15600
Epoch: 0084 train_loss= 0.72560 train_acc= 0.83620 val_loss= 0.69734 val_acc= 0.84533 time= 0.13801
Epoch: 0085 train_loss= 0.70493 train_acc= 0.84623 val_loss= 0.68571 val_acc= 0.84992 time= 0.13299
Epoch: 0086 train_loss= 0.69533 train_acc= 0.84895 val_loss= 0.67452 val_acc= 0.85299 time= 0.13401
Epoch: 0087 train_loss= 0.68669 train_acc= 0.84725 val_loss= 0.66386 val_acc= 0.85452 time= 0.13600
Epoch: 0088 train_loss= 0.66351 train_acc= 0.84997 val_loss= 0.65360 val_acc= 0.86064 time= 0.15700
Epoch: 0089 train_loss= 0.66701 train_acc= 0.85406 val_loss= 0.64353 val_acc= 0.86371 time= 0.13200
Epoch: 0090 train_loss= 0.64192 train_acc= 0.86290 val_loss= 0.63337 val_acc= 0.86371 time= 0.13200
Epoch: 0091 train_loss= 0.63839 train_acc= 0.85848 val_loss= 0.62319 val_acc= 0.86217 time= 0.15600
Epoch: 0092 train_loss= 0.64090 train_acc= 0.86120 val_loss= 0.61312 val_acc= 0.86217 time= 0.13100
Epoch: 0093 train_loss= 0.63236 train_acc= 0.85746 val_loss= 0.60270 val_acc= 0.86677 time= 0.13200
Epoch: 0094 train_loss= 0.60228 train_acc= 0.86647 val_loss= 0.59280 val_acc= 0.86830 time= 0.13303
Epoch: 0095 train_loss= 0.61311 train_acc= 0.86426 val_loss= 0.58297 val_acc= 0.86830 time= 0.15200
Epoch: 0096 train_loss= 0.59233 train_acc= 0.86273 val_loss= 0.57380 val_acc= 0.87289 time= 0.13297
Epoch: 0097 train_loss= 0.57179 train_acc= 0.87328 val_loss= 0.56469 val_acc= 0.87289 time= 0.13300
Epoch: 0098 train_loss= 0.56816 train_acc= 0.86920 val_loss= 0.55597 val_acc= 0.87443 time= 0.14639
Epoch: 0099 train_loss= 0.56063 train_acc= 0.87209 val_loss= 0.54765 val_acc= 0.87749 time= 0.13300
Epoch: 0100 train_loss= 0.56000 train_acc= 0.87226 val_loss= 0.53980 val_acc= 0.87749 time= 0.13200
Epoch: 0101 train_loss= 0.53561 train_acc= 0.87753 val_loss= 0.53195 val_acc= 0.87902 time= 0.13203
Epoch: 0102 train_loss= 0.52620 train_acc= 0.88263 val_loss= 0.52431 val_acc= 0.87902 time= 0.15199
Epoch: 0103 train_loss= 0.54385 train_acc= 0.87328 val_loss= 0.51673 val_acc= 0.88361 time= 0.13000
Epoch: 0104 train_loss= 0.51612 train_acc= 0.87566 val_loss= 0.50945 val_acc= 0.88515 time= 0.13104
Epoch: 0105 train_loss= 0.51253 train_acc= 0.88161 val_loss= 0.50288 val_acc= 0.88821 time= 0.15103
Epoch: 0106 train_loss= 0.49844 train_acc= 0.88995 val_loss= 0.49646 val_acc= 0.88821 time= 0.13474
Epoch: 0107 train_loss= 0.50085 train_acc= 0.88961 val_loss= 0.49018 val_acc= 0.88361 time= 0.13342
Epoch: 0108 train_loss= 0.49311 train_acc= 0.88876 val_loss= 0.48436 val_acc= 0.88361 time= 0.13500
Epoch: 0109 train_loss= 0.48604 train_acc= 0.88876 val_loss= 0.47845 val_acc= 0.88361 time= 0.16000
Epoch: 0110 train_loss= 0.48233 train_acc= 0.89199 val_loss= 0.47224 val_acc= 0.88515 time= 0.13500
Epoch: 0111 train_loss= 0.47510 train_acc= 0.89131 val_loss= 0.46634 val_acc= 0.88668 time= 0.13000
Epoch: 0112 train_loss= 0.46617 train_acc= 0.89624 val_loss= 0.46082 val_acc= 0.88361 time= 0.13404
Epoch: 0113 train_loss= 0.45727 train_acc= 0.89658 val_loss= 0.45481 val_acc= 0.88821 time= 0.15300
Epoch: 0114 train_loss= 0.44885 train_acc= 0.89675 val_loss= 0.44871 val_acc= 0.88974 time= 0.13096
Epoch: 0115 train_loss= 0.44761 train_acc= 0.89607 val_loss= 0.44288 val_acc= 0.89127 time= 0.13000
Epoch: 0116 train_loss= 0.43787 train_acc= 0.89709 val_loss= 0.43751 val_acc= 0.89280 time= 0.15500
Epoch: 0117 train_loss= 0.44237 train_acc= 0.89675 val_loss= 0.43255 val_acc= 0.89587 time= 0.13100
Epoch: 0118 train_loss= 0.42900 train_acc= 0.90032 val_loss= 0.42809 val_acc= 0.89740 time= 0.13500
Epoch: 0119 train_loss= 0.43641 train_acc= 0.89658 val_loss= 0.42380 val_acc= 0.89740 time= 0.13816
Epoch: 0120 train_loss= 0.41113 train_acc= 0.90202 val_loss= 0.42027 val_acc= 0.89893 time= 0.15400
Epoch: 0121 train_loss= 0.40744 train_acc= 0.90185 val_loss= 0.41774 val_acc= 0.89740 time= 0.13400
Epoch: 0122 train_loss= 0.41452 train_acc= 0.90917 val_loss= 0.41538 val_acc= 0.89740 time= 0.13011
Epoch: 0123 train_loss= 0.40030 train_acc= 0.91206 val_loss= 0.41214 val_acc= 0.89893 time= 0.13399
Epoch: 0124 train_loss= 0.41390 train_acc= 0.90560 val_loss= 0.40804 val_acc= 0.89893 time= 0.15700
Epoch: 0125 train_loss= 0.40035 train_acc= 0.90815 val_loss= 0.40369 val_acc= 0.89587 time= 0.13030
Epoch: 0126 train_loss= 0.40236 train_acc= 0.90917 val_loss= 0.39934 val_acc= 0.89893 time= 0.13102
Epoch: 0127 train_loss= 0.40257 train_acc= 0.90900 val_loss= 0.39521 val_acc= 0.90046 time= 0.15598
Epoch: 0128 train_loss= 0.39056 train_acc= 0.90985 val_loss= 0.39139 val_acc= 0.90046 time= 0.13000
Epoch: 0129 train_loss= 0.39167 train_acc= 0.90594 val_loss= 0.38808 val_acc= 0.89893 time= 0.13500
Epoch: 0130 train_loss= 0.37902 train_acc= 0.91155 val_loss= 0.38547 val_acc= 0.89893 time= 0.15700
Epoch: 0131 train_loss= 0.37481 train_acc= 0.91563 val_loss= 0.38271 val_acc= 0.89893 time= 0.13406
Epoch: 0132 train_loss= 0.37455 train_acc= 0.91189 val_loss= 0.38019 val_acc= 0.89893 time= 0.13300
Epoch: 0133 train_loss= 0.36657 train_acc= 0.91614 val_loss= 0.37720 val_acc= 0.90046 time= 0.13300
Epoch: 0134 train_loss= 0.35950 train_acc= 0.91614 val_loss= 0.37415 val_acc= 0.90505 time= 0.15300
Epoch: 0135 train_loss= 0.35868 train_acc= 0.91563 val_loss= 0.37159 val_acc= 0.90658 time= 0.13000
Epoch: 0136 train_loss= 0.36586 train_acc= 0.90968 val_loss= 0.36935 val_acc= 0.90199 time= 0.13100
Epoch: 0137 train_loss= 0.35354 train_acc= 0.91614 val_loss= 0.36724 val_acc= 0.90352 time= 0.13400
Epoch: 0138 train_loss= 0.34087 train_acc= 0.92329 val_loss= 0.36469 val_acc= 0.90352 time= 0.15108
Epoch: 0139 train_loss= 0.34317 train_acc= 0.91869 val_loss= 0.36209 val_acc= 0.90199 time= 0.13191
Epoch: 0140 train_loss= 0.32802 train_acc= 0.92465 val_loss= 0.35957 val_acc= 0.90199 time= 0.13399
Epoch: 0141 train_loss= 0.33759 train_acc= 0.92108 val_loss= 0.35702 val_acc= 0.90199 time= 0.16000
Epoch: 0142 train_loss= 0.34040 train_acc= 0.92295 val_loss= 0.35456 val_acc= 0.90352 time= 0.13771
Epoch: 0143 train_loss= 0.34534 train_acc= 0.91971 val_loss= 0.35241 val_acc= 0.90505 time= 0.13400
Epoch: 0144 train_loss= 0.33184 train_acc= 0.92567 val_loss= 0.35008 val_acc= 0.90352 time= 0.13000
Epoch: 0145 train_loss= 0.32837 train_acc= 0.92397 val_loss= 0.34793 val_acc= 0.90199 time= 0.15500
Epoch: 0146 train_loss= 0.32064 train_acc= 0.92227 val_loss= 0.34549 val_acc= 0.90046 time= 0.13119
Epoch: 0147 train_loss= 0.32145 train_acc= 0.92499 val_loss= 0.34328 val_acc= 0.90046 time= 0.13001
Epoch: 0148 train_loss= 0.31738 train_acc= 0.92329 val_loss= 0.34113 val_acc= 0.90046 time= 0.15398
Epoch: 0149 train_loss= 0.30679 train_acc= 0.92516 val_loss= 0.33880 val_acc= 0.90046 time= 0.13200
Epoch: 0150 train_loss= 0.29681 train_acc= 0.92992 val_loss= 0.33611 val_acc= 0.90505 time= 0.13101
Epoch: 0151 train_loss= 0.30638 train_acc= 0.92686 val_loss= 0.33401 val_acc= 0.90658 time= 0.13418
Epoch: 0152 train_loss= 0.29631 train_acc= 0.93179 val_loss= 0.33212 val_acc= 0.90658 time= 0.16000
Epoch: 0153 train_loss= 0.30728 train_acc= 0.92737 val_loss= 0.33046 val_acc= 0.90965 time= 0.13364
Epoch: 0154 train_loss= 0.29788 train_acc= 0.92941 val_loss= 0.32918 val_acc= 0.90812 time= 0.13373
Epoch: 0155 train_loss= 0.29484 train_acc= 0.93553 val_loss= 0.32739 val_acc= 0.90965 time= 0.13500
Epoch: 0156 train_loss= 0.30036 train_acc= 0.92907 val_loss= 0.32575 val_acc= 0.90965 time= 0.15103
Epoch: 0157 train_loss= 0.27977 train_acc= 0.93536 val_loss= 0.32401 val_acc= 0.91118 time= 0.13402
Epoch: 0158 train_loss= 0.28147 train_acc= 0.93400 val_loss= 0.32187 val_acc= 0.91271 time= 0.13099
Epoch: 0159 train_loss= 0.28669 train_acc= 0.93400 val_loss= 0.31998 val_acc= 0.91118 time= 0.15599
Epoch: 0160 train_loss= 0.28674 train_acc= 0.93434 val_loss= 0.31833 val_acc= 0.91118 time= 0.13100
Epoch: 0161 train_loss= 0.27126 train_acc= 0.93808 val_loss= 0.31707 val_acc= 0.91118 time= 0.13201
Epoch: 0162 train_loss= 0.27840 train_acc= 0.93400 val_loss= 0.31517 val_acc= 0.91271 time= 0.13529
Epoch: 0163 train_loss= 0.28034 train_acc= 0.93536 val_loss= 0.31359 val_acc= 0.91424 time= 0.16067
Epoch: 0164 train_loss= 0.28432 train_acc= 0.93451 val_loss= 0.31190 val_acc= 0.91424 time= 0.13400
Epoch: 0165 train_loss= 0.27252 train_acc= 0.93485 val_loss= 0.30998 val_acc= 0.91424 time= 0.13303
Epoch: 0166 train_loss= 0.27195 train_acc= 0.93621 val_loss= 0.30812 val_acc= 0.91424 time= 0.13297
Epoch: 0167 train_loss= 0.25971 train_acc= 0.94200 val_loss= 0.30622 val_acc= 0.91577 time= 0.15103
Epoch: 0168 train_loss= 0.26246 train_acc= 0.94047 val_loss= 0.30524 val_acc= 0.91577 time= 0.13100
Epoch: 0169 train_loss= 0.25496 train_acc= 0.94047 val_loss= 0.30430 val_acc= 0.91577 time= 0.13000
Epoch: 0170 train_loss= 0.26209 train_acc= 0.93791 val_loss= 0.30332 val_acc= 0.91118 time= 0.14500
Epoch: 0171 train_loss= 0.24648 train_acc= 0.94404 val_loss= 0.30195 val_acc= 0.91424 time= 0.13000
Epoch: 0172 train_loss= 0.26444 train_acc= 0.93417 val_loss= 0.30091 val_acc= 0.91118 time= 0.13397
Epoch: 0173 train_loss= 0.25598 train_acc= 0.93962 val_loss= 0.29997 val_acc= 0.91271 time= 0.13400
Epoch: 0174 train_loss= 0.24657 train_acc= 0.93740 val_loss= 0.29844 val_acc= 0.91271 time= 0.16000
Epoch: 0175 train_loss= 0.24510 train_acc= 0.94489 val_loss= 0.29678 val_acc= 0.91271 time= 0.13300
Epoch: 0176 train_loss= 0.23365 train_acc= 0.94115 val_loss= 0.29538 val_acc= 0.91271 time= 0.13200
Epoch: 0177 train_loss= 0.24314 train_acc= 0.94489 val_loss= 0.29337 val_acc= 0.91577 time= 0.15613
Epoch: 0178 train_loss= 0.25070 train_acc= 0.94353 val_loss= 0.29153 val_acc= 0.92037 time= 0.13400
Epoch: 0179 train_loss= 0.23982 train_acc= 0.94353 val_loss= 0.28994 val_acc= 0.92037 time= 0.13100
Epoch: 0180 train_loss= 0.23572 train_acc= 0.94591 val_loss= 0.28842 val_acc= 0.92190 time= 0.13300
Epoch: 0181 train_loss= 0.24764 train_acc= 0.93894 val_loss= 0.28718 val_acc= 0.92190 time= 0.14600
Epoch: 0182 train_loss= 0.23850 train_acc= 0.94608 val_loss= 0.28589 val_acc= 0.92190 time= 0.13100
Epoch: 0183 train_loss= 0.24353 train_acc= 0.94166 val_loss= 0.28477 val_acc= 0.92190 time= 0.13300
Epoch: 0184 train_loss= 0.23498 train_acc= 0.94404 val_loss= 0.28339 val_acc= 0.92190 time= 0.13600
Epoch: 0185 train_loss= 0.22176 train_acc= 0.94778 val_loss= 0.28211 val_acc= 0.92190 time= 0.15700
Epoch: 0186 train_loss= 0.22221 train_acc= 0.94897 val_loss= 0.28076 val_acc= 0.92190 time= 0.13400
Epoch: 0187 train_loss= 0.21912 train_acc= 0.94846 val_loss= 0.27943 val_acc= 0.92343 time= 0.13300
Epoch: 0188 train_loss= 0.22356 train_acc= 0.94540 val_loss= 0.27864 val_acc= 0.92343 time= 0.15400
Epoch: 0189 train_loss= 0.22077 train_acc= 0.94880 val_loss= 0.27771 val_acc= 0.92496 time= 0.13105
Epoch: 0190 train_loss= 0.21880 train_acc= 0.94676 val_loss= 0.27665 val_acc= 0.92343 time= 0.13205
Epoch: 0191 train_loss= 0.20850 train_acc= 0.95407 val_loss= 0.27609 val_acc= 0.91884 time= 0.13399
Epoch: 0192 train_loss= 0.22308 train_acc= 0.94812 val_loss= 0.27595 val_acc= 0.91730 time= 0.15701
Epoch: 0193 train_loss= 0.21945 train_acc= 0.94421 val_loss= 0.27610 val_acc= 0.91730 time= 0.13101
Epoch: 0194 train_loss= 0.21476 train_acc= 0.95016 val_loss= 0.27612 val_acc= 0.91730 time= 0.13195
Epoch: 0195 train_loss= 0.21369 train_acc= 0.94982 val_loss= 0.27605 val_acc= 0.91730 time= 0.13700
Epoch: 0196 train_loss= 0.21071 train_acc= 0.94982 val_loss= 0.27562 val_acc= 0.91730 time= 0.15937
Epoch: 0197 train_loss= 0.20774 train_acc= 0.95152 val_loss= 0.27440 val_acc= 0.91730 time= 0.13400
Epoch: 0198 train_loss= 0.21426 train_acc= 0.94744 val_loss= 0.27315 val_acc= 0.92037 time= 0.13200
Epoch: 0199 train_loss= 0.21546 train_acc= 0.94846 val_loss= 0.27166 val_acc= 0.92190 time= 0.15500
Epoch: 0200 train_loss= 0.21223 train_acc= 0.94948 val_loss= 0.27027 val_acc= 0.92496 time= 0.13000
Epoch: 0201 train_loss= 0.19825 train_acc= 0.95560 val_loss= 0.26948 val_acc= 0.92190 time= 0.13000
Epoch: 0202 train_loss= 0.20066 train_acc= 0.95475 val_loss= 0.26894 val_acc= 0.92496 time= 0.13600
Epoch: 0203 train_loss= 0.19356 train_acc= 0.95356 val_loss= 0.26849 val_acc= 0.92496 time= 0.15100
Epoch: 0204 train_loss= 0.20547 train_acc= 0.94676 val_loss= 0.26802 val_acc= 0.92649 time= 0.13400
Epoch: 0205 train_loss= 0.19869 train_acc= 0.95407 val_loss= 0.26749 val_acc= 0.92190 time= 0.13200
Epoch: 0206 train_loss= 0.19795 train_acc= 0.95101 val_loss= 0.26741 val_acc= 0.92037 time= 0.15944
Epoch: 0207 train_loss= 0.19525 train_acc= 0.95237 val_loss= 0.26769 val_acc= 0.91730 time= 0.13400
Epoch: 0208 train_loss= 0.20050 train_acc= 0.95135 val_loss= 0.26709 val_acc= 0.92037 time= 0.13400
Epoch: 0209 train_loss= 0.19213 train_acc= 0.95356 val_loss= 0.26617 val_acc= 0.92037 time= 0.13700
Epoch: 0210 train_loss= 0.18914 train_acc= 0.95407 val_loss= 0.26452 val_acc= 0.91884 time= 0.15304
Epoch: 0211 train_loss= 0.18463 train_acc= 0.95680 val_loss= 0.26299 val_acc= 0.91884 time= 0.13105
Epoch: 0212 train_loss= 0.18614 train_acc= 0.95271 val_loss= 0.26132 val_acc= 0.92037 time= 0.13100
Epoch: 0213 train_loss= 0.19873 train_acc= 0.95543 val_loss= 0.26024 val_acc= 0.92649 time= 0.15600
Epoch: 0214 train_loss= 0.18574 train_acc= 0.95714 val_loss= 0.25915 val_acc= 0.92649 time= 0.13405
Epoch: 0215 train_loss= 0.18273 train_acc= 0.95509 val_loss= 0.25856 val_acc= 0.92802 time= 0.13115
Epoch: 0216 train_loss= 0.19030 train_acc= 0.95594 val_loss= 0.25800 val_acc= 0.92802 time= 0.13100
Epoch: 0217 train_loss= 0.17970 train_acc= 0.95271 val_loss= 0.25815 val_acc= 0.92496 time= 0.16200
Epoch: 0218 train_loss= 0.17913 train_acc= 0.95577 val_loss= 0.25838 val_acc= 0.92496 time= 0.13400
Epoch: 0219 train_loss= 0.18761 train_acc= 0.95577 val_loss= 0.25794 val_acc= 0.92649 time= 0.13400
Epoch: 0220 train_loss= 0.18242 train_acc= 0.95629 val_loss= 0.25727 val_acc= 0.92802 time= 0.13600
Epoch: 0221 train_loss= 0.17548 train_acc= 0.95765 val_loss= 0.25677 val_acc= 0.92496 time= 0.15000
Epoch: 0222 train_loss= 0.16331 train_acc= 0.96122 val_loss= 0.25636 val_acc= 0.92496 time= 0.13100
Epoch: 0223 train_loss= 0.17134 train_acc= 0.96173 val_loss= 0.25611 val_acc= 0.92343 time= 0.13100
Epoch: 0224 train_loss= 0.17218 train_acc= 0.95714 val_loss= 0.25572 val_acc= 0.92190 time= 0.15704
Epoch: 0225 train_loss= 0.17154 train_acc= 0.96020 val_loss= 0.25468 val_acc= 0.92343 time= 0.12999
Epoch: 0226 train_loss= 0.15817 train_acc= 0.96173 val_loss= 0.25329 val_acc= 0.92802 time= 0.13100
Epoch: 0227 train_loss= 0.17964 train_acc= 0.95441 val_loss= 0.25232 val_acc= 0.93109 time= 0.13627
Epoch: 0228 train_loss= 0.17619 train_acc= 0.95833 val_loss= 0.25208 val_acc= 0.92956 time= 0.15700
Epoch: 0229 train_loss= 0.16954 train_acc= 0.95731 val_loss= 0.25157 val_acc= 0.92802 time= 0.13543
Epoch: 0230 train_loss= 0.17120 train_acc= 0.96037 val_loss= 0.25091 val_acc= 0.93109 time= 0.13400
Epoch: 0231 train_loss= 0.17124 train_acc= 0.96105 val_loss= 0.25065 val_acc= 0.92956 time= 0.15800
Epoch: 0232 train_loss= 0.15421 train_acc= 0.96564 val_loss= 0.25068 val_acc= 0.92802 time= 0.13223
Epoch: 0233 train_loss= 0.17030 train_acc= 0.95646 val_loss= 0.25086 val_acc= 0.92649 time= 0.13195
Epoch: 0234 train_loss= 0.15640 train_acc= 0.96190 val_loss= 0.25075 val_acc= 0.92496 time= 0.13300
Epoch: 0235 train_loss= 0.15900 train_acc= 0.96156 val_loss= 0.25007 val_acc= 0.92649 time= 0.15604
Epoch: 0236 train_loss= 0.15883 train_acc= 0.96224 val_loss= 0.24940 val_acc= 0.92802 time= 0.13010
Epoch: 0237 train_loss= 0.16696 train_acc= 0.95901 val_loss= 0.24901 val_acc= 0.92802 time= 0.13599
Epoch: 0238 train_loss= 0.15548 train_acc= 0.96258 val_loss= 0.24867 val_acc= 0.92802 time= 0.13600
Epoch: 0239 train_loss= 0.15757 train_acc= 0.96122 val_loss= 0.24822 val_acc= 0.92649 time= 0.15187
Epoch: 0240 train_loss= 0.14887 train_acc= 0.96445 val_loss= 0.24710 val_acc= 0.92649 time= 0.13436
Epoch: 0241 train_loss= 0.15188 train_acc= 0.96683 val_loss= 0.24615 val_acc= 0.92956 time= 0.13344
Epoch: 0242 train_loss= 0.15174 train_acc= 0.96292 val_loss= 0.24499 val_acc= 0.92802 time= 0.15800
Epoch: 0243 train_loss= 0.16228 train_acc= 0.96173 val_loss= 0.24367 val_acc= 0.93109 time= 0.13100
Epoch: 0244 train_loss= 0.14814 train_acc= 0.96734 val_loss= 0.24243 val_acc= 0.93109 time= 0.13116
Epoch: 0245 train_loss= 0.15308 train_acc= 0.96173 val_loss= 0.24140 val_acc= 0.93109 time= 0.15300
Epoch: 0246 train_loss= 0.14832 train_acc= 0.97074 val_loss= 0.24057 val_acc= 0.93262 time= 0.13097
Epoch: 0247 train_loss= 0.15120 train_acc= 0.96496 val_loss= 0.24016 val_acc= 0.93568 time= 0.13303
Epoch: 0248 train_loss= 0.15131 train_acc= 0.96428 val_loss= 0.24072 val_acc= 0.93262 time= 0.13500
Epoch: 0249 train_loss= 0.15140 train_acc= 0.96394 val_loss= 0.24185 val_acc= 0.93415 time= 0.15697
Epoch: 0250 train_loss= 0.13454 train_acc= 0.96734 val_loss= 0.24284 val_acc= 0.93415 time= 0.13500
Epoch: 0251 train_loss= 0.14648 train_acc= 0.96700 val_loss= 0.24383 val_acc= 0.93415 time= 0.13400
Early stopping...
Optimization Finished!
Test set results: cost= 0.29096 accuracy= 0.92757 time= 0.06300
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7500    0.9600    0.8421        75
           4     1.0000    1.0000    1.0000         9
           5     0.7980    0.9080    0.8495        87
           6     0.9583    0.9200    0.9388        25
           7     0.7857    0.8462    0.8148        13
           8     0.5833    0.6364    0.6087        11
           9     0.6667    0.2222    0.3333         9
          10     0.8571    0.6667    0.7500        36
          11     1.0000    0.9167    0.9565        12
          12     0.8741    0.9752    0.9219       121
          13     0.7500    0.7895    0.7692        19
          14     0.8276    0.8571    0.8421        28
          15     0.0000    0.0000    0.0000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.8000    0.8889        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.8095    0.8500    0.8293        20
          22     0.2857    0.4000    0.3333         5
          23     0.0000    0.0000    0.0000         1
          24     0.7059    0.7059    0.7059        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.6364    0.7778        11
          29     0.9547    0.9684    0.9615       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    0.6667    0.8000         3
          32     0.4762    1.0000    0.6452        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8816    0.8272    0.8535        81
          36     1.0000    0.2500    0.4000        12
          37     1.0000    0.5000    0.6667         4
          38     0.0000    0.0000    0.0000         1
          39     0.9799    0.9908    0.9853      1083
          40     1.0000    0.6000    0.7500         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     0.0000    0.0000    0.0000         3
          44     0.8182    0.7500    0.7826        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.1429    0.2500         7
          47     0.7368    0.9333    0.8235        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     0.7500    0.7500    0.7500         4

    accuracy                         0.9276      2568
   macro avg     0.6772    0.5867    0.6011      2568
weighted avg     0.9234    0.9276    0.9201      2568

Macro average Test Precision, Recall and F1-Score...
(0.6772306521875571, 0.5866971514835044, 0.6011376134877567, None)
Micro average Test Precision, Recall and F1-Score...
(0.927570093457944, 0.927570093457944, 0.927570093457944, None)
embeddings:
8892 6532 2568
[[-0.15350784  2.6702178  -0.18951416 ... -0.25026914 -0.23678845
   0.17111781]
 [ 0.37331858  0.8405649   0.3934656  ...  0.30205014  0.36211747
   0.01277999]
 [ 0.6360738   0.18752867  0.11646216 ...  0.2458124   0.36169475
   0.4516507 ]
 ...
 [ 0.36882737  0.5416842   0.3280721  ...  0.20446445  0.07880856
   0.2949976 ]
 [ 0.26511973  0.31636116  0.12774329 ...  0.16106997  0.17105432
   0.21664762]
 [ 0.758675    0.6952346   0.5429788  ...  0.52337676  0.6296637
   0.3627857 ]]

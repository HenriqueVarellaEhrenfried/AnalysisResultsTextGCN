(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95130 train_acc= 0.01021 val_loss= 3.89773 val_acc= 0.66769 time= 0.46621
Epoch: 0002 train_loss= 3.89908 train_acc= 0.64246 val_loss= 3.79821 val_acc= 0.66769 time= 0.18000
Epoch: 0003 train_loss= 3.80006 train_acc= 0.64365 val_loss= 3.64593 val_acc= 0.67075 time= 0.17400
Epoch: 0004 train_loss= 3.65097 train_acc= 0.64637 val_loss= 3.44159 val_acc= 0.67075 time= 0.16664
Epoch: 0005 train_loss= 3.44926 train_acc= 0.64569 val_loss= 3.19517 val_acc= 0.66922 time= 0.16700
Epoch: 0006 train_loss= 3.19163 train_acc= 0.64331 val_loss= 2.92868 val_acc= 0.66769 time= 0.16996
Epoch: 0007 train_loss= 2.92262 train_acc= 0.64314 val_loss= 2.67267 val_acc= 0.66616 time= 0.18303
Epoch: 0008 train_loss= 2.67150 train_acc= 0.64076 val_loss= 2.46065 val_acc= 0.66309 time= 0.16902
Epoch: 0009 train_loss= 2.46665 train_acc= 0.64263 val_loss= 2.31850 val_acc= 0.66922 time= 0.17349
Epoch: 0010 train_loss= 2.30756 train_acc= 0.64212 val_loss= 2.23908 val_acc= 0.63400 time= 0.18000
Epoch: 0011 train_loss= 2.26559 train_acc= 0.60367 val_loss= 2.19164 val_acc= 0.50995 time= 0.17001
Epoch: 0012 train_loss= 2.19451 train_acc= 0.50366 val_loss= 2.14799 val_acc= 0.46861 time= 0.18504
Epoch: 0013 train_loss= 2.15194 train_acc= 0.44429 val_loss= 2.09285 val_acc= 0.46248 time= 0.16999
Epoch: 0014 train_loss= 2.10872 train_acc= 0.43817 val_loss= 2.02037 val_acc= 0.46248 time= 0.16700
Epoch: 0015 train_loss= 2.04146 train_acc= 0.43783 val_loss= 1.93382 val_acc= 0.47167 time= 0.17097
Epoch: 0016 train_loss= 1.96031 train_acc= 0.44514 val_loss= 1.84365 val_acc= 0.50383 time= 0.18503
Epoch: 0017 train_loss= 1.86436 train_acc= 0.48699 val_loss= 1.76205 val_acc= 0.57427 time= 0.16700
Epoch: 0018 train_loss= 1.77846 train_acc= 0.55877 val_loss= 1.69529 val_acc= 0.64778 time= 0.17197
Epoch: 0019 train_loss= 1.71664 train_acc= 0.63871 val_loss= 1.64024 val_acc= 0.67381 time= 0.17600
Epoch: 0020 train_loss= 1.66305 train_acc= 0.65249 val_loss= 1.58926 val_acc= 0.67994 time= 0.17000
Epoch: 0021 train_loss= 1.62764 train_acc= 0.65130 val_loss= 1.53670 val_acc= 0.67381 time= 0.17100
Epoch: 0022 train_loss= 1.56007 train_acc= 0.66015 val_loss= 1.48307 val_acc= 0.67688 time= 0.17503
Epoch: 0023 train_loss= 1.50905 train_acc= 0.66134 val_loss= 1.43018 val_acc= 0.68453 time= 0.16643
Epoch: 0024 train_loss= 1.46652 train_acc= 0.66797 val_loss= 1.38007 val_acc= 0.69219 time= 0.18514
Epoch: 0025 train_loss= 1.40887 train_acc= 0.67494 val_loss= 1.33426 val_acc= 0.69985 time= 0.16801
Epoch: 0026 train_loss= 1.35918 train_acc= 0.68447 val_loss= 1.29311 val_acc= 0.71210 time= 0.16696
Epoch: 0027 train_loss= 1.32199 train_acc= 0.69604 val_loss= 1.25594 val_acc= 0.71822 time= 0.19368
Epoch: 0028 train_loss= 1.28663 train_acc= 0.70352 val_loss= 1.22185 val_acc= 0.72435 time= 0.17100
Epoch: 0029 train_loss= 1.24452 train_acc= 0.71679 val_loss= 1.18986 val_acc= 0.72894 time= 0.17004
Epoch: 0030 train_loss= 1.21314 train_acc= 0.72495 val_loss= 1.15916 val_acc= 0.73354 time= 0.16700
Epoch: 0031 train_loss= 1.18157 train_acc= 0.73159 val_loss= 1.12915 val_acc= 0.73660 time= 0.16801
Epoch: 0032 train_loss= 1.14775 train_acc= 0.74009 val_loss= 1.09953 val_acc= 0.74119 time= 0.16699
Epoch: 0033 train_loss= 1.12434 train_acc= 0.74996 val_loss= 1.07019 val_acc= 0.75038 time= 0.19000
Epoch: 0034 train_loss= 1.08715 train_acc= 0.75251 val_loss= 1.04126 val_acc= 0.75498 time= 0.16610
Epoch: 0035 train_loss= 1.05457 train_acc= 0.75965 val_loss= 1.01287 val_acc= 0.76417 time= 0.16932
Epoch: 0036 train_loss= 1.03064 train_acc= 0.76578 val_loss= 0.98537 val_acc= 0.77642 time= 0.19200
Epoch: 0037 train_loss= 0.99528 train_acc= 0.77717 val_loss= 0.95874 val_acc= 0.78407 time= 0.17061
Epoch: 0038 train_loss= 0.96863 train_acc= 0.78585 val_loss= 0.93280 val_acc= 0.79479 time= 0.17223
Epoch: 0039 train_loss= 0.94340 train_acc= 0.79792 val_loss= 0.90758 val_acc= 0.80704 time= 0.18400
Epoch: 0040 train_loss= 0.91754 train_acc= 0.80626 val_loss= 0.88292 val_acc= 0.81164 time= 0.16800
Epoch: 0041 train_loss= 0.89351 train_acc= 0.81306 val_loss= 0.85885 val_acc= 0.82389 time= 0.17300
Epoch: 0042 train_loss= 0.86874 train_acc= 0.81800 val_loss= 0.83518 val_acc= 0.82848 time= 0.17300
Epoch: 0043 train_loss= 0.84421 train_acc= 0.82616 val_loss= 0.81203 val_acc= 0.83461 time= 0.16804
Epoch: 0044 train_loss= 0.82442 train_acc= 0.83143 val_loss= 0.78945 val_acc= 0.84074 time= 0.17303
Epoch: 0045 train_loss= 0.79254 train_acc= 0.83756 val_loss= 0.76746 val_acc= 0.84533 time= 0.17800
Epoch: 0046 train_loss= 0.77589 train_acc= 0.84045 val_loss= 0.74593 val_acc= 0.84839 time= 0.17000
Epoch: 0047 train_loss= 0.75121 train_acc= 0.84419 val_loss= 0.72498 val_acc= 0.85299 time= 0.18404
Epoch: 0048 train_loss= 0.72500 train_acc= 0.84946 val_loss= 0.70459 val_acc= 0.85605 time= 0.16799
Epoch: 0049 train_loss= 0.70312 train_acc= 0.85202 val_loss= 0.68481 val_acc= 0.85605 time= 0.16701
Epoch: 0050 train_loss= 0.68356 train_acc= 0.85712 val_loss= 0.66538 val_acc= 0.85911 time= 0.18800
Epoch: 0051 train_loss= 0.66500 train_acc= 0.85831 val_loss= 0.64650 val_acc= 0.86371 time= 0.16800
Epoch: 0052 train_loss= 0.64653 train_acc= 0.86120 val_loss= 0.62831 val_acc= 0.86524 time= 0.16800
Epoch: 0053 train_loss= 0.62401 train_acc= 0.86596 val_loss= 0.61082 val_acc= 0.86677 time= 0.17700
Epoch: 0054 train_loss= 0.60461 train_acc= 0.86800 val_loss= 0.59392 val_acc= 0.87136 time= 0.17001
Epoch: 0055 train_loss= 0.58792 train_acc= 0.87158 val_loss= 0.57765 val_acc= 0.87443 time= 0.16797
Epoch: 0056 train_loss= 0.56496 train_acc= 0.87685 val_loss= 0.56204 val_acc= 0.87596 time= 0.18703
Epoch: 0057 train_loss= 0.55248 train_acc= 0.88127 val_loss= 0.54704 val_acc= 0.87749 time= 0.16700
Epoch: 0058 train_loss= 0.52997 train_acc= 0.88110 val_loss= 0.53287 val_acc= 0.87749 time= 0.16707
Epoch: 0059 train_loss= 0.52312 train_acc= 0.88161 val_loss= 0.51956 val_acc= 0.87902 time= 0.19000
Epoch: 0060 train_loss= 0.49621 train_acc= 0.88706 val_loss= 0.50686 val_acc= 0.88055 time= 0.17001
Epoch: 0061 train_loss= 0.48506 train_acc= 0.88706 val_loss= 0.49444 val_acc= 0.88055 time= 0.17000
Epoch: 0062 train_loss= 0.47319 train_acc= 0.89029 val_loss= 0.48219 val_acc= 0.88208 time= 0.19000
Epoch: 0063 train_loss= 0.45556 train_acc= 0.89335 val_loss= 0.46958 val_acc= 0.88668 time= 0.17100
Epoch: 0064 train_loss= 0.45191 train_acc= 0.89726 val_loss= 0.45718 val_acc= 0.89127 time= 0.16900
Epoch: 0065 train_loss= 0.42765 train_acc= 0.90253 val_loss= 0.44551 val_acc= 0.89127 time= 0.16701
Epoch: 0066 train_loss= 0.41906 train_acc= 0.90662 val_loss= 0.43475 val_acc= 0.89433 time= 0.16999
Epoch: 0067 train_loss= 0.40681 train_acc= 0.90934 val_loss= 0.42475 val_acc= 0.89587 time= 0.17400
Epoch: 0068 train_loss= 0.39456 train_acc= 0.91087 val_loss= 0.41510 val_acc= 0.89587 time= 0.18201
Epoch: 0069 train_loss= 0.38886 train_acc= 0.91563 val_loss= 0.40566 val_acc= 0.89893 time= 0.17087
Epoch: 0070 train_loss= 0.36772 train_acc= 0.92142 val_loss= 0.39743 val_acc= 0.90199 time= 0.19200
Epoch: 0071 train_loss= 0.36815 train_acc= 0.91801 val_loss= 0.38984 val_acc= 0.90199 time= 0.17000
Epoch: 0072 train_loss= 0.34872 train_acc= 0.92363 val_loss= 0.38203 val_acc= 0.90352 time= 0.16900
Epoch: 0073 train_loss= 0.33614 train_acc= 0.92822 val_loss= 0.37470 val_acc= 0.90352 time= 0.18910
Epoch: 0074 train_loss= 0.33216 train_acc= 0.92346 val_loss= 0.36758 val_acc= 0.90505 time= 0.16605
Epoch: 0075 train_loss= 0.32141 train_acc= 0.93383 val_loss= 0.36028 val_acc= 0.90505 time= 0.16700
Epoch: 0076 train_loss= 0.31030 train_acc= 0.93009 val_loss= 0.35321 val_acc= 0.90658 time= 0.16703
Epoch: 0077 train_loss= 0.30029 train_acc= 0.93366 val_loss= 0.34692 val_acc= 0.91118 time= 0.16700
Epoch: 0078 train_loss= 0.29436 train_acc= 0.93621 val_loss= 0.34072 val_acc= 0.91424 time= 0.17292
Epoch: 0079 train_loss= 0.27786 train_acc= 0.94574 val_loss= 0.33483 val_acc= 0.91424 time= 0.18900
Epoch: 0080 train_loss= 0.28350 train_acc= 0.94098 val_loss= 0.32902 val_acc= 0.90965 time= 0.16900
Epoch: 0081 train_loss= 0.26966 train_acc= 0.94455 val_loss= 0.32339 val_acc= 0.91118 time= 0.16900
Epoch: 0082 train_loss= 0.25633 train_acc= 0.94472 val_loss= 0.31784 val_acc= 0.91118 time= 0.16813
Epoch: 0083 train_loss= 0.25292 train_acc= 0.94540 val_loss= 0.31231 val_acc= 0.91118 time= 0.16844
Epoch: 0084 train_loss= 0.24520 train_acc= 0.94744 val_loss= 0.30694 val_acc= 0.91271 time= 0.17094
Epoch: 0085 train_loss= 0.24175 train_acc= 0.95271 val_loss= 0.30143 val_acc= 0.91730 time= 0.18500
Epoch: 0086 train_loss= 0.23330 train_acc= 0.95305 val_loss= 0.29770 val_acc= 0.92190 time= 0.17000
Epoch: 0087 train_loss= 0.22823 train_acc= 0.95322 val_loss= 0.29456 val_acc= 0.92037 time= 0.17300
Epoch: 0088 train_loss= 0.21745 train_acc= 0.95782 val_loss= 0.29179 val_acc= 0.91884 time= 0.18100
Epoch: 0089 train_loss= 0.21489 train_acc= 0.95731 val_loss= 0.28885 val_acc= 0.92037 time= 0.16800
Epoch: 0090 train_loss= 0.20922 train_acc= 0.95765 val_loss= 0.28541 val_acc= 0.92496 time= 0.17103
Epoch: 0091 train_loss= 0.19699 train_acc= 0.95969 val_loss= 0.28281 val_acc= 0.92496 time= 0.18401
Epoch: 0092 train_loss= 0.19492 train_acc= 0.96275 val_loss= 0.28018 val_acc= 0.92496 time= 0.16701
Epoch: 0093 train_loss= 0.19072 train_acc= 0.96020 val_loss= 0.27763 val_acc= 0.92496 time= 0.18395
Epoch: 0094 train_loss= 0.18115 train_acc= 0.96326 val_loss= 0.27419 val_acc= 0.92496 time= 0.16800
Epoch: 0095 train_loss= 0.17697 train_acc= 0.96598 val_loss= 0.27081 val_acc= 0.92649 time= 0.17000
Epoch: 0096 train_loss= 0.17730 train_acc= 0.96496 val_loss= 0.26702 val_acc= 0.92802 time= 0.19300
Epoch: 0097 train_loss= 0.16812 train_acc= 0.96292 val_loss= 0.26332 val_acc= 0.93109 time= 0.17100
Epoch: 0098 train_loss= 0.16783 train_acc= 0.96853 val_loss= 0.26031 val_acc= 0.93262 time= 0.17100
Epoch: 0099 train_loss= 0.16108 train_acc= 0.97006 val_loss= 0.25790 val_acc= 0.93262 time= 0.18705
Epoch: 0100 train_loss= 0.15724 train_acc= 0.96972 val_loss= 0.25593 val_acc= 0.93415 time= 0.16805
Epoch: 0101 train_loss= 0.14987 train_acc= 0.97227 val_loss= 0.25484 val_acc= 0.93415 time= 0.17103
Epoch: 0102 train_loss= 0.14764 train_acc= 0.97227 val_loss= 0.25455 val_acc= 0.93262 time= 0.18401
Epoch: 0103 train_loss= 0.14384 train_acc= 0.97346 val_loss= 0.25441 val_acc= 0.93262 time= 0.16896
Epoch: 0104 train_loss= 0.13960 train_acc= 0.97261 val_loss= 0.25193 val_acc= 0.93262 time= 0.17382
Epoch: 0105 train_loss= 0.14335 train_acc= 0.97074 val_loss= 0.24877 val_acc= 0.92956 time= 0.19100
Epoch: 0106 train_loss= 0.13363 train_acc= 0.97568 val_loss= 0.24576 val_acc= 0.93262 time= 0.16868
Epoch: 0107 train_loss= 0.12955 train_acc= 0.97551 val_loss= 0.24346 val_acc= 0.93415 time= 0.16999
Epoch: 0108 train_loss= 0.12568 train_acc= 0.97619 val_loss= 0.24199 val_acc= 0.93415 time= 0.18397
Epoch: 0109 train_loss= 0.12073 train_acc= 0.97755 val_loss= 0.24084 val_acc= 0.93874 time= 0.16700
Epoch: 0110 train_loss= 0.12122 train_acc= 0.97908 val_loss= 0.24005 val_acc= 0.93568 time= 0.18500
Epoch: 0111 train_loss= 0.11448 train_acc= 0.97925 val_loss= 0.24104 val_acc= 0.93415 time= 0.16700
Epoch: 0112 train_loss= 0.11540 train_acc= 0.97925 val_loss= 0.24171 val_acc= 0.93262 time= 0.16929
Epoch: 0113 train_loss= 0.10890 train_acc= 0.98010 val_loss= 0.24010 val_acc= 0.93262 time= 0.19300
Epoch: 0114 train_loss= 0.10917 train_acc= 0.97874 val_loss= 0.23739 val_acc= 0.93415 time= 0.17000
Epoch: 0115 train_loss= 0.10561 train_acc= 0.97857 val_loss= 0.23521 val_acc= 0.93415 time= 0.16834
Epoch: 0116 train_loss= 0.10137 train_acc= 0.98265 val_loss= 0.23443 val_acc= 0.93568 time= 0.16800
Epoch: 0117 train_loss= 0.10446 train_acc= 0.97908 val_loss= 0.23270 val_acc= 0.93874 time= 0.16800
Epoch: 0118 train_loss= 0.09459 train_acc= 0.98435 val_loss= 0.23088 val_acc= 0.93721 time= 0.17100
Epoch: 0119 train_loss= 0.09447 train_acc= 0.98112 val_loss= 0.22907 val_acc= 0.93874 time= 0.18400
Epoch: 0120 train_loss= 0.09240 train_acc= 0.98350 val_loss= 0.22963 val_acc= 0.93568 time= 0.16700
Epoch: 0121 train_loss= 0.09017 train_acc= 0.98605 val_loss= 0.23257 val_acc= 0.93262 time= 0.19300
Epoch: 0122 train_loss= 0.09149 train_acc= 0.98299 val_loss= 0.23512 val_acc= 0.93262 time= 0.17500
Early stopping...
Optimization Finished!
Test set results: cost= 0.26240 accuracy= 0.93692 time= 0.07500
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.8161    0.9467    0.8765        75
           4     1.0000    1.0000    1.0000         9
           5     0.7615    0.9540    0.8469        87
           6     0.9200    0.9200    0.9200        25
           7     0.8571    0.9231    0.8889        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.6667    0.8000         9
          10     0.8750    0.5833    0.7000        36
          11     1.0000    0.9167    0.9565        12
          12     0.8440    0.9835    0.9084       121
          13     0.9375    0.7895    0.8571        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7647    0.7647    0.7647        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9000    0.7500    0.8182        12
          28     1.0000    0.8182    0.9000        11
          29     0.9642    0.9684    0.9663       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    0.6667    0.8000         3
          32     0.7692    1.0000    0.8696        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8986    0.7654    0.8267        81
          36     0.8333    0.4167    0.5556        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9908    0.9849      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.8889    0.6667    0.7619        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7372    0.6799    0.6913      2568
weighted avg     0.9331    0.9369    0.9317      2568

Macro average Test Precision, Recall and F1-Score...
(0.7372446030201361, 0.6798683958531642, 0.6912747838742122, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[-0.07886509 -0.14478289 -0.12060248 ...  0.15288967  0.00800799
   0.07786465]
 [ 0.14369708  0.17517455  0.08587877 ... -0.00623828  0.18796197
   0.02124804]
 [-0.00273042  0.11551211  0.04797691 ...  0.17283668  0.18271995
   0.05740061]
 ...
 [ 0.01040509  0.046583    0.18321511 ...  0.11710232  0.13008696
   0.07732326]
 [ 0.02803195  0.07116305  0.06246994 ...  0.17372936  0.09330053
   0.06469613]
 [ 0.26852196  0.2553963   0.23111218 ...  0.28568897  0.2568417
   0.2389259 ]]

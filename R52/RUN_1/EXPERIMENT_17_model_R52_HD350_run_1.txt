(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95122 train_acc= 0.00850 val_loss= 3.87480 val_acc= 0.65391 time= 2.37404
Epoch: 0002 train_loss= 3.87508 train_acc= 0.65198 val_loss= 3.71432 val_acc= 0.64778 time= 2.23000
Epoch: 0003 train_loss= 3.71981 train_acc= 0.64246 val_loss= 3.46136 val_acc= 0.63553 time= 2.23500
Epoch: 0004 train_loss= 3.46447 train_acc= 0.62119 val_loss= 3.13143 val_acc= 0.60490 time= 2.22701
Epoch: 0005 train_loss= 3.12881 train_acc= 0.59891 val_loss= 2.77512 val_acc= 0.57274 time= 2.25099
Epoch: 0006 train_loss= 2.77957 train_acc= 0.57017 val_loss= 2.46668 val_acc= 0.55130 time= 2.22652
Epoch: 0007 train_loss= 2.45306 train_acc= 0.53342 val_loss= 2.27613 val_acc= 0.52833 time= 2.22699
Epoch: 0008 train_loss= 2.26692 train_acc= 0.50604 val_loss= 2.19765 val_acc= 0.49617 time= 2.24302
Epoch: 0009 train_loss= 2.19262 train_acc= 0.47984 val_loss= 2.15887 val_acc= 0.47167 time= 2.24266
Epoch: 0010 train_loss= 2.17100 train_acc= 0.45008 val_loss= 2.10289 val_acc= 0.46707 time= 2.22601
Epoch: 0011 train_loss= 2.11660 train_acc= 0.44157 val_loss= 2.01271 val_acc= 0.47779 time= 2.23300
Epoch: 0012 train_loss= 2.03321 train_acc= 0.45450 val_loss= 1.89931 val_acc= 0.50995 time= 2.24999
Epoch: 0013 train_loss= 1.92041 train_acc= 0.49889 val_loss= 1.78701 val_acc= 0.58959 time= 2.23101
Epoch: 0014 train_loss= 1.81446 train_acc= 0.58241 val_loss= 1.69746 val_acc= 0.65237 time= 2.22800
Epoch: 0015 train_loss= 1.72934 train_acc= 0.64229 val_loss= 1.63064 val_acc= 0.67075 time= 2.23499
Epoch: 0016 train_loss= 1.65485 train_acc= 0.65079 val_loss= 1.57176 val_acc= 0.67534 time= 2.21700
Epoch: 0017 train_loss= 1.60706 train_acc= 0.65079 val_loss= 1.51057 val_acc= 0.67688 time= 2.24501
Epoch: 0018 train_loss= 1.53412 train_acc= 0.65964 val_loss= 1.44787 val_acc= 0.67688 time= 2.24702
Epoch: 0019 train_loss= 1.47592 train_acc= 0.66338 val_loss= 1.38763 val_acc= 0.68913 time= 2.23474
Epoch: 0020 train_loss= 1.41724 train_acc= 0.67069 val_loss= 1.33316 val_acc= 0.69678 time= 2.22672
Epoch: 0021 train_loss= 1.35706 train_acc= 0.68022 val_loss= 1.28527 val_acc= 0.69985 time= 2.24199
Epoch: 0022 train_loss= 1.31295 train_acc= 0.69042 val_loss= 1.24311 val_acc= 0.71516 time= 2.21300
Epoch: 0023 train_loss= 1.26422 train_acc= 0.69995 val_loss= 1.20520 val_acc= 0.71822 time= 2.23701
Epoch: 0024 train_loss= 1.22862 train_acc= 0.70998 val_loss= 1.17000 val_acc= 0.72741 time= 2.34800
Epoch: 0025 train_loss= 1.19061 train_acc= 0.72716 val_loss= 1.13637 val_acc= 0.73507 time= 2.25398
Epoch: 0026 train_loss= 1.15924 train_acc= 0.73669 val_loss= 1.10347 val_acc= 0.74273 time= 2.24598
Epoch: 0027 train_loss= 1.12360 train_acc= 0.75030 val_loss= 1.07086 val_acc= 0.74885 time= 2.23799
Epoch: 0028 train_loss= 1.08481 train_acc= 0.75812 val_loss= 1.03856 val_acc= 0.75804 time= 2.22101
Epoch: 0029 train_loss= 1.05497 train_acc= 0.76322 val_loss= 1.00666 val_acc= 0.76876 time= 2.24700
Epoch: 0030 train_loss= 1.01784 train_acc= 0.76884 val_loss= 0.97555 val_acc= 0.77182 time= 2.23999
Epoch: 0031 train_loss= 0.98978 train_acc= 0.77292 val_loss= 0.94549 val_acc= 0.77489 time= 2.23900
Epoch: 0032 train_loss= 0.95404 train_acc= 0.78330 val_loss= 0.91661 val_acc= 0.78407 time= 2.22601
Epoch: 0033 train_loss= 0.92932 train_acc= 0.79197 val_loss= 0.88876 val_acc= 0.80245 time= 2.24100
Epoch: 0034 train_loss= 0.90479 train_acc= 0.79690 val_loss= 0.86181 val_acc= 0.81317 time= 2.23405
Epoch: 0035 train_loss= 0.87666 train_acc= 0.80813 val_loss= 0.83558 val_acc= 0.82236 time= 2.24700
Epoch: 0036 train_loss= 0.84534 train_acc= 0.82259 val_loss= 0.81002 val_acc= 0.83461 time= 2.24401
Epoch: 0037 train_loss= 0.81941 train_acc= 0.82650 val_loss= 0.78511 val_acc= 0.84380 time= 2.25600
Epoch: 0038 train_loss= 0.79082 train_acc= 0.83960 val_loss= 0.76063 val_acc= 0.84839 time= 2.23700
Epoch: 0039 train_loss= 0.77240 train_acc= 0.83926 val_loss= 0.73680 val_acc= 0.85146 time= 2.25499
Epoch: 0040 train_loss= 0.74437 train_acc= 0.84470 val_loss= 0.71333 val_acc= 0.85299 time= 2.23800
Epoch: 0041 train_loss= 0.71692 train_acc= 0.85099 val_loss= 0.69059 val_acc= 0.85299 time= 2.22836
Epoch: 0042 train_loss= 0.70013 train_acc= 0.85253 val_loss= 0.66852 val_acc= 0.85146 time= 2.22998
Epoch: 0043 train_loss= 0.66672 train_acc= 0.85661 val_loss= 0.64697 val_acc= 0.85605 time= 2.23001
Epoch: 0044 train_loss= 0.64814 train_acc= 0.86035 val_loss= 0.62610 val_acc= 0.85758 time= 2.27097
Epoch: 0045 train_loss= 0.62638 train_acc= 0.86018 val_loss= 0.60543 val_acc= 0.86524 time= 2.22700
Epoch: 0046 train_loss= 0.60458 train_acc= 0.86494 val_loss= 0.58546 val_acc= 0.87136 time= 2.23899
Epoch: 0047 train_loss= 0.57770 train_acc= 0.87107 val_loss= 0.56596 val_acc= 0.87902 time= 2.22300
Epoch: 0048 train_loss= 0.56223 train_acc= 0.87464 val_loss= 0.54766 val_acc= 0.87749 time= 2.24101
Epoch: 0049 train_loss= 0.54071 train_acc= 0.88144 val_loss= 0.53034 val_acc= 0.88055 time= 2.23683
Epoch: 0050 train_loss= 0.51981 train_acc= 0.88229 val_loss= 0.51402 val_acc= 0.88055 time= 2.23870
Epoch: 0051 train_loss= 0.50043 train_acc= 0.88739 val_loss= 0.49882 val_acc= 0.88208 time= 2.24698
Epoch: 0052 train_loss= 0.48060 train_acc= 0.89148 val_loss= 0.48433 val_acc= 0.88361 time= 2.22688
Epoch: 0053 train_loss= 0.47020 train_acc= 0.89437 val_loss= 0.47057 val_acc= 0.88361 time= 2.24283
Epoch: 0054 train_loss= 0.44406 train_acc= 0.89862 val_loss= 0.45726 val_acc= 0.88974 time= 2.23875
Epoch: 0055 train_loss= 0.43369 train_acc= 0.90236 val_loss= 0.44435 val_acc= 0.89433 time= 2.24098
Epoch: 0056 train_loss= 0.41523 train_acc= 0.90628 val_loss= 0.43173 val_acc= 0.89893 time= 2.23901
Epoch: 0057 train_loss= 0.40443 train_acc= 0.91325 val_loss= 0.41930 val_acc= 0.89740 time= 2.25200
Epoch: 0058 train_loss= 0.38473 train_acc= 0.91359 val_loss= 0.40719 val_acc= 0.90199 time= 2.24256
Epoch: 0059 train_loss= 0.37357 train_acc= 0.92107 val_loss= 0.39541 val_acc= 0.90352 time= 2.24782
Epoch: 0060 train_loss= 0.36202 train_acc= 0.92329 val_loss= 0.38443 val_acc= 0.90505 time= 2.22575
Epoch: 0061 train_loss= 0.34261 train_acc= 0.92839 val_loss= 0.37431 val_acc= 0.90812 time= 2.24500
Epoch: 0062 train_loss= 0.33612 train_acc= 0.92703 val_loss= 0.36532 val_acc= 0.90659 time= 2.26100
Epoch: 0063 train_loss= 0.32481 train_acc= 0.93451 val_loss= 0.35747 val_acc= 0.90812 time= 2.23421
Epoch: 0064 train_loss= 0.30771 train_acc= 0.93451 val_loss= 0.34978 val_acc= 0.90965 time= 2.22902
Epoch: 0065 train_loss= 0.29679 train_acc= 0.93978 val_loss= 0.34256 val_acc= 0.91424 time= 2.23036
Epoch: 0066 train_loss= 0.28842 train_acc= 0.93978 val_loss= 0.33586 val_acc= 0.91731 time= 2.24600
Epoch: 0067 train_loss= 0.27716 train_acc= 0.94132 val_loss= 0.32973 val_acc= 0.91577 time= 2.24268
Epoch: 0068 train_loss= 0.26598 train_acc= 0.94710 val_loss= 0.32414 val_acc= 0.91424 time= 2.22700
Epoch: 0069 train_loss= 0.25833 train_acc= 0.94846 val_loss= 0.31889 val_acc= 0.91577 time= 2.24032
Epoch: 0070 train_loss= 0.24850 train_acc= 0.94999 val_loss= 0.31366 val_acc= 0.91884 time= 2.23101
Epoch: 0071 train_loss= 0.24109 train_acc= 0.94931 val_loss= 0.30795 val_acc= 0.92037 time= 2.24500
Epoch: 0072 train_loss= 0.22827 train_acc= 0.95509 val_loss= 0.30246 val_acc= 0.92343 time= 2.22138
Epoch: 0073 train_loss= 0.22374 train_acc= 0.95543 val_loss= 0.29616 val_acc= 0.92343 time= 2.22400
Epoch: 0074 train_loss= 0.21618 train_acc= 0.95458 val_loss= 0.28991 val_acc= 0.92190 time= 2.24099
Epoch: 0075 train_loss= 0.20613 train_acc= 0.95918 val_loss= 0.28437 val_acc= 0.92496 time= 2.25203
Epoch: 0076 train_loss= 0.20145 train_acc= 0.95662 val_loss= 0.28057 val_acc= 0.92649 time= 2.22898
Epoch: 0077 train_loss= 0.19271 train_acc= 0.96020 val_loss= 0.27807 val_acc= 0.92649 time= 2.23701
Epoch: 0078 train_loss= 0.18632 train_acc= 0.96054 val_loss= 0.27627 val_acc= 0.92649 time= 2.31700
Epoch: 0079 train_loss= 0.17835 train_acc= 0.96649 val_loss= 0.27446 val_acc= 0.92496 time= 2.27254
Epoch: 0080 train_loss= 0.17160 train_acc= 0.96411 val_loss= 0.27257 val_acc= 0.92802 time= 2.24601
Epoch: 0081 train_loss= 0.16711 train_acc= 0.96802 val_loss= 0.26930 val_acc= 0.92956 time= 2.25800
Epoch: 0082 train_loss= 0.16067 train_acc= 0.96734 val_loss= 0.26533 val_acc= 0.92802 time= 2.23700
Epoch: 0083 train_loss= 0.15359 train_acc= 0.96836 val_loss= 0.26126 val_acc= 0.92956 time= 2.23676
Epoch: 0084 train_loss= 0.14932 train_acc= 0.96972 val_loss= 0.25751 val_acc= 0.92956 time= 2.23200
Epoch: 0085 train_loss= 0.14556 train_acc= 0.97278 val_loss= 0.25428 val_acc= 0.92956 time= 2.23976
Epoch: 0086 train_loss= 0.13832 train_acc= 0.97176 val_loss= 0.25206 val_acc= 0.93262 time= 2.23177
Epoch: 0087 train_loss= 0.13055 train_acc= 0.97346 val_loss= 0.24928 val_acc= 0.93109 time= 2.22703
Epoch: 0088 train_loss= 0.12763 train_acc= 0.97346 val_loss= 0.24594 val_acc= 0.93568 time= 2.22800
Epoch: 0089 train_loss= 0.12295 train_acc= 0.97618 val_loss= 0.24383 val_acc= 0.93415 time= 2.22900
Epoch: 0090 train_loss= 0.11972 train_acc= 0.97653 val_loss= 0.24240 val_acc= 0.93568 time= 2.23901
Epoch: 0091 train_loss= 0.11779 train_acc= 0.97789 val_loss= 0.24236 val_acc= 0.93568 time= 2.23799
Epoch: 0092 train_loss= 0.11110 train_acc= 0.98061 val_loss= 0.24239 val_acc= 0.93568 time= 2.22149
Epoch: 0093 train_loss= 0.10597 train_acc= 0.97993 val_loss= 0.24336 val_acc= 0.93568 time= 2.23502
Epoch: 0094 train_loss= 0.10377 train_acc= 0.98010 val_loss= 0.24253 val_acc= 0.93262 time= 2.24599
Epoch: 0095 train_loss= 0.10228 train_acc= 0.97959 val_loss= 0.23928 val_acc= 0.93874 time= 2.22576
Epoch: 0096 train_loss= 0.09441 train_acc= 0.98095 val_loss= 0.23549 val_acc= 0.94028 time= 2.23713
Epoch: 0097 train_loss= 0.09567 train_acc= 0.98520 val_loss= 0.23298 val_acc= 0.94028 time= 2.22700
Epoch: 0098 train_loss= 0.09007 train_acc= 0.98537 val_loss= 0.23163 val_acc= 0.94181 time= 2.22199
Epoch: 0099 train_loss= 0.08787 train_acc= 0.98384 val_loss= 0.23154 val_acc= 0.94181 time= 2.22736
Epoch: 0100 train_loss= 0.08633 train_acc= 0.98520 val_loss= 0.23253 val_acc= 0.93721 time= 2.23636
Epoch: 0101 train_loss= 0.08147 train_acc= 0.98639 val_loss= 0.23550 val_acc= 0.93721 time= 2.23762
Epoch: 0102 train_loss= 0.07940 train_acc= 0.98554 val_loss= 0.23750 val_acc= 0.93721 time= 2.23512
Early stopping...
Optimization Finished!
Test set results: cost= 0.25789 accuracy= 0.93536 time= 0.76399
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.5000    1.0000    0.6667         1
           3     0.7667    0.9200    0.8364        75
           4     1.0000    1.0000    1.0000         9
           5     0.8200    0.9425    0.8770        87
           6     0.9200    0.9200    0.9200        25
           7     0.7059    0.9231    0.8000        13
           8     1.0000    1.0000    1.0000        11
           9     1.0000    0.2222    0.3636         9
          10     0.8800    0.6111    0.7213        36
          11     1.0000    0.9167    0.9565        12
          12     0.8392    0.9917    0.9091       121
          13     1.0000    0.7368    0.8485        19
          14     0.8621    0.8929    0.8772        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.6500    0.7647    0.7027        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9670    0.9676       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8354    0.8148    0.8250        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9808    0.9917    0.9862      1083
          40     0.8333    1.0000    0.9091         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.8889    0.6667    0.7619        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.7647    0.8667    0.8125        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9354      2568
   macro avg     0.7284    0.6503    0.6651      2568
weighted avg     0.9328    0.9354    0.9296      2568

Macro average Test Precision, Recall and F1-Score...
(0.72843139142517, 0.6502604896770928, 0.6651486905393897, None)
Micro average Test Precision, Recall and F1-Score...
(0.9353582554517134, 0.9353582554517134, 0.9353582554517134, None)
embeddings:
8892 6532 2568
[[ 0.13144904 -0.0750339   0.08222318 ...  0.117184   -0.08646018
  -0.11039344]
 [ 0.0861621  -0.02062187 -0.0329927  ...  0.03532814 -0.05882821
   0.00508636]
 [ 0.32744482 -0.01172051  0.2215806  ...  0.233368    0.31757784
   0.04608375]
 ...
 [ 0.02276322 -0.00250305  0.27851582 ...  0.03894571  0.20185912
   0.04573182]
 [ 0.21838786  0.01983142  0.13241823 ...  0.13935207  0.13726006
   0.04040287]
 [ 0.39462742  0.21400732  0.23356366 ...  0.3289129   0.1560379
   0.1860695 ]]

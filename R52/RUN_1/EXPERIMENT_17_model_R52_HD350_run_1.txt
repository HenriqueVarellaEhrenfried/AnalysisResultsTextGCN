(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95132 train_acc= 0.00357 val_loss= 3.88058 val_acc= 0.66922 time= 0.53462
Epoch: 0002 train_loss= 3.88067 train_acc= 0.65691 val_loss= 3.73109 val_acc= 0.66922 time= 0.21455
Epoch: 0003 train_loss= 3.73238 train_acc= 0.65674 val_loss= 3.49401 val_acc= 0.66616 time= 0.21301
Epoch: 0004 train_loss= 3.49987 train_acc= 0.65351 val_loss= 3.18061 val_acc= 0.65850 time= 0.21196
Epoch: 0005 train_loss= 3.18537 train_acc= 0.64756 val_loss= 2.83468 val_acc= 0.65084 time= 0.21303
Epoch: 0006 train_loss= 2.83609 train_acc= 0.63701 val_loss= 2.52119 val_acc= 0.63859 time= 0.23500
Epoch: 0007 train_loss= 2.52780 train_acc= 0.61711 val_loss= 2.30877 val_acc= 0.59418 time= 0.21200
Epoch: 0008 train_loss= 2.30479 train_acc= 0.59024 val_loss= 2.21215 val_acc= 0.51914 time= 0.21400
Epoch: 0009 train_loss= 2.21780 train_acc= 0.50485 val_loss= 2.17059 val_acc= 0.47473 time= 0.20900
Epoch: 0010 train_loss= 2.17760 train_acc= 0.45314 val_loss= 2.12307 val_acc= 0.46708 time= 0.23900
Epoch: 0011 train_loss= 2.13405 train_acc= 0.44208 val_loss= 2.04408 val_acc= 0.46861 time= 0.21000
Epoch: 0012 train_loss= 2.06104 train_acc= 0.44259 val_loss= 1.93751 val_acc= 0.48545 time= 0.21008
Epoch: 0013 train_loss= 1.95942 train_acc= 0.46079 val_loss= 1.82334 val_acc= 0.53599 time= 0.21397
Epoch: 0014 train_loss= 1.84429 train_acc= 0.53274 val_loss= 1.72525 val_acc= 0.62787 time= 0.21514
Epoch: 0015 train_loss= 1.75012 train_acc= 0.61133 val_loss= 1.65253 val_acc= 0.66462 time= 0.23603
Epoch: 0016 train_loss= 1.68196 train_acc= 0.64518 val_loss= 1.59437 val_acc= 0.67534 time= 0.21800
Epoch: 0017 train_loss= 1.62209 train_acc= 0.65742 val_loss= 1.53632 val_acc= 0.68147 time= 0.21397
Epoch: 0018 train_loss= 1.55935 train_acc= 0.65930 val_loss= 1.47415 val_acc= 0.67688 time= 0.21203
Epoch: 0019 train_loss= 1.50715 train_acc= 0.66423 val_loss= 1.41097 val_acc= 0.68300 time= 0.21497
Epoch: 0020 train_loss= 1.43961 train_acc= 0.66644 val_loss= 1.35175 val_acc= 0.69066 time= 0.24351
Epoch: 0021 train_loss= 1.37711 train_acc= 0.67597 val_loss= 1.29903 val_acc= 0.69525 time= 0.21200
Epoch: 0022 train_loss= 1.32326 train_acc= 0.68736 val_loss= 1.25310 val_acc= 0.71363 time= 0.21000
Epoch: 0023 train_loss= 1.26926 train_acc= 0.69808 val_loss= 1.21231 val_acc= 0.71975 time= 0.21000
Epoch: 0024 train_loss= 1.23406 train_acc= 0.71067 val_loss= 1.17462 val_acc= 0.72588 time= 0.23903
Epoch: 0025 train_loss= 1.19291 train_acc= 0.72325 val_loss= 1.13835 val_acc= 0.73201 time= 0.21595
Epoch: 0026 train_loss= 1.15971 train_acc= 0.73754 val_loss= 1.10233 val_acc= 0.73507 time= 0.21367
Epoch: 0027 train_loss= 1.11571 train_acc= 0.75268 val_loss= 1.06589 val_acc= 0.74732 time= 0.21097
Epoch: 0028 train_loss= 1.08283 train_acc= 0.75795 val_loss= 1.02922 val_acc= 0.76110 time= 0.21503
Epoch: 0029 train_loss= 1.04335 train_acc= 0.76816 val_loss= 0.99304 val_acc= 0.76876 time= 0.24000
Epoch: 0030 train_loss= 1.00568 train_acc= 0.78177 val_loss= 0.95828 val_acc= 0.78101 time= 0.21200
Epoch: 0031 train_loss= 0.96643 train_acc= 0.78993 val_loss= 0.92551 val_acc= 0.79786 time= 0.21397
Epoch: 0032 train_loss= 0.93689 train_acc= 0.79180 val_loss= 0.89476 val_acc= 0.80398 time= 0.21504
Epoch: 0033 train_loss= 0.90940 train_acc= 0.80235 val_loss= 0.86562 val_acc= 0.80858 time= 0.23800
Epoch: 0034 train_loss= 0.87918 train_acc= 0.81510 val_loss= 0.83763 val_acc= 0.81317 time= 0.21055
Epoch: 0035 train_loss= 0.84611 train_acc= 0.82310 val_loss= 0.81063 val_acc= 0.82389 time= 0.21003
Epoch: 0036 train_loss= 0.82307 train_acc= 0.82718 val_loss= 0.78470 val_acc= 0.83614 time= 0.21200
Epoch: 0037 train_loss= 0.79961 train_acc= 0.83399 val_loss= 0.75975 val_acc= 0.83767 time= 0.21692
Epoch: 0038 train_loss= 0.76933 train_acc= 0.84198 val_loss= 0.73539 val_acc= 0.83767 time= 0.24251
Epoch: 0039 train_loss= 0.74035 train_acc= 0.84453 val_loss= 0.71146 val_acc= 0.84380 time= 0.21103
Epoch: 0040 train_loss= 0.71593 train_acc= 0.84487 val_loss= 0.68786 val_acc= 0.84992 time= 0.20944
Epoch: 0041 train_loss= 0.68835 train_acc= 0.85117 val_loss= 0.66455 val_acc= 0.85605 time= 0.21100
Epoch: 0042 train_loss= 0.66350 train_acc= 0.85865 val_loss= 0.64164 val_acc= 0.85911 time= 0.23600
Epoch: 0043 train_loss= 0.63356 train_acc= 0.86256 val_loss= 0.61942 val_acc= 0.86524 time= 0.21342
Epoch: 0044 train_loss= 0.61041 train_acc= 0.86869 val_loss= 0.59788 val_acc= 0.86677 time= 0.21551
Epoch: 0045 train_loss= 0.58868 train_acc= 0.87413 val_loss= 0.57706 val_acc= 0.87136 time= 0.21004
Epoch: 0046 train_loss= 0.57027 train_acc= 0.87770 val_loss= 0.55692 val_acc= 0.87443 time= 0.22401
Epoch: 0047 train_loss= 0.54796 train_acc= 0.88042 val_loss= 0.53783 val_acc= 0.87749 time= 0.21095
Epoch: 0048 train_loss= 0.52292 train_acc= 0.88246 val_loss= 0.51997 val_acc= 0.87902 time= 0.22300
Epoch: 0049 train_loss= 0.50906 train_acc= 0.88757 val_loss= 0.50332 val_acc= 0.88208 time= 0.21200
Epoch: 0050 train_loss= 0.48705 train_acc= 0.89114 val_loss= 0.48818 val_acc= 0.88361 time= 0.21401
Epoch: 0051 train_loss= 0.46852 train_acc= 0.89488 val_loss= 0.47425 val_acc= 0.88361 time= 0.21300
Epoch: 0052 train_loss= 0.45300 train_acc= 0.89947 val_loss= 0.46098 val_acc= 0.88361 time= 0.22667
Epoch: 0053 train_loss= 0.43369 train_acc= 0.89981 val_loss= 0.44778 val_acc= 0.88515 time= 0.21000
Epoch: 0054 train_loss= 0.41785 train_acc= 0.90577 val_loss= 0.43391 val_acc= 0.88668 time= 0.21096
Epoch: 0055 train_loss= 0.39779 train_acc= 0.91325 val_loss= 0.42023 val_acc= 0.89433 time= 0.21500
Epoch: 0056 train_loss= 0.38524 train_acc= 0.91427 val_loss= 0.40703 val_acc= 0.89893 time= 0.22305
Epoch: 0057 train_loss= 0.37088 train_acc= 0.91818 val_loss= 0.39447 val_acc= 0.90199 time= 0.21395
Epoch: 0058 train_loss= 0.35570 train_acc= 0.92312 val_loss= 0.38401 val_acc= 0.90046 time= 0.21200
Epoch: 0059 train_loss= 0.34590 train_acc= 0.92567 val_loss= 0.37447 val_acc= 0.90046 time= 0.21005
Epoch: 0060 train_loss= 0.33264 train_acc= 0.92907 val_loss= 0.36556 val_acc= 0.90046 time= 0.21395
Epoch: 0061 train_loss= 0.31726 train_acc= 0.93400 val_loss= 0.35785 val_acc= 0.90199 time= 0.23900
Epoch: 0062 train_loss= 0.30632 train_acc= 0.93434 val_loss= 0.35093 val_acc= 0.90658 time= 0.21704
Epoch: 0063 train_loss= 0.29264 train_acc= 0.93842 val_loss= 0.34400 val_acc= 0.90658 time= 0.21096
Epoch: 0064 train_loss= 0.28586 train_acc= 0.94064 val_loss= 0.33665 val_acc= 0.90658 time= 0.21304
Epoch: 0065 train_loss= 0.27638 train_acc= 0.94098 val_loss= 0.32838 val_acc= 0.90812 time= 0.21501
Epoch: 0066 train_loss= 0.26778 train_acc= 0.94200 val_loss= 0.32024 val_acc= 0.91271 time= 0.22559
Epoch: 0067 train_loss= 0.25243 train_acc= 0.94812 val_loss= 0.31265 val_acc= 0.91577 time= 0.21380
Epoch: 0068 train_loss= 0.24437 train_acc= 0.94931 val_loss= 0.30602 val_acc= 0.92037 time= 0.21603
Epoch: 0069 train_loss= 0.23547 train_acc= 0.95152 val_loss= 0.30027 val_acc= 0.91884 time= 0.21197
Epoch: 0070 train_loss= 0.22912 train_acc= 0.95441 val_loss= 0.29552 val_acc= 0.91730 time= 0.23900
Epoch: 0071 train_loss= 0.21957 train_acc= 0.95458 val_loss= 0.29272 val_acc= 0.91271 time= 0.21435
Epoch: 0072 train_loss= 0.21343 train_acc= 0.95748 val_loss= 0.28994 val_acc= 0.91271 time= 0.21104
Epoch: 0073 train_loss= 0.20471 train_acc= 0.95782 val_loss= 0.28629 val_acc= 0.91424 time= 0.21399
Epoch: 0074 train_loss= 0.19553 train_acc= 0.95731 val_loss= 0.28184 val_acc= 0.91730 time= 0.21600
Epoch: 0075 train_loss= 0.18960 train_acc= 0.96224 val_loss= 0.27735 val_acc= 0.92343 time= 0.24001
Epoch: 0076 train_loss= 0.18362 train_acc= 0.96275 val_loss= 0.27446 val_acc= 0.92496 time= 0.21200
Epoch: 0077 train_loss= 0.17685 train_acc= 0.96326 val_loss= 0.27292 val_acc= 0.91884 time= 0.21000
Epoch: 0078 train_loss= 0.16780 train_acc= 0.96717 val_loss= 0.27167 val_acc= 0.92190 time= 0.21000
Epoch: 0079 train_loss= 0.16146 train_acc= 0.96615 val_loss= 0.26921 val_acc= 0.92343 time= 0.24503
Epoch: 0080 train_loss= 0.15752 train_acc= 0.96513 val_loss= 0.26441 val_acc= 0.92037 time= 0.21412
Epoch: 0081 train_loss= 0.15026 train_acc= 0.96870 val_loss= 0.25959 val_acc= 0.92956 time= 0.21601
Epoch: 0082 train_loss= 0.14433 train_acc= 0.97176 val_loss= 0.25545 val_acc= 0.92956 time= 0.21113
Epoch: 0083 train_loss= 0.14050 train_acc= 0.97193 val_loss= 0.25123 val_acc= 0.92956 time= 0.21297
Epoch: 0084 train_loss= 0.13542 train_acc= 0.97398 val_loss= 0.24729 val_acc= 0.93109 time= 0.23586
Epoch: 0085 train_loss= 0.12894 train_acc= 0.97653 val_loss= 0.24483 val_acc= 0.92956 time= 0.21097
Epoch: 0086 train_loss= 0.12619 train_acc= 0.97398 val_loss= 0.24172 val_acc= 0.93262 time= 0.21303
Epoch: 0087 train_loss= 0.12094 train_acc= 0.97398 val_loss= 0.23974 val_acc= 0.93568 time= 0.20997
Epoch: 0088 train_loss= 0.11600 train_acc= 0.97840 val_loss= 0.23848 val_acc= 0.93262 time= 0.21503
Epoch: 0089 train_loss= 0.11371 train_acc= 0.97789 val_loss= 0.23750 val_acc= 0.93568 time= 0.21500
Epoch: 0090 train_loss= 0.10492 train_acc= 0.98061 val_loss= 0.23709 val_acc= 0.93568 time= 0.21697
Epoch: 0091 train_loss= 0.10450 train_acc= 0.98163 val_loss= 0.23700 val_acc= 0.93262 time= 0.21900
Epoch: 0092 train_loss= 0.10078 train_acc= 0.98282 val_loss= 0.23672 val_acc= 0.93415 time= 0.21803
Epoch: 0093 train_loss= 0.10020 train_acc= 0.98180 val_loss= 0.23666 val_acc= 0.93262 time= 0.23800
Epoch: 0094 train_loss= 0.09393 train_acc= 0.98316 val_loss= 0.23615 val_acc= 0.93415 time= 0.22000
Epoch: 0095 train_loss= 0.08937 train_acc= 0.98486 val_loss= 0.23458 val_acc= 0.93262 time= 0.21100
Epoch: 0096 train_loss= 0.08793 train_acc= 0.98554 val_loss= 0.23195 val_acc= 0.93721 time= 0.21200
Epoch: 0097 train_loss= 0.08430 train_acc= 0.98520 val_loss= 0.22956 val_acc= 0.93721 time= 0.23097
Epoch: 0098 train_loss= 0.08213 train_acc= 0.98639 val_loss= 0.22856 val_acc= 0.93721 time= 0.22204
Epoch: 0099 train_loss= 0.08086 train_acc= 0.98554 val_loss= 0.22853 val_acc= 0.93721 time= 0.21000
Epoch: 0100 train_loss= 0.07714 train_acc= 0.98690 val_loss= 0.22869 val_acc= 0.94028 time= 0.20999
Epoch: 0101 train_loss= 0.07201 train_acc= 0.98741 val_loss= 0.22986 val_acc= 0.93568 time= 0.21300
Epoch: 0102 train_loss= 0.07119 train_acc= 0.98911 val_loss= 0.23025 val_acc= 0.93568 time= 0.21700
Epoch: 0103 train_loss= 0.07035 train_acc= 0.98741 val_loss= 0.22987 val_acc= 0.93568 time= 0.21029
Epoch: 0104 train_loss= 0.06988 train_acc= 0.98860 val_loss= 0.22835 val_acc= 0.93874 time= 0.21365
Epoch: 0105 train_loss= 0.06725 train_acc= 0.98792 val_loss= 0.22567 val_acc= 0.94334 time= 0.21100
Epoch: 0106 train_loss= 0.06501 train_acc= 0.98911 val_loss= 0.22409 val_acc= 0.94028 time= 0.21300
Epoch: 0107 train_loss= 0.06040 train_acc= 0.99013 val_loss= 0.22377 val_acc= 0.94028 time= 0.23700
Epoch: 0108 train_loss= 0.06116 train_acc= 0.98996 val_loss= 0.22362 val_acc= 0.94181 time= 0.21100
Epoch: 0109 train_loss= 0.05654 train_acc= 0.99081 val_loss= 0.22328 val_acc= 0.94334 time= 0.21424
Epoch: 0110 train_loss= 0.05720 train_acc= 0.99167 val_loss= 0.22279 val_acc= 0.94181 time= 0.21403
Epoch: 0111 train_loss= 0.05501 train_acc= 0.99116 val_loss= 0.22336 val_acc= 0.94028 time= 0.23701
Epoch: 0112 train_loss= 0.05403 train_acc= 0.99133 val_loss= 0.22398 val_acc= 0.93568 time= 0.20998
Epoch: 0113 train_loss= 0.05318 train_acc= 0.99252 val_loss= 0.22341 val_acc= 0.93568 time= 0.21101
Epoch: 0114 train_loss= 0.05094 train_acc= 0.99116 val_loss= 0.22098 val_acc= 0.93874 time= 0.21375
Epoch: 0115 train_loss= 0.04722 train_acc= 0.99354 val_loss= 0.21905 val_acc= 0.94028 time= 0.21497
Epoch: 0116 train_loss= 0.04743 train_acc= 0.99235 val_loss= 0.21872 val_acc= 0.94181 time= 0.24553
Epoch: 0117 train_loss= 0.04596 train_acc= 0.99371 val_loss= 0.21833 val_acc= 0.93874 time= 0.21603
Epoch: 0118 train_loss= 0.04390 train_acc= 0.99439 val_loss= 0.21823 val_acc= 0.93874 time= 0.21000
Epoch: 0119 train_loss= 0.04350 train_acc= 0.99371 val_loss= 0.21794 val_acc= 0.94334 time= 0.21100
Epoch: 0120 train_loss= 0.04333 train_acc= 0.99490 val_loss= 0.21835 val_acc= 0.94028 time= 0.23000
Epoch: 0121 train_loss= 0.04362 train_acc= 0.99218 val_loss= 0.21910 val_acc= 0.94334 time= 0.22201
Epoch: 0122 train_loss= 0.03993 train_acc= 0.99303 val_loss= 0.22014 val_acc= 0.94334 time= 0.21401
Early stopping...
Optimization Finished!
Test set results: cost= 0.24491 accuracy= 0.93925 time= 0.08803
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7955    0.9333    0.8589        75
           4     1.0000    1.0000    1.0000         9
           5     0.8333    0.9195    0.8743        87
           6     0.8846    0.9200    0.9020        25
           7     0.8462    0.8462    0.8462        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.6667    0.8000         9
          10     0.8846    0.6389    0.7419        36
          11     1.0000    0.9167    0.9565        12
          12     0.8521    1.0000    0.9202       121
          13     0.9333    0.7368    0.8235        19
          14     0.8571    0.8571    0.8571        28
          15     1.0000    1.0000    1.0000         4
          16     0.5000    0.2500    0.3333         4
          17     1.0000    0.3333    0.5000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.9333    0.8235    0.8750        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9641    0.9662       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8072    0.8272    0.8171        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9799    0.9917    0.9858      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     1.0000    0.6667    0.8000         3
          44     0.8182    0.7500    0.7826        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9393      2568
   macro avg     0.8105    0.7038    0.7281      2568
weighted avg     0.9403    0.9393    0.9355      2568

Macro average Test Precision, Recall and F1-Score...
(0.8105297390826173, 0.7037613618067516, 0.7281376164006872, None)
Micro average Test Precision, Recall and F1-Score...
(0.9392523364485982, 0.9392523364485982, 0.9392523364485983, None)
embeddings:
8892 6532 2568
[[-6.58775168e-03 -9.49035287e-02 -1.53891549e-01 ...  9.44319963e-02
   1.74927905e-01  1.40790403e+00]
 [-9.30918194e-03 -6.26873598e-02 -4.21613380e-02 ...  1.69047326e-01
   3.23086083e-02  5.70652306e-01]
 [-6.21296912e-02 -6.92626685e-02 -5.42341769e-02 ...  3.42878491e-01
   1.88647538e-01  1.91361696e-01]
 ...
 [ 2.91559957e-02 -1.85175072e-02 -2.52625905e-04 ...  3.61911058e-01
   1.08169295e-01  2.37914458e-01]
 [ 1.80310477e-02 -3.53827029e-02  6.45372178e-03 ...  2.70669073e-01
   1.42299354e-01  2.73040980e-01]
 [ 1.97633654e-01 -5.57247177e-02  2.03938156e-01 ...  3.80311906e-01
   1.48721844e-01  3.08116853e-01]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.01242 val_loss= 3.90295 val_acc= 0.63093 time= 0.46618
Epoch: 0002 train_loss= 3.90215 train_acc= 0.62034 val_loss= 3.80865 val_acc= 0.60643 time= 0.17210
Epoch: 0003 train_loss= 3.80985 train_acc= 0.59891 val_loss= 3.66192 val_acc= 0.59571 time= 0.16600
Epoch: 0004 train_loss= 3.66211 train_acc= 0.59279 val_loss= 3.46255 val_acc= 0.58652 time= 0.16999
Epoch: 0005 train_loss= 3.46617 train_acc= 0.57935 val_loss= 3.21964 val_acc= 0.57887 time= 0.20142
Epoch: 0006 train_loss= 3.22699 train_acc= 0.56948 val_loss= 2.95603 val_acc= 0.57121 time= 0.17207
Epoch: 0007 train_loss= 2.96269 train_acc= 0.56149 val_loss= 2.70124 val_acc= 0.55896 time= 0.16902
Epoch: 0008 train_loss= 2.70276 train_acc= 0.56064 val_loss= 2.48719 val_acc= 0.55436 time= 0.16603
Epoch: 0009 train_loss= 2.50225 train_acc= 0.55486 val_loss= 2.33944 val_acc= 0.56968 time= 0.19703
Epoch: 0010 train_loss= 2.35699 train_acc= 0.56217 val_loss= 2.25025 val_acc= 0.61103 time= 0.16797
Epoch: 0011 train_loss= 2.24928 train_acc= 0.59959 val_loss= 2.18973 val_acc= 0.65084 time= 0.18608
Epoch: 0012 train_loss= 2.19966 train_acc= 0.63718 val_loss= 2.13890 val_acc= 0.47320 time= 0.17200
Epoch: 0013 train_loss= 2.15173 train_acc= 0.44991 val_loss= 2.08362 val_acc= 0.45636 time= 0.17201
Epoch: 0014 train_loss= 2.10458 train_acc= 0.43528 val_loss= 2.01463 val_acc= 0.45636 time= 0.17004
Epoch: 0015 train_loss= 2.03177 train_acc= 0.43358 val_loss= 1.93166 val_acc= 0.46095 time= 0.19901
Epoch: 0016 train_loss= 1.95221 train_acc= 0.43630 val_loss= 1.84355 val_acc= 0.47626 time= 0.17496
Epoch: 0017 train_loss= 1.86639 train_acc= 0.44940 val_loss= 1.76238 val_acc= 0.52527 time= 0.16661
Epoch: 0018 train_loss= 1.79338 train_acc= 0.52543 val_loss= 1.69432 val_acc= 0.63093 time= 0.16945
Epoch: 0019 train_loss= 1.72334 train_acc= 0.61813 val_loss= 1.63741 val_acc= 0.66769 time= 0.16700
Epoch: 0020 train_loss= 1.66428 train_acc= 0.65215 val_loss= 1.58518 val_acc= 0.67688 time= 0.17300
Epoch: 0021 train_loss= 1.60529 train_acc= 0.64977 val_loss= 1.53374 val_acc= 0.66922 time= 0.20181
Epoch: 0022 train_loss= 1.55734 train_acc= 0.65028 val_loss= 1.48172 val_acc= 0.66769 time= 0.16801
Epoch: 0023 train_loss= 1.50304 train_acc= 0.65164 val_loss= 1.43042 val_acc= 0.67381 time= 0.16699
Epoch: 0024 train_loss= 1.44976 train_acc= 0.65555 val_loss= 1.38144 val_acc= 0.67994 time= 0.17034
Epoch: 0025 train_loss= 1.40711 train_acc= 0.66644 val_loss= 1.33588 val_acc= 0.69219 time= 0.16899
Epoch: 0026 train_loss= 1.35829 train_acc= 0.67341 val_loss= 1.29427 val_acc= 0.70597 time= 0.20102
Epoch: 0027 train_loss= 1.32328 train_acc= 0.68549 val_loss= 1.25643 val_acc= 0.71975 time= 0.16913
Epoch: 0028 train_loss= 1.28194 train_acc= 0.70080 val_loss= 1.22166 val_acc= 0.72282 time= 0.18472
Epoch: 0029 train_loss= 1.25090 train_acc= 0.71730 val_loss= 1.18907 val_acc= 0.73201 time= 0.16901
Epoch: 0030 train_loss= 1.21487 train_acc= 0.73006 val_loss= 1.15782 val_acc= 0.73660 time= 0.16900
Epoch: 0031 train_loss= 1.17744 train_acc= 0.74162 val_loss= 1.12722 val_acc= 0.74119 time= 0.16800
Epoch: 0032 train_loss= 1.14816 train_acc= 0.75285 val_loss= 1.09680 val_acc= 0.74579 time= 0.20099
Epoch: 0033 train_loss= 1.11636 train_acc= 0.75880 val_loss= 1.06637 val_acc= 0.75651 time= 0.17101
Epoch: 0034 train_loss= 1.08089 train_acc= 0.76510 val_loss= 1.03612 val_acc= 0.76723 time= 0.16696
Epoch: 0035 train_loss= 1.04830 train_acc= 0.77343 val_loss= 1.00658 val_acc= 0.77642 time= 0.17200
Epoch: 0036 train_loss= 1.02204 train_acc= 0.78057 val_loss= 0.97815 val_acc= 0.78101 time= 0.17504
Epoch: 0037 train_loss= 0.99473 train_acc= 0.78687 val_loss= 0.95083 val_acc= 0.79020 time= 0.17107
Epoch: 0038 train_loss= 0.96786 train_acc= 0.79180 val_loss= 0.92448 val_acc= 0.80092 time= 0.19800
Epoch: 0039 train_loss= 0.94466 train_acc= 0.80201 val_loss= 0.89901 val_acc= 0.80858 time= 0.17500
Epoch: 0040 train_loss= 0.91232 train_acc= 0.80745 val_loss= 0.87419 val_acc= 0.81623 time= 0.16800
Epoch: 0041 train_loss= 0.89544 train_acc= 0.81289 val_loss= 0.84994 val_acc= 0.82542 time= 0.17000
Epoch: 0042 train_loss= 0.86477 train_acc= 0.82361 val_loss= 0.82614 val_acc= 0.83155 time= 0.17204
Epoch: 0043 train_loss= 0.84463 train_acc= 0.82650 val_loss= 0.80299 val_acc= 0.83767 time= 0.20200
Epoch: 0044 train_loss= 0.81701 train_acc= 0.83245 val_loss= 0.78047 val_acc= 0.84227 time= 0.16897
Epoch: 0045 train_loss= 0.79520 train_acc= 0.83586 val_loss= 0.75825 val_acc= 0.84380 time= 0.18000
Epoch: 0046 train_loss= 0.77587 train_acc= 0.83926 val_loss= 0.73690 val_acc= 0.84992 time= 0.16703
Epoch: 0047 train_loss= 0.74028 train_acc= 0.84640 val_loss= 0.71601 val_acc= 0.85452 time= 0.16800
Epoch: 0048 train_loss= 0.72274 train_acc= 0.84963 val_loss= 0.69541 val_acc= 0.85299 time= 0.17000
Epoch: 0049 train_loss= 0.69695 train_acc= 0.85355 val_loss= 0.67540 val_acc= 0.85452 time= 0.17899
Epoch: 0050 train_loss= 0.67598 train_acc= 0.85678 val_loss= 0.65545 val_acc= 0.86371 time= 0.17358
Epoch: 0051 train_loss= 0.65878 train_acc= 0.86290 val_loss= 0.63600 val_acc= 0.86524 time= 0.17705
Epoch: 0052 train_loss= 0.64307 train_acc= 0.86358 val_loss= 0.61725 val_acc= 0.87136 time= 0.16696
Epoch: 0053 train_loss= 0.61321 train_acc= 0.86715 val_loss= 0.59913 val_acc= 0.87443 time= 0.16903
Epoch: 0054 train_loss= 0.59925 train_acc= 0.86834 val_loss= 0.58184 val_acc= 0.87443 time= 0.16900
Epoch: 0055 train_loss= 0.57425 train_acc= 0.87396 val_loss= 0.56524 val_acc= 0.87749 time= 0.20000
Epoch: 0056 train_loss= 0.56357 train_acc= 0.87736 val_loss= 0.54950 val_acc= 0.87902 time= 0.17500
Epoch: 0057 train_loss= 0.54989 train_acc= 0.87617 val_loss= 0.53467 val_acc= 0.88361 time= 0.16900
Epoch: 0058 train_loss= 0.52246 train_acc= 0.88365 val_loss= 0.52051 val_acc= 0.88515 time= 0.17100
Epoch: 0059 train_loss= 0.51496 train_acc= 0.88501 val_loss= 0.50713 val_acc= 0.88668 time= 0.16700
Epoch: 0060 train_loss= 0.48540 train_acc= 0.89165 val_loss= 0.49447 val_acc= 0.88821 time= 0.20100
Epoch: 0061 train_loss= 0.48503 train_acc= 0.89267 val_loss= 0.48203 val_acc= 0.88974 time= 0.16700
Epoch: 0062 train_loss= 0.47123 train_acc= 0.89607 val_loss= 0.47014 val_acc= 0.88974 time= 0.16800
Epoch: 0063 train_loss= 0.45300 train_acc= 0.90253 val_loss= 0.45793 val_acc= 0.88974 time= 0.16699
Epoch: 0064 train_loss= 0.43597 train_acc= 0.90696 val_loss= 0.44579 val_acc= 0.89587 time= 0.16900
Epoch: 0065 train_loss= 0.42349 train_acc= 0.91002 val_loss= 0.43384 val_acc= 0.90046 time= 0.17201
Epoch: 0066 train_loss= 0.40575 train_acc= 0.90900 val_loss= 0.42257 val_acc= 0.90199 time= 0.18304
Epoch: 0067 train_loss= 0.39607 train_acc= 0.91444 val_loss= 0.41195 val_acc= 0.90046 time= 0.16854
Epoch: 0068 train_loss= 0.38314 train_acc= 0.91478 val_loss= 0.40224 val_acc= 0.90352 time= 0.18500
Epoch: 0069 train_loss= 0.38057 train_acc= 0.91835 val_loss= 0.39362 val_acc= 0.90352 time= 0.16800
Epoch: 0070 train_loss= 0.36770 train_acc= 0.92022 val_loss= 0.38578 val_acc= 0.90352 time= 0.16900
Epoch: 0071 train_loss= 0.35743 train_acc= 0.92295 val_loss= 0.37819 val_acc= 0.90505 time= 0.16800
Epoch: 0072 train_loss= 0.34290 train_acc= 0.93128 val_loss= 0.37033 val_acc= 0.90658 time= 0.20697
Epoch: 0073 train_loss= 0.33628 train_acc= 0.92822 val_loss= 0.36210 val_acc= 0.90812 time= 0.18103
Epoch: 0074 train_loss= 0.31545 train_acc= 0.93485 val_loss= 0.35400 val_acc= 0.90965 time= 0.16797
Epoch: 0075 train_loss= 0.31665 train_acc= 0.93536 val_loss= 0.34659 val_acc= 0.90965 time= 0.16703
Epoch: 0076 train_loss= 0.30922 train_acc= 0.93638 val_loss= 0.33939 val_acc= 0.90658 time= 0.16900
Epoch: 0077 train_loss= 0.29721 train_acc= 0.94047 val_loss= 0.33266 val_acc= 0.90658 time= 0.17045
Epoch: 0078 train_loss= 0.28114 train_acc= 0.94268 val_loss= 0.32705 val_acc= 0.90965 time= 0.20107
Epoch: 0079 train_loss= 0.28035 train_acc= 0.94251 val_loss= 0.32244 val_acc= 0.90965 time= 0.17097
Epoch: 0080 train_loss= 0.26993 train_acc= 0.94574 val_loss= 0.31776 val_acc= 0.91118 time= 0.16944
Epoch: 0081 train_loss= 0.26307 train_acc= 0.94557 val_loss= 0.31277 val_acc= 0.91118 time= 0.17304
Epoch: 0082 train_loss= 0.25353 train_acc= 0.95220 val_loss= 0.30809 val_acc= 0.91884 time= 0.17000
Epoch: 0083 train_loss= 0.24985 train_acc= 0.94982 val_loss= 0.30373 val_acc= 0.92343 time= 0.16996
Epoch: 0084 train_loss= 0.23616 train_acc= 0.95101 val_loss= 0.29925 val_acc= 0.92496 time= 0.20200
Epoch: 0085 train_loss= 0.23687 train_acc= 0.95101 val_loss= 0.29499 val_acc= 0.92343 time= 0.17705
Epoch: 0086 train_loss= 0.22892 train_acc= 0.95305 val_loss= 0.29099 val_acc= 0.92649 time= 0.16800
Epoch: 0087 train_loss= 0.22100 train_acc= 0.95543 val_loss= 0.28624 val_acc= 0.92649 time= 0.16900
Epoch: 0088 train_loss= 0.21193 train_acc= 0.95901 val_loss= 0.28259 val_acc= 0.92496 time= 0.17162
Epoch: 0089 train_loss= 0.20925 train_acc= 0.95935 val_loss= 0.27995 val_acc= 0.92649 time= 0.20101
Epoch: 0090 train_loss= 0.20231 train_acc= 0.96156 val_loss= 0.27733 val_acc= 0.92802 time= 0.17099
Epoch: 0091 train_loss= 0.19678 train_acc= 0.96122 val_loss= 0.27485 val_acc= 0.92343 time= 0.17001
Epoch: 0092 train_loss= 0.19155 train_acc= 0.96394 val_loss= 0.27185 val_acc= 0.92343 time= 0.16700
Epoch: 0093 train_loss= 0.18532 train_acc= 0.96326 val_loss= 0.26848 val_acc= 0.92496 time= 0.16900
Epoch: 0094 train_loss= 0.18260 train_acc= 0.96530 val_loss= 0.26557 val_acc= 0.92496 time= 0.19896
Epoch: 0095 train_loss= 0.17145 train_acc= 0.96581 val_loss= 0.26173 val_acc= 0.92802 time= 0.17161
Epoch: 0096 train_loss= 0.16915 train_acc= 0.96547 val_loss= 0.25880 val_acc= 0.93109 time= 0.18197
Epoch: 0097 train_loss= 0.17024 train_acc= 0.96428 val_loss= 0.25540 val_acc= 0.93109 time= 0.16903
Epoch: 0098 train_loss= 0.16100 train_acc= 0.96887 val_loss= 0.25293 val_acc= 0.93109 time= 0.16801
Epoch: 0099 train_loss= 0.15418 train_acc= 0.96853 val_loss= 0.25102 val_acc= 0.93262 time= 0.16700
Epoch: 0100 train_loss= 0.15605 train_acc= 0.96836 val_loss= 0.24974 val_acc= 0.93262 time= 0.17199
Epoch: 0101 train_loss= 0.14702 train_acc= 0.97329 val_loss= 0.24814 val_acc= 0.93262 time= 0.18000
Epoch: 0102 train_loss= 0.15024 train_acc= 0.97244 val_loss= 0.24711 val_acc= 0.93262 time= 0.17147
Epoch: 0103 train_loss= 0.13754 train_acc= 0.97329 val_loss= 0.24606 val_acc= 0.93109 time= 0.17200
Epoch: 0104 train_loss= 0.13773 train_acc= 0.97329 val_loss= 0.24417 val_acc= 0.93109 time= 0.16601
Epoch: 0105 train_loss= 0.13104 train_acc= 0.97415 val_loss= 0.24198 val_acc= 0.93109 time= 0.17099
Epoch: 0106 train_loss= 0.12727 train_acc= 0.97534 val_loss= 0.24007 val_acc= 0.93262 time= 0.19801
Epoch: 0107 train_loss= 0.12778 train_acc= 0.97585 val_loss= 0.23747 val_acc= 0.93415 time= 0.16800
Epoch: 0108 train_loss= 0.12212 train_acc= 0.97619 val_loss= 0.23579 val_acc= 0.93262 time= 0.18400
Epoch: 0109 train_loss= 0.12693 train_acc= 0.97840 val_loss= 0.23512 val_acc= 0.93109 time= 0.17391
Epoch: 0110 train_loss= 0.11800 train_acc= 0.98010 val_loss= 0.23555 val_acc= 0.92802 time= 0.17001
Epoch: 0111 train_loss= 0.11558 train_acc= 0.97823 val_loss= 0.23634 val_acc= 0.93109 time= 0.16803
Epoch: 0112 train_loss= 0.11436 train_acc= 0.97789 val_loss= 0.23584 val_acc= 0.92956 time= 0.19801
Epoch: 0113 train_loss= 0.10651 train_acc= 0.98027 val_loss= 0.23507 val_acc= 0.93109 time= 0.17500
Epoch: 0114 train_loss= 0.10497 train_acc= 0.97993 val_loss= 0.23402 val_acc= 0.93262 time= 0.16700
Epoch: 0115 train_loss= 0.10331 train_acc= 0.98129 val_loss= 0.23300 val_acc= 0.93415 time= 0.16899
Epoch: 0116 train_loss= 0.10181 train_acc= 0.98129 val_loss= 0.23151 val_acc= 0.93262 time= 0.16900
Epoch: 0117 train_loss= 0.10128 train_acc= 0.98010 val_loss= 0.23061 val_acc= 0.93415 time= 0.17497
Epoch: 0118 train_loss= 0.09903 train_acc= 0.98214 val_loss= 0.22952 val_acc= 0.93415 time= 0.20100
Epoch: 0119 train_loss= 0.09704 train_acc= 0.98282 val_loss= 0.22900 val_acc= 0.93262 time= 0.16704
Epoch: 0120 train_loss= 0.09251 train_acc= 0.98367 val_loss= 0.22911 val_acc= 0.93415 time= 0.16800
Epoch: 0121 train_loss= 0.09118 train_acc= 0.98333 val_loss= 0.22837 val_acc= 0.93415 time= 0.17199
Epoch: 0122 train_loss= 0.08696 train_acc= 0.98452 val_loss= 0.22717 val_acc= 0.93721 time= 0.17201
Epoch: 0123 train_loss= 0.08455 train_acc= 0.98707 val_loss= 0.22540 val_acc= 0.93874 time= 0.20099
Epoch: 0124 train_loss= 0.08400 train_acc= 0.98520 val_loss= 0.22360 val_acc= 0.93874 time= 0.17054
Epoch: 0125 train_loss= 0.08372 train_acc= 0.98299 val_loss= 0.22217 val_acc= 0.94028 time= 0.18341
Epoch: 0126 train_loss= 0.08260 train_acc= 0.98622 val_loss= 0.22061 val_acc= 0.94028 time= 0.16800
Epoch: 0127 train_loss= 0.07880 train_acc= 0.98792 val_loss= 0.21948 val_acc= 0.94028 time= 0.17202
Epoch: 0128 train_loss= 0.07990 train_acc= 0.98418 val_loss= 0.21933 val_acc= 0.93721 time= 0.17099
Epoch: 0129 train_loss= 0.07605 train_acc= 0.98775 val_loss= 0.21993 val_acc= 0.93721 time= 0.19900
Epoch: 0130 train_loss= 0.07477 train_acc= 0.98792 val_loss= 0.22004 val_acc= 0.93721 time= 0.17400
Epoch: 0131 train_loss= 0.07225 train_acc= 0.98809 val_loss= 0.21975 val_acc= 0.94028 time= 0.16997
Epoch: 0132 train_loss= 0.07114 train_acc= 0.98843 val_loss= 0.21907 val_acc= 0.93874 time= 0.16951
Epoch: 0133 train_loss= 0.06877 train_acc= 0.98911 val_loss= 0.21868 val_acc= 0.93874 time= 0.17100
Epoch: 0134 train_loss= 0.06673 train_acc= 0.98928 val_loss= 0.21873 val_acc= 0.93568 time= 0.19700
Epoch: 0135 train_loss= 0.06892 train_acc= 0.98809 val_loss= 0.21895 val_acc= 0.93568 time= 0.16800
Epoch: 0136 train_loss= 0.06724 train_acc= 0.98809 val_loss= 0.21913 val_acc= 0.93721 time= 0.16800
Epoch: 0137 train_loss= 0.06332 train_acc= 0.98928 val_loss= 0.21910 val_acc= 0.93568 time= 0.16801
Epoch: 0138 train_loss= 0.06561 train_acc= 0.98911 val_loss= 0.21873 val_acc= 0.93721 time= 0.16598
Epoch: 0139 train_loss= 0.06381 train_acc= 0.98809 val_loss= 0.21861 val_acc= 0.93874 time= 0.17300
Epoch: 0140 train_loss= 0.06157 train_acc= 0.98911 val_loss= 0.21762 val_acc= 0.93721 time= 0.20403
Epoch: 0141 train_loss= 0.06032 train_acc= 0.98945 val_loss= 0.21678 val_acc= 0.93874 time= 0.16701
Epoch: 0142 train_loss= 0.05949 train_acc= 0.98911 val_loss= 0.21634 val_acc= 0.93721 time= 0.17999
Epoch: 0143 train_loss= 0.06127 train_acc= 0.98979 val_loss= 0.21565 val_acc= 0.93721 time= 0.16778
Epoch: 0144 train_loss= 0.05611 train_acc= 0.99081 val_loss= 0.21508 val_acc= 0.93415 time= 0.16901
Epoch: 0145 train_loss= 0.05835 train_acc= 0.99081 val_loss= 0.21501 val_acc= 0.93721 time= 0.17099
Epoch: 0146 train_loss= 0.05286 train_acc= 0.99218 val_loss= 0.21600 val_acc= 0.93568 time= 0.19896
Epoch: 0147 train_loss= 0.05605 train_acc= 0.99133 val_loss= 0.21852 val_acc= 0.93415 time= 0.18304
Epoch: 0148 train_loss= 0.05708 train_acc= 0.99013 val_loss= 0.21867 val_acc= 0.93568 time= 0.16804
Epoch: 0149 train_loss= 0.05207 train_acc= 0.99286 val_loss= 0.21734 val_acc= 0.93874 time= 0.16700
Epoch: 0150 train_loss= 0.05048 train_acc= 0.99201 val_loss= 0.21623 val_acc= 0.93874 time= 0.16796
Epoch: 0151 train_loss= 0.05190 train_acc= 0.99235 val_loss= 0.21583 val_acc= 0.93874 time= 0.20103
Epoch: 0152 train_loss= 0.04813 train_acc= 0.99235 val_loss= 0.21631 val_acc= 0.93721 time= 0.16936
Epoch: 0153 train_loss= 0.04690 train_acc= 0.99337 val_loss= 0.21604 val_acc= 0.93721 time= 0.17000
Epoch: 0154 train_loss= 0.04791 train_acc= 0.99167 val_loss= 0.21654 val_acc= 0.93568 time= 0.17046
Epoch: 0155 train_loss= 0.04628 train_acc= 0.99320 val_loss= 0.21684 val_acc= 0.93262 time= 0.16901
Epoch: 0156 train_loss= 0.04794 train_acc= 0.99184 val_loss= 0.21608 val_acc= 0.93721 time= 0.16799
Epoch: 0157 train_loss= 0.04642 train_acc= 0.99354 val_loss= 0.21539 val_acc= 0.93568 time= 0.19100
Epoch: 0158 train_loss= 0.04531 train_acc= 0.99354 val_loss= 0.21611 val_acc= 0.94028 time= 0.17100
Epoch: 0159 train_loss= 0.04249 train_acc= 0.99337 val_loss= 0.21665 val_acc= 0.94181 time= 0.18221
Epoch: 0160 train_loss= 0.04283 train_acc= 0.99473 val_loss= 0.21656 val_acc= 0.94028 time= 0.17122
Epoch: 0161 train_loss= 0.04225 train_acc= 0.99337 val_loss= 0.21659 val_acc= 0.93568 time= 0.17196
Epoch: 0162 train_loss= 0.04434 train_acc= 0.99422 val_loss= 0.21833 val_acc= 0.93874 time= 0.17200
Epoch: 0163 train_loss= 0.04246 train_acc= 0.99354 val_loss= 0.21974 val_acc= 0.93721 time= 0.20209
Epoch: 0164 train_loss= 0.03908 train_acc= 0.99405 val_loss= 0.21999 val_acc= 0.94028 time= 0.17703
Epoch: 0165 train_loss= 0.04026 train_acc= 0.99252 val_loss= 0.21813 val_acc= 0.94181 time= 0.16800
Epoch: 0166 train_loss= 0.03948 train_acc= 0.99320 val_loss= 0.21601 val_acc= 0.94181 time= 0.16799
Epoch: 0167 train_loss= 0.03879 train_acc= 0.99388 val_loss= 0.21372 val_acc= 0.94334 time= 0.16800
Epoch: 0168 train_loss= 0.03826 train_acc= 0.99405 val_loss= 0.21343 val_acc= 0.94334 time= 0.17418
Epoch: 0169 train_loss= 0.04004 train_acc= 0.99456 val_loss= 0.21346 val_acc= 0.94181 time= 0.20400
Epoch: 0170 train_loss= 0.03644 train_acc= 0.99524 val_loss= 0.21382 val_acc= 0.94028 time= 0.17004
Epoch: 0171 train_loss= 0.03459 train_acc= 0.99490 val_loss= 0.21568 val_acc= 0.93874 time= 0.16800
Epoch: 0172 train_loss= 0.03842 train_acc= 0.99558 val_loss= 0.21768 val_acc= 0.93874 time= 0.16696
Epoch: 0173 train_loss= 0.03624 train_acc= 0.99456 val_loss= 0.21815 val_acc= 0.93721 time= 0.16903
Epoch: 0174 train_loss= 0.03445 train_acc= 0.99541 val_loss= 0.21778 val_acc= 0.93721 time= 0.20000
Epoch: 0175 train_loss= 0.03467 train_acc= 0.99490 val_loss= 0.21689 val_acc= 0.94028 time= 0.16901
Epoch: 0176 train_loss= 0.03217 train_acc= 0.99660 val_loss= 0.21620 val_acc= 0.94028 time= 0.18503
Epoch: 0177 train_loss= 0.03531 train_acc= 0.99541 val_loss= 0.21625 val_acc= 0.94181 time= 0.17103
Epoch: 0178 train_loss= 0.03423 train_acc= 0.99558 val_loss= 0.21722 val_acc= 0.94334 time= 0.16900
Epoch: 0179 train_loss= 0.03371 train_acc= 0.99422 val_loss= 0.21801 val_acc= 0.94181 time= 0.16801
Epoch: 0180 train_loss= 0.03329 train_acc= 0.99439 val_loss= 0.21857 val_acc= 0.94028 time= 0.20396
Epoch: 0181 train_loss= 0.03106 train_acc= 0.99677 val_loss= 0.21845 val_acc= 0.94181 time= 0.17905
Epoch: 0182 train_loss= 0.03223 train_acc= 0.99626 val_loss= 0.21790 val_acc= 0.94334 time= 0.17097
Epoch: 0183 train_loss= 0.03073 train_acc= 0.99694 val_loss= 0.21769 val_acc= 0.94028 time= 0.16954
Epoch: 0184 train_loss= 0.03005 train_acc= 0.99660 val_loss= 0.21907 val_acc= 0.94028 time= 0.17205
Epoch: 0185 train_loss= 0.02916 train_acc= 0.99609 val_loss= 0.21961 val_acc= 0.94181 time= 0.17900
Epoch: 0186 train_loss= 0.02965 train_acc= 0.99626 val_loss= 0.21981 val_acc= 0.94181 time= 0.17707
Epoch: 0187 train_loss= 0.03073 train_acc= 0.99490 val_loss= 0.21791 val_acc= 0.94028 time= 0.17100
Epoch: 0188 train_loss= 0.03069 train_acc= 0.99473 val_loss= 0.21701 val_acc= 0.94028 time= 0.17107
Epoch: 0189 train_loss= 0.02820 train_acc= 0.99626 val_loss= 0.21699 val_acc= 0.94028 time= 0.16847
Epoch: 0190 train_loss= 0.02874 train_acc= 0.99439 val_loss= 0.21705 val_acc= 0.94028 time= 0.16897
Epoch: 0191 train_loss= 0.02645 train_acc= 0.99643 val_loss= 0.21796 val_acc= 0.94334 time= 0.20500
Epoch: 0192 train_loss= 0.02680 train_acc= 0.99609 val_loss= 0.21902 val_acc= 0.94181 time= 0.17078
Epoch: 0193 train_loss= 0.03089 train_acc= 0.99490 val_loss= 0.21938 val_acc= 0.94334 time= 0.17903
Epoch: 0194 train_loss= 0.02880 train_acc= 0.99473 val_loss= 0.21977 val_acc= 0.94334 time= 0.16900
Epoch: 0195 train_loss= 0.02542 train_acc= 0.99711 val_loss= 0.22021 val_acc= 0.94181 time= 0.16800
Epoch: 0196 train_loss= 0.02811 train_acc= 0.99575 val_loss= 0.21925 val_acc= 0.94334 time= 0.16897
Epoch: 0197 train_loss= 0.02684 train_acc= 0.99677 val_loss= 0.21776 val_acc= 0.94028 time= 0.20001
Epoch: 0198 train_loss= 0.02684 train_acc= 0.99575 val_loss= 0.21727 val_acc= 0.94028 time= 0.17954
Epoch: 0199 train_loss= 0.02604 train_acc= 0.99609 val_loss= 0.21664 val_acc= 0.94334 time= 0.17197
Epoch: 0200 train_loss= 0.02823 train_acc= 0.99694 val_loss= 0.21736 val_acc= 0.94181 time= 0.17003
Epoch: 0201 train_loss= 0.02582 train_acc= 0.99558 val_loss= 0.21832 val_acc= 0.94181 time= 0.17111
Epoch: 0202 train_loss= 0.02583 train_acc= 0.99592 val_loss= 0.22002 val_acc= 0.94028 time= 0.17199
Epoch: 0203 train_loss= 0.02479 train_acc= 0.99592 val_loss= 0.22227 val_acc= 0.94028 time= 0.20001
Early stopping...
Optimization Finished!
Test set results: cost= 0.25178 accuracy= 0.93731 time= 0.07796
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8750    0.8750    0.8750         8
           1     0.5000    0.1667    0.2500         6
           2     1.0000    1.0000    1.0000         1
           3     0.8171    0.8933    0.8535        75
           4     1.0000    1.0000    1.0000         9
           5     0.8511    0.9195    0.8840        87
           6     0.9583    0.9200    0.9388        25
           7     0.6875    0.8462    0.7586        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.5556    0.7143         9
          10     0.9000    0.7500    0.8182        36
          11     1.0000    0.9167    0.9565        12
          12     0.8561    0.9835    0.9154       121
          13     1.0000    0.6842    0.8125        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    1.0000    1.0000         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.6250    0.5556    0.5882         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     1.0000    1.0000    1.0000         1
          24     0.8235    0.8235    0.8235        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9639    0.9598    0.9618       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.6429    0.9000    0.7500        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8272    0.8272    0.8272        81
          36     0.8571    0.5000    0.6316        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9917    0.9844      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9000    0.7500    0.8182        12
          45     0.5000    0.1667    0.2500         6
          46     1.0000    0.1429    0.2500         7
          47     0.7895    1.0000    0.8824        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     0.6000    0.7500    0.6667         4

    accuracy                         0.9373      2568
   macro avg     0.7755    0.6904    0.7098      2568
weighted avg     0.9358    0.9373    0.9332      2568

Macro average Test Precision, Recall and F1-Score...
(0.7754571224067957, 0.690384175142566, 0.709824462596501, None)
Micro average Test Precision, Recall and F1-Score...
(0.9373052959501558, 0.9373052959501558, 0.9373052959501558, None)
embeddings:
8892 6532 2568
[[-0.02772585  0.15781622  0.0120355  ...  0.0292251  -0.13111705
   1.6591616 ]
 [ 0.08594527  0.06521748 -0.02210109 ...  0.10794821  0.172351
   0.9871244 ]
 [ 0.00185175  0.2080594   0.3827357  ...  0.17206092  0.00856834
   0.81535876]
 ...
 [ 0.03657556  0.04132214  0.23068471 ...  0.4430603   0.02603212
   0.08229897]
 [ 0.07246408  0.12155394  0.16819926 ...  0.16595906  0.04167955
   0.35128805]
 [ 0.3691302   0.25564447  0.26617792 ...  0.2093456   0.26561195
   0.2607921 ]]

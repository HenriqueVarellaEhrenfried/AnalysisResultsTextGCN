(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95118 train_acc= 0.02415 val_loss= 3.91021 val_acc= 0.67688 time= 0.44152
Epoch: 0002 train_loss= 3.90959 train_acc= 0.63429 val_loss= 3.82178 val_acc= 0.66922 time= 0.19715
Epoch: 0003 train_loss= 3.81740 train_acc= 0.63106 val_loss= 3.68309 val_acc= 0.66003 time= 0.16600
Epoch: 0004 train_loss= 3.68387 train_acc= 0.62732 val_loss= 3.49466 val_acc= 0.64931 time= 0.16958
Epoch: 0005 train_loss= 3.51025 train_acc= 0.61014 val_loss= 3.26388 val_acc= 0.62940 time= 0.17000
Epoch: 0006 train_loss= 3.25677 train_acc= 0.60061 val_loss= 3.01085 val_acc= 0.61715 time= 0.20232
Epoch: 0007 train_loss= 3.00477 train_acc= 0.58717 val_loss= 2.76390 val_acc= 0.60949 time= 0.16597
Epoch: 0008 train_loss= 2.80705 train_acc= 0.57969 val_loss= 2.55282 val_acc= 0.60337 time= 0.16600
Epoch: 0009 train_loss= 2.57396 train_acc= 0.57867 val_loss= 2.40307 val_acc= 0.60184 time= 0.16606
Epoch: 0010 train_loss= 2.41996 train_acc= 0.56064 val_loss= 2.31622 val_acc= 0.63247 time= 0.17000
Epoch: 0011 train_loss= 2.34267 train_acc= 0.58854 val_loss= 2.26761 val_acc= 0.64472 time= 0.19200
Epoch: 0012 train_loss= 2.30140 train_acc= 0.58377 val_loss= 2.22970 val_acc= 0.46708 time= 0.16876
Epoch: 0013 train_loss= 2.26852 train_acc= 0.47100 val_loss= 2.18712 val_acc= 0.45636 time= 0.17002
Epoch: 0014 train_loss= 2.20660 train_acc= 0.44497 val_loss= 2.13017 val_acc= 0.45636 time= 0.18804
Epoch: 0015 train_loss= 2.16563 train_acc= 0.43460 val_loss= 2.05670 val_acc= 0.45636 time= 0.16596
Epoch: 0016 train_loss= 2.07023 train_acc= 0.43392 val_loss= 1.97172 val_acc= 0.45636 time= 0.16703
Epoch: 0017 train_loss= 2.00238 train_acc= 0.43664 val_loss= 1.88565 val_acc= 0.46401 time= 0.19407
Epoch: 0018 train_loss= 1.92218 train_acc= 0.44736 val_loss= 1.80946 val_acc= 0.49923 time= 0.16600
Epoch: 0019 train_loss= 1.86941 train_acc= 0.51131 val_loss= 1.74685 val_acc= 0.59571 time= 0.17797
Epoch: 0020 train_loss= 1.76949 train_acc= 0.57459 val_loss= 1.69361 val_acc= 0.65697 time= 0.16800
Epoch: 0021 train_loss= 1.71085 train_acc= 0.63038 val_loss= 1.64439 val_acc= 0.67228 time= 0.17100
Epoch: 0022 train_loss= 1.68379 train_acc= 0.62970 val_loss= 1.59464 val_acc= 0.67381 time= 0.16900
Epoch: 0023 train_loss= 1.64350 train_acc= 0.65198 val_loss= 1.54274 val_acc= 0.67228 time= 0.19503
Epoch: 0024 train_loss= 1.62641 train_acc= 0.64280 val_loss= 1.49041 val_acc= 0.67688 time= 0.16600
Epoch: 0025 train_loss= 1.56327 train_acc= 0.63684 val_loss= 1.44056 val_acc= 0.67841 time= 0.18201
Epoch: 0026 train_loss= 1.47020 train_acc= 0.65981 val_loss= 1.39546 val_acc= 0.68760 time= 0.16697
Epoch: 0027 train_loss= 1.43868 train_acc= 0.66797 val_loss= 1.35525 val_acc= 0.70291 time= 0.16800
Epoch: 0028 train_loss= 1.38334 train_acc= 0.67546 val_loss= 1.31974 val_acc= 0.70904 time= 0.17000
Epoch: 0029 train_loss= 1.37480 train_acc= 0.67733 val_loss= 1.28752 val_acc= 0.71363 time= 0.20100
Epoch: 0030 train_loss= 1.33142 train_acc= 0.68515 val_loss= 1.25780 val_acc= 0.71516 time= 0.16703
Epoch: 0031 train_loss= 1.29845 train_acc= 0.70369 val_loss= 1.22987 val_acc= 0.72129 time= 0.18397
Epoch: 0032 train_loss= 1.28332 train_acc= 0.69280 val_loss= 1.20333 val_acc= 0.72894 time= 0.16676
Epoch: 0033 train_loss= 1.23839 train_acc= 0.71118 val_loss= 1.17777 val_acc= 0.73201 time= 0.16605
Epoch: 0034 train_loss= 1.21675 train_acc= 0.71543 val_loss= 1.15264 val_acc= 0.73354 time= 0.16700
Epoch: 0035 train_loss= 1.18645 train_acc= 0.72665 val_loss= 1.12806 val_acc= 0.73660 time= 0.20301
Epoch: 0036 train_loss= 1.15239 train_acc= 0.72682 val_loss= 1.10430 val_acc= 0.73660 time= 0.16989
Epoch: 0037 train_loss= 1.14636 train_acc= 0.73312 val_loss= 1.08139 val_acc= 0.74732 time= 0.16850
Epoch: 0038 train_loss= 1.11446 train_acc= 0.73686 val_loss= 1.05925 val_acc= 0.74885 time= 0.16700
Epoch: 0039 train_loss= 1.10130 train_acc= 0.74485 val_loss= 1.03744 val_acc= 0.75498 time= 0.16601
Epoch: 0040 train_loss= 1.07457 train_acc= 0.73924 val_loss= 1.01605 val_acc= 0.76263 time= 0.19200
Epoch: 0041 train_loss= 1.05489 train_acc= 0.75387 val_loss= 0.99501 val_acc= 0.76876 time= 0.16999
Epoch: 0042 train_loss= 1.03616 train_acc= 0.75438 val_loss= 0.97441 val_acc= 0.77795 time= 0.18400
Epoch: 0043 train_loss= 1.00473 train_acc= 0.77377 val_loss= 0.95426 val_acc= 0.78254 time= 0.17100
Epoch: 0044 train_loss= 0.99850 train_acc= 0.77428 val_loss= 0.93469 val_acc= 0.79326 time= 0.17097
Epoch: 0045 train_loss= 0.97453 train_acc= 0.78143 val_loss= 0.91586 val_acc= 0.80551 time= 0.16703
Epoch: 0046 train_loss= 0.97006 train_acc= 0.77751 val_loss= 0.89764 val_acc= 0.81011 time= 0.19600
Epoch: 0047 train_loss= 0.94966 train_acc= 0.80014 val_loss= 0.87989 val_acc= 0.82389 time= 0.17100
Epoch: 0048 train_loss= 0.91440 train_acc= 0.79946 val_loss= 0.86282 val_acc= 0.83002 time= 0.16700
Epoch: 0049 train_loss= 0.89702 train_acc= 0.81817 val_loss= 0.84601 val_acc= 0.82848 time= 0.16597
Epoch: 0050 train_loss= 0.89289 train_acc= 0.80830 val_loss= 0.82920 val_acc= 0.83002 time= 0.17000
Epoch: 0051 train_loss= 0.87639 train_acc= 0.80575 val_loss= 0.81191 val_acc= 0.83767 time= 0.17411
Epoch: 0052 train_loss= 0.87657 train_acc= 0.80660 val_loss= 0.79382 val_acc= 0.83920 time= 0.19597
Epoch: 0053 train_loss= 0.84602 train_acc= 0.80643 val_loss= 0.77547 val_acc= 0.84686 time= 0.17003
Epoch: 0054 train_loss= 0.82863 train_acc= 0.81204 val_loss= 0.75760 val_acc= 0.84992 time= 0.18409
Epoch: 0055 train_loss= 0.81094 train_acc= 0.81596 val_loss= 0.74053 val_acc= 0.84992 time= 0.16800
Epoch: 0056 train_loss= 0.79716 train_acc= 0.81613 val_loss= 0.72392 val_acc= 0.84839 time= 0.17001
Epoch: 0057 train_loss= 0.77443 train_acc= 0.82072 val_loss= 0.70800 val_acc= 0.84839 time= 0.16943
Epoch: 0058 train_loss= 0.76974 train_acc= 0.82378 val_loss= 0.69279 val_acc= 0.85299 time= 0.17900
Epoch: 0059 train_loss= 0.74930 train_acc= 0.83228 val_loss= 0.67781 val_acc= 0.85605 time= 0.18851
Epoch: 0060 train_loss= 0.72865 train_acc= 0.82905 val_loss= 0.66311 val_acc= 0.85758 time= 0.16706
Epoch: 0061 train_loss= 0.71442 train_acc= 0.83416 val_loss= 0.64892 val_acc= 0.85758 time= 0.16798
Epoch: 0062 train_loss= 0.70685 train_acc= 0.83382 val_loss= 0.63546 val_acc= 0.85758 time= 0.16701
Epoch: 0063 train_loss= 0.68149 train_acc= 0.83773 val_loss= 0.62267 val_acc= 0.86524 time= 0.19501
Epoch: 0064 train_loss= 0.65562 train_acc= 0.85304 val_loss= 0.61054 val_acc= 0.86371 time= 0.16701
Epoch: 0065 train_loss= 0.66622 train_acc= 0.83365 val_loss= 0.59917 val_acc= 0.86524 time= 0.17495
Epoch: 0066 train_loss= 0.64435 train_acc= 0.84708 val_loss= 0.58878 val_acc= 0.86677 time= 0.17202
Epoch: 0067 train_loss= 0.62382 train_acc= 0.85780 val_loss= 0.57932 val_acc= 0.86983 time= 0.16903
Epoch: 0068 train_loss= 0.62479 train_acc= 0.85048 val_loss= 0.56921 val_acc= 0.86983 time= 0.16800
Epoch: 0069 train_loss= 0.61750 train_acc= 0.85644 val_loss= 0.55815 val_acc= 0.87596 time= 0.19700
Epoch: 0070 train_loss= 0.60182 train_acc= 0.85916 val_loss= 0.54715 val_acc= 0.87902 time= 0.16660
Epoch: 0071 train_loss= 0.58080 train_acc= 0.86596 val_loss= 0.53616 val_acc= 0.88055 time= 0.18296
Epoch: 0072 train_loss= 0.58081 train_acc= 0.86477 val_loss= 0.52522 val_acc= 0.88055 time= 0.17003
Epoch: 0073 train_loss= 0.56862 train_acc= 0.86477 val_loss= 0.51465 val_acc= 0.88515 time= 0.16898
Epoch: 0074 train_loss= 0.54400 train_acc= 0.86886 val_loss= 0.50454 val_acc= 0.88668 time= 0.17300
Epoch: 0075 train_loss= 0.52805 train_acc= 0.87277 val_loss= 0.49471 val_acc= 0.88821 time= 0.19103
Epoch: 0076 train_loss= 0.53360 train_acc= 0.87311 val_loss= 0.48555 val_acc= 0.88974 time= 0.16700
Epoch: 0077 train_loss= 0.51359 train_acc= 0.87583 val_loss= 0.47674 val_acc= 0.89127 time= 0.16700
Epoch: 0078 train_loss= 0.51894 train_acc= 0.87600 val_loss= 0.46772 val_acc= 0.89280 time= 0.17297
Epoch: 0079 train_loss= 0.51265 train_acc= 0.87413 val_loss= 0.45890 val_acc= 0.89587 time= 0.16700
Epoch: 0080 train_loss= 0.49112 train_acc= 0.87668 val_loss= 0.45039 val_acc= 0.89740 time= 0.20203
Epoch: 0081 train_loss= 0.49235 train_acc= 0.88076 val_loss= 0.44373 val_acc= 0.89740 time= 0.17303
Epoch: 0082 train_loss= 0.48190 train_acc= 0.87855 val_loss= 0.43834 val_acc= 0.89587 time= 0.18604
Epoch: 0083 train_loss= 0.45919 train_acc= 0.88842 val_loss= 0.43413 val_acc= 0.89587 time= 0.16800
Epoch: 0084 train_loss= 0.46176 train_acc= 0.88621 val_loss= 0.42925 val_acc= 0.89893 time= 0.17000
Epoch: 0085 train_loss= 0.46181 train_acc= 0.88621 val_loss= 0.42335 val_acc= 0.90046 time= 0.16600
Epoch: 0086 train_loss= 0.46695 train_acc= 0.89216 val_loss= 0.41638 val_acc= 0.90046 time= 0.18500
Epoch: 0087 train_loss= 0.45654 train_acc= 0.89216 val_loss= 0.40899 val_acc= 0.90046 time= 0.16900
Epoch: 0088 train_loss= 0.42866 train_acc= 0.89403 val_loss= 0.40236 val_acc= 0.90199 time= 0.16904
Epoch: 0089 train_loss= 0.43471 train_acc= 0.89658 val_loss= 0.39623 val_acc= 0.90352 time= 0.17000
Epoch: 0090 train_loss= 0.42101 train_acc= 0.89981 val_loss= 0.39032 val_acc= 0.89893 time= 0.17358
Epoch: 0091 train_loss= 0.41263 train_acc= 0.89981 val_loss= 0.38444 val_acc= 0.89893 time= 0.17300
Epoch: 0092 train_loss= 0.40091 train_acc= 0.89845 val_loss= 0.37800 val_acc= 0.90199 time= 0.19500
Epoch: 0093 train_loss= 0.42032 train_acc= 0.89930 val_loss= 0.37277 val_acc= 0.90352 time= 0.16900
Epoch: 0094 train_loss= 0.39961 train_acc= 0.89981 val_loss= 0.36910 val_acc= 0.90352 time= 0.18797
Epoch: 0095 train_loss= 0.39412 train_acc= 0.90287 val_loss= 0.36461 val_acc= 0.90199 time= 0.16759
Epoch: 0096 train_loss= 0.39359 train_acc= 0.90270 val_loss= 0.35894 val_acc= 0.90352 time= 0.17584
Epoch: 0097 train_loss= 0.39709 train_acc= 0.90662 val_loss= 0.35361 val_acc= 0.90505 time= 0.19222
Epoch: 0098 train_loss= 0.38556 train_acc= 0.90390 val_loss= 0.34855 val_acc= 0.90965 time= 0.16601
Epoch: 0099 train_loss= 0.37582 train_acc= 0.90934 val_loss= 0.34513 val_acc= 0.90658 time= 0.18599
Epoch: 0100 train_loss= 0.37659 train_acc= 0.90764 val_loss= 0.34211 val_acc= 0.90658 time= 0.16600
Epoch: 0101 train_loss= 0.36689 train_acc= 0.90934 val_loss= 0.33881 val_acc= 0.90812 time= 0.16900
Epoch: 0102 train_loss= 0.37600 train_acc= 0.90900 val_loss= 0.33608 val_acc= 0.91271 time= 0.16900
Epoch: 0103 train_loss= 0.34422 train_acc= 0.91886 val_loss= 0.33278 val_acc= 0.91424 time= 0.20099
Epoch: 0104 train_loss= 0.33667 train_acc= 0.90951 val_loss= 0.32917 val_acc= 0.91730 time= 0.17104
Epoch: 0105 train_loss= 0.32647 train_acc= 0.92125 val_loss= 0.32542 val_acc= 0.91884 time= 0.17100
Epoch: 0106 train_loss= 0.34034 train_acc= 0.91580 val_loss= 0.32288 val_acc= 0.91884 time= 0.16601
Epoch: 0107 train_loss= 0.32656 train_acc= 0.92090 val_loss= 0.32052 val_acc= 0.91884 time= 0.16799
Epoch: 0108 train_loss= 0.32866 train_acc= 0.91733 val_loss= 0.31855 val_acc= 0.91730 time= 0.19700
Epoch: 0109 train_loss= 0.31951 train_acc= 0.92261 val_loss= 0.31625 val_acc= 0.91730 time= 0.16702
Epoch: 0110 train_loss= 0.32756 train_acc= 0.91903 val_loss= 0.31381 val_acc= 0.91577 time= 0.16935
Epoch: 0111 train_loss= 0.30743 train_acc= 0.92039 val_loss= 0.31222 val_acc= 0.91577 time= 0.19100
Epoch: 0112 train_loss= 0.31093 train_acc= 0.92346 val_loss= 0.31077 val_acc= 0.91424 time= 0.17000
Epoch: 0113 train_loss= 0.31193 train_acc= 0.91920 val_loss= 0.30867 val_acc= 0.91577 time= 0.16804
Epoch: 0114 train_loss= 0.28916 train_acc= 0.92431 val_loss= 0.30588 val_acc= 0.91424 time= 0.19400
Epoch: 0115 train_loss= 0.29275 train_acc= 0.92924 val_loss= 0.30382 val_acc= 0.91424 time= 0.16804
Epoch: 0116 train_loss= 0.30229 train_acc= 0.92533 val_loss= 0.30190 val_acc= 0.91424 time= 0.18097
Epoch: 0117 train_loss= 0.30380 train_acc= 0.92380 val_loss= 0.30055 val_acc= 0.91424 time= 0.16603
Epoch: 0118 train_loss= 0.29497 train_acc= 0.92533 val_loss= 0.29851 val_acc= 0.91271 time= 0.16897
Epoch: 0119 train_loss= 0.28373 train_acc= 0.92822 val_loss= 0.29513 val_acc= 0.91271 time= 0.17036
Epoch: 0120 train_loss= 0.27472 train_acc= 0.92822 val_loss= 0.29143 val_acc= 0.91577 time= 0.19200
Epoch: 0121 train_loss= 0.26913 train_acc= 0.93128 val_loss= 0.28823 val_acc= 0.92037 time= 0.16600
Epoch: 0122 train_loss= 0.26179 train_acc= 0.93400 val_loss= 0.28588 val_acc= 0.92037 time= 0.17303
Epoch: 0123 train_loss= 0.26229 train_acc= 0.93417 val_loss= 0.28464 val_acc= 0.91730 time= 0.16500
Epoch: 0124 train_loss= 0.25973 train_acc= 0.93417 val_loss= 0.28313 val_acc= 0.91730 time= 0.16597
Epoch: 0125 train_loss= 0.26591 train_acc= 0.93349 val_loss= 0.28155 val_acc= 0.91884 time= 0.16703
Epoch: 0126 train_loss= 0.26440 train_acc= 0.93332 val_loss= 0.27986 val_acc= 0.92037 time= 0.18297
Epoch: 0127 train_loss= 0.27659 train_acc= 0.92992 val_loss= 0.27777 val_acc= 0.92343 time= 0.17000
Epoch: 0128 train_loss= 0.25396 train_acc= 0.93962 val_loss= 0.27591 val_acc= 0.92649 time= 0.16700
Epoch: 0129 train_loss= 0.24032 train_acc= 0.93791 val_loss= 0.27490 val_acc= 0.92802 time= 0.16706
Epoch: 0130 train_loss= 0.25931 train_acc= 0.93298 val_loss= 0.27382 val_acc= 0.92496 time= 0.16600
Epoch: 0131 train_loss= 0.25771 train_acc= 0.93332 val_loss= 0.27324 val_acc= 0.92649 time= 0.16897
Epoch: 0132 train_loss= 0.25006 train_acc= 0.94064 val_loss= 0.27220 val_acc= 0.92037 time= 0.18800
Epoch: 0133 train_loss= 0.25182 train_acc= 0.93672 val_loss= 0.27006 val_acc= 0.92190 time= 0.16804
Epoch: 0134 train_loss= 0.23680 train_acc= 0.94336 val_loss= 0.26851 val_acc= 0.92037 time= 0.18478
Epoch: 0135 train_loss= 0.23615 train_acc= 0.93894 val_loss= 0.26766 val_acc= 0.92037 time= 0.17004
Epoch: 0136 train_loss= 0.23760 train_acc= 0.94064 val_loss= 0.26591 val_acc= 0.91884 time= 0.16699
Epoch: 0137 train_loss= 0.22085 train_acc= 0.95016 val_loss= 0.26515 val_acc= 0.91884 time= 0.16997
Epoch: 0138 train_loss= 0.22569 train_acc= 0.94438 val_loss= 0.26467 val_acc= 0.91884 time= 0.19500
Epoch: 0139 train_loss= 0.23425 train_acc= 0.93808 val_loss= 0.26364 val_acc= 0.91884 time= 0.18303
Epoch: 0140 train_loss= 0.21732 train_acc= 0.94149 val_loss= 0.26156 val_acc= 0.91577 time= 0.16800
Epoch: 0141 train_loss= 0.22165 train_acc= 0.94166 val_loss= 0.26001 val_acc= 0.91884 time= 0.16899
Epoch: 0142 train_loss= 0.21403 train_acc= 0.94863 val_loss= 0.25856 val_acc= 0.92649 time= 0.17201
Epoch: 0143 train_loss= 0.21790 train_acc= 0.94455 val_loss= 0.25643 val_acc= 0.92496 time= 0.18700
Epoch: 0144 train_loss= 0.21264 train_acc= 0.94472 val_loss= 0.25470 val_acc= 0.92802 time= 0.16801
Epoch: 0145 train_loss= 0.20713 train_acc= 0.94778 val_loss= 0.25308 val_acc= 0.93109 time= 0.18101
Epoch: 0146 train_loss= 0.22461 train_acc= 0.94285 val_loss= 0.25204 val_acc= 0.93109 time= 0.16796
Epoch: 0147 train_loss= 0.21084 train_acc= 0.94574 val_loss= 0.25091 val_acc= 0.93109 time= 0.16703
Epoch: 0148 train_loss= 0.19499 train_acc= 0.95016 val_loss= 0.24953 val_acc= 0.92649 time= 0.16700
Epoch: 0149 train_loss= 0.20436 train_acc= 0.94795 val_loss= 0.24816 val_acc= 0.92802 time= 0.18963
Epoch: 0150 train_loss= 0.20278 train_acc= 0.95288 val_loss= 0.24680 val_acc= 0.92802 time= 0.17100
Epoch: 0151 train_loss= 0.19171 train_acc= 0.95271 val_loss= 0.24595 val_acc= 0.92649 time= 0.17463
Epoch: 0152 train_loss= 0.20363 train_acc= 0.94523 val_loss= 0.24652 val_acc= 0.92343 time= 0.16603
Epoch: 0153 train_loss= 0.19452 train_acc= 0.95067 val_loss= 0.24705 val_acc= 0.92496 time= 0.16700
Epoch: 0154 train_loss= 0.19378 train_acc= 0.95186 val_loss= 0.24760 val_acc= 0.92496 time= 0.16901
Epoch: 0155 train_loss= 0.20266 train_acc= 0.94812 val_loss= 0.24707 val_acc= 0.92496 time= 0.19400
Epoch: 0156 train_loss= 0.20062 train_acc= 0.94965 val_loss= 0.24570 val_acc= 0.92496 time= 0.16900
Epoch: 0157 train_loss= 0.19348 train_acc= 0.95067 val_loss= 0.24400 val_acc= 0.92496 time= 0.18828
Epoch: 0158 train_loss= 0.19291 train_acc= 0.94829 val_loss= 0.24277 val_acc= 0.92496 time= 0.16997
Epoch: 0159 train_loss= 0.19270 train_acc= 0.95254 val_loss= 0.24206 val_acc= 0.92649 time= 0.16735
Epoch: 0160 train_loss= 0.19191 train_acc= 0.95101 val_loss= 0.24216 val_acc= 0.92496 time= 0.17000
Epoch: 0161 train_loss= 0.19556 train_acc= 0.95186 val_loss= 0.24170 val_acc= 0.92802 time= 0.19500
Epoch: 0162 train_loss= 0.17679 train_acc= 0.95441 val_loss= 0.24140 val_acc= 0.92802 time= 0.17705
Epoch: 0163 train_loss= 0.18341 train_acc= 0.95441 val_loss= 0.24101 val_acc= 0.92496 time= 0.16899
Epoch: 0164 train_loss= 0.19138 train_acc= 0.94795 val_loss= 0.24020 val_acc= 0.92956 time= 0.16796
Epoch: 0165 train_loss= 0.18098 train_acc= 0.95612 val_loss= 0.24052 val_acc= 0.92649 time= 0.17052
Epoch: 0166 train_loss= 0.17896 train_acc= 0.95441 val_loss= 0.24175 val_acc= 0.92649 time= 0.19103
Epoch: 0167 train_loss= 0.18074 train_acc= 0.95424 val_loss= 0.24307 val_acc= 0.92649 time= 0.17000
Early stopping...
Optimization Finished!
Test set results: cost= 0.27463 accuracy= 0.93107 time= 0.07700
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.8750    0.9333         8
           1     1.0000    0.3333    0.5000         6
           2     0.3333    1.0000    0.5000         1
           3     0.7778    0.9333    0.8485        75
           4     1.0000    1.0000    1.0000         9
           5     0.8351    0.9310    0.8804        87
           6     0.9231    0.9600    0.9412        25
           7     0.6875    0.8462    0.7586        13
           8     0.8462    1.0000    0.9167        11
           9     1.0000    0.2222    0.3636         9
          10     0.9167    0.6111    0.7333        36
          11     1.0000    1.0000    1.0000        12
          12     0.8333    0.9917    0.9057       121
          13     0.8125    0.6842    0.7429        19
          14     0.8333    0.8929    0.8621        28
          15     1.0000    0.2500    0.4000         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.6000    0.7500        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.6667    0.7059    0.6857        17
          25     0.8571    0.8000    0.8276        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.5000    0.6667        12
          28     0.8889    0.7273    0.8000        11
          29     0.9588    0.9698    0.9643       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.6667    0.8000         3
          32     0.8182    0.9000    0.8571        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8750    0.8642    0.8696        81
          36     1.0000    0.4167    0.5882        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9763    0.9908    0.9835      1083
          40     0.8000    0.8000    0.8000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8889    0.8889    0.8889         9
          43     0.0000    0.0000    0.0000         3
          44     0.7778    0.5833    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9311      2568
   macro avg     0.7046    0.6050    0.6249      2568
weighted avg     0.9270    0.9311    0.9240      2568

Macro average Test Precision, Recall and F1-Score...
(0.7045649035760853, 0.6049612642551241, 0.6249070141503462, None)
Micro average Test Precision, Recall and F1-Score...
(0.9310747663551402, 0.9310747663551402, 0.9310747663551402, None)
embeddings:
8892 6532 2568
[[-0.06740572 -0.1029552  -0.15864612 ...  0.751664    1.1316458
  -0.0528165 ]
 [ 0.06092319  0.20867781  0.03586661 ...  0.11068814  0.01596085
   0.07333286]
 [ 0.06440582  0.07608924 -0.00331146 ...  0.29207504  0.70350003
   0.09421871]
 ...
 [ 0.01384563  0.24350272  0.29607576 ...  0.26532966  0.3168231
   0.00671491]
 [ 0.03882765  0.11764749  0.13185377 ...  0.22259334  0.40705332
   0.07344218]
 [ 0.19902252  0.19453597  0.05920079 ...  0.25098962  0.16091406
   0.20991954]]

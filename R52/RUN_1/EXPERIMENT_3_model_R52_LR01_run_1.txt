(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95112 train_acc= 0.03096 val_loss= 3.30452 val_acc= 0.54824 time= 0.44403
Epoch: 0002 train_loss= 3.30569 train_acc= 0.54193 val_loss= 2.28790 val_acc= 0.49923 time= 0.17697
Epoch: 0003 train_loss= 2.28407 train_acc= 0.47525 val_loss= 2.19860 val_acc= 0.53905 time= 0.17952
Epoch: 0004 train_loss= 2.21167 train_acc= 0.52305 val_loss= 1.91700 val_acc= 0.67381 time= 0.16906
Epoch: 0005 train_loss= 1.93510 train_acc= 0.65215 val_loss= 1.56089 val_acc= 0.67075 time= 0.16699
Epoch: 0006 train_loss= 1.59889 train_acc= 0.65419 val_loss= 1.45157 val_acc= 0.68453 time= 0.16800
Epoch: 0007 train_loss= 1.49142 train_acc= 0.66916 val_loss= 1.36270 val_acc= 0.69832 time= 0.19200
Epoch: 0008 train_loss= 1.39617 train_acc= 0.68532 val_loss= 1.26163 val_acc= 0.69219 time= 0.16800
Epoch: 0009 train_loss= 1.29187 train_acc= 0.67597 val_loss= 1.19260 val_acc= 0.69219 time= 0.17002
Epoch: 0010 train_loss= 1.21725 train_acc= 0.67988 val_loss= 1.13599 val_acc= 0.70291 time= 0.17057
Epoch: 0011 train_loss= 1.14986 train_acc= 0.69757 val_loss= 1.08236 val_acc= 0.71822 time= 0.17100
Epoch: 0012 train_loss= 1.09481 train_acc= 0.72121 val_loss= 1.03537 val_acc= 0.73966 time= 0.17303
Epoch: 0013 train_loss= 1.03812 train_acc= 0.75030 val_loss= 0.98983 val_acc= 0.75345 time= 0.19899
Epoch: 0014 train_loss= 0.97768 train_acc= 0.76935 val_loss= 0.93944 val_acc= 0.77182 time= 0.16801
Epoch: 0015 train_loss= 0.93687 train_acc= 0.78415 val_loss= 0.88504 val_acc= 0.77642 time= 0.18497
Epoch: 0016 train_loss= 0.88263 train_acc= 0.78993 val_loss= 0.82959 val_acc= 0.79020 time= 0.17000
Epoch: 0017 train_loss= 0.83669 train_acc= 0.80575 val_loss= 0.77518 val_acc= 0.81470 time= 0.17061
Epoch: 0018 train_loss= 0.78665 train_acc= 0.82412 val_loss= 0.72480 val_acc= 0.83614 time= 0.16924
Epoch: 0019 train_loss= 0.73802 train_acc= 0.83296 val_loss= 0.68297 val_acc= 0.84686 time= 0.19565
Epoch: 0020 train_loss= 0.68787 train_acc= 0.84895 val_loss= 0.64849 val_acc= 0.84839 time= 0.17896
Epoch: 0021 train_loss= 0.66464 train_acc= 0.85236 val_loss= 0.61836 val_acc= 0.84686 time= 0.16900
Epoch: 0022 train_loss= 0.61376 train_acc= 0.85695 val_loss= 0.58982 val_acc= 0.85145 time= 0.16903
Epoch: 0023 train_loss= 0.57847 train_acc= 0.86154 val_loss= 0.56114 val_acc= 0.85145 time= 0.16873
Epoch: 0024 train_loss= 0.54192 train_acc= 0.86528 val_loss= 0.52823 val_acc= 0.85758 time= 0.18051
Epoch: 0025 train_loss= 0.49997 train_acc= 0.87617 val_loss= 0.49553 val_acc= 0.86677 time= 0.17083
Epoch: 0026 train_loss= 0.47372 train_acc= 0.88348 val_loss= 0.46785 val_acc= 0.87289 time= 0.17597
Epoch: 0027 train_loss= 0.44315 train_acc= 0.89029 val_loss= 0.44536 val_acc= 0.87749 time= 0.17003
Epoch: 0028 train_loss= 0.41408 train_acc= 0.89726 val_loss= 0.42690 val_acc= 0.87749 time= 0.16800
Epoch: 0029 train_loss= 0.38087 train_acc= 0.90543 val_loss= 0.41414 val_acc= 0.88208 time= 0.16792
Epoch: 0030 train_loss= 0.36385 train_acc= 0.90815 val_loss= 0.40377 val_acc= 0.88668 time= 0.19600
Epoch: 0031 train_loss= 0.33447 train_acc= 0.91529 val_loss= 0.38975 val_acc= 0.89280 time= 0.17200
Epoch: 0032 train_loss= 0.31208 train_acc= 0.92312 val_loss= 0.36890 val_acc= 0.90812 time= 0.18401
Epoch: 0033 train_loss= 0.29271 train_acc= 0.92924 val_loss= 0.34726 val_acc= 0.90812 time= 0.16896
Epoch: 0034 train_loss= 0.26396 train_acc= 0.93638 val_loss= 0.33098 val_acc= 0.90965 time= 0.17003
Epoch: 0035 train_loss= 0.24519 train_acc= 0.94200 val_loss= 0.31932 val_acc= 0.91424 time= 0.17100
Epoch: 0036 train_loss= 0.23523 train_acc= 0.93894 val_loss= 0.31574 val_acc= 0.91424 time= 0.19245
Epoch: 0037 train_loss= 0.21026 train_acc= 0.94863 val_loss= 0.31881 val_acc= 0.91424 time= 0.17504
Epoch: 0038 train_loss= 0.19769 train_acc= 0.94914 val_loss= 0.31782 val_acc= 0.91577 time= 0.17256
Epoch: 0039 train_loss= 0.18718 train_acc= 0.95458 val_loss= 0.31214 val_acc= 0.92190 time= 0.17600
Epoch: 0040 train_loss= 0.16679 train_acc= 0.95952 val_loss= 0.30277 val_acc= 0.92343 time= 0.17204
Epoch: 0041 train_loss= 0.15965 train_acc= 0.95731 val_loss= 0.29396 val_acc= 0.92649 time= 0.19738
Epoch: 0042 train_loss= 0.14894 train_acc= 0.96445 val_loss= 0.28953 val_acc= 0.92956 time= 0.17004
Epoch: 0043 train_loss= 0.13652 train_acc= 0.96258 val_loss= 0.28809 val_acc= 0.92802 time= 0.17500
Epoch: 0044 train_loss= 0.12763 train_acc= 0.96445 val_loss= 0.28617 val_acc= 0.93109 time= 0.16901
Epoch: 0045 train_loss= 0.11681 train_acc= 0.97244 val_loss= 0.28267 val_acc= 0.92956 time= 0.17195
Epoch: 0046 train_loss= 0.10896 train_acc= 0.97278 val_loss= 0.27889 val_acc= 0.92956 time= 0.17141
Epoch: 0047 train_loss= 0.10759 train_acc= 0.97278 val_loss= 0.27779 val_acc= 0.93262 time= 0.17100
Epoch: 0048 train_loss= 0.09665 train_acc= 0.97670 val_loss= 0.27868 val_acc= 0.93568 time= 0.16705
Epoch: 0049 train_loss= 0.09538 train_acc= 0.97653 val_loss= 0.28669 val_acc= 0.92649 time= 0.16998
Epoch: 0050 train_loss= 0.08777 train_acc= 0.97551 val_loss= 0.30371 val_acc= 0.92649 time= 0.17100
Early stopping...
Optimization Finished!
Test set results: cost= 0.31263 accuracy= 0.92523 time= 0.07302
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    0.5000    0.6667         8
           1     0.6667    0.3333    0.4444         6
           2     0.0000    0.0000    0.0000         1
           3     0.8395    0.9067    0.8718        75
           4     1.0000    1.0000    1.0000         9
           5     0.6967    0.9770    0.8134        87
           6     0.9583    0.9200    0.9388        25
           7     0.7857    0.8462    0.8148        13
           8     1.0000    0.7273    0.8421        11
           9     1.0000    0.3333    0.5000         9
          10     0.8889    0.6667    0.7619        36
          11     1.0000    1.0000    1.0000        12
          12     0.7857    1.0000    0.8800       121
          13     1.0000    0.6316    0.7742        19
          14     0.8276    0.8571    0.8421        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     1.0000    0.7000    0.8235        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.2222    0.3636         9
          21     0.9048    0.9500    0.9268        20
          22     0.5000    0.6000    0.5455         5
          23     0.0000    0.0000    0.0000         1
          24     0.5417    0.7647    0.6341        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.8182    0.9000        11
          29     0.9669    0.9655    0.9662       696
          30     0.9167    1.0000    0.9565        22
          31     1.0000    0.6667    0.8000         3
          32     0.5385    0.7000    0.6087        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8871    0.6790    0.7692        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9817    0.9917    0.9867      1083
          40     1.0000    0.8000    0.8889         5
          41     0.0000    0.0000    0.0000         2
          42     0.7273    0.8889    0.8000         9
          43     1.0000    0.6667    0.8000         3
          44     0.5556    0.8333    0.6667        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9252      2568
   macro avg     0.7274    0.5946    0.6298      2568
weighted avg     0.9297    0.9252    0.9204      2568

Macro average Test Precision, Recall and F1-Score...
(0.7274303996427218, 0.5946278435164861, 0.6298210186118939, None)
Micro average Test Precision, Recall and F1-Score...
(0.9252336448598131, 0.9252336448598131, 0.9252336448598131, None)
embeddings:
8892 6532 2568
[[-0.6259353  -0.48051643  2.7046437  ...  0.15705037 -0.05007329
  -0.53749   ]
 [-0.11247209 -0.14939605  1.4111334  ... -0.02056936 -0.17046249
  -0.2466373 ]
 [-0.23651579  0.11889639  0.8127678  ... -0.29464382  0.83638024
  -0.11188105]
 ...
 [-0.17393993  0.37098458  0.21331014 ... -0.19714414  0.34502685
  -0.16723871]
 [-0.09655249  0.0935997   0.4192904  ... -0.0637932   0.5391517
  -0.02332138]
 [ 0.1546555   0.03000486  0.46554497 ...  0.21311569  0.06999313
   0.11560068]]

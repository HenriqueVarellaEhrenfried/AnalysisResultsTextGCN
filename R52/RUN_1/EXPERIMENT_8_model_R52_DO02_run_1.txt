(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95127 train_acc= 0.00408 val_loss= 3.90036 val_acc= 0.66616 time= 0.48373
Epoch: 0002 train_loss= 3.90081 train_acc= 0.65589 val_loss= 3.80144 val_acc= 0.67075 time= 0.17703
Epoch: 0003 train_loss= 3.80298 train_acc= 0.65674 val_loss= 3.64622 val_acc= 0.67075 time= 0.16901
Epoch: 0004 train_loss= 3.64719 train_acc= 0.65572 val_loss= 3.43388 val_acc= 0.67075 time= 0.17095
Epoch: 0005 train_loss= 3.43432 train_acc= 0.65436 val_loss= 3.17354 val_acc= 0.66462 time= 0.19404
Epoch: 0006 train_loss= 3.18048 train_acc= 0.65113 val_loss= 2.88954 val_acc= 0.66156 time= 0.17900
Epoch: 0007 train_loss= 2.89165 train_acc= 0.64926 val_loss= 2.61583 val_acc= 0.65850 time= 0.17196
Epoch: 0008 train_loss= 2.62235 train_acc= 0.64297 val_loss= 2.39380 val_acc= 0.64931 time= 0.16971
Epoch: 0009 train_loss= 2.38965 train_acc= 0.63718 val_loss= 2.25279 val_acc= 0.62328 time= 0.16904
Epoch: 0010 train_loss= 2.26244 train_acc= 0.61626 val_loss= 2.18043 val_acc= 0.56662 time= 0.19600
Epoch: 0011 train_loss= 2.18779 train_acc= 0.56574 val_loss= 2.13673 val_acc= 0.51455 time= 0.17000
Epoch: 0012 train_loss= 2.15531 train_acc= 0.50842 val_loss= 2.08893 val_acc= 0.49617 time= 0.16659
Epoch: 0013 train_loss= 2.10454 train_acc= 0.47236 val_loss= 2.02235 val_acc= 0.49617 time= 0.16700
Epoch: 0014 train_loss= 2.03888 train_acc= 0.47542 val_loss= 1.93755 val_acc= 0.50995 time= 0.16700
Epoch: 0015 train_loss= 1.95976 train_acc= 0.50162 val_loss= 1.84450 val_acc= 0.55896 time= 0.19051
Epoch: 0016 train_loss= 1.86978 train_acc= 0.54873 val_loss= 1.75658 val_acc= 0.60643 time= 0.17156
Epoch: 0017 train_loss= 1.78477 train_acc= 0.60044 val_loss= 1.68356 val_acc= 0.65084 time= 0.16997
Epoch: 0018 train_loss= 1.71492 train_acc= 0.64007 val_loss= 1.62525 val_acc= 0.66769 time= 0.18503
Epoch: 0019 train_loss= 1.65037 train_acc= 0.65062 val_loss= 1.57416 val_acc= 0.67534 time= 0.16861
Epoch: 0020 train_loss= 1.60276 train_acc= 0.65555 val_loss= 1.52336 val_acc= 0.68913 time= 0.16701
Epoch: 0021 train_loss= 1.54620 train_acc= 0.66304 val_loss= 1.47111 val_acc= 0.69219 time= 0.19228
Epoch: 0022 train_loss= 1.49851 train_acc= 0.66933 val_loss= 1.41870 val_acc= 0.68913 time= 0.16897
Epoch: 0023 train_loss= 1.44045 train_acc= 0.67511 val_loss= 1.36905 val_acc= 0.69525 time= 0.19883
Epoch: 0024 train_loss= 1.39110 train_acc= 0.67954 val_loss= 1.32376 val_acc= 0.69832 time= 0.17603
Epoch: 0025 train_loss= 1.34948 train_acc= 0.68243 val_loss= 1.28317 val_acc= 0.70750 time= 0.17300
Epoch: 0026 train_loss= 1.30215 train_acc= 0.69025 val_loss= 1.24675 val_acc= 0.71669 time= 0.17200
Epoch: 0027 train_loss= 1.26817 train_acc= 0.70199 val_loss= 1.21339 val_acc= 0.71822 time= 0.17400
Epoch: 0028 train_loss= 1.23641 train_acc= 0.71203 val_loss= 1.18222 val_acc= 0.72588 time= 0.17000
Epoch: 0029 train_loss= 1.20294 train_acc= 0.72172 val_loss= 1.15247 val_acc= 0.73354 time= 0.16997
Epoch: 0030 train_loss= 1.17276 train_acc= 0.73533 val_loss= 1.12357 val_acc= 0.73813 time= 0.17200
Epoch: 0031 train_loss= 1.14173 train_acc= 0.74451 val_loss= 1.09518 val_acc= 0.74579 time= 0.16952
Epoch: 0032 train_loss= 1.11116 train_acc= 0.75472 val_loss= 1.06715 val_acc= 0.75038 time= 0.16603
Epoch: 0033 train_loss= 1.08204 train_acc= 0.75982 val_loss= 1.03959 val_acc= 0.75804 time= 0.17900
Epoch: 0034 train_loss= 1.05424 train_acc= 0.76527 val_loss= 1.01259 val_acc= 0.76570 time= 0.16700
Epoch: 0035 train_loss= 1.02525 train_acc= 0.77258 val_loss= 0.98627 val_acc= 0.77029 time= 0.16800
Epoch: 0036 train_loss= 0.99751 train_acc= 0.77989 val_loss= 0.96063 val_acc= 0.78101 time= 0.17100
Epoch: 0037 train_loss= 0.97140 train_acc= 0.78415 val_loss= 0.93558 val_acc= 0.79632 time= 0.16860
Epoch: 0038 train_loss= 0.94808 train_acc= 0.79265 val_loss= 0.91112 val_acc= 0.80551 time= 0.17247
Epoch: 0039 train_loss= 0.91977 train_acc= 0.80252 val_loss= 0.88715 val_acc= 0.81011 time= 0.19700
Epoch: 0040 train_loss= 0.89531 train_acc= 0.80932 val_loss= 0.86366 val_acc= 0.81776 time= 0.16823
Epoch: 0041 train_loss= 0.87043 train_acc= 0.81834 val_loss= 0.84066 val_acc= 0.82389 time= 0.18500
Epoch: 0042 train_loss= 0.84617 train_acc= 0.82599 val_loss= 0.81810 val_acc= 0.82542 time= 0.17100
Epoch: 0043 train_loss= 0.81943 train_acc= 0.83143 val_loss= 0.79597 val_acc= 0.83155 time= 0.16900
Epoch: 0044 train_loss= 0.79998 train_acc= 0.83331 val_loss= 0.77420 val_acc= 0.83308 time= 0.17100
Epoch: 0045 train_loss= 0.77706 train_acc= 0.83994 val_loss= 0.75280 val_acc= 0.83920 time= 0.19500
Epoch: 0046 train_loss= 0.75884 train_acc= 0.84164 val_loss= 0.73191 val_acc= 0.84533 time= 0.18502
Epoch: 0047 train_loss= 0.73360 train_acc= 0.84623 val_loss= 0.71161 val_acc= 0.85145 time= 0.16700
Epoch: 0048 train_loss= 0.71327 train_acc= 0.85202 val_loss= 0.69213 val_acc= 0.85452 time= 0.16900
Epoch: 0049 train_loss= 0.69052 train_acc= 0.85542 val_loss= 0.67307 val_acc= 0.85605 time= 0.16700
Epoch: 0050 train_loss= 0.67033 train_acc= 0.85797 val_loss= 0.65457 val_acc= 0.85605 time= 0.19300
Epoch: 0051 train_loss= 0.64990 train_acc= 0.85814 val_loss= 0.63643 val_acc= 0.85911 time= 0.16700
Epoch: 0052 train_loss= 0.63124 train_acc= 0.86188 val_loss= 0.61858 val_acc= 0.86677 time= 0.16900
Epoch: 0053 train_loss= 0.60929 train_acc= 0.86937 val_loss= 0.60129 val_acc= 0.87443 time= 0.17100
Epoch: 0054 train_loss= 0.58996 train_acc= 0.87192 val_loss= 0.58450 val_acc= 0.87902 time= 0.16941
Epoch: 0055 train_loss= 0.57104 train_acc= 0.87447 val_loss= 0.56833 val_acc= 0.87902 time= 0.17100
Epoch: 0056 train_loss= 0.55624 train_acc= 0.87855 val_loss= 0.55291 val_acc= 0.88055 time= 0.19500
Epoch: 0057 train_loss= 0.53906 train_acc= 0.88450 val_loss= 0.53796 val_acc= 0.88208 time= 0.16700
Epoch: 0058 train_loss= 0.52269 train_acc= 0.88876 val_loss= 0.52353 val_acc= 0.88361 time= 0.18200
Epoch: 0059 train_loss= 0.50325 train_acc= 0.89114 val_loss= 0.50965 val_acc= 0.88515 time= 0.16800
Epoch: 0060 train_loss= 0.49064 train_acc= 0.89233 val_loss= 0.49633 val_acc= 0.88668 time= 0.17071
Epoch: 0061 train_loss= 0.47158 train_acc= 0.89556 val_loss= 0.48333 val_acc= 0.88821 time= 0.19561
Epoch: 0062 train_loss= 0.45875 train_acc= 0.90202 val_loss= 0.47077 val_acc= 0.88821 time= 0.16700
Epoch: 0063 train_loss= 0.44121 train_acc= 0.90543 val_loss= 0.45868 val_acc= 0.88821 time= 0.18300
Epoch: 0064 train_loss= 0.42934 train_acc= 0.90815 val_loss= 0.44700 val_acc= 0.89127 time= 0.16700
Epoch: 0065 train_loss= 0.41777 train_acc= 0.90985 val_loss= 0.43582 val_acc= 0.89893 time= 0.16800
Epoch: 0066 train_loss= 0.39979 train_acc= 0.91376 val_loss= 0.42518 val_acc= 0.89893 time= 0.16800
Epoch: 0067 train_loss= 0.38946 train_acc= 0.92005 val_loss= 0.41512 val_acc= 0.89740 time= 0.19903
Epoch: 0068 train_loss= 0.37953 train_acc= 0.92227 val_loss= 0.40576 val_acc= 0.90046 time= 0.17123
Epoch: 0069 train_loss= 0.36517 train_acc= 0.92210 val_loss= 0.39686 val_acc= 0.90199 time= 0.17204
Epoch: 0070 train_loss= 0.35474 train_acc= 0.92941 val_loss= 0.38822 val_acc= 0.90199 time= 0.16800
Epoch: 0071 train_loss= 0.34495 train_acc= 0.93094 val_loss= 0.37960 val_acc= 0.90046 time= 0.16806
Epoch: 0072 train_loss= 0.33370 train_acc= 0.92907 val_loss= 0.37140 val_acc= 0.90199 time= 0.16800
Epoch: 0073 train_loss= 0.32270 train_acc= 0.93451 val_loss= 0.36370 val_acc= 0.90199 time= 0.19500
Epoch: 0074 train_loss= 0.31263 train_acc= 0.93638 val_loss= 0.35655 val_acc= 0.90352 time= 0.16700
Epoch: 0075 train_loss= 0.30173 train_acc= 0.93911 val_loss= 0.34950 val_acc= 0.90505 time= 0.18600
Epoch: 0076 train_loss= 0.29285 train_acc= 0.93877 val_loss= 0.34290 val_acc= 0.90658 time= 0.17040
Epoch: 0077 train_loss= 0.28434 train_acc= 0.94472 val_loss= 0.33687 val_acc= 0.90965 time= 0.16686
Epoch: 0078 train_loss= 0.27626 train_acc= 0.94727 val_loss= 0.33086 val_acc= 0.91271 time= 0.19800
Epoch: 0079 train_loss= 0.26577 train_acc= 0.94846 val_loss= 0.32507 val_acc= 0.91271 time= 0.16700
Epoch: 0080 train_loss= 0.26111 train_acc= 0.94744 val_loss= 0.31941 val_acc= 0.91271 time= 0.16900
Epoch: 0081 train_loss= 0.24976 train_acc= 0.95084 val_loss= 0.31402 val_acc= 0.91424 time= 0.17297
Epoch: 0082 train_loss= 0.24253 train_acc= 0.95050 val_loss= 0.30899 val_acc= 0.91577 time= 0.16900
Epoch: 0083 train_loss= 0.23726 train_acc= 0.95288 val_loss= 0.30428 val_acc= 0.92190 time= 0.17052
Epoch: 0084 train_loss= 0.22949 train_acc= 0.95509 val_loss= 0.29984 val_acc= 0.92190 time= 0.18403
Epoch: 0085 train_loss= 0.22194 train_acc= 0.95560 val_loss= 0.29560 val_acc= 0.92649 time= 0.16800
Epoch: 0086 train_loss= 0.21312 train_acc= 0.95782 val_loss= 0.29158 val_acc= 0.92496 time= 0.18300
Epoch: 0087 train_loss= 0.20841 train_acc= 0.95986 val_loss= 0.28835 val_acc= 0.92496 time= 0.16700
Epoch: 0088 train_loss= 0.20125 train_acc= 0.96037 val_loss= 0.28470 val_acc= 0.92802 time= 0.16830
Epoch: 0089 train_loss= 0.19335 train_acc= 0.96360 val_loss= 0.28146 val_acc= 0.92802 time= 0.16800
Epoch: 0090 train_loss= 0.18967 train_acc= 0.96428 val_loss= 0.27734 val_acc= 0.92956 time= 0.19283
Epoch: 0091 train_loss= 0.18253 train_acc= 0.96479 val_loss= 0.27315 val_acc= 0.92802 time= 0.17301
Epoch: 0092 train_loss= 0.17673 train_acc= 0.96615 val_loss= 0.26935 val_acc= 0.92802 time= 0.16700
Epoch: 0093 train_loss= 0.17205 train_acc= 0.96700 val_loss= 0.26596 val_acc= 0.92956 time= 0.16700
Epoch: 0094 train_loss= 0.16539 train_acc= 0.96921 val_loss= 0.26344 val_acc= 0.93109 time= 0.16629
Epoch: 0095 train_loss= 0.15983 train_acc= 0.96955 val_loss= 0.26153 val_acc= 0.93262 time= 0.17200
Epoch: 0096 train_loss= 0.15831 train_acc= 0.97125 val_loss= 0.25944 val_acc= 0.93262 time= 0.19300
Epoch: 0097 train_loss= 0.15195 train_acc= 0.97346 val_loss= 0.25732 val_acc= 0.93262 time= 0.17197
Epoch: 0098 train_loss= 0.14776 train_acc= 0.97295 val_loss= 0.25532 val_acc= 0.92956 time= 0.18700
Epoch: 0099 train_loss= 0.14131 train_acc= 0.97466 val_loss= 0.25278 val_acc= 0.92956 time= 0.16803
Epoch: 0100 train_loss= 0.13546 train_acc= 0.97381 val_loss= 0.24967 val_acc= 0.93262 time= 0.16697
Epoch: 0101 train_loss= 0.13339 train_acc= 0.97687 val_loss= 0.24680 val_acc= 0.93262 time= 0.17103
Epoch: 0102 train_loss= 0.12932 train_acc= 0.97772 val_loss= 0.24407 val_acc= 0.93262 time= 0.19300
Epoch: 0103 train_loss= 0.12582 train_acc= 0.97806 val_loss= 0.24188 val_acc= 0.93415 time= 0.18200
Epoch: 0104 train_loss= 0.12322 train_acc= 0.98027 val_loss= 0.24050 val_acc= 0.93415 time= 0.16597
Epoch: 0105 train_loss= 0.11944 train_acc= 0.97976 val_loss= 0.23927 val_acc= 0.93415 time= 0.17115
Epoch: 0106 train_loss= 0.11377 train_acc= 0.98265 val_loss= 0.23823 val_acc= 0.93568 time= 0.17057
Epoch: 0107 train_loss= 0.10991 train_acc= 0.98299 val_loss= 0.23730 val_acc= 0.93262 time= 0.19603
Epoch: 0108 train_loss= 0.10728 train_acc= 0.98197 val_loss= 0.23642 val_acc= 0.93415 time= 0.16797
Epoch: 0109 train_loss= 0.10582 train_acc= 0.98435 val_loss= 0.23509 val_acc= 0.93415 time= 0.17903
Epoch: 0110 train_loss= 0.10173 train_acc= 0.98367 val_loss= 0.23419 val_acc= 0.93568 time= 0.16700
Epoch: 0111 train_loss= 0.09701 train_acc= 0.98673 val_loss= 0.23359 val_acc= 0.93568 time= 0.16800
Epoch: 0112 train_loss= 0.09593 train_acc= 0.98486 val_loss= 0.23269 val_acc= 0.93415 time= 0.16897
Epoch: 0113 train_loss= 0.09233 train_acc= 0.98571 val_loss= 0.23147 val_acc= 0.93568 time= 0.20258
Epoch: 0114 train_loss= 0.09222 train_acc= 0.98605 val_loss= 0.23091 val_acc= 0.93721 time= 0.16903
Epoch: 0115 train_loss= 0.08794 train_acc= 0.98758 val_loss= 0.23018 val_acc= 0.93874 time= 0.18100
Epoch: 0116 train_loss= 0.08625 train_acc= 0.98707 val_loss= 0.22934 val_acc= 0.93721 time= 0.16712
Epoch: 0117 train_loss= 0.08301 train_acc= 0.98741 val_loss= 0.22857 val_acc= 0.93721 time= 0.16901
Epoch: 0118 train_loss= 0.08204 train_acc= 0.98758 val_loss= 0.22728 val_acc= 0.93568 time= 0.16999
Epoch: 0119 train_loss= 0.08001 train_acc= 0.98707 val_loss= 0.22621 val_acc= 0.93415 time= 0.19300
Epoch: 0120 train_loss= 0.07519 train_acc= 0.98911 val_loss= 0.22512 val_acc= 0.93415 time= 0.17174
Epoch: 0121 train_loss= 0.07467 train_acc= 0.98962 val_loss= 0.22415 val_acc= 0.93262 time= 0.17129
Epoch: 0122 train_loss= 0.07319 train_acc= 0.99013 val_loss= 0.22332 val_acc= 0.93721 time= 0.17100
Epoch: 0123 train_loss= 0.06879 train_acc= 0.99064 val_loss= 0.22314 val_acc= 0.93568 time= 0.17197
Epoch: 0124 train_loss= 0.06997 train_acc= 0.98996 val_loss= 0.22338 val_acc= 0.93721 time= 0.17603
Epoch: 0125 train_loss= 0.06769 train_acc= 0.99081 val_loss= 0.22307 val_acc= 0.93874 time= 0.19500
Epoch: 0126 train_loss= 0.06402 train_acc= 0.99167 val_loss= 0.22310 val_acc= 0.93874 time= 0.17800
Epoch: 0127 train_loss= 0.06253 train_acc= 0.99064 val_loss= 0.22265 val_acc= 0.93874 time= 0.17500
Epoch: 0128 train_loss= 0.06277 train_acc= 0.99150 val_loss= 0.22202 val_acc= 0.94028 time= 0.17168
Epoch: 0129 train_loss= 0.06022 train_acc= 0.99167 val_loss= 0.22167 val_acc= 0.93874 time= 0.16700
Epoch: 0130 train_loss= 0.05932 train_acc= 0.99150 val_loss= 0.22119 val_acc= 0.93874 time= 0.16800
Epoch: 0131 train_loss= 0.05824 train_acc= 0.99167 val_loss= 0.22042 val_acc= 0.93568 time= 0.16800
Epoch: 0132 train_loss= 0.05621 train_acc= 0.99218 val_loss= 0.21889 val_acc= 0.93568 time= 0.16900
Epoch: 0133 train_loss= 0.05633 train_acc= 0.99252 val_loss= 0.21807 val_acc= 0.93874 time= 0.16800
Epoch: 0134 train_loss= 0.05366 train_acc= 0.99167 val_loss= 0.21808 val_acc= 0.93874 time= 0.16800
Epoch: 0135 train_loss= 0.05248 train_acc= 0.99269 val_loss= 0.21792 val_acc= 0.93874 time= 0.17503
Epoch: 0136 train_loss= 0.05039 train_acc= 0.99337 val_loss= 0.21846 val_acc= 0.93874 time= 0.19600
Epoch: 0137 train_loss= 0.05031 train_acc= 0.99354 val_loss= 0.21917 val_acc= 0.93874 time= 0.16800
Epoch: 0138 train_loss= 0.04783 train_acc= 0.99422 val_loss= 0.21948 val_acc= 0.94028 time= 0.18100
Epoch: 0139 train_loss= 0.04602 train_acc= 0.99422 val_loss= 0.22025 val_acc= 0.94028 time= 0.16697
Early stopping...
Optimization Finished!
Test set results: cost= 0.24957 accuracy= 0.94042 time= 0.07403
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7889    0.9467    0.8606        75
           4     1.0000    1.0000    1.0000         9
           5     0.8351    0.9310    0.8804        87
           6     0.9200    0.9200    0.9200        25
           7     0.8571    0.9231    0.8889        13
           8     0.7857    1.0000    0.8800        11
           9     1.0000    0.4444    0.6154         9
          10     0.8929    0.6944    0.7812        36
          11     1.0000    0.9167    0.9565        12
          12     0.8696    0.9917    0.9266       121
          13     0.8824    0.7895    0.8333        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.8333    0.5556    0.6667         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.8235    0.8235    0.8235        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.8333    0.9091        12
          28     1.0000    0.8182    0.9000        11
          29     0.9641    0.9655    0.9648       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7692    1.0000    0.8696        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8395    0.8395    0.8395        81
          36     1.0000    0.3333    0.5000        12
          37     0.7500    0.7500    0.7500         4
          38     0.0000    0.0000    0.0000         1
          39     0.9790    0.9917    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     0.0000    0.0000    0.0000         3
          44     0.9000    0.7500    0.8182        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.8667    0.8667    0.8667        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9404      2568
   macro avg     0.7485    0.6654    0.6888      2568
weighted avg     0.9365    0.9404    0.9351      2568

Macro average Test Precision, Recall and F1-Score...
(0.7484863590441262, 0.665429162512797, 0.6887627773554426, None)
Micro average Test Precision, Recall and F1-Score...
(0.9404205607476636, 0.9404205607476636, 0.9404205607476636, None)
embeddings:
8892 6532 2568
[[ 0.09750208  1.6036828   0.03821849 ...  0.57173395 -0.07787757
   0.8168954 ]
 [ 0.3336892   1.1326891   0.579781   ...  0.38227272 -0.02231327
   0.58798933]
 [ 0.5618887   0.8678344   0.6451421  ...  0.13214457  0.02177191
   0.1373503 ]
 ...
 [ 0.2935859   0.2715814   0.36376125 ...  0.16281874 -0.00914716
   0.18182728]
 [ 0.33477414  0.23263007  0.1925063  ...  0.17411612  0.06022264
   0.07651442]
 [ 0.5164722   0.41643852  0.19785975 ...  0.33414528  0.23706089
   0.25596136]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95141 train_acc= 0.00885 val_loss= 3.90129 val_acc= 0.62787 time= 0.46200
Epoch: 0002 train_loss= 3.90188 train_acc= 0.62153 val_loss= 3.80912 val_acc= 0.61409 time= 0.17199
Epoch: 0003 train_loss= 3.81056 train_acc= 0.61643 val_loss= 3.66747 val_acc= 0.61103 time= 0.17097
Epoch: 0004 train_loss= 3.67173 train_acc= 0.61507 val_loss= 3.47559 val_acc= 0.61103 time= 0.17108
Epoch: 0005 train_loss= 3.48078 train_acc= 0.60980 val_loss= 3.24043 val_acc= 0.61715 time= 0.17478
Epoch: 0006 train_loss= 3.24312 train_acc= 0.61388 val_loss= 2.97981 val_acc= 0.61868 time= 0.16703
Epoch: 0007 train_loss= 2.98196 train_acc= 0.61201 val_loss= 2.72175 val_acc= 0.61562 time= 0.17506
Epoch: 0008 train_loss= 2.72360 train_acc= 0.61167 val_loss= 2.49716 val_acc= 0.61103 time= 0.16812
Epoch: 0009 train_loss= 2.49783 train_acc= 0.60895 val_loss= 2.33550 val_acc= 0.60337 time= 0.16697
Epoch: 0010 train_loss= 2.33410 train_acc= 0.59908 val_loss= 2.24164 val_acc= 0.55743 time= 0.16700
Epoch: 0011 train_loss= 2.23568 train_acc= 0.55367 val_loss= 2.18884 val_acc= 0.50077 time= 0.20368
Epoch: 0012 train_loss= 2.18493 train_acc= 0.47967 val_loss= 2.14516 val_acc= 0.47167 time= 0.17100
Epoch: 0013 train_loss= 2.14359 train_acc= 0.44344 val_loss= 2.09104 val_acc= 0.46401 time= 0.17903
Epoch: 0014 train_loss= 2.09768 train_acc= 0.43715 val_loss= 2.01999 val_acc= 0.46401 time= 0.16797
Epoch: 0015 train_loss= 2.03017 train_acc= 0.43732 val_loss= 1.93457 val_acc= 0.46708 time= 0.16727
Epoch: 0016 train_loss= 1.94975 train_acc= 0.44072 val_loss= 1.84415 val_acc= 0.48698 time= 0.19099
Epoch: 0017 train_loss= 1.86377 train_acc= 0.46300 val_loss= 1.76029 val_acc= 0.53139 time= 0.16802
Epoch: 0018 train_loss= 1.78204 train_acc= 0.53598 val_loss= 1.69016 val_acc= 0.62175 time= 0.17195
Epoch: 0019 train_loss= 1.71778 train_acc= 0.61796 val_loss= 1.63141 val_acc= 0.66462 time= 0.17952
Epoch: 0020 train_loss= 1.65482 train_acc= 0.65794 val_loss= 1.57662 val_acc= 0.68606 time= 0.17000
Epoch: 0021 train_loss= 1.60250 train_acc= 0.67035 val_loss= 1.52082 val_acc= 0.69678 time= 0.16804
Epoch: 0022 train_loss= 1.54756 train_acc= 0.67614 val_loss= 1.46409 val_acc= 0.69219 time= 0.19299
Epoch: 0023 train_loss= 1.49148 train_acc= 0.68022 val_loss= 1.40907 val_acc= 0.69219 time= 0.16796
Epoch: 0024 train_loss= 1.43396 train_acc= 0.68260 val_loss= 1.35819 val_acc= 0.69985 time= 0.18403
Epoch: 0025 train_loss= 1.38273 train_acc= 0.68957 val_loss= 1.31244 val_acc= 0.70444 time= 0.16702
Epoch: 0026 train_loss= 1.33811 train_acc= 0.69383 val_loss= 1.27154 val_acc= 0.70904 time= 0.17195
Epoch: 0027 train_loss= 1.29537 train_acc= 0.70471 val_loss= 1.23466 val_acc= 0.72588 time= 0.17100
Epoch: 0028 train_loss= 1.25764 train_acc= 0.71254 val_loss= 1.20081 val_acc= 0.72894 time= 0.19804
Epoch: 0029 train_loss= 1.22019 train_acc= 0.72359 val_loss= 1.16890 val_acc= 0.73201 time= 0.16999
Epoch: 0030 train_loss= 1.18606 train_acc= 0.73295 val_loss= 1.13795 val_acc= 0.73813 time= 0.17500
Epoch: 0031 train_loss= 1.15368 train_acc= 0.74553 val_loss= 1.10731 val_acc= 0.74426 time= 0.16801
Epoch: 0032 train_loss= 1.12280 train_acc= 0.75234 val_loss= 1.07672 val_acc= 0.75038 time= 0.16700
Epoch: 0033 train_loss= 1.08963 train_acc= 0.75999 val_loss= 1.04621 val_acc= 0.76110 time= 0.16897
Epoch: 0034 train_loss= 1.05751 train_acc= 0.76765 val_loss= 1.01607 val_acc= 0.76876 time= 0.18600
Epoch: 0035 train_loss= 1.02432 train_acc= 0.77309 val_loss= 0.98656 val_acc= 0.78101 time= 0.17100
Epoch: 0036 train_loss= 0.99318 train_acc= 0.78075 val_loss= 0.95785 val_acc= 0.78254 time= 0.18503
Epoch: 0037 train_loss= 0.96373 train_acc= 0.78772 val_loss= 0.92985 val_acc= 0.78714 time= 0.16920
Epoch: 0038 train_loss= 0.93585 train_acc= 0.79520 val_loss= 0.90251 val_acc= 0.80092 time= 0.16700
Epoch: 0039 train_loss= 0.90915 train_acc= 0.80371 val_loss= 0.87573 val_acc= 0.81011 time= 0.16895
Epoch: 0040 train_loss= 0.87916 train_acc= 0.81578 val_loss= 0.84946 val_acc= 0.81164 time= 0.18004
Epoch: 0041 train_loss= 0.85125 train_acc= 0.82225 val_loss= 0.82372 val_acc= 0.82083 time= 0.18596
Epoch: 0042 train_loss= 0.82427 train_acc= 0.82922 val_loss= 0.79855 val_acc= 0.83461 time= 0.17500
Epoch: 0043 train_loss= 0.79721 train_acc= 0.83518 val_loss= 0.77401 val_acc= 0.83920 time= 0.16805
Epoch: 0044 train_loss= 0.77251 train_acc= 0.84130 val_loss= 0.75003 val_acc= 0.84992 time= 0.16895
Epoch: 0045 train_loss= 0.74813 train_acc= 0.84742 val_loss= 0.72668 val_acc= 0.85145 time= 0.19504
Epoch: 0046 train_loss= 0.72110 train_acc= 0.85355 val_loss= 0.70385 val_acc= 0.85452 time= 0.16699
Epoch: 0047 train_loss= 0.69688 train_acc= 0.85457 val_loss= 0.68162 val_acc= 0.85911 time= 0.17096
Epoch: 0048 train_loss= 0.67210 train_acc= 0.85780 val_loss= 0.66010 val_acc= 0.86064 time= 0.17104
Epoch: 0049 train_loss= 0.65052 train_acc= 0.86273 val_loss= 0.63926 val_acc= 0.86677 time= 0.17096
Epoch: 0050 train_loss= 0.62858 train_acc= 0.86426 val_loss= 0.61913 val_acc= 0.86983 time= 0.18085
Epoch: 0051 train_loss= 0.60678 train_acc= 0.86988 val_loss= 0.59963 val_acc= 0.87443 time= 0.17299
Epoch: 0052 train_loss= 0.58581 train_acc= 0.87396 val_loss= 0.58075 val_acc= 0.87289 time= 0.16901
Epoch: 0053 train_loss= 0.56413 train_acc= 0.88025 val_loss= 0.56261 val_acc= 0.87443 time= 0.18499
Epoch: 0054 train_loss= 0.54770 train_acc= 0.88246 val_loss= 0.54530 val_acc= 0.87902 time= 0.16800
Epoch: 0055 train_loss= 0.52560 train_acc= 0.88774 val_loss= 0.52877 val_acc= 0.88208 time= 0.16701
Epoch: 0056 train_loss= 0.50602 train_acc= 0.89301 val_loss= 0.51304 val_acc= 0.88361 time= 0.19398
Epoch: 0057 train_loss= 0.48888 train_acc= 0.89641 val_loss= 0.49815 val_acc= 0.88515 time= 0.17155
Epoch: 0058 train_loss= 0.47040 train_acc= 0.90032 val_loss= 0.48378 val_acc= 0.88668 time= 0.17154
Epoch: 0059 train_loss= 0.45531 train_acc= 0.90373 val_loss= 0.47012 val_acc= 0.88821 time= 0.17199
Epoch: 0060 train_loss= 0.43843 train_acc= 0.90730 val_loss= 0.45688 val_acc= 0.89127 time= 0.17000
Epoch: 0061 train_loss= 0.42264 train_acc= 0.91308 val_loss= 0.44376 val_acc= 0.89280 time= 0.17097
Epoch: 0062 train_loss= 0.40955 train_acc= 0.91580 val_loss= 0.43103 val_acc= 0.89587 time= 0.17503
Epoch: 0063 train_loss= 0.39315 train_acc= 0.92090 val_loss= 0.41898 val_acc= 0.89587 time= 0.19400
Epoch: 0064 train_loss= 0.37872 train_acc= 0.92465 val_loss= 0.40779 val_acc= 0.89740 time= 0.18299
Epoch: 0065 train_loss= 0.36400 train_acc= 0.92499 val_loss= 0.39752 val_acc= 0.90352 time= 0.17001
Epoch: 0066 train_loss= 0.35317 train_acc= 0.92873 val_loss= 0.38797 val_acc= 0.90505 time= 0.17103
Epoch: 0067 train_loss= 0.33880 train_acc= 0.93247 val_loss= 0.37920 val_acc= 0.90352 time= 0.16800
Epoch: 0068 train_loss= 0.32796 train_acc= 0.93315 val_loss= 0.37082 val_acc= 0.90199 time= 0.19300
Epoch: 0069 train_loss= 0.31479 train_acc= 0.93587 val_loss= 0.36273 val_acc= 0.90505 time= 0.16900
Epoch: 0070 train_loss= 0.30624 train_acc= 0.93791 val_loss= 0.35481 val_acc= 0.90965 time= 0.17800
Epoch: 0071 train_loss= 0.29405 train_acc= 0.94132 val_loss= 0.34730 val_acc= 0.90965 time= 0.16799
Epoch: 0072 train_loss= 0.28315 train_acc= 0.94234 val_loss= 0.34032 val_acc= 0.90965 time= 0.17536
Epoch: 0073 train_loss= 0.27230 train_acc= 0.94625 val_loss= 0.33379 val_acc= 0.90812 time= 0.20101
Epoch: 0074 train_loss= 0.26198 train_acc= 0.95050 val_loss= 0.32765 val_acc= 0.91118 time= 0.16801
Epoch: 0075 train_loss= 0.25288 train_acc= 0.95084 val_loss= 0.32169 val_acc= 0.91118 time= 0.17099
Epoch: 0076 train_loss= 0.24225 train_acc= 0.95288 val_loss= 0.31561 val_acc= 0.91118 time= 0.17501
Epoch: 0077 train_loss= 0.23292 train_acc= 0.95441 val_loss= 0.30974 val_acc= 0.91424 time= 0.16899
Epoch: 0078 train_loss= 0.22524 train_acc= 0.95782 val_loss= 0.30431 val_acc= 0.91577 time= 0.16900
Epoch: 0079 train_loss= 0.21533 train_acc= 0.95901 val_loss= 0.29908 val_acc= 0.91577 time= 0.18502
Epoch: 0080 train_loss= 0.20837 train_acc= 0.95969 val_loss= 0.29452 val_acc= 0.91730 time= 0.17100
Epoch: 0081 train_loss= 0.20012 train_acc= 0.96343 val_loss= 0.29023 val_acc= 0.92037 time= 0.18905
Epoch: 0082 train_loss= 0.19330 train_acc= 0.96411 val_loss= 0.28610 val_acc= 0.92190 time= 0.16701
Epoch: 0083 train_loss= 0.18609 train_acc= 0.96649 val_loss= 0.28278 val_acc= 0.92343 time= 0.16800
Epoch: 0084 train_loss= 0.17969 train_acc= 0.96564 val_loss= 0.27955 val_acc= 0.92190 time= 0.17097
Epoch: 0085 train_loss= 0.17048 train_acc= 0.96836 val_loss= 0.27629 val_acc= 0.92190 time= 0.19604
Epoch: 0086 train_loss= 0.16554 train_acc= 0.97006 val_loss= 0.27264 val_acc= 0.92496 time= 0.16800
Epoch: 0087 train_loss= 0.15984 train_acc= 0.97006 val_loss= 0.26876 val_acc= 0.92802 time= 0.16999
Epoch: 0088 train_loss= 0.15321 train_acc= 0.97295 val_loss= 0.26504 val_acc= 0.92956 time= 0.17000
Epoch: 0089 train_loss= 0.14716 train_acc= 0.97415 val_loss= 0.26192 val_acc= 0.92956 time= 0.16897
Epoch: 0090 train_loss= 0.14057 train_acc= 0.97534 val_loss= 0.25915 val_acc= 0.92956 time= 0.17103
Epoch: 0091 train_loss= 0.13599 train_acc= 0.97585 val_loss= 0.25679 val_acc= 0.92802 time= 0.19700
Epoch: 0092 train_loss= 0.13027 train_acc= 0.97823 val_loss= 0.25466 val_acc= 0.92649 time= 0.16800
Epoch: 0093 train_loss= 0.12649 train_acc= 0.97874 val_loss= 0.25286 val_acc= 0.92649 time= 0.18597
Epoch: 0094 train_loss= 0.11950 train_acc= 0.98078 val_loss= 0.25066 val_acc= 0.92956 time= 0.16903
Epoch: 0095 train_loss= 0.11708 train_acc= 0.98044 val_loss= 0.24808 val_acc= 0.93109 time= 0.17095
Epoch: 0096 train_loss= 0.11223 train_acc= 0.98163 val_loss= 0.24517 val_acc= 0.93109 time= 0.17200
Epoch: 0097 train_loss= 0.10692 train_acc= 0.98401 val_loss= 0.24248 val_acc= 0.93568 time= 0.19601
Epoch: 0098 train_loss= 0.10290 train_acc= 0.98350 val_loss= 0.24044 val_acc= 0.93721 time= 0.18301
Epoch: 0099 train_loss= 0.09896 train_acc= 0.98537 val_loss= 0.23936 val_acc= 0.93415 time= 0.16710
Epoch: 0100 train_loss= 0.09649 train_acc= 0.98639 val_loss= 0.23866 val_acc= 0.93568 time= 0.16800
Epoch: 0101 train_loss= 0.09242 train_acc= 0.98639 val_loss= 0.23826 val_acc= 0.93568 time= 0.16908
Epoch: 0102 train_loss= 0.08824 train_acc= 0.98622 val_loss= 0.23681 val_acc= 0.93568 time= 0.20197
Epoch: 0103 train_loss= 0.08621 train_acc= 0.98656 val_loss= 0.23498 val_acc= 0.93874 time= 0.17303
Epoch: 0104 train_loss= 0.08141 train_acc= 0.98843 val_loss= 0.23312 val_acc= 0.93721 time= 0.16897
Epoch: 0105 train_loss= 0.07942 train_acc= 0.98945 val_loss= 0.23185 val_acc= 0.93721 time= 0.16803
Epoch: 0106 train_loss= 0.07616 train_acc= 0.98979 val_loss= 0.23099 val_acc= 0.93568 time= 0.16765
Epoch: 0107 train_loss= 0.07304 train_acc= 0.99013 val_loss= 0.23061 val_acc= 0.93721 time= 0.17072
Epoch: 0108 train_loss= 0.07088 train_acc= 0.98996 val_loss= 0.23135 val_acc= 0.93721 time= 0.19452
Epoch: 0109 train_loss= 0.06839 train_acc= 0.99081 val_loss= 0.23180 val_acc= 0.93568 time= 0.17103
Epoch: 0110 train_loss= 0.06604 train_acc= 0.99098 val_loss= 0.23185 val_acc= 0.93262 time= 0.18600
Epoch: 0111 train_loss= 0.06345 train_acc= 0.99116 val_loss= 0.23036 val_acc= 0.93721 time= 0.17104
Epoch: 0112 train_loss= 0.06169 train_acc= 0.99184 val_loss= 0.22850 val_acc= 0.93874 time= 0.16900
Epoch: 0113 train_loss= 0.05864 train_acc= 0.99184 val_loss= 0.22658 val_acc= 0.94181 time= 0.17047
Epoch: 0114 train_loss= 0.05670 train_acc= 0.99337 val_loss= 0.22551 val_acc= 0.94181 time= 0.19200
Epoch: 0115 train_loss= 0.05477 train_acc= 0.99269 val_loss= 0.22521 val_acc= 0.94028 time= 0.18300
Epoch: 0116 train_loss= 0.05401 train_acc= 0.99286 val_loss= 0.22529 val_acc= 0.94028 time= 0.16901
Epoch: 0117 train_loss= 0.05153 train_acc= 0.99303 val_loss= 0.22563 val_acc= 0.93721 time= 0.17170
Epoch: 0118 train_loss= 0.04875 train_acc= 0.99320 val_loss= 0.22523 val_acc= 0.93721 time= 0.17000
Epoch: 0119 train_loss= 0.04798 train_acc= 0.99371 val_loss= 0.22420 val_acc= 0.93721 time= 0.19601
Epoch: 0120 train_loss= 0.04713 train_acc= 0.99320 val_loss= 0.22359 val_acc= 0.93874 time= 0.16733
Epoch: 0121 train_loss= 0.04447 train_acc= 0.99456 val_loss= 0.22254 val_acc= 0.93874 time= 0.17100
Epoch: 0122 train_loss= 0.04347 train_acc= 0.99473 val_loss= 0.22079 val_acc= 0.93874 time= 0.16800
Epoch: 0123 train_loss= 0.04232 train_acc= 0.99524 val_loss= 0.21972 val_acc= 0.94028 time= 0.16700
Epoch: 0124 train_loss= 0.04101 train_acc= 0.99541 val_loss= 0.21998 val_acc= 0.94028 time= 0.16996
Epoch: 0125 train_loss= 0.04093 train_acc= 0.99524 val_loss= 0.22150 val_acc= 0.93874 time= 0.19898
Epoch: 0126 train_loss= 0.03849 train_acc= 0.99643 val_loss= 0.22342 val_acc= 0.93874 time= 0.17300
Early stopping...
Optimization Finished!
Test set results: cost= 0.25229 accuracy= 0.93692 time= 0.08800
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.7778    0.9333    0.8485        75
           4     1.0000    1.0000    1.0000         9
           5     0.8587    0.9080    0.8827        87
           6     0.9200    0.9200    0.9200        25
           7     0.6875    0.8462    0.7586        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.5556    0.7143         9
          10     0.9130    0.5833    0.7119        36
          11     1.0000    0.9167    0.9565        12
          12     0.8231    1.0000    0.9030       121
          13     1.0000    0.7895    0.8824        19
          14     0.9231    0.8571    0.8889        28
          15     1.0000    0.7500    0.8571         4
          16     0.0000    0.0000    0.0000         4
          17     1.0000    0.3333    0.5000         3
          18     1.0000    1.0000    1.0000        10
          19     1.0000    1.0000    1.0000         2
          20     0.6667    0.4444    0.5333         9
          21     0.9048    0.9500    0.9268        20
          22     0.6000    0.6000    0.6000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7000    0.8235    0.7568        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9683    0.9641    0.9662       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    1.0000    1.0000         3
          32     0.6923    0.9000    0.7826        10
          33     1.0000    0.6667    0.8000         3
          34     0.0000    0.0000    0.0000         1
          35     0.8193    0.8395    0.8293        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9799    0.9917    0.9858      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.3333    0.5000         3
          44     0.9091    0.8333    0.8696        12
          45     1.0000    0.1667    0.2857         6
          46     1.0000    0.2857    0.4444         7
          47     0.8571    0.8000    0.8276        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9369      2568
   macro avg     0.7815    0.6673    0.6972      2568
weighted avg     0.9378    0.9369    0.9327      2568

Macro average Test Precision, Recall and F1-Score...
(0.7815093474498217, 0.6672898811353315, 0.6971605966239197, None)
Micro average Test Precision, Recall and F1-Score...
(0.9369158878504673, 0.9369158878504673, 0.9369158878504673, None)
embeddings:
8892 6532 2568
[[-0.04158612 -0.15181752  1.7478473  ... -0.04754449  0.20010918
   0.14484343]
 [ 0.23349531  0.00647528  0.2628357  ... -0.0321174   0.0372039
   0.04142924]
 [ 0.02903933  0.00922887  0.8633668  ...  0.12979202  0.44242078
   0.33736402]
 ...
 [ 0.08746122  0.07715273  0.27097172 ...  0.03943948  0.4541703
   0.49246073]
 [ 0.02085768  0.02242951  0.49947494 ...  0.08233931  0.27637112
   0.30668166]
 [ 0.2673811   0.21966255  0.4704224  ...  0.22599421  0.39091042
   0.34309554]]

(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95125 train_acc= 0.02347 val_loss= 3.89749 val_acc= 0.65697 time= 0.45372
Epoch: 0002 train_loss= 3.89747 train_acc= 0.63446 val_loss= 3.79667 val_acc= 0.65237 time= 0.17817
Epoch: 0003 train_loss= 3.79651 train_acc= 0.62919 val_loss= 3.64074 val_acc= 0.65237 time= 0.16800
Epoch: 0004 train_loss= 3.64101 train_acc= 0.62715 val_loss= 3.43016 val_acc= 0.64931 time= 0.16997
Epoch: 0005 train_loss= 3.43069 train_acc= 0.62494 val_loss= 3.17691 val_acc= 0.64012 time= 0.19603
Epoch: 0006 train_loss= 3.17909 train_acc= 0.62119 val_loss= 2.90605 val_acc= 0.63859 time= 0.17000
Epoch: 0007 train_loss= 2.90630 train_acc= 0.61915 val_loss= 2.64779 val_acc= 0.63093 time= 0.16918
Epoch: 0008 train_loss= 2.64404 train_acc= 0.61524 val_loss= 2.43744 val_acc= 0.62787 time= 0.16804
Epoch: 0009 train_loss= 2.43111 train_acc= 0.61405 val_loss= 2.29981 val_acc= 0.64931 time= 0.16959
Epoch: 0010 train_loss= 2.29456 train_acc= 0.62630 val_loss= 2.22223 val_acc= 0.67075 time= 0.19600
Epoch: 0011 train_loss= 2.21927 train_acc= 0.65198 val_loss= 2.17134 val_acc= 0.54671 time= 0.16703
Epoch: 0012 train_loss= 2.17559 train_acc= 0.54108 val_loss= 2.12235 val_acc= 0.47779 time= 0.18201
Epoch: 0013 train_loss= 2.12887 train_acc= 0.45246 val_loss= 2.06098 val_acc= 0.46554 time= 0.17303
Epoch: 0014 train_loss= 2.07649 train_acc= 0.44089 val_loss= 1.98207 val_acc= 0.46861 time= 0.16797
Epoch: 0015 train_loss= 2.00179 train_acc= 0.44191 val_loss= 1.89031 val_acc= 0.48239 time= 0.16803
Epoch: 0016 train_loss= 1.91536 train_acc= 0.46317 val_loss= 1.79808 val_acc= 0.52527 time= 0.19997
Epoch: 0017 train_loss= 1.82687 train_acc= 0.52764 val_loss= 1.71677 val_acc= 0.61868 time= 0.17117
Epoch: 0018 train_loss= 1.74525 train_acc= 0.61150 val_loss= 1.65012 val_acc= 0.66462 time= 0.16900
Epoch: 0019 train_loss= 1.67849 train_acc= 0.64994 val_loss= 1.59382 val_acc= 0.67534 time= 0.16803
Epoch: 0020 train_loss= 1.62215 train_acc= 0.65759 val_loss= 1.54107 val_acc= 0.68147 time= 0.16800
Epoch: 0021 train_loss= 1.56713 train_acc= 0.66236 val_loss= 1.48797 val_acc= 0.68300 time= 0.17000
Epoch: 0022 train_loss= 1.51129 train_acc= 0.66593 val_loss= 1.43460 val_acc= 0.68606 time= 0.19397
Epoch: 0023 train_loss= 1.45851 train_acc= 0.66882 val_loss= 1.38278 val_acc= 0.69219 time= 0.17300
Epoch: 0024 train_loss= 1.40439 train_acc= 0.67614 val_loss= 1.33440 val_acc= 0.69832 time= 0.19064
Epoch: 0025 train_loss= 1.35550 train_acc= 0.68430 val_loss= 1.29026 val_acc= 0.71057 time= 0.17275
Epoch: 0026 train_loss= 1.31125 train_acc= 0.69536 val_loss= 1.25053 val_acc= 0.72282 time= 0.17000
Epoch: 0027 train_loss= 1.27120 train_acc= 0.70658 val_loss= 1.21465 val_acc= 0.72282 time= 0.19900
Epoch: 0028 train_loss= 1.23285 train_acc= 0.71968 val_loss= 1.18178 val_acc= 0.73047 time= 0.17100
Epoch: 0029 train_loss= 1.20025 train_acc= 0.72989 val_loss= 1.15088 val_acc= 0.73354 time= 0.18503
Epoch: 0030 train_loss= 1.16912 train_acc= 0.73992 val_loss= 1.12115 val_acc= 0.73507 time= 0.16800
Epoch: 0031 train_loss= 1.13350 train_acc= 0.74792 val_loss= 1.09192 val_acc= 0.74273 time= 0.17297
Epoch: 0032 train_loss= 1.10523 train_acc= 0.75421 val_loss= 1.06290 val_acc= 0.75038 time= 0.17093
Epoch: 0033 train_loss= 1.07494 train_acc= 0.75897 val_loss= 1.03425 val_acc= 0.76110 time= 0.17103
Epoch: 0034 train_loss= 1.04488 train_acc= 0.76561 val_loss= 1.00621 val_acc= 0.77029 time= 0.16997
Epoch: 0035 train_loss= 1.01393 train_acc= 0.77105 val_loss= 0.97899 val_acc= 0.77182 time= 0.17000
Epoch: 0036 train_loss= 0.98719 train_acc= 0.77836 val_loss= 0.95270 val_acc= 0.77795 time= 0.16803
Epoch: 0037 train_loss= 0.95983 train_acc= 0.79061 val_loss= 0.92736 val_acc= 0.78560 time= 0.16798
Epoch: 0038 train_loss= 0.93572 train_acc= 0.79639 val_loss= 0.90284 val_acc= 0.80092 time= 0.17302
Epoch: 0039 train_loss= 0.90905 train_acc= 0.81051 val_loss= 0.87891 val_acc= 0.81164 time= 0.19497
Epoch: 0040 train_loss= 0.88516 train_acc= 0.81732 val_loss= 0.85545 val_acc= 0.81623 time= 0.16959
Epoch: 0041 train_loss= 0.86251 train_acc= 0.82276 val_loss= 0.83241 val_acc= 0.82389 time= 0.18704
Epoch: 0042 train_loss= 0.83649 train_acc= 0.83041 val_loss= 0.80980 val_acc= 0.83461 time= 0.16999
Epoch: 0043 train_loss= 0.81509 train_acc= 0.83382 val_loss= 0.78764 val_acc= 0.83767 time= 0.17000
Epoch: 0044 train_loss= 0.79134 train_acc= 0.83620 val_loss= 0.76591 val_acc= 0.84227 time= 0.17197
Epoch: 0045 train_loss= 0.76582 train_acc= 0.84266 val_loss= 0.74456 val_acc= 0.84533 time= 0.19303
Epoch: 0046 train_loss= 0.74266 train_acc= 0.84657 val_loss= 0.72359 val_acc= 0.84686 time= 0.18197
Epoch: 0047 train_loss= 0.71900 train_acc= 0.84946 val_loss= 0.70313 val_acc= 0.85145 time= 0.17400
Epoch: 0048 train_loss= 0.70044 train_acc= 0.85338 val_loss= 0.68309 val_acc= 0.85299 time= 0.17226
Epoch: 0049 train_loss= 0.67740 train_acc= 0.85763 val_loss= 0.66357 val_acc= 0.85758 time= 0.16800
Epoch: 0050 train_loss= 0.65611 train_acc= 0.85933 val_loss= 0.64458 val_acc= 0.85911 time= 0.19696
Epoch: 0051 train_loss= 0.63722 train_acc= 0.86290 val_loss= 0.62619 val_acc= 0.86371 time= 0.17400
Epoch: 0052 train_loss= 0.61673 train_acc= 0.86749 val_loss= 0.60843 val_acc= 0.86217 time= 0.16900
Epoch: 0053 train_loss= 0.59780 train_acc= 0.87090 val_loss= 0.59142 val_acc= 0.86524 time= 0.16903
Epoch: 0054 train_loss= 0.57818 train_acc= 0.87753 val_loss= 0.57515 val_acc= 0.87289 time= 0.16967
Epoch: 0055 train_loss= 0.55873 train_acc= 0.87957 val_loss= 0.55963 val_acc= 0.87443 time= 0.17333
Epoch: 0056 train_loss= 0.54250 train_acc= 0.88076 val_loss= 0.54485 val_acc= 0.87902 time= 0.19603
Epoch: 0057 train_loss= 0.52663 train_acc= 0.88416 val_loss= 0.53056 val_acc= 0.88208 time= 0.16800
Epoch: 0058 train_loss= 0.50578 train_acc= 0.88655 val_loss= 0.51675 val_acc= 0.88361 time= 0.18597
Epoch: 0059 train_loss= 0.49301 train_acc= 0.89250 val_loss= 0.50312 val_acc= 0.88515 time= 0.17100
Epoch: 0060 train_loss= 0.47451 train_acc= 0.89522 val_loss= 0.48974 val_acc= 0.88361 time= 0.16803
Epoch: 0061 train_loss= 0.45947 train_acc= 0.89896 val_loss= 0.47663 val_acc= 0.88821 time= 0.17000
Epoch: 0062 train_loss= 0.44299 train_acc= 0.90287 val_loss= 0.46381 val_acc= 0.89127 time= 0.19901
Epoch: 0063 train_loss= 0.43154 train_acc= 0.90543 val_loss= 0.45167 val_acc= 0.89433 time= 0.17997
Epoch: 0064 train_loss= 0.41792 train_acc= 0.91002 val_loss= 0.44020 val_acc= 0.89433 time= 0.16868
Epoch: 0065 train_loss= 0.40452 train_acc= 0.91444 val_loss= 0.42924 val_acc= 0.89587 time= 0.17100
Epoch: 0066 train_loss= 0.39203 train_acc= 0.92073 val_loss= 0.41884 val_acc= 0.89587 time= 0.16897
Epoch: 0067 train_loss= 0.37840 train_acc= 0.92142 val_loss= 0.40920 val_acc= 0.89587 time= 0.16900
Epoch: 0068 train_loss= 0.36626 train_acc= 0.92567 val_loss= 0.40002 val_acc= 0.89893 time= 0.16700
Epoch: 0069 train_loss= 0.35428 train_acc= 0.92856 val_loss= 0.39119 val_acc= 0.90352 time= 0.19217
Epoch: 0070 train_loss= 0.34565 train_acc= 0.93196 val_loss= 0.38277 val_acc= 0.90505 time= 0.17101
Epoch: 0071 train_loss= 0.33275 train_acc= 0.93230 val_loss= 0.37481 val_acc= 0.90812 time= 0.17103
Epoch: 0072 train_loss= 0.32163 train_acc= 0.93553 val_loss= 0.36751 val_acc= 0.90812 time= 0.16906
Epoch: 0073 train_loss= 0.31092 train_acc= 0.93689 val_loss= 0.36047 val_acc= 0.90812 time= 0.19600
Epoch: 0074 train_loss= 0.30299 train_acc= 0.93842 val_loss= 0.35360 val_acc= 0.90812 time= 0.16704
Epoch: 0075 train_loss= 0.29155 train_acc= 0.94234 val_loss= 0.34682 val_acc= 0.90965 time= 0.18200
Epoch: 0076 train_loss= 0.28255 train_acc= 0.94438 val_loss= 0.34034 val_acc= 0.91271 time= 0.16896
Epoch: 0077 train_loss= 0.27135 train_acc= 0.94642 val_loss= 0.33399 val_acc= 0.91271 time= 0.17604
Epoch: 0078 train_loss= 0.26452 train_acc= 0.94965 val_loss= 0.32798 val_acc= 0.91577 time= 0.17458
Epoch: 0079 train_loss= 0.25816 train_acc= 0.94999 val_loss= 0.32223 val_acc= 0.92190 time= 0.19505
Epoch: 0080 train_loss= 0.24740 train_acc= 0.95152 val_loss= 0.31704 val_acc= 0.92343 time= 0.18259
Epoch: 0081 train_loss= 0.24144 train_acc= 0.95458 val_loss= 0.31224 val_acc= 0.92496 time= 0.16811
Epoch: 0082 train_loss= 0.23266 train_acc= 0.95612 val_loss= 0.30793 val_acc= 0.92496 time= 0.16899
Epoch: 0083 train_loss= 0.22762 train_acc= 0.95629 val_loss= 0.30358 val_acc= 0.92649 time= 0.17197
Epoch: 0084 train_loss= 0.21629 train_acc= 0.95850 val_loss= 0.29943 val_acc= 0.92649 time= 0.20000
Epoch: 0085 train_loss= 0.21069 train_acc= 0.95935 val_loss= 0.29524 val_acc= 0.92802 time= 0.17000
Epoch: 0086 train_loss= 0.20321 train_acc= 0.96224 val_loss= 0.29111 val_acc= 0.92802 time= 0.17103
Epoch: 0087 train_loss= 0.19785 train_acc= 0.96224 val_loss= 0.28721 val_acc= 0.92649 time= 0.16800
Epoch: 0088 train_loss= 0.18951 train_acc= 0.96411 val_loss= 0.28379 val_acc= 0.92649 time= 0.16901
Epoch: 0089 train_loss= 0.18507 train_acc= 0.96377 val_loss= 0.28021 val_acc= 0.92802 time= 0.19699
Epoch: 0090 train_loss= 0.17870 train_acc= 0.96564 val_loss= 0.27650 val_acc= 0.92956 time= 0.16901
Epoch: 0091 train_loss= 0.17105 train_acc= 0.96836 val_loss= 0.27275 val_acc= 0.93109 time= 0.16896
Epoch: 0092 train_loss= 0.16537 train_acc= 0.96955 val_loss= 0.26937 val_acc= 0.93109 time= 0.19297
Epoch: 0093 train_loss= 0.15968 train_acc= 0.96972 val_loss= 0.26619 val_acc= 0.93109 time= 0.17100
Epoch: 0094 train_loss= 0.15607 train_acc= 0.97125 val_loss= 0.26367 val_acc= 0.93109 time= 0.16904
Epoch: 0095 train_loss= 0.14817 train_acc= 0.97278 val_loss= 0.26145 val_acc= 0.93109 time= 0.19700
Epoch: 0096 train_loss= 0.14359 train_acc= 0.97415 val_loss= 0.25936 val_acc= 0.92956 time= 0.16896
Epoch: 0097 train_loss= 0.13966 train_acc= 0.97670 val_loss= 0.25709 val_acc= 0.92956 time= 0.18704
Epoch: 0098 train_loss= 0.13356 train_acc= 0.97636 val_loss= 0.25481 val_acc= 0.93109 time= 0.16800
Epoch: 0099 train_loss= 0.13112 train_acc= 0.97823 val_loss= 0.25244 val_acc= 0.93415 time= 0.17099
Epoch: 0100 train_loss= 0.12414 train_acc= 0.97823 val_loss= 0.25005 val_acc= 0.93874 time= 0.17204
Epoch: 0101 train_loss= 0.12256 train_acc= 0.98044 val_loss= 0.24801 val_acc= 0.93874 time= 0.17605
Epoch: 0102 train_loss= 0.11708 train_acc= 0.98180 val_loss= 0.24639 val_acc= 0.93721 time= 0.16897
Epoch: 0103 train_loss= 0.11422 train_acc= 0.98112 val_loss= 0.24495 val_acc= 0.93721 time= 0.18203
Epoch: 0104 train_loss= 0.10991 train_acc= 0.98418 val_loss= 0.24408 val_acc= 0.93415 time= 0.16897
Epoch: 0105 train_loss= 0.10495 train_acc= 0.98469 val_loss= 0.24351 val_acc= 0.93568 time= 0.17000
Epoch: 0106 train_loss= 0.10335 train_acc= 0.98503 val_loss= 0.24288 val_acc= 0.93568 time= 0.17003
Epoch: 0107 train_loss= 0.09743 train_acc= 0.98588 val_loss= 0.24195 val_acc= 0.93568 time= 0.19290
Epoch: 0108 train_loss= 0.09492 train_acc= 0.98605 val_loss= 0.24051 val_acc= 0.93568 time= 0.17512
Epoch: 0109 train_loss= 0.09207 train_acc= 0.98673 val_loss= 0.23881 val_acc= 0.93721 time= 0.18204
Epoch: 0110 train_loss= 0.08847 train_acc= 0.98826 val_loss= 0.23703 val_acc= 0.93721 time= 0.16700
Epoch: 0111 train_loss= 0.08578 train_acc= 0.98826 val_loss= 0.23529 val_acc= 0.93721 time= 0.16852
Epoch: 0112 train_loss= 0.08310 train_acc= 0.98860 val_loss= 0.23403 val_acc= 0.93721 time= 0.17701
Epoch: 0113 train_loss= 0.08067 train_acc= 0.98826 val_loss= 0.23296 val_acc= 0.93721 time= 0.20099
Epoch: 0114 train_loss= 0.07644 train_acc= 0.98877 val_loss= 0.23230 val_acc= 0.93721 time= 0.18397
Epoch: 0115 train_loss= 0.07457 train_acc= 0.98962 val_loss= 0.23192 val_acc= 0.93721 time= 0.17175
Epoch: 0116 train_loss= 0.07344 train_acc= 0.98877 val_loss= 0.23171 val_acc= 0.94028 time= 0.17300
Epoch: 0117 train_loss= 0.06914 train_acc= 0.98996 val_loss= 0.23158 val_acc= 0.94181 time= 0.16999
Epoch: 0118 train_loss= 0.06754 train_acc= 0.99064 val_loss= 0.23173 val_acc= 0.94181 time= 0.19834
Epoch: 0119 train_loss= 0.06637 train_acc= 0.99081 val_loss= 0.23140 val_acc= 0.94028 time= 0.16899
Epoch: 0120 train_loss= 0.06334 train_acc= 0.99133 val_loss= 0.23092 val_acc= 0.94181 time= 0.16900
Epoch: 0121 train_loss= 0.06095 train_acc= 0.99167 val_loss= 0.23064 val_acc= 0.94028 time= 0.16800
Epoch: 0122 train_loss= 0.05945 train_acc= 0.99150 val_loss= 0.22986 val_acc= 0.94181 time= 0.16997
Epoch: 0123 train_loss= 0.05775 train_acc= 0.99235 val_loss= 0.22905 val_acc= 0.94334 time= 0.19774
Epoch: 0124 train_loss= 0.05635 train_acc= 0.99252 val_loss= 0.22846 val_acc= 0.94334 time= 0.17061
Epoch: 0125 train_loss= 0.05430 train_acc= 0.99286 val_loss= 0.22755 val_acc= 0.94334 time= 0.17097
Epoch: 0126 train_loss= 0.05195 train_acc= 0.99388 val_loss= 0.22718 val_acc= 0.94334 time= 0.19505
Epoch: 0127 train_loss= 0.05049 train_acc= 0.99286 val_loss= 0.22673 val_acc= 0.94181 time= 0.17890
Epoch: 0128 train_loss= 0.04855 train_acc= 0.99405 val_loss= 0.22674 val_acc= 0.94181 time= 0.16954
Epoch: 0129 train_loss= 0.04773 train_acc= 0.99439 val_loss= 0.22702 val_acc= 0.94181 time= 0.19158
Epoch: 0130 train_loss= 0.04625 train_acc= 0.99422 val_loss= 0.22712 val_acc= 0.94181 time= 0.17200
Epoch: 0131 train_loss= 0.04489 train_acc= 0.99439 val_loss= 0.22688 val_acc= 0.94181 time= 0.19565
Epoch: 0132 train_loss= 0.04376 train_acc= 0.99524 val_loss= 0.22608 val_acc= 0.94181 time= 0.17200
Epoch: 0133 train_loss= 0.04272 train_acc= 0.99473 val_loss= 0.22525 val_acc= 0.94487 time= 0.17000
Epoch: 0134 train_loss= 0.04154 train_acc= 0.99439 val_loss= 0.22525 val_acc= 0.94334 time= 0.17000
Epoch: 0135 train_loss= 0.03977 train_acc= 0.99592 val_loss= 0.22546 val_acc= 0.94334 time= 0.19900
Epoch: 0136 train_loss= 0.03937 train_acc= 0.99541 val_loss= 0.22566 val_acc= 0.94334 time= 0.17151
Epoch: 0137 train_loss= 0.03789 train_acc= 0.99609 val_loss= 0.22563 val_acc= 0.94487 time= 0.19100
Epoch: 0138 train_loss= 0.03638 train_acc= 0.99575 val_loss= 0.22583 val_acc= 0.94487 time= 0.18606
Epoch: 0139 train_loss= 0.03568 train_acc= 0.99626 val_loss= 0.22564 val_acc= 0.94487 time= 0.17273
Epoch: 0140 train_loss= 0.03487 train_acc= 0.99643 val_loss= 0.22578 val_acc= 0.94487 time= 0.17580
Epoch: 0141 train_loss= 0.03423 train_acc= 0.99660 val_loss= 0.22542 val_acc= 0.94487 time= 0.19731
Epoch: 0142 train_loss= 0.03301 train_acc= 0.99677 val_loss= 0.22458 val_acc= 0.94487 time= 0.18400
Epoch: 0143 train_loss= 0.03216 train_acc= 0.99694 val_loss= 0.22413 val_acc= 0.94487 time= 0.17300
Epoch: 0144 train_loss= 0.03117 train_acc= 0.99677 val_loss= 0.22388 val_acc= 0.94334 time= 0.16900
Epoch: 0145 train_loss= 0.03014 train_acc= 0.99694 val_loss= 0.22403 val_acc= 0.94334 time= 0.17100
Epoch: 0146 train_loss= 0.03028 train_acc= 0.99711 val_loss= 0.22435 val_acc= 0.94334 time= 0.20295
Epoch: 0147 train_loss= 0.02862 train_acc= 0.99660 val_loss= 0.22533 val_acc= 0.94181 time= 0.17260
Early stopping...
Optimization Finished!
Test set results: cost= 0.24697 accuracy= 0.93808 time= 0.08900
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8889    1.0000    0.9412         8
           1     0.6667    0.3333    0.4444         6
           2     1.0000    1.0000    1.0000         1
           3     0.8068    0.9467    0.8712        75
           4     1.0000    1.0000    1.0000         9
           5     0.8298    0.8966    0.8619        87
           6     0.9200    0.9200    0.9200        25
           7     0.8462    0.8462    0.8462        13
           8     0.9167    1.0000    0.9565        11
           9     1.0000    0.5556    0.7143         9
          10     0.9167    0.6111    0.7333        36
          11     1.0000    0.9167    0.9565        12
          12     0.8403    1.0000    0.9132       121
          13     0.9333    0.7368    0.8235        19
          14     0.8889    0.8571    0.8727        28
          15     1.0000    1.0000    1.0000         4
          16     1.0000    0.2500    0.4000         4
          17     1.0000    0.3333    0.5000         3
          18     0.9091    1.0000    0.9524        10
          19     1.0000    1.0000    1.0000         2
          20     0.7143    0.5556    0.6250         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7368    0.8235    0.7778        17
          25     1.0000    0.8667    0.9286        15
          26     0.0000    0.0000    0.0000         1
          27     0.9091    0.8333    0.8696        12
          28     1.0000    0.8182    0.9000        11
          29     0.9640    0.9612    0.9626       696
          30     1.0000    1.0000    1.0000        22
          31     1.0000    1.0000    1.0000         3
          32     0.7500    0.9000    0.8182        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8171    0.8272    0.8221        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9782    0.9926    0.9853      1083
          40     1.0000    1.0000    1.0000         5
          41     0.0000    0.0000    0.0000         2
          42     0.8000    0.8889    0.8421         9
          43     1.0000    0.6667    0.8000         3
          44     0.9091    0.8333    0.8696        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9231    0.8000    0.8571        15
          48     1.0000    1.0000    1.0000         9
          49     0.0000    0.0000    0.0000         1
          50     0.5000    0.2000    0.2857         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9381      2568
   macro avg     0.7923    0.7020    0.7231      2568
weighted avg     0.9366    0.9381    0.9335      2568

Macro average Test Precision, Recall and F1-Score...
(0.7922725199141365, 0.702042455030294, 0.7231382096234544, None)
Micro average Test Precision, Recall and F1-Score...
(0.9380841121495327, 0.9380841121495327, 0.9380841121495327, None)
embeddings:
8892 6532 2568
[[ 0.02468993  1.0967792   1.2638603  ... -0.15285766 -0.08981257
   1.1629113 ]
 [ 0.5512348   0.46188632  0.6881563  ...  0.03991535  0.06161539
  -0.09832065]
 [ 0.5531854   0.32839242  0.32183453 ...  0.0033373  -0.02215176
   0.97291297]
 ...
 [ 0.5611807   0.34849015  0.16947775 ...  0.0108399   0.04318926
   0.10958214]
 [ 0.16319987  0.19985704  0.23675403 ...  0.05044333  0.0476597
   0.43388772]
 [ 0.50426626  0.17112054  0.2021257  ...  0.26180726  0.27355048
   0.2595049 ]]

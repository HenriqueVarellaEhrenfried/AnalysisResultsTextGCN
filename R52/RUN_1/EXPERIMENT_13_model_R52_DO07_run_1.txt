(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)
17992
  (0, 7394)	6.686038286434835
  (0, 7579)	9.544554181485928
  (0, 8435)	1.1089956803115342
  (0, 9187)	1.4870258028519843
  (0, 9412)	3.4992585948383703
  (0, 9715)	8.687769937101656
  (0, 9722)	2.6831883644039674
  (0, 10649)	2.428921084638427
  (0, 10738)	0.09818243264420962
  (0, 10836)	3.739323224804961
  (0, 11937)	2.330442047497012
  (0, 12542)	3.606641355876964
  (0, 12888)	3.02851632033319
  (0, 13067)	4.35029811356276
  (0, 13073)	2.3388242585209373
  (0, 13236)	2.5284796776801453
  (0, 13573)	2.03853163893571
  (0, 13630)	1.7178555995344762
  (1, 6773)	4.6162200221746765
  (1, 7352)	2.48534630686257
  (1, 7394)	10.029057429652251
  (1, 8228)	3.3602874789180293
  (1, 8236)	11.429664621685571
  (1, 8707)	2.7241125791123393
  (1, 9657)	22.280113863795975
  :	:
  (17991, 6619)	5.00515582833163
  (17991, 6688)	2.5465482720906456
  (17991, 6690)	4.132423070796605
  (17991, 8122)	2.991346301610737
  (17991, 8579)	0.5254001830155217
  (17991, 9087)	5.452468046375295
  (17991, 9104)	3.2299256610547857
  (17991, 10380)	2.6292767444269107
  (17991, 10738)	0.09818243264420962
  (17991, 11118)	2.473542891137685
  (17991, 12137)	2.8079312509954106
  (17991, 12172)	4.065283689759292
  (17991, 12422)	4.74658184003792
  (17991, 12449)	6.0249872391466255
  (17991, 12450)	5.798847182840153
  (17991, 12762)	4.203374806768889
  (17991, 13101)	2.2203469947570738
  (17991, 13183)	4.231390464459422
  (17991, 13186)	2.330442047497012
  (17991, 13283)	1.2562305119428314
  (17991, 14115)	2.001260244138478
  (17991, 14362)	23.189205994155152
  (17991, 14446)	2.588071774882391
  (17991, 15151)	2.9569343040130085
  (17991, 15162)	2.6687238299637284
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95133 train_acc= 0.00425 val_loss= 3.90443 val_acc= 0.65084 time= 0.48093
Epoch: 0002 train_loss= 3.90675 train_acc= 0.63565 val_loss= 3.80933 val_acc= 0.65850 time= 0.17904
Epoch: 0003 train_loss= 3.81102 train_acc= 0.64416 val_loss= 3.66179 val_acc= 0.66003 time= 0.17401
Epoch: 0004 train_loss= 3.66923 train_acc= 0.64195 val_loss= 3.46193 val_acc= 0.65391 time= 0.16795
Epoch: 0005 train_loss= 3.46928 train_acc= 0.63854 val_loss= 3.21948 val_acc= 0.65084 time= 0.17204
Epoch: 0006 train_loss= 3.23127 train_acc= 0.61932 val_loss= 2.95560 val_acc= 0.65084 time= 0.19800
Epoch: 0007 train_loss= 2.97458 train_acc= 0.63497 val_loss= 2.69874 val_acc= 0.65084 time= 0.17774
Epoch: 0008 train_loss= 2.70919 train_acc= 0.61881 val_loss= 2.48024 val_acc= 0.65084 time= 0.17600
Epoch: 0009 train_loss= 2.48395 train_acc= 0.63072 val_loss= 2.32928 val_acc= 0.62481 time= 0.16800
Epoch: 0010 train_loss= 2.35636 train_acc= 0.60555 val_loss= 2.24740 val_acc= 0.55743 time= 0.16809
Epoch: 0011 train_loss= 2.23563 train_acc= 0.55520 val_loss= 2.20440 val_acc= 0.48545 time= 0.18103
Epoch: 0012 train_loss= 2.22311 train_acc= 0.48103 val_loss= 2.16749 val_acc= 0.46401 time= 0.18200
Epoch: 0013 train_loss= 2.17262 train_acc= 0.44344 val_loss= 2.11755 val_acc= 0.45789 time= 0.16700
Epoch: 0014 train_loss= 2.12449 train_acc= 0.43766 val_loss= 2.04912 val_acc= 0.45789 time= 0.17197
Epoch: 0015 train_loss= 2.06585 train_acc= 0.43545 val_loss= 1.96566 val_acc= 0.46401 time= 0.17251
Epoch: 0016 train_loss= 1.99194 train_acc= 0.44344 val_loss= 1.87648 val_acc= 0.47933 time= 0.17200
Epoch: 0017 train_loss= 1.89205 train_acc= 0.45960 val_loss= 1.79280 val_acc= 0.52527 time= 0.19728
Epoch: 0018 train_loss= 1.82081 train_acc= 0.53223 val_loss= 1.72216 val_acc= 0.60949 time= 0.16600
Epoch: 0019 train_loss= 1.76360 train_acc= 0.59415 val_loss= 1.66364 val_acc= 0.66003 time= 0.17000
Epoch: 0020 train_loss= 1.68807 train_acc= 0.63718 val_loss= 1.61214 val_acc= 0.67688 time= 0.17000
Epoch: 0021 train_loss= 1.64542 train_acc= 0.65181 val_loss= 1.56133 val_acc= 0.68300 time= 0.16804
Epoch: 0022 train_loss= 1.58515 train_acc= 0.66015 val_loss= 1.50972 val_acc= 0.67841 time= 0.19895
Epoch: 0023 train_loss= 1.52955 train_acc= 0.65725 val_loss= 1.45849 val_acc= 0.67994 time= 0.17054
Epoch: 0024 train_loss= 1.48990 train_acc= 0.66882 val_loss= 1.40912 val_acc= 0.69066 time= 0.18300
Epoch: 0025 train_loss= 1.43514 train_acc= 0.66593 val_loss= 1.36333 val_acc= 0.69219 time= 0.16655
Epoch: 0026 train_loss= 1.40774 train_acc= 0.67699 val_loss= 1.32127 val_acc= 0.69985 time= 0.16995
Epoch: 0027 train_loss= 1.35056 train_acc= 0.68107 val_loss= 1.28335 val_acc= 0.70750 time= 0.17004
Epoch: 0028 train_loss= 1.30801 train_acc= 0.69638 val_loss= 1.24862 val_acc= 0.71975 time= 0.20096
Epoch: 0029 train_loss= 1.27791 train_acc= 0.70896 val_loss= 1.21641 val_acc= 0.72282 time= 0.18005
Epoch: 0030 train_loss= 1.24502 train_acc= 0.72053 val_loss= 1.18581 val_acc= 0.72894 time= 0.17208
Epoch: 0031 train_loss= 1.20348 train_acc= 0.73159 val_loss= 1.15617 val_acc= 0.73047 time= 0.16903
Epoch: 0032 train_loss= 1.18987 train_acc= 0.73635 val_loss= 1.12700 val_acc= 0.73660 time= 0.16997
Epoch: 0033 train_loss= 1.14617 train_acc= 0.74400 val_loss= 1.09808 val_acc= 0.73966 time= 0.17303
Epoch: 0034 train_loss= 1.12191 train_acc= 0.74894 val_loss= 1.06951 val_acc= 0.75038 time= 0.19700
Epoch: 0035 train_loss= 1.09047 train_acc= 0.75727 val_loss= 1.04160 val_acc= 0.75651 time= 0.16604
Epoch: 0036 train_loss= 1.06483 train_acc= 0.75965 val_loss= 1.01482 val_acc= 0.76417 time= 0.17300
Epoch: 0037 train_loss= 1.03242 train_acc= 0.76680 val_loss= 0.98914 val_acc= 0.77335 time= 0.17302
Epoch: 0038 train_loss= 1.00679 train_acc= 0.77309 val_loss= 0.96468 val_acc= 0.77642 time= 0.17697
Epoch: 0039 train_loss= 0.98308 train_acc= 0.77870 val_loss= 0.94105 val_acc= 0.78254 time= 0.21403
Epoch: 0040 train_loss= 0.95862 train_acc= 0.78194 val_loss= 0.91807 val_acc= 0.79479 time= 0.17200
Epoch: 0041 train_loss= 0.94442 train_acc= 0.79010 val_loss= 0.89562 val_acc= 0.80858 time= 0.18121
Epoch: 0042 train_loss= 0.92201 train_acc= 0.80201 val_loss= 0.87383 val_acc= 0.81776 time= 0.17200
Epoch: 0043 train_loss= 0.89685 train_acc= 0.81204 val_loss= 0.85279 val_acc= 0.82848 time= 0.17900
Epoch: 0044 train_loss= 0.86851 train_acc= 0.82633 val_loss= 0.83224 val_acc= 0.83308 time= 0.16997
Epoch: 0045 train_loss= 0.84594 train_acc= 0.82616 val_loss= 0.81176 val_acc= 0.83614 time= 0.21197
Epoch: 0046 train_loss= 0.82882 train_acc= 0.83058 val_loss= 0.79152 val_acc= 0.83767 time= 0.17207
Epoch: 0047 train_loss= 0.81520 train_acc= 0.83484 val_loss= 0.77134 val_acc= 0.83920 time= 0.16701
Epoch: 0048 train_loss= 0.78617 train_acc= 0.83909 val_loss= 0.75149 val_acc= 0.84380 time= 0.16700
Epoch: 0049 train_loss= 0.76102 train_acc= 0.84198 val_loss= 0.73201 val_acc= 0.84839 time= 0.16900
Epoch: 0050 train_loss= 0.73932 train_acc= 0.84402 val_loss= 0.71327 val_acc= 0.84839 time= 0.17000
Epoch: 0051 train_loss= 0.71890 train_acc= 0.84249 val_loss= 0.69537 val_acc= 0.84992 time= 0.16999
Epoch: 0052 train_loss= 0.70418 train_acc= 0.84674 val_loss= 0.67800 val_acc= 0.84992 time= 0.16900
Epoch: 0053 train_loss= 0.68627 train_acc= 0.84708 val_loss= 0.66113 val_acc= 0.85605 time= 0.17004
Epoch: 0054 train_loss= 0.67357 train_acc= 0.85219 val_loss= 0.64485 val_acc= 0.86064 time= 0.16632
Epoch: 0055 train_loss= 0.65748 train_acc= 0.85236 val_loss= 0.62904 val_acc= 0.86524 time= 0.16800
Epoch: 0056 train_loss= 0.62701 train_acc= 0.86069 val_loss= 0.61352 val_acc= 0.86830 time= 0.19800
Epoch: 0057 train_loss= 0.62233 train_acc= 0.86103 val_loss= 0.59804 val_acc= 0.87289 time= 0.16900
Epoch: 0058 train_loss= 0.59009 train_acc= 0.87362 val_loss= 0.58293 val_acc= 0.87136 time= 0.18000
Epoch: 0059 train_loss= 0.58527 train_acc= 0.87379 val_loss= 0.56839 val_acc= 0.87443 time= 0.16997
Epoch: 0060 train_loss= 0.56945 train_acc= 0.87022 val_loss= 0.55441 val_acc= 0.87596 time= 0.17300
Epoch: 0061 train_loss= 0.56345 train_acc= 0.87583 val_loss= 0.54107 val_acc= 0.87749 time= 0.17004
Epoch: 0062 train_loss= 0.54112 train_acc= 0.88042 val_loss= 0.52810 val_acc= 0.87902 time= 0.19599
Epoch: 0063 train_loss= 0.52565 train_acc= 0.87617 val_loss= 0.51526 val_acc= 0.88055 time= 0.18300
Epoch: 0064 train_loss= 0.51091 train_acc= 0.88569 val_loss= 0.50233 val_acc= 0.88361 time= 0.16701
Epoch: 0065 train_loss= 0.50818 train_acc= 0.88416 val_loss= 0.48953 val_acc= 0.88515 time= 0.16999
Epoch: 0066 train_loss= 0.46828 train_acc= 0.89573 val_loss= 0.47736 val_acc= 0.88974 time= 0.16600
Epoch: 0067 train_loss= 0.46796 train_acc= 0.88876 val_loss= 0.46659 val_acc= 0.89127 time= 0.17406
Epoch: 0068 train_loss= 0.45322 train_acc= 0.90236 val_loss= 0.45678 val_acc= 0.89740 time= 0.20200
Epoch: 0069 train_loss= 0.43744 train_acc= 0.90475 val_loss= 0.44729 val_acc= 0.89433 time= 0.16900
Epoch: 0070 train_loss= 0.43448 train_acc= 0.90287 val_loss= 0.43791 val_acc= 0.89587 time= 0.16801
Epoch: 0071 train_loss= 0.41945 train_acc= 0.90730 val_loss= 0.42874 val_acc= 0.89740 time= 0.16799
Epoch: 0072 train_loss= 0.41398 train_acc= 0.90900 val_loss= 0.41994 val_acc= 0.89740 time= 0.16601
Epoch: 0073 train_loss= 0.40077 train_acc= 0.90985 val_loss= 0.41117 val_acc= 0.89893 time= 0.20396
Epoch: 0074 train_loss= 0.39112 train_acc= 0.91240 val_loss= 0.40283 val_acc= 0.89893 time= 0.16907
Epoch: 0075 train_loss= 0.38232 train_acc= 0.91138 val_loss= 0.39503 val_acc= 0.89587 time= 0.18762
Epoch: 0076 train_loss= 0.37036 train_acc= 0.91563 val_loss= 0.38745 val_acc= 0.89587 time= 0.16800
Epoch: 0077 train_loss= 0.36814 train_acc= 0.91750 val_loss= 0.38055 val_acc= 0.89740 time= 0.16900
Epoch: 0078 train_loss= 0.34920 train_acc= 0.92022 val_loss= 0.37335 val_acc= 0.89893 time= 0.16800
Epoch: 0079 train_loss= 0.34050 train_acc= 0.92703 val_loss= 0.36587 val_acc= 0.90199 time= 0.20100
Epoch: 0080 train_loss= 0.33654 train_acc= 0.92142 val_loss= 0.35914 val_acc= 0.90199 time= 0.17800
Epoch: 0081 train_loss= 0.32610 train_acc= 0.92533 val_loss= 0.35280 val_acc= 0.90352 time= 0.16897
Epoch: 0082 train_loss= 0.31791 train_acc= 0.92380 val_loss= 0.34707 val_acc= 0.90658 time= 0.18151
Epoch: 0083 train_loss= 0.30729 train_acc= 0.93094 val_loss= 0.34169 val_acc= 0.91118 time= 0.17203
Epoch: 0084 train_loss= 0.29882 train_acc= 0.93213 val_loss= 0.33765 val_acc= 0.90965 time= 0.17100
Epoch: 0085 train_loss= 0.29318 train_acc= 0.93791 val_loss= 0.33404 val_acc= 0.90965 time= 0.19900
Epoch: 0086 train_loss= 0.28720 train_acc= 0.93859 val_loss= 0.33032 val_acc= 0.90965 time= 0.16900
Epoch: 0087 train_loss= 0.27636 train_acc= 0.93996 val_loss= 0.32633 val_acc= 0.90965 time= 0.17001
Epoch: 0088 train_loss= 0.28268 train_acc= 0.94234 val_loss= 0.32147 val_acc= 0.90965 time= 0.16799
Epoch: 0089 train_loss= 0.26018 train_acc= 0.94370 val_loss= 0.31715 val_acc= 0.90965 time= 0.16951
Epoch: 0090 train_loss= 0.26222 train_acc= 0.94081 val_loss= 0.31261 val_acc= 0.91424 time= 0.20700
Epoch: 0091 train_loss= 0.25616 train_acc= 0.94472 val_loss= 0.30799 val_acc= 0.91271 time= 0.16801
Epoch: 0092 train_loss= 0.24362 train_acc= 0.94676 val_loss= 0.30385 val_acc= 0.91424 time= 0.17985
Epoch: 0093 train_loss= 0.23815 train_acc= 0.94506 val_loss= 0.29995 val_acc= 0.91424 time= 0.16951
Epoch: 0094 train_loss= 0.22724 train_acc= 0.95152 val_loss= 0.29672 val_acc= 0.91577 time= 0.16901
Epoch: 0095 train_loss= 0.23988 train_acc= 0.94761 val_loss= 0.29426 val_acc= 0.91271 time= 0.17199
Epoch: 0096 train_loss= 0.22169 train_acc= 0.94727 val_loss= 0.29320 val_acc= 0.90812 time= 0.20200
Epoch: 0097 train_loss= 0.21412 train_acc= 0.95373 val_loss= 0.29069 val_acc= 0.91424 time= 0.18752
Epoch: 0098 train_loss= 0.21361 train_acc= 0.95458 val_loss= 0.28744 val_acc= 0.91424 time= 0.17200
Epoch: 0099 train_loss= 0.20819 train_acc= 0.95782 val_loss= 0.28309 val_acc= 0.91730 time= 0.17088
Epoch: 0100 train_loss= 0.20231 train_acc= 0.95612 val_loss= 0.27847 val_acc= 0.91884 time= 0.17201
Epoch: 0101 train_loss= 0.20042 train_acc= 0.95714 val_loss= 0.27488 val_acc= 0.92037 time= 0.18603
Epoch: 0102 train_loss= 0.19921 train_acc= 0.95118 val_loss= 0.27270 val_acc= 0.92343 time= 0.17497
Epoch: 0103 train_loss= 0.19330 train_acc= 0.95850 val_loss= 0.27053 val_acc= 0.92343 time= 0.16804
Epoch: 0104 train_loss= 0.18222 train_acc= 0.95748 val_loss= 0.26830 val_acc= 0.92190 time= 0.17010
Epoch: 0105 train_loss= 0.18674 train_acc= 0.95305 val_loss= 0.26657 val_acc= 0.92649 time= 0.17278
Epoch: 0106 train_loss= 0.17342 train_acc= 0.96139 val_loss= 0.26691 val_acc= 0.92496 time= 0.16800
Epoch: 0107 train_loss= 0.18133 train_acc= 0.95884 val_loss= 0.26755 val_acc= 0.92037 time= 0.19121
Epoch: 0108 train_loss= 0.16446 train_acc= 0.96496 val_loss= 0.26854 val_acc= 0.92496 time= 0.16796
Epoch: 0109 train_loss= 0.16879 train_acc= 0.96360 val_loss= 0.26757 val_acc= 0.92343 time= 0.18703
Epoch: 0110 train_loss= 0.16181 train_acc= 0.96649 val_loss= 0.26466 val_acc= 0.92037 time= 0.16900
Epoch: 0111 train_loss= 0.15909 train_acc= 0.96683 val_loss= 0.25891 val_acc= 0.92496 time= 0.16997
Epoch: 0112 train_loss= 0.15953 train_acc= 0.96462 val_loss= 0.25375 val_acc= 0.92649 time= 0.17104
Epoch: 0113 train_loss= 0.16075 train_acc= 0.96581 val_loss= 0.24973 val_acc= 0.92956 time= 0.20100
Epoch: 0114 train_loss= 0.14590 train_acc= 0.96870 val_loss= 0.24756 val_acc= 0.93109 time= 0.17500
Epoch: 0115 train_loss= 0.14894 train_acc= 0.96666 val_loss= 0.24640 val_acc= 0.93109 time= 0.16796
Epoch: 0116 train_loss= 0.14548 train_acc= 0.96598 val_loss= 0.24708 val_acc= 0.93109 time= 0.16803
Epoch: 0117 train_loss= 0.14629 train_acc= 0.96887 val_loss= 0.24946 val_acc= 0.93262 time= 0.17000
Epoch: 0118 train_loss= 0.14003 train_acc= 0.96972 val_loss= 0.25167 val_acc= 0.92649 time= 0.19997
Epoch: 0119 train_loss= 0.14364 train_acc= 0.96768 val_loss= 0.25343 val_acc= 0.92802 time= 0.16960
Epoch: 0120 train_loss= 0.13243 train_acc= 0.97329 val_loss= 0.25341 val_acc= 0.92649 time= 0.17200
Early stopping...
Optimization Finished!
Test set results: cost= 0.28063 accuracy= 0.93497 time= 0.07300
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     1.0000    1.0000    1.0000         8
           1     0.6667    0.3333    0.4444         6
           2     0.3333    1.0000    0.5000         1
           3     0.7826    0.9600    0.8623        75
           4     1.0000    1.0000    1.0000         9
           5     0.7810    0.9425    0.8542        87
           6     0.8519    0.9200    0.8846        25
           7     0.8571    0.9231    0.8889        13
           8     0.7333    1.0000    0.8462        11
           9     1.0000    0.4444    0.6154         9
          10     0.9583    0.6389    0.7667        36
          11     1.0000    0.9167    0.9565        12
          12     0.8511    0.9917    0.9160       121
          13     0.9286    0.6842    0.7879        19
          14     0.8929    0.8929    0.8929        28
          15     1.0000    0.5000    0.6667         4
          16     0.0000    0.0000    0.0000         4
          17     0.0000    0.0000    0.0000         3
          18     0.9000    0.9000    0.9000        10
          19     1.0000    1.0000    1.0000         2
          20     1.0000    0.4444    0.6154         9
          21     0.9048    0.9500    0.9268        20
          22     0.4286    0.6000    0.5000         5
          23     0.0000    0.0000    0.0000         1
          24     0.7333    0.6471    0.6875        17
          25     1.0000    0.8000    0.8889        15
          26     0.0000    0.0000    0.0000         1
          27     1.0000    0.7500    0.8571        12
          28     1.0000    0.7273    0.8421        11
          29     0.9669    0.9655    0.9662       696
          30     0.9565    1.0000    0.9778        22
          31     1.0000    0.6667    0.8000         3
          32     0.6154    0.8000    0.6957        10
          33     1.0000    0.6667    0.8000         3
          34     0.5000    1.0000    0.6667         1
          35     0.8919    0.8148    0.8516        81
          36     1.0000    0.3333    0.5000        12
          37     1.0000    0.7500    0.8571         4
          38     0.0000    0.0000    0.0000         1
          39     0.9773    0.9926    0.9849      1083
          40     0.7143    1.0000    0.8333         5
          41     0.0000    0.0000    0.0000         2
          42     0.7273    0.8889    0.8000         9
          43     1.0000    0.6667    0.8000         3
          44     0.7500    0.7500    0.7500        12
          45     0.0000    0.0000    0.0000         6
          46     1.0000    0.2857    0.4444         7
          47     0.9286    0.8667    0.8966        15
          48     1.0000    0.8889    0.9412         9
          49     0.0000    0.0000    0.0000         1
          50     1.0000    0.2000    0.3333         5
          51     1.0000    0.7500    0.8571         4

    accuracy                         0.9350      2568
   macro avg     0.7429    0.6587    0.6703      2568
weighted avg     0.9341    0.9350    0.9294      2568

Macro average Test Precision, Recall and F1-Score...
(0.7429132468995786, 0.6587106812512483, 0.6703140541228355, None)
Micro average Test Precision, Recall and F1-Score...
(0.9349688473520249, 0.9349688473520249, 0.9349688473520249, None)
embeddings:
8892 6532 2568
[[-0.18635328  0.21961156  0.1754998  ... -0.06738806 -0.11383602
   1.0118325 ]
 [ 0.10175122  0.04273735  0.05297629 ...  0.13830015 -0.00569624
   0.45510456]
 [ 0.03205606  0.04471319 -0.02282957 ... -0.05383956  0.20047383
   0.36350358]
 ...
 [ 0.03076563  0.14442544  0.00397654 ...  0.00713263  0.06435366
   0.03036047]
 [ 0.04937345  0.05455749  0.02430875 ...  0.02841774  0.10459738
   0.15306339]
 [ 0.24654715  0.2252412   0.2352369  ...  0.24164948  0.22886474
   0.2222427 ]]

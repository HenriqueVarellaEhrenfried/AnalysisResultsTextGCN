(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07950 train_acc= 0.06218 val_loss= 2.02548 val_acc= 0.75182 time= 0.39952
Epoch: 0002 train_loss= 2.02229 train_acc= 0.76706 val_loss= 1.93576 val_acc= 0.75365 time= 0.12795
Epoch: 0003 train_loss= 1.93029 train_acc= 0.76524 val_loss= 1.81404 val_acc= 0.74088 time= 0.12505
Epoch: 0004 train_loss= 1.81061 train_acc= 0.75694 val_loss= 1.67233 val_acc= 0.73540 time= 0.12406
Epoch: 0005 train_loss= 1.65707 train_acc= 0.74742 val_loss= 1.53184 val_acc= 0.72263 time= 0.12306
Epoch: 0006 train_loss= 1.51545 train_acc= 0.73911 val_loss= 1.41319 val_acc= 0.71715 time= 0.12704
Epoch: 0007 train_loss= 1.38817 train_acc= 0.72595 val_loss= 1.32287 val_acc= 0.70620 time= 0.12496
Epoch: 0008 train_loss= 1.29582 train_acc= 0.72615 val_loss= 1.25269 val_acc= 0.68978 time= 0.12700
Epoch: 0009 train_loss= 1.21257 train_acc= 0.70488 val_loss= 1.19003 val_acc= 0.68613 time= 0.17400
Epoch: 0010 train_loss= 1.14709 train_acc= 0.71136 val_loss= 1.12652 val_acc= 0.69891 time= 0.12300
Epoch: 0011 train_loss= 1.08432 train_acc= 0.71643 val_loss= 1.05864 val_acc= 0.72628 time= 0.12600
Epoch: 0012 train_loss= 1.01752 train_acc= 0.73506 val_loss= 0.98716 val_acc= 0.74453 time= 0.12385
Epoch: 0013 train_loss= 0.95009 train_acc= 0.76220 val_loss= 0.91582 val_acc= 0.75912 time= 0.12300
Epoch: 0014 train_loss= 0.87546 train_acc= 0.77821 val_loss= 0.84958 val_acc= 0.75912 time= 0.12397
Epoch: 0015 train_loss= 0.80976 train_acc= 0.78631 val_loss= 0.79240 val_acc= 0.76095 time= 0.12511
Epoch: 0016 train_loss= 0.76155 train_acc= 0.78550 val_loss= 0.74570 val_acc= 0.76460 time= 0.16696
Epoch: 0017 train_loss= 0.70915 train_acc= 0.78874 val_loss= 0.70873 val_acc= 0.77555 time= 0.12700
Epoch: 0018 train_loss= 0.67932 train_acc= 0.79583 val_loss= 0.67887 val_acc= 0.79562 time= 0.12704
Epoch: 0019 train_loss= 0.65000 train_acc= 0.81365 val_loss= 0.65314 val_acc= 0.81934 time= 0.12299
Epoch: 0020 train_loss= 0.62272 train_acc= 0.83492 val_loss= 0.62902 val_acc= 0.83212 time= 0.12401
Epoch: 0021 train_loss= 0.59472 train_acc= 0.84910 val_loss= 0.60519 val_acc= 0.83942 time= 0.12300
Epoch: 0022 train_loss= 0.56751 train_acc= 0.86409 val_loss= 0.58145 val_acc= 0.85036 time= 0.12403
Epoch: 0023 train_loss= 0.54581 train_acc= 0.87016 val_loss= 0.55826 val_acc= 0.85584 time= 0.12703
Epoch: 0024 train_loss= 0.51572 train_acc= 0.87462 val_loss= 0.53631 val_acc= 0.85584 time= 0.14900
Epoch: 0025 train_loss= 0.49589 train_acc= 0.87604 val_loss= 0.51598 val_acc= 0.85584 time= 0.12397
Epoch: 0026 train_loss= 0.47231 train_acc= 0.88130 val_loss= 0.49744 val_acc= 0.85766 time= 0.12400
Epoch: 0027 train_loss= 0.45127 train_acc= 0.88455 val_loss= 0.48047 val_acc= 0.85949 time= 0.12500
Epoch: 0028 train_loss= 0.43619 train_acc= 0.88941 val_loss= 0.46465 val_acc= 0.87226 time= 0.12448
Epoch: 0029 train_loss= 0.41818 train_acc= 0.89244 val_loss= 0.44957 val_acc= 0.87591 time= 0.12303
Epoch: 0030 train_loss= 0.39787 train_acc= 0.89791 val_loss= 0.43495 val_acc= 0.88321 time= 0.12297
Epoch: 0031 train_loss= 0.37854 train_acc= 0.90217 val_loss= 0.42064 val_acc= 0.88686 time= 0.12551
Epoch: 0032 train_loss= 0.36889 train_acc= 0.90298 val_loss= 0.40652 val_acc= 0.89234 time= 0.16500
Epoch: 0033 train_loss= 0.34988 train_acc= 0.90602 val_loss= 0.39273 val_acc= 0.89416 time= 0.12197
Epoch: 0034 train_loss= 0.33272 train_acc= 0.91331 val_loss= 0.37923 val_acc= 0.90146 time= 0.12400
Epoch: 0035 train_loss= 0.32586 train_acc= 0.91128 val_loss= 0.36599 val_acc= 0.90693 time= 0.12300
Epoch: 0036 train_loss= 0.30707 train_acc= 0.91837 val_loss= 0.35322 val_acc= 0.91058 time= 0.12587
Epoch: 0037 train_loss= 0.29595 train_acc= 0.92202 val_loss= 0.34089 val_acc= 0.91058 time= 0.12527
Epoch: 0038 train_loss= 0.28311 train_acc= 0.92566 val_loss= 0.32906 val_acc= 0.91423 time= 0.12400
Epoch: 0039 train_loss= 0.26582 train_acc= 0.93113 val_loss= 0.31794 val_acc= 0.91788 time= 0.13197
Epoch: 0040 train_loss= 0.25616 train_acc= 0.93579 val_loss= 0.30727 val_acc= 0.92153 time= 0.15403
Epoch: 0041 train_loss= 0.24615 train_acc= 0.94146 val_loss= 0.29715 val_acc= 0.92336 time= 0.12300
Epoch: 0042 train_loss= 0.23205 train_acc= 0.95037 val_loss= 0.28746 val_acc= 0.92518 time= 0.12401
Epoch: 0043 train_loss= 0.22326 train_acc= 0.94693 val_loss= 0.27812 val_acc= 0.92883 time= 0.12200
Epoch: 0044 train_loss= 0.21373 train_acc= 0.95362 val_loss= 0.26918 val_acc= 0.92883 time= 0.12384
Epoch: 0045 train_loss= 0.19856 train_acc= 0.95443 val_loss= 0.26076 val_acc= 0.93248 time= 0.12311
Epoch: 0046 train_loss= 0.19292 train_acc= 0.95341 val_loss= 0.25269 val_acc= 0.93431 time= 0.12697
Epoch: 0047 train_loss= 0.18301 train_acc= 0.95746 val_loss= 0.24499 val_acc= 0.93431 time= 0.17400
Epoch: 0048 train_loss= 0.17042 train_acc= 0.95888 val_loss= 0.23781 val_acc= 0.93431 time= 0.12503
Epoch: 0049 train_loss= 0.16516 train_acc= 0.96070 val_loss= 0.23127 val_acc= 0.93613 time= 0.12401
Epoch: 0050 train_loss= 0.15623 train_acc= 0.96111 val_loss= 0.22523 val_acc= 0.93978 time= 0.12303
Epoch: 0051 train_loss= 0.14837 train_acc= 0.96293 val_loss= 0.21958 val_acc= 0.94161 time= 0.12287
Epoch: 0052 train_loss= 0.14295 train_acc= 0.96476 val_loss= 0.21404 val_acc= 0.94161 time= 0.12304
Epoch: 0053 train_loss= 0.13664 train_acc= 0.96476 val_loss= 0.20854 val_acc= 0.94343 time= 0.12299
Epoch: 0054 train_loss= 0.12791 train_acc= 0.96638 val_loss= 0.20300 val_acc= 0.94343 time= 0.12300
Epoch: 0055 train_loss= 0.12431 train_acc= 0.96820 val_loss= 0.19782 val_acc= 0.94708 time= 0.15096
Epoch: 0056 train_loss= 0.11607 train_acc= 0.97104 val_loss= 0.19312 val_acc= 0.94526 time= 0.12800
Epoch: 0057 train_loss= 0.11167 train_acc= 0.96881 val_loss= 0.18891 val_acc= 0.94343 time= 0.12500
Epoch: 0058 train_loss= 0.10651 train_acc= 0.97286 val_loss= 0.18515 val_acc= 0.94526 time= 0.12300
Epoch: 0059 train_loss= 0.10167 train_acc= 0.97428 val_loss= 0.18194 val_acc= 0.94708 time= 0.12304
Epoch: 0060 train_loss= 0.09964 train_acc= 0.97650 val_loss= 0.17904 val_acc= 0.94891 time= 0.12396
Epoch: 0061 train_loss= 0.09611 train_acc= 0.97671 val_loss= 0.17631 val_acc= 0.94708 time= 0.12505
Epoch: 0062 train_loss= 0.09292 train_acc= 0.97792 val_loss= 0.17358 val_acc= 0.94891 time= 0.12300
Epoch: 0063 train_loss= 0.08584 train_acc= 0.98197 val_loss= 0.17092 val_acc= 0.95255 time= 0.16807
Epoch: 0064 train_loss= 0.08427 train_acc= 0.98116 val_loss= 0.16842 val_acc= 0.95255 time= 0.12300
Epoch: 0065 train_loss= 0.08214 train_acc= 0.98076 val_loss= 0.16606 val_acc= 0.95255 time= 0.12781
Epoch: 0066 train_loss= 0.07974 train_acc= 0.98238 val_loss= 0.16422 val_acc= 0.95255 time= 0.12570
Epoch: 0067 train_loss= 0.07606 train_acc= 0.98137 val_loss= 0.16287 val_acc= 0.95255 time= 0.12501
Epoch: 0068 train_loss= 0.07221 train_acc= 0.98420 val_loss= 0.16169 val_acc= 0.95255 time= 0.12307
Epoch: 0069 train_loss= 0.06914 train_acc= 0.98481 val_loss= 0.16054 val_acc= 0.95255 time= 0.12196
Epoch: 0070 train_loss= 0.06890 train_acc= 0.98461 val_loss= 0.15939 val_acc= 0.95255 time= 0.14603
Epoch: 0071 train_loss= 0.06359 train_acc= 0.98481 val_loss= 0.15796 val_acc= 0.95255 time= 0.13808
Epoch: 0072 train_loss= 0.06390 train_acc= 0.98542 val_loss= 0.15678 val_acc= 0.95255 time= 0.12207
Epoch: 0073 train_loss= 0.06120 train_acc= 0.98501 val_loss= 0.15532 val_acc= 0.95255 time= 0.12800
Epoch: 0074 train_loss= 0.05880 train_acc= 0.98562 val_loss= 0.15405 val_acc= 0.95438 time= 0.12397
Epoch: 0075 train_loss= 0.05599 train_acc= 0.98805 val_loss= 0.15253 val_acc= 0.95255 time= 0.12700
Epoch: 0076 train_loss= 0.05543 train_acc= 0.98866 val_loss= 0.15139 val_acc= 0.95255 time= 0.12600
Epoch: 0077 train_loss= 0.05412 train_acc= 0.98744 val_loss= 0.15033 val_acc= 0.95438 time= 0.12300
Epoch: 0078 train_loss= 0.05082 train_acc= 0.98845 val_loss= 0.14972 val_acc= 0.95255 time= 0.16200
Epoch: 0079 train_loss= 0.04876 train_acc= 0.98967 val_loss= 0.14973 val_acc= 0.95255 time= 0.12403
Epoch: 0080 train_loss= 0.04698 train_acc= 0.98987 val_loss= 0.14997 val_acc= 0.95438 time= 0.12300
Epoch: 0081 train_loss= 0.04710 train_acc= 0.98886 val_loss= 0.14993 val_acc= 0.95438 time= 0.12597
Epoch: 0082 train_loss= 0.04475 train_acc= 0.98987 val_loss= 0.14997 val_acc= 0.95438 time= 0.12500
Epoch: 0083 train_loss= 0.04413 train_acc= 0.99007 val_loss= 0.15038 val_acc= 0.95255 time= 0.12319
Epoch: 0084 train_loss= 0.04170 train_acc= 0.99068 val_loss= 0.15062 val_acc= 0.95803 time= 0.12497
Epoch: 0085 train_loss= 0.04313 train_acc= 0.99129 val_loss= 0.15001 val_acc= 0.95985 time= 0.12611
Epoch: 0086 train_loss= 0.04213 train_acc= 0.99089 val_loss= 0.14853 val_acc= 0.95985 time= 0.15403
Epoch: 0087 train_loss= 0.04026 train_acc= 0.99190 val_loss= 0.14704 val_acc= 0.95620 time= 0.12300
Epoch: 0088 train_loss= 0.03823 train_acc= 0.99271 val_loss= 0.14591 val_acc= 0.95620 time= 0.12501
Epoch: 0089 train_loss= 0.03626 train_acc= 0.99190 val_loss= 0.14483 val_acc= 0.95620 time= 0.12500
Epoch: 0090 train_loss= 0.03701 train_acc= 0.99129 val_loss= 0.14469 val_acc= 0.95803 time= 0.12603
Epoch: 0091 train_loss= 0.03676 train_acc= 0.99190 val_loss= 0.14550 val_acc= 0.95803 time= 0.12408
Epoch: 0092 train_loss= 0.03741 train_acc= 0.99149 val_loss= 0.14716 val_acc= 0.95620 time= 0.12399
Epoch: 0093 train_loss= 0.03503 train_acc= 0.99311 val_loss= 0.14927 val_acc= 0.95438 time= 0.12400
Early stopping...
Optimization Finished!
Test set results: cost= 0.10556 accuracy= 0.97442 time= 0.10299
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9512    0.9669    0.9590       121
           1     0.9136    0.9867    0.9487        75
           2     0.9844    0.9917    0.9880      1083
           3     1.0000    1.0000    1.0000        10
           4     0.9643    0.7500    0.8437        36
           5     0.9231    0.8889    0.9057        81
           6     0.9091    0.9195    0.9143        87
           7     0.9841    0.9756    0.9798       696

    accuracy                         0.9744      2189
   macro avg     0.9537    0.9349    0.9424      2189
weighted avg     0.9745    0.9744    0.9742      2189

Macro average Test Precision, Recall and F1-Score...
(0.9537161552182889, 0.9349127996921427, 0.9424086114979526, None)
Micro average Test Precision, Recall and F1-Score...
(0.9744175422567383, 0.9744175422567383, 0.9744175422567383, None)
embeddings:
7688 5485 2189
[[ 0.3694946   0.18688993  0.29870307 ...  0.21140584  0.08164334
   0.2223318 ]
 [ 0.2552308   0.24359289  0.16757779 ...  0.06583407  0.22850122
   0.05820662]
 [-0.01657812  0.20391983 -0.01182279 ...  0.14421068  0.40516463
   0.17476436]
 ...
 [ 0.14852932  0.01641051  0.08667905 ...  0.23653355  0.4521466
   0.24755934]
 [ 0.33150935  0.35625988  0.24796796 ...  0.04646499  0.26553386
   0.04397008]
 [ 0.05426344  0.02939744  0.03709218 ...  0.15865475  0.40647045
   0.19373064]]

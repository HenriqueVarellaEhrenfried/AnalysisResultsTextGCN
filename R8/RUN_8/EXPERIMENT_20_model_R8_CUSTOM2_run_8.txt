(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07936 train_acc= 0.14948 val_loss= 1.59856 val_acc= 0.75730 time= 0.38601
Epoch: 0002 train_loss= 1.57407 train_acc= 0.78813 val_loss= 1.26448 val_acc= 0.75912 time= 0.13206
Epoch: 0003 train_loss= 1.21925 train_acc= 0.78367 val_loss= 1.04746 val_acc= 0.63686 time= 0.13061
Epoch: 0004 train_loss= 0.99412 train_acc= 0.65728 val_loss= 0.79045 val_acc= 0.75547 time= 0.13000
Epoch: 0005 train_loss= 0.74830 train_acc= 0.78469 val_loss= 0.68375 val_acc= 0.75365 time= 0.13200
Epoch: 0006 train_loss= 0.64231 train_acc= 0.77193 val_loss= 0.62662 val_acc= 0.77920 time= 0.12300
Epoch: 0007 train_loss= 0.58240 train_acc= 0.80332 val_loss= 0.57903 val_acc= 0.82847 time= 0.12401
Epoch: 0008 train_loss= 0.53029 train_acc= 0.84707 val_loss= 0.53352 val_acc= 0.83394 time= 0.12308
Epoch: 0009 train_loss= 0.47837 train_acc= 0.85963 val_loss= 0.49295 val_acc= 0.84124 time= 0.12300
Epoch: 0010 train_loss= 0.42813 train_acc= 0.86672 val_loss= 0.45904 val_acc= 0.85401 time= 0.12336
Epoch: 0011 train_loss= 0.39035 train_acc= 0.87219 val_loss= 0.43017 val_acc= 0.86131 time= 0.17500
Epoch: 0012 train_loss= 0.34817 train_acc= 0.88596 val_loss= 0.40241 val_acc= 0.87591 time= 0.12673
Epoch: 0013 train_loss= 0.30779 train_acc= 0.90602 val_loss= 0.37371 val_acc= 0.89599 time= 0.12600
Epoch: 0014 train_loss= 0.28265 train_acc= 0.91938 val_loss= 0.34696 val_acc= 0.90876 time= 0.12416
Epoch: 0015 train_loss= 0.24870 train_acc= 0.93174 val_loss= 0.32292 val_acc= 0.91788 time= 0.12400
Epoch: 0016 train_loss= 0.21803 train_acc= 0.94065 val_loss= 0.30185 val_acc= 0.92883 time= 0.12503
Epoch: 0017 train_loss= 0.19958 train_acc= 0.94430 val_loss= 0.28473 val_acc= 0.92883 time= 0.12309
Epoch: 0018 train_loss= 0.17394 train_acc= 0.94815 val_loss= 0.27140 val_acc= 0.93431 time= 0.12199
Epoch: 0019 train_loss= 0.14881 train_acc= 0.95868 val_loss= 0.26154 val_acc= 0.93796 time= 0.12597
Epoch: 0020 train_loss= 0.13438 train_acc= 0.96273 val_loss= 0.25217 val_acc= 0.93978 time= 0.17303
Epoch: 0021 train_loss= 0.12051 train_acc= 0.96212 val_loss= 0.24260 val_acc= 0.94161 time= 0.12200
Epoch: 0022 train_loss= 0.10064 train_acc= 0.96800 val_loss= 0.23907 val_acc= 0.94161 time= 0.12500
Epoch: 0023 train_loss= 0.09653 train_acc= 0.97103 val_loss= 0.23657 val_acc= 0.94161 time= 0.12600
Epoch: 0024 train_loss= 0.08392 train_acc= 0.97245 val_loss= 0.23590 val_acc= 0.94526 time= 0.12417
Epoch: 0025 train_loss= 0.07530 train_acc= 0.97671 val_loss= 0.23462 val_acc= 0.94526 time= 0.12200
Epoch: 0026 train_loss= 0.06964 train_acc= 0.97752 val_loss= 0.23163 val_acc= 0.94891 time= 0.12300
Epoch: 0027 train_loss= 0.06117 train_acc= 0.98177 val_loss= 0.22913 val_acc= 0.94708 time= 0.12299
Epoch: 0028 train_loss= 0.05459 train_acc= 0.98299 val_loss= 0.22822 val_acc= 0.94708 time= 0.15500
Epoch: 0029 train_loss= 0.04909 train_acc= 0.98582 val_loss= 0.22733 val_acc= 0.95073 time= 0.12403
Epoch: 0030 train_loss= 0.04505 train_acc= 0.98704 val_loss= 0.22630 val_acc= 0.95255 time= 0.12300
Epoch: 0031 train_loss= 0.04151 train_acc= 0.98764 val_loss= 0.22966 val_acc= 0.95073 time= 0.12201
Early stopping...
Optimization Finished!
Test set results: cost= 0.14934 accuracy= 0.96482 time= 0.05593
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9000    0.9669    0.9323       121
           1     0.8765    0.9467    0.9103        75
           2     0.9835    0.9917    0.9876      1083
           3     0.5556    0.5000    0.5263        10
           4     0.9259    0.6944    0.7937        36
           5     0.9041    0.8148    0.8571        81
           6     0.8542    0.9425    0.8962        87
           7     0.9868    0.9655    0.9760       696

    accuracy                         0.9648      2189
   macro avg     0.8733    0.8528    0.8599      2189
weighted avg     0.9653    0.9648    0.9644      2189

Macro average Test Precision, Recall and F1-Score...
(0.8733251964439291, 0.8528254752987838, 0.8599290869413421, None)
Micro average Test Precision, Recall and F1-Score...
(0.964824120603015, 0.964824120603015, 0.964824120603015, None)
embeddings:
7688 5485 2189
[[-0.08399824 -0.03164649  0.03246908 ...  0.12247322  0.07436963
   0.08079799]
 [-0.0990968   0.07619113  0.11188553 ... -0.1493081  -0.15623601
  -0.13734847]
 [ 0.4097095   0.52320695  0.69595706 ...  0.0650003  -0.00076988
   0.22274764]
 ...
 [ 0.37859303  0.5010742   0.6484553  ...  0.22010165  0.18700983
   0.3759857 ]
 [-0.01808324  0.11325201  0.13802946 ... -0.2911391  -0.30671057
  -0.28423652]
 [ 0.44930717  0.43170118  0.61619633 ...  0.195582    0.1323833
   0.23405594]]

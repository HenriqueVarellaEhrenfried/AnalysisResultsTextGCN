(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07958 train_acc= 0.04861 val_loss= 2.02955 val_acc= 0.75547 time= 0.39432
Epoch: 0002 train_loss= 2.02783 train_acc= 0.78185 val_loss= 1.94685 val_acc= 0.75547 time= 0.13100
Epoch: 0003 train_loss= 1.94325 train_acc= 0.78064 val_loss= 1.83204 val_acc= 0.75365 time= 0.12700
Epoch: 0004 train_loss= 1.82472 train_acc= 0.78428 val_loss= 1.69438 val_acc= 0.75365 time= 0.15200
Epoch: 0005 train_loss= 1.68185 train_acc= 0.78367 val_loss= 1.55134 val_acc= 0.75912 time= 0.12216
Epoch: 0006 train_loss= 1.53369 train_acc= 0.78084 val_loss= 1.42375 val_acc= 0.75365 time= 0.12500
Epoch: 0007 train_loss= 1.39679 train_acc= 0.77395 val_loss= 1.32329 val_acc= 0.73905 time= 0.12300
Epoch: 0008 train_loss= 1.28840 train_acc= 0.76058 val_loss= 1.24652 val_acc= 0.72810 time= 0.12408
Epoch: 0009 train_loss= 1.20712 train_acc= 0.74175 val_loss= 1.18261 val_acc= 0.71715 time= 0.12800
Epoch: 0010 train_loss= 1.13903 train_acc= 0.73223 val_loss= 1.12125 val_acc= 0.72080 time= 0.12400
Epoch: 0011 train_loss= 1.07546 train_acc= 0.73567 val_loss= 1.05643 val_acc= 0.73175 time= 0.12759
Epoch: 0012 train_loss= 1.01208 train_acc= 0.75167 val_loss= 0.98746 val_acc= 0.75000 time= 0.17100
Epoch: 0013 train_loss= 0.94428 train_acc= 0.77031 val_loss= 0.91741 val_acc= 0.76277 time= 0.12401
Epoch: 0014 train_loss= 0.87655 train_acc= 0.78104 val_loss= 0.85050 val_acc= 0.76095 time= 0.12293
Epoch: 0015 train_loss= 0.81302 train_acc= 0.78833 val_loss= 0.79063 val_acc= 0.76277 time= 0.12400
Epoch: 0016 train_loss= 0.75499 train_acc= 0.78914 val_loss= 0.74050 val_acc= 0.76825 time= 0.12400
Epoch: 0017 train_loss= 0.70630 train_acc= 0.79623 val_loss= 0.70070 val_acc= 0.78467 time= 0.12601
Epoch: 0018 train_loss= 0.66843 train_acc= 0.81041 val_loss= 0.66931 val_acc= 0.81022 time= 0.12200
Epoch: 0019 train_loss= 0.63643 train_acc= 0.83107 val_loss= 0.64283 val_acc= 0.82847 time= 0.16800
Epoch: 0020 train_loss= 0.60910 train_acc= 0.84910 val_loss= 0.61788 val_acc= 0.83577 time= 0.12600
Epoch: 0021 train_loss= 0.58182 train_acc= 0.86004 val_loss= 0.59270 val_acc= 0.83942 time= 0.12600
Epoch: 0022 train_loss= 0.55632 train_acc= 0.86530 val_loss= 0.56725 val_acc= 0.84672 time= 0.12400
Epoch: 0023 train_loss= 0.52769 train_acc= 0.86996 val_loss= 0.54243 val_acc= 0.85401 time= 0.12300
Epoch: 0024 train_loss= 0.50186 train_acc= 0.87401 val_loss= 0.51921 val_acc= 0.85584 time= 0.12300
Epoch: 0025 train_loss= 0.47839 train_acc= 0.87705 val_loss= 0.49815 val_acc= 0.85766 time= 0.12607
Epoch: 0026 train_loss= 0.45333 train_acc= 0.88070 val_loss= 0.47928 val_acc= 0.86131 time= 0.12600
Epoch: 0027 train_loss= 0.43318 train_acc= 0.88637 val_loss= 0.46222 val_acc= 0.87226 time= 0.14901
Epoch: 0028 train_loss= 0.41371 train_acc= 0.89184 val_loss= 0.44644 val_acc= 0.88686 time= 0.12314
Epoch: 0029 train_loss= 0.39594 train_acc= 0.89589 val_loss= 0.43139 val_acc= 0.88869 time= 0.12299
Epoch: 0030 train_loss= 0.37861 train_acc= 0.90176 val_loss= 0.41665 val_acc= 0.89234 time= 0.12716
Epoch: 0031 train_loss= 0.36203 train_acc= 0.90966 val_loss= 0.40206 val_acc= 0.89599 time= 0.12599
Epoch: 0032 train_loss= 0.34708 train_acc= 0.91189 val_loss= 0.38763 val_acc= 0.89781 time= 0.12300
Epoch: 0033 train_loss= 0.33086 train_acc= 0.91635 val_loss= 0.37348 val_acc= 0.89964 time= 0.12200
Epoch: 0034 train_loss= 0.31463 train_acc= 0.91878 val_loss= 0.35979 val_acc= 0.90511 time= 0.12601
Epoch: 0035 train_loss= 0.30149 train_acc= 0.92222 val_loss= 0.34663 val_acc= 0.90876 time= 0.15899
Epoch: 0036 train_loss= 0.28834 train_acc= 0.92546 val_loss= 0.33409 val_acc= 0.90876 time= 0.12301
Epoch: 0037 train_loss= 0.27398 train_acc= 0.93073 val_loss= 0.32213 val_acc= 0.91423 time= 0.12304
Epoch: 0038 train_loss= 0.26207 train_acc= 0.93539 val_loss= 0.31067 val_acc= 0.91606 time= 0.12206
Epoch: 0039 train_loss= 0.24924 train_acc= 0.94085 val_loss= 0.29971 val_acc= 0.92336 time= 0.12597
Epoch: 0040 train_loss= 0.23772 train_acc= 0.94592 val_loss= 0.28920 val_acc= 0.93248 time= 0.12600
Epoch: 0041 train_loss= 0.22697 train_acc= 0.95037 val_loss= 0.27917 val_acc= 0.93248 time= 0.12600
Epoch: 0042 train_loss= 0.21612 train_acc= 0.95220 val_loss= 0.26962 val_acc= 0.93066 time= 0.12803
Epoch: 0043 train_loss= 0.20462 train_acc= 0.95524 val_loss= 0.26057 val_acc= 0.92883 time= 0.15997
Epoch: 0044 train_loss= 0.19306 train_acc= 0.95868 val_loss= 0.25184 val_acc= 0.93066 time= 0.12400
Epoch: 0045 train_loss= 0.18461 train_acc= 0.96070 val_loss= 0.24349 val_acc= 0.93431 time= 0.12303
Epoch: 0046 train_loss= 0.17497 train_acc= 0.96111 val_loss= 0.23551 val_acc= 0.93613 time= 0.12397
Epoch: 0047 train_loss= 0.16632 train_acc= 0.96293 val_loss= 0.22788 val_acc= 0.93978 time= 0.12503
Epoch: 0048 train_loss= 0.15665 train_acc= 0.96395 val_loss= 0.22062 val_acc= 0.94161 time= 0.12397
Epoch: 0049 train_loss= 0.15000 train_acc= 0.96759 val_loss= 0.21383 val_acc= 0.94526 time= 0.12606
Epoch: 0050 train_loss= 0.14161 train_acc= 0.96800 val_loss= 0.20747 val_acc= 0.94526 time= 0.17511
Epoch: 0051 train_loss= 0.13432 train_acc= 0.96941 val_loss= 0.20151 val_acc= 0.94708 time= 0.12603
Epoch: 0052 train_loss= 0.12755 train_acc= 0.96941 val_loss= 0.19585 val_acc= 0.94708 time= 0.12300
Epoch: 0053 train_loss= 0.12131 train_acc= 0.97266 val_loss= 0.19060 val_acc= 0.94708 time= 0.12200
Epoch: 0054 train_loss= 0.11524 train_acc= 0.97306 val_loss= 0.18582 val_acc= 0.94708 time= 0.12297
Epoch: 0055 train_loss= 0.10999 train_acc= 0.97488 val_loss= 0.18142 val_acc= 0.94708 time= 0.12400
Epoch: 0056 train_loss= 0.10517 train_acc= 0.97569 val_loss= 0.17744 val_acc= 0.94343 time= 0.12240
Epoch: 0057 train_loss= 0.09992 train_acc= 0.97853 val_loss= 0.17379 val_acc= 0.94343 time= 0.12242
Epoch: 0058 train_loss= 0.09483 train_acc= 0.97873 val_loss= 0.17038 val_acc= 0.94343 time= 0.14801
Epoch: 0059 train_loss= 0.09131 train_acc= 0.98015 val_loss= 0.16736 val_acc= 0.94343 time= 0.12899
Epoch: 0060 train_loss= 0.08657 train_acc= 0.98278 val_loss= 0.16457 val_acc= 0.94708 time= 0.12700
Epoch: 0061 train_loss= 0.08334 train_acc= 0.98238 val_loss= 0.16197 val_acc= 0.94708 time= 0.12600
Epoch: 0062 train_loss= 0.07980 train_acc= 0.98339 val_loss= 0.15971 val_acc= 0.95255 time= 0.12300
Epoch: 0063 train_loss= 0.07663 train_acc= 0.98440 val_loss= 0.15765 val_acc= 0.95255 time= 0.12400
Epoch: 0064 train_loss= 0.07349 train_acc= 0.98461 val_loss= 0.15578 val_acc= 0.95255 time= 0.12200
Epoch: 0065 train_loss= 0.07036 train_acc= 0.98461 val_loss= 0.15396 val_acc= 0.95255 time= 0.12345
Epoch: 0066 train_loss= 0.06708 train_acc= 0.98602 val_loss= 0.15228 val_acc= 0.95255 time= 0.16101
Epoch: 0067 train_loss= 0.06467 train_acc= 0.98582 val_loss= 0.15077 val_acc= 0.95255 time= 0.12501
Epoch: 0068 train_loss= 0.06247 train_acc= 0.98663 val_loss= 0.14952 val_acc= 0.95255 time= 0.12598
Epoch: 0069 train_loss= 0.05980 train_acc= 0.98663 val_loss= 0.14862 val_acc= 0.95255 time= 0.12535
Epoch: 0070 train_loss= 0.05836 train_acc= 0.98764 val_loss= 0.14807 val_acc= 0.95255 time= 0.12600
Epoch: 0071 train_loss= 0.05562 train_acc= 0.98744 val_loss= 0.14746 val_acc= 0.95438 time= 0.12500
Epoch: 0072 train_loss= 0.05393 train_acc= 0.98866 val_loss= 0.14657 val_acc= 0.95803 time= 0.12310
Epoch: 0073 train_loss= 0.05256 train_acc= 0.98845 val_loss= 0.14552 val_acc= 0.95620 time= 0.13200
Epoch: 0074 train_loss= 0.05041 train_acc= 0.98886 val_loss= 0.14476 val_acc= 0.95620 time= 0.15197
Epoch: 0075 train_loss= 0.04878 train_acc= 0.99048 val_loss= 0.14424 val_acc= 0.95438 time= 0.12304
Epoch: 0076 train_loss= 0.04647 train_acc= 0.99007 val_loss= 0.14380 val_acc= 0.95438 time= 0.12596
Epoch: 0077 train_loss= 0.04556 train_acc= 0.98987 val_loss= 0.14340 val_acc= 0.95620 time= 0.12404
Epoch: 0078 train_loss= 0.04390 train_acc= 0.99089 val_loss= 0.14325 val_acc= 0.95620 time= 0.12196
Epoch: 0079 train_loss= 0.04221 train_acc= 0.99149 val_loss= 0.14338 val_acc= 0.95803 time= 0.12800
Epoch: 0080 train_loss= 0.04083 train_acc= 0.99190 val_loss= 0.14344 val_acc= 0.95803 time= 0.12587
Epoch: 0081 train_loss= 0.03937 train_acc= 0.99210 val_loss= 0.14342 val_acc= 0.95803 time= 0.16800
Epoch: 0082 train_loss= 0.03835 train_acc= 0.99271 val_loss= 0.14335 val_acc= 0.95803 time= 0.12400
Epoch: 0083 train_loss= 0.03652 train_acc= 0.99291 val_loss= 0.14322 val_acc= 0.95255 time= 0.12410
Epoch: 0084 train_loss= 0.03546 train_acc= 0.99352 val_loss= 0.14325 val_acc= 0.95255 time= 0.12701
Epoch: 0085 train_loss= 0.03517 train_acc= 0.99352 val_loss= 0.14349 val_acc= 0.95255 time= 0.12330
Early stopping...
Optimization Finished!
Test set results: cost= 0.10719 accuracy= 0.97305 time= 0.05501
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9370    0.9835    0.9597       121
           1     0.9125    0.9733    0.9419        75
           2     0.9853    0.9917    0.9885      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9630    0.7222    0.8254        36
           5     0.9452    0.8519    0.8961        81
           6     0.8710    0.9310    0.9000        87
           7     0.9855    0.9741    0.9798       696

    accuracy                         0.9730      2189
   macro avg     0.9386    0.9285    0.9305      2189
weighted avg     0.9735    0.9730    0.9727      2189

Macro average Test Precision, Recall and F1-Score...
(0.9385651480817074, 0.9284675807841496, 0.9304698164006495, None)
Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)
embeddings:
7688 5485 2189
[[ 0.21871012  0.11875132  0.16324352 ... -0.0045891   0.34614685
   0.07783273]
 [ 0.07051546  0.2515798   0.06316029 ...  0.03811034  0.16807757
   0.17705384]
 [ 0.21707961  0.63045406  0.18586099 ...  0.03445297  0.02107979
  -0.01579903]
 ...
 [ 0.2599687   0.5338103   0.26072973 ... -0.04014951  0.10381925
   0.08047614]
 [ 0.05545795  0.2894592   0.06622151 ...  0.1395201   0.18605264
   0.23152852]
 [ 0.20073757  0.437064    0.18871585 ... -0.01520302  0.00541693
   0.01763521]]

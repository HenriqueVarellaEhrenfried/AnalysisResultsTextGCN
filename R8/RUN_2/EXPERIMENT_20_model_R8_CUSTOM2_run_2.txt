(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07930 train_acc= 0.36054 val_loss= 1.58217 val_acc= 0.66058 time= 0.41103
Epoch: 0002 train_loss= 1.55594 train_acc= 0.68645 val_loss= 1.27322 val_acc= 0.65511 time= 0.12997
Epoch: 0003 train_loss= 1.22522 train_acc= 0.67531 val_loss= 1.04488 val_acc= 0.63869 time= 0.12687
Epoch: 0004 train_loss= 0.99718 train_acc= 0.65971 val_loss= 0.80336 val_acc= 0.75547 time= 0.12304
Epoch: 0005 train_loss= 0.76410 train_acc= 0.77618 val_loss= 0.69939 val_acc= 0.75365 time= 0.12309
Epoch: 0006 train_loss= 0.65624 train_acc= 0.77334 val_loss= 0.64977 val_acc= 0.75730 time= 0.12704
Epoch: 0007 train_loss= 0.60303 train_acc= 0.78104 val_loss= 0.61328 val_acc= 0.77007 time= 0.15200
Epoch: 0008 train_loss= 0.55608 train_acc= 0.80008 val_loss= 0.57930 val_acc= 0.79380 time= 0.12400
Epoch: 0009 train_loss= 0.51405 train_acc= 0.82682 val_loss= 0.54744 val_acc= 0.81569 time= 0.12704
Epoch: 0010 train_loss= 0.47737 train_acc= 0.84140 val_loss= 0.51621 val_acc= 0.83394 time= 0.12396
Epoch: 0011 train_loss= 0.44180 train_acc= 0.85031 val_loss= 0.48367 val_acc= 0.84672 time= 0.12300
Epoch: 0012 train_loss= 0.40410 train_acc= 0.87259 val_loss= 0.44922 val_acc= 0.86496 time= 0.12291
Epoch: 0013 train_loss= 0.36188 train_acc= 0.88657 val_loss= 0.41644 val_acc= 0.88139 time= 0.12300
Epoch: 0014 train_loss= 0.33274 train_acc= 0.89953 val_loss= 0.38878 val_acc= 0.88686 time= 0.16000
Epoch: 0015 train_loss= 0.29941 train_acc= 0.91128 val_loss= 0.36717 val_acc= 0.89234 time= 0.12596
Epoch: 0016 train_loss= 0.27422 train_acc= 0.91857 val_loss= 0.34913 val_acc= 0.90146 time= 0.12403
Epoch: 0017 train_loss= 0.24462 train_acc= 0.93093 val_loss= 0.33142 val_acc= 0.91788 time= 0.12487
Epoch: 0018 train_loss= 0.21935 train_acc= 0.94045 val_loss= 0.31554 val_acc= 0.93066 time= 0.12414
Epoch: 0019 train_loss= 0.19299 train_acc= 0.94774 val_loss= 0.30046 val_acc= 0.93431 time= 0.12197
Epoch: 0020 train_loss= 0.16927 train_acc= 0.95341 val_loss= 0.28481 val_acc= 0.93613 time= 0.12411
Epoch: 0021 train_loss= 0.15032 train_acc= 0.95706 val_loss= 0.27312 val_acc= 0.93796 time= 0.12400
Epoch: 0022 train_loss= 0.13026 train_acc= 0.96192 val_loss= 0.26368 val_acc= 0.93613 time= 0.16100
Epoch: 0023 train_loss= 0.11755 train_acc= 0.96617 val_loss= 0.25502 val_acc= 0.93978 time= 0.12297
Epoch: 0024 train_loss= 0.10692 train_acc= 0.96658 val_loss= 0.24957 val_acc= 0.94161 time= 0.12703
Epoch: 0025 train_loss= 0.09673 train_acc= 0.96982 val_loss= 0.24832 val_acc= 0.93978 time= 0.12400
Epoch: 0026 train_loss= 0.08330 train_acc= 0.97630 val_loss= 0.24725 val_acc= 0.93613 time= 0.12401
Epoch: 0027 train_loss= 0.07791 train_acc= 0.97367 val_loss= 0.24621 val_acc= 0.93978 time= 0.12500
Epoch: 0028 train_loss= 0.07126 train_acc= 0.97509 val_loss= 0.24459 val_acc= 0.94708 time= 0.12305
Epoch: 0029 train_loss= 0.06567 train_acc= 0.97630 val_loss= 0.24385 val_acc= 0.94526 time= 0.12196
Epoch: 0030 train_loss= 0.05782 train_acc= 0.98359 val_loss= 0.24434 val_acc= 0.94891 time= 0.15000
Epoch: 0031 train_loss= 0.04877 train_acc= 0.98582 val_loss= 0.24495 val_acc= 0.95073 time= 0.12300
Epoch: 0032 train_loss= 0.04500 train_acc= 0.98663 val_loss= 0.24510 val_acc= 0.95255 time= 0.12368
Early stopping...
Optimization Finished!
Test set results: cost= 0.15416 accuracy= 0.96300 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9421    0.9421    0.9421       121
           1     0.8861    0.9333    0.9091        75
           2     0.9808    0.9917    0.9862      1083
           3     0.3333    0.5000    0.4000        10
           4     0.9259    0.6944    0.7937        36
           5     0.9545    0.7778    0.8571        81
           6     0.8300    0.9540    0.8877        87
           7     0.9825    0.9684    0.9754       696

    accuracy                         0.9630      2189
   macro avg     0.8544    0.8452    0.8439      2189
weighted avg     0.9651    0.9630    0.9632      2189

Macro average Test Precision, Recall and F1-Score...
(0.8544198287425422, 0.8452259824602629, 0.8439197155302621, None)
Micro average Test Precision, Recall and F1-Score...
(0.962996802192782, 0.962996802192782, 0.962996802192782, None)
embeddings:
7688 5485 2189
[[ 0.25404873  0.83290374  0.35179818 ... -0.32443234 -0.3205204
   0.11155287]
 [-0.07230818  0.37425637  0.2126162  ... -0.29026392 -0.30771795
  -0.17156261]
 [ 0.10959466  0.13247518  0.11102219 ... -0.3335814  -0.36409098
  -0.04320653]
 ...
 [ 0.20703104  0.39941463  0.21967141 ... -0.3523322  -0.37966058
   0.1700513 ]
 [-0.19991356  0.46303535  0.19769464 ... -0.42131045 -0.4437889
  -0.32349512]
 [ 0.20368235  0.09736329  0.02024237 ... -0.2734721  -0.27373326
   0.07652482]]

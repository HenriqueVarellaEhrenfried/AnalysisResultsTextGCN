(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07964 train_acc= 0.03646 val_loss= 1.99648 val_acc= 0.75182 time= 0.51137
Epoch: 0002 train_loss= 1.99341 train_acc= 0.77213 val_loss= 1.83596 val_acc= 0.73540 time= 0.19400
Epoch: 0003 train_loss= 1.82679 train_acc= 0.74985 val_loss= 1.62343 val_acc= 0.67701 time= 0.19900
Epoch: 0004 train_loss= 1.60394 train_acc= 0.69901 val_loss= 1.42297 val_acc= 0.64051 time= 0.19203
Epoch: 0005 train_loss= 1.39134 train_acc= 0.65708 val_loss= 1.28697 val_acc= 0.64234 time= 0.19301
Epoch: 0006 train_loss= 1.24981 train_acc= 0.67490 val_loss= 1.20149 val_acc= 0.66241 time= 0.19296
Epoch: 0007 train_loss= 1.15802 train_acc= 0.68058 val_loss= 1.12504 val_acc= 0.70438 time= 0.19389
Epoch: 0008 train_loss= 1.07172 train_acc= 0.72210 val_loss= 1.03776 val_acc= 0.74635 time= 0.20200
Epoch: 0009 train_loss= 0.99218 train_acc= 0.76544 val_loss= 0.94394 val_acc= 0.76095 time= 0.19100
Epoch: 0010 train_loss= 0.90021 train_acc= 0.78509 val_loss= 0.85453 val_acc= 0.75730 time= 0.19303
Epoch: 0011 train_loss= 0.81113 train_acc= 0.78813 val_loss= 0.77799 val_acc= 0.75730 time= 0.19400
Epoch: 0012 train_loss= 0.73957 train_acc= 0.78550 val_loss= 0.71820 val_acc= 0.75730 time= 0.18982
Epoch: 0013 train_loss= 0.67946 train_acc= 0.78529 val_loss= 0.67389 val_acc= 0.76825 time= 0.21700
Epoch: 0014 train_loss= 0.63669 train_acc= 0.79400 val_loss= 0.64031 val_acc= 0.78467 time= 0.19600
Epoch: 0015 train_loss= 0.60231 train_acc= 0.81426 val_loss= 0.61226 val_acc= 0.81934 time= 0.19497
Epoch: 0016 train_loss= 0.57502 train_acc= 0.84079 val_loss= 0.58596 val_acc= 0.83029 time= 0.19303
Epoch: 0017 train_loss= 0.54574 train_acc= 0.85700 val_loss= 0.55987 val_acc= 0.84307 time= 0.19297
Epoch: 0018 train_loss= 0.51639 train_acc= 0.86632 val_loss= 0.53391 val_acc= 0.84854 time= 0.22272
Epoch: 0019 train_loss= 0.48640 train_acc= 0.87422 val_loss= 0.50873 val_acc= 0.85766 time= 0.19400
Epoch: 0020 train_loss= 0.45926 train_acc= 0.87624 val_loss= 0.48493 val_acc= 0.85949 time= 0.19200
Epoch: 0021 train_loss= 0.43190 train_acc= 0.88272 val_loss= 0.46285 val_acc= 0.86861 time= 0.19500
Epoch: 0022 train_loss= 0.40546 train_acc= 0.88819 val_loss= 0.44237 val_acc= 0.87956 time= 0.19103
Epoch: 0023 train_loss= 0.38425 train_acc= 0.89791 val_loss= 0.42306 val_acc= 0.89416 time= 0.22429
Epoch: 0024 train_loss= 0.36210 train_acc= 0.90440 val_loss= 0.40454 val_acc= 0.90146 time= 0.19512
Epoch: 0025 train_loss= 0.34148 train_acc= 0.91290 val_loss= 0.38646 val_acc= 0.90876 time= 0.19300
Epoch: 0026 train_loss= 0.32378 train_acc= 0.91857 val_loss= 0.36873 val_acc= 0.91058 time= 0.19296
Epoch: 0027 train_loss= 0.30287 train_acc= 0.92425 val_loss= 0.35167 val_acc= 0.91241 time= 0.19204
Epoch: 0028 train_loss= 0.28571 train_acc= 0.93174 val_loss= 0.33549 val_acc= 0.91606 time= 0.22310
Epoch: 0029 train_loss= 0.26430 train_acc= 0.93741 val_loss= 0.32026 val_acc= 0.92153 time= 0.19397
Epoch: 0030 train_loss= 0.24967 train_acc= 0.94146 val_loss= 0.30601 val_acc= 0.92336 time= 0.19503
Epoch: 0031 train_loss= 0.23321 train_acc= 0.94491 val_loss= 0.29250 val_acc= 0.92883 time= 0.19297
Epoch: 0032 train_loss= 0.21864 train_acc= 0.94815 val_loss= 0.27987 val_acc= 0.92883 time= 0.19000
Epoch: 0033 train_loss= 0.20175 train_acc= 0.95260 val_loss= 0.26808 val_acc= 0.92883 time= 0.22103
Epoch: 0034 train_loss= 0.18976 train_acc= 0.95503 val_loss= 0.25698 val_acc= 0.92883 time= 0.19497
Epoch: 0035 train_loss= 0.17759 train_acc= 0.95787 val_loss= 0.24674 val_acc= 0.92883 time= 0.19300
Epoch: 0036 train_loss= 0.16533 train_acc= 0.96091 val_loss= 0.23709 val_acc= 0.93431 time= 0.19400
Epoch: 0037 train_loss= 0.15208 train_acc= 0.96273 val_loss= 0.22820 val_acc= 0.93796 time= 0.19104
Epoch: 0038 train_loss= 0.14553 train_acc= 0.96395 val_loss= 0.21989 val_acc= 0.93796 time= 0.22196
Epoch: 0039 train_loss= 0.13372 train_acc= 0.96719 val_loss= 0.21223 val_acc= 0.93978 time= 0.19114
Epoch: 0040 train_loss= 0.12390 train_acc= 0.96941 val_loss= 0.20541 val_acc= 0.94161 time= 0.19403
Epoch: 0041 train_loss= 0.11723 train_acc= 0.96901 val_loss= 0.19923 val_acc= 0.94343 time= 0.19400
Epoch: 0042 train_loss= 0.10909 train_acc= 0.97083 val_loss= 0.19371 val_acc= 0.94891 time= 0.19201
Epoch: 0043 train_loss= 0.10112 train_acc= 0.97549 val_loss= 0.18889 val_acc= 0.95255 time= 0.22000
Epoch: 0044 train_loss= 0.09811 train_acc= 0.97326 val_loss= 0.18480 val_acc= 0.95255 time= 0.19299
Epoch: 0045 train_loss= 0.09021 train_acc= 0.97569 val_loss= 0.18143 val_acc= 0.94891 time= 0.19300
Epoch: 0046 train_loss= 0.08570 train_acc= 0.97731 val_loss= 0.17890 val_acc= 0.94708 time= 0.19301
Epoch: 0047 train_loss= 0.07938 train_acc= 0.98076 val_loss= 0.17687 val_acc= 0.95073 time= 0.19200
Epoch: 0048 train_loss= 0.07569 train_acc= 0.98035 val_loss= 0.17466 val_acc= 0.94891 time= 0.20899
Epoch: 0049 train_loss= 0.07234 train_acc= 0.98035 val_loss= 0.17208 val_acc= 0.94891 time= 0.19300
Epoch: 0050 train_loss= 0.06608 train_acc= 0.98440 val_loss= 0.16951 val_acc= 0.94891 time= 0.19297
Epoch: 0051 train_loss= 0.06407 train_acc= 0.98521 val_loss= 0.16701 val_acc= 0.95073 time= 0.19596
Epoch: 0052 train_loss= 0.06180 train_acc= 0.98420 val_loss= 0.16532 val_acc= 0.95255 time= 0.19201
Epoch: 0053 train_loss= 0.05768 train_acc= 0.98643 val_loss= 0.16454 val_acc= 0.95255 time= 0.19699
Epoch: 0054 train_loss= 0.05444 train_acc= 0.98623 val_loss= 0.16419 val_acc= 0.95438 time= 0.19708
Epoch: 0055 train_loss= 0.05040 train_acc= 0.98886 val_loss= 0.16410 val_acc= 0.95073 time= 0.19103
Epoch: 0056 train_loss= 0.05086 train_acc= 0.98744 val_loss= 0.16347 val_acc= 0.95073 time= 0.19297
Epoch: 0057 train_loss= 0.04704 train_acc= 0.98805 val_loss= 0.16234 val_acc= 0.95438 time= 0.19400
Epoch: 0058 train_loss= 0.04464 train_acc= 0.98967 val_loss= 0.16080 val_acc= 0.95255 time= 0.19400
Epoch: 0059 train_loss= 0.04204 train_acc= 0.98906 val_loss= 0.15940 val_acc= 0.95255 time= 0.21520
Epoch: 0060 train_loss= 0.04077 train_acc= 0.99007 val_loss= 0.15819 val_acc= 0.95438 time= 0.19301
Epoch: 0061 train_loss= 0.03943 train_acc= 0.98825 val_loss= 0.15742 val_acc= 0.95255 time= 0.19196
Epoch: 0062 train_loss= 0.03767 train_acc= 0.99149 val_loss= 0.15708 val_acc= 0.95255 time= 0.19603
Epoch: 0063 train_loss= 0.03519 train_acc= 0.99170 val_loss= 0.15741 val_acc= 0.95620 time= 0.19300
Epoch: 0064 train_loss= 0.03489 train_acc= 0.99190 val_loss= 0.15814 val_acc= 0.95985 time= 0.22201
Epoch: 0065 train_loss= 0.03311 train_acc= 0.99210 val_loss= 0.15862 val_acc= 0.95985 time= 0.19299
Epoch: 0066 train_loss= 0.03097 train_acc= 0.99291 val_loss= 0.15866 val_acc= 0.95620 time= 0.19300
Epoch: 0067 train_loss= 0.03011 train_acc= 0.99271 val_loss= 0.15782 val_acc= 0.95620 time= 0.19197
Epoch: 0068 train_loss= 0.02888 train_acc= 0.99311 val_loss= 0.15686 val_acc= 0.95803 time= 0.19303
Epoch: 0069 train_loss= 0.02826 train_acc= 0.99210 val_loss= 0.15625 val_acc= 0.95803 time= 0.22100
Epoch: 0070 train_loss= 0.02694 train_acc= 0.99352 val_loss= 0.15619 val_acc= 0.95803 time= 0.19100
Epoch: 0071 train_loss= 0.02521 train_acc= 0.99413 val_loss= 0.15714 val_acc= 0.95620 time= 0.19300
Epoch: 0072 train_loss= 0.02468 train_acc= 0.99392 val_loss= 0.15755 val_acc= 0.95438 time= 0.19424
Early stopping...
Optimization Finished!
Test set results: cost= 0.11347 accuracy= 0.97031 time= 0.07904
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9370    0.9835    0.9597       121
           1     0.9125    0.9733    0.9419        75
           2     0.9808    0.9917    0.9862      1083
           3     0.9091    1.0000    0.9524        10
           4     1.0000    0.7222    0.8387        36
           5     0.8974    0.8642    0.8805        81
           6     0.8876    0.9080    0.8977        87
           7     0.9854    0.9670    0.9761       696

    accuracy                         0.9703      2189
   macro avg     0.9387    0.9262    0.9292      2189
weighted avg     0.9707    0.9703    0.9700      2189

Macro average Test Precision, Recall and F1-Score...
(0.9387319699194492, 0.9262392389365548, 0.9291536826734753, None)
Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)
embeddings:
7688 5485 2189
[[ 0.1923697   0.2250793   0.21655071 ... -0.05905973  0.08674404
   0.2550379 ]
 [ 0.16771735  0.08361682  0.13480048 ... -0.05304223  0.01551495
   0.10346954]
 [ 0.27182445  0.00654925 -0.01624086 ... -0.05942472  0.1540846
   0.04438556]
 ...
 [ 0.23112696  0.04060635  0.06590384 ... -0.0617208   0.20912719
   0.09551952]
 [ 0.12630671  0.08302003  0.17393698 ... -0.07681003 -0.0116848
   0.14606269]
 [ 0.16139054  0.02146718  0.02910521 ... -0.04611319  0.15731335
   0.02577917]]

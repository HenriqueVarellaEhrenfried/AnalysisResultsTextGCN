(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07939 train_acc= 0.15151 val_loss= 2.03641 val_acc= 0.71168 time= 0.38604
Epoch: 0002 train_loss= 2.03627 train_acc= 0.71258 val_loss= 1.95669 val_acc= 0.72080 time= 0.12996
Epoch: 0003 train_loss= 1.95480 train_acc= 0.73000 val_loss= 1.84743 val_acc= 0.72445 time= 0.12600
Epoch: 0004 train_loss= 1.84118 train_acc= 0.72433 val_loss= 1.72022 val_acc= 0.72445 time= 0.12566
Epoch: 0005 train_loss= 1.71239 train_acc= 0.72615 val_loss= 1.59221 val_acc= 0.72628 time= 0.15100
Epoch: 0006 train_loss= 1.57229 train_acc= 0.72757 val_loss= 1.48148 val_acc= 0.73905 time= 0.13500
Epoch: 0007 train_loss= 1.44972 train_acc= 0.75430 val_loss= 1.39514 val_acc= 0.75547 time= 0.12300
Epoch: 0008 train_loss= 1.37317 train_acc= 0.76544 val_loss= 1.32714 val_acc= 0.75912 time= 0.12313
Epoch: 0009 train_loss= 1.30187 train_acc= 0.74357 val_loss= 1.26795 val_acc= 0.70438 time= 0.12307
Epoch: 0010 train_loss= 1.22102 train_acc= 0.71076 val_loss= 1.21024 val_acc= 0.64416 time= 0.12300
Epoch: 0011 train_loss= 1.16789 train_acc= 0.69475 val_loss= 1.15009 val_acc= 0.62956 time= 0.12300
Epoch: 0012 train_loss= 1.10720 train_acc= 0.67612 val_loss= 1.08556 val_acc= 0.65511 time= 0.12300
Epoch: 0013 train_loss= 1.03181 train_acc= 0.65850 val_loss= 1.01749 val_acc= 0.70803 time= 0.16900
Epoch: 0014 train_loss= 0.99208 train_acc= 0.71258 val_loss= 0.95055 val_acc= 0.74453 time= 0.12797
Epoch: 0015 train_loss= 0.89353 train_acc= 0.74965 val_loss= 0.88923 val_acc= 0.76095 time= 0.12503
Epoch: 0016 train_loss= 0.86461 train_acc= 0.76889 val_loss= 0.83715 val_acc= 0.75912 time= 0.12300
Epoch: 0017 train_loss= 0.80026 train_acc= 0.78003 val_loss= 0.79445 val_acc= 0.75730 time= 0.12399
Epoch: 0018 train_loss= 0.75613 train_acc= 0.78286 val_loss= 0.75941 val_acc= 0.76095 time= 0.12300
Epoch: 0019 train_loss= 0.73254 train_acc= 0.77456 val_loss= 0.72972 val_acc= 0.76277 time= 0.12399
Epoch: 0020 train_loss= 0.69913 train_acc= 0.78023 val_loss= 0.70364 val_acc= 0.76642 time= 0.12305
Epoch: 0021 train_loss= 0.67628 train_acc= 0.78347 val_loss= 0.67926 val_acc= 0.77555 time= 0.14900
Epoch: 0022 train_loss= 0.66012 train_acc= 0.79866 val_loss= 0.65569 val_acc= 0.79562 time= 0.12301
Epoch: 0023 train_loss= 0.63636 train_acc= 0.80920 val_loss= 0.63283 val_acc= 0.82117 time= 0.12697
Epoch: 0024 train_loss= 0.60681 train_acc= 0.83836 val_loss= 0.61070 val_acc= 0.83212 time= 0.12700
Epoch: 0025 train_loss= 0.59044 train_acc= 0.84140 val_loss= 0.58951 val_acc= 0.84307 time= 0.12600
Epoch: 0026 train_loss= 0.55585 train_acc= 0.85659 val_loss= 0.56916 val_acc= 0.84672 time= 0.12300
Epoch: 0027 train_loss= 0.52947 train_acc= 0.86530 val_loss= 0.54965 val_acc= 0.84854 time= 0.12303
Epoch: 0028 train_loss= 0.51249 train_acc= 0.86976 val_loss= 0.53107 val_acc= 0.85584 time= 0.12300
Epoch: 0029 train_loss= 0.49127 train_acc= 0.87685 val_loss= 0.51344 val_acc= 0.86131 time= 0.16802
Epoch: 0030 train_loss= 0.48459 train_acc= 0.87847 val_loss= 0.49677 val_acc= 0.86314 time= 0.12309
Epoch: 0031 train_loss= 0.45618 train_acc= 0.88110 val_loss= 0.48107 val_acc= 0.86679 time= 0.12500
Epoch: 0032 train_loss= 0.45276 train_acc= 0.87199 val_loss= 0.46623 val_acc= 0.86679 time= 0.12400
Epoch: 0033 train_loss= 0.42217 train_acc= 0.88515 val_loss= 0.45200 val_acc= 0.86679 time= 0.12300
Epoch: 0034 train_loss= 0.41500 train_acc= 0.88515 val_loss= 0.43833 val_acc= 0.87226 time= 0.12500
Epoch: 0035 train_loss= 0.39144 train_acc= 0.88920 val_loss= 0.42528 val_acc= 0.87409 time= 0.12600
Epoch: 0036 train_loss= 0.39607 train_acc= 0.88758 val_loss= 0.41232 val_acc= 0.87591 time= 0.14100
Epoch: 0037 train_loss= 0.36946 train_acc= 0.89650 val_loss= 0.39946 val_acc= 0.88504 time= 0.14500
Epoch: 0038 train_loss= 0.35015 train_acc= 0.89569 val_loss= 0.38695 val_acc= 0.88869 time= 0.12300
Epoch: 0039 train_loss= 0.33843 train_acc= 0.90359 val_loss= 0.37485 val_acc= 0.89416 time= 0.12297
Epoch: 0040 train_loss= 0.32056 train_acc= 0.91270 val_loss= 0.36326 val_acc= 0.90328 time= 0.12403
Epoch: 0041 train_loss= 0.31912 train_acc= 0.91027 val_loss= 0.35217 val_acc= 0.91241 time= 0.12300
Epoch: 0042 train_loss= 0.30340 train_acc= 0.92323 val_loss= 0.34173 val_acc= 0.91423 time= 0.12407
Epoch: 0043 train_loss= 0.29247 train_acc= 0.92283 val_loss= 0.33163 val_acc= 0.91423 time= 0.12397
Epoch: 0044 train_loss= 0.28052 train_acc= 0.92870 val_loss= 0.32212 val_acc= 0.92153 time= 0.17300
Epoch: 0045 train_loss= 0.29013 train_acc= 0.91938 val_loss= 0.31311 val_acc= 0.92336 time= 0.12500
Epoch: 0046 train_loss= 0.27131 train_acc= 0.93296 val_loss= 0.30508 val_acc= 0.91971 time= 0.12304
Epoch: 0047 train_loss= 0.26713 train_acc= 0.93154 val_loss= 0.29743 val_acc= 0.92518 time= 0.12304
Epoch: 0048 train_loss= 0.25852 train_acc= 0.92749 val_loss= 0.28971 val_acc= 0.92518 time= 0.12596
Epoch: 0049 train_loss= 0.22157 train_acc= 0.93964 val_loss= 0.28175 val_acc= 0.92883 time= 0.12404
Epoch: 0050 train_loss= 0.23360 train_acc= 0.93984 val_loss= 0.27362 val_acc= 0.92883 time= 0.12300
Epoch: 0051 train_loss= 0.23122 train_acc= 0.93741 val_loss= 0.26598 val_acc= 0.93248 time= 0.12296
Epoch: 0052 train_loss= 0.20906 train_acc= 0.94612 val_loss= 0.25892 val_acc= 0.93248 time= 0.15100
Epoch: 0053 train_loss= 0.20319 train_acc= 0.94329 val_loss= 0.25196 val_acc= 0.93248 time= 0.12400
Epoch: 0054 train_loss= 0.19449 train_acc= 0.94531 val_loss= 0.24532 val_acc= 0.93248 time= 0.12690
Epoch: 0055 train_loss= 0.19734 train_acc= 0.94916 val_loss= 0.23925 val_acc= 0.93248 time= 0.12700
Epoch: 0056 train_loss= 0.18259 train_acc= 0.94997 val_loss= 0.23330 val_acc= 0.93431 time= 0.12400
Epoch: 0057 train_loss= 0.18492 train_acc= 0.95037 val_loss= 0.22743 val_acc= 0.93248 time= 0.12500
Epoch: 0058 train_loss= 0.17637 train_acc= 0.95200 val_loss= 0.22214 val_acc= 0.93066 time= 0.12300
Epoch: 0059 train_loss= 0.17140 train_acc= 0.95139 val_loss= 0.21760 val_acc= 0.93431 time= 0.12400
Epoch: 0060 train_loss= 0.16866 train_acc= 0.94956 val_loss= 0.21322 val_acc= 0.93248 time= 0.16600
Epoch: 0061 train_loss= 0.15859 train_acc= 0.95908 val_loss= 0.20875 val_acc= 0.93431 time= 0.12400
Epoch: 0062 train_loss= 0.15490 train_acc= 0.95868 val_loss= 0.20464 val_acc= 0.93613 time= 0.12200
Epoch: 0063 train_loss= 0.15694 train_acc= 0.95402 val_loss= 0.20100 val_acc= 0.93796 time= 0.12500
Epoch: 0064 train_loss= 0.15846 train_acc= 0.95443 val_loss= 0.19745 val_acc= 0.93978 time= 0.12800
Epoch: 0065 train_loss= 0.15051 train_acc= 0.95787 val_loss= 0.19499 val_acc= 0.94161 time= 0.12804
Epoch: 0066 train_loss= 0.13304 train_acc= 0.96273 val_loss= 0.19375 val_acc= 0.94708 time= 0.12300
Epoch: 0067 train_loss= 0.13969 train_acc= 0.95868 val_loss= 0.19265 val_acc= 0.94891 time= 0.15895
Epoch: 0068 train_loss= 0.13467 train_acc= 0.96293 val_loss= 0.19092 val_acc= 0.94708 time= 0.12400
Epoch: 0069 train_loss= 0.12920 train_acc= 0.96638 val_loss= 0.18780 val_acc= 0.94708 time= 0.12300
Epoch: 0070 train_loss= 0.13515 train_acc= 0.96152 val_loss= 0.18315 val_acc= 0.94891 time= 0.12200
Epoch: 0071 train_loss= 0.11634 train_acc= 0.96820 val_loss= 0.17876 val_acc= 0.94708 time= 0.12305
Epoch: 0072 train_loss= 0.12275 train_acc= 0.96617 val_loss= 0.17477 val_acc= 0.95073 time= 0.12295
Epoch: 0073 train_loss= 0.11910 train_acc= 0.96698 val_loss= 0.17122 val_acc= 0.95073 time= 0.12700
Epoch: 0074 train_loss= 0.11865 train_acc= 0.96658 val_loss= 0.16850 val_acc= 0.94891 time= 0.12600
Epoch: 0075 train_loss= 0.11300 train_acc= 0.96820 val_loss= 0.16601 val_acc= 0.95073 time= 0.16800
Epoch: 0076 train_loss= 0.11608 train_acc= 0.96840 val_loss= 0.16436 val_acc= 0.95255 time= 0.12400
Epoch: 0077 train_loss= 0.10096 train_acc= 0.96962 val_loss= 0.16347 val_acc= 0.95073 time= 0.12400
Epoch: 0078 train_loss= 0.10631 train_acc= 0.96840 val_loss= 0.16341 val_acc= 0.95073 time= 0.12300
Epoch: 0079 train_loss= 0.10335 train_acc= 0.96962 val_loss= 0.16419 val_acc= 0.95073 time= 0.12400
Epoch: 0080 train_loss= 0.10168 train_acc= 0.97083 val_loss= 0.16512 val_acc= 0.95255 time= 0.12412
Epoch: 0081 train_loss= 0.09369 train_acc= 0.97124 val_loss= 0.16556 val_acc= 0.95255 time= 0.12300
Epoch: 0082 train_loss= 0.10478 train_acc= 0.96941 val_loss= 0.16409 val_acc= 0.95255 time= 0.12597
Epoch: 0083 train_loss= 0.09185 train_acc= 0.97488 val_loss= 0.16218 val_acc= 0.95073 time= 0.15492
Epoch: 0084 train_loss= 0.09154 train_acc= 0.97448 val_loss= 0.15946 val_acc= 0.95073 time= 0.12658
Epoch: 0085 train_loss= 0.09763 train_acc= 0.97286 val_loss= 0.15672 val_acc= 0.95073 time= 0.12600
Epoch: 0086 train_loss= 0.09186 train_acc= 0.97407 val_loss= 0.15410 val_acc= 0.95255 time= 0.12303
Epoch: 0087 train_loss= 0.09129 train_acc= 0.97590 val_loss= 0.15139 val_acc= 0.95438 time= 0.12300
Epoch: 0088 train_loss= 0.08707 train_acc= 0.97549 val_loss= 0.14963 val_acc= 0.95255 time= 0.12300
Epoch: 0089 train_loss= 0.08483 train_acc= 0.97650 val_loss= 0.14856 val_acc= 0.95255 time= 0.12338
Epoch: 0090 train_loss= 0.08774 train_acc= 0.97671 val_loss= 0.14733 val_acc= 0.95803 time= 0.12996
Epoch: 0091 train_loss= 0.08518 train_acc= 0.97731 val_loss= 0.14631 val_acc= 0.95620 time= 0.16200
Epoch: 0092 train_loss= 0.07715 train_acc= 0.97954 val_loss= 0.14592 val_acc= 0.95620 time= 0.12300
Epoch: 0093 train_loss= 0.07990 train_acc= 0.97833 val_loss= 0.14593 val_acc= 0.95620 time= 0.12400
Epoch: 0094 train_loss= 0.07662 train_acc= 0.97833 val_loss= 0.14576 val_acc= 0.95620 time= 0.12660
Epoch: 0095 train_loss= 0.07893 train_acc= 0.97792 val_loss= 0.14599 val_acc= 0.95438 time= 0.12600
Epoch: 0096 train_loss= 0.07066 train_acc= 0.98055 val_loss= 0.14646 val_acc= 0.95438 time= 0.12300
Epoch: 0097 train_loss= 0.07277 train_acc= 0.98015 val_loss= 0.14637 val_acc= 0.95438 time= 0.12300
Epoch: 0098 train_loss= 0.07210 train_acc= 0.98076 val_loss= 0.14640 val_acc= 0.95438 time= 0.17407
Epoch: 0099 train_loss= 0.06743 train_acc= 0.98096 val_loss= 0.14642 val_acc= 0.95438 time= 0.12200
Epoch: 0100 train_loss= 0.06632 train_acc= 0.98238 val_loss= 0.14680 val_acc= 0.95438 time= 0.12300
Early stopping...
Optimization Finished!
Test set results: cost= 0.10855 accuracy= 0.97213 time= 0.05703
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9508    0.9587    0.9547       121
           1     0.8409    0.9867    0.9080        75
           2     0.9862    0.9908    0.9885      1083
           3     1.0000    0.9000    0.9474        10
           4     0.9630    0.7222    0.8254        36
           5     0.9333    0.8642    0.8974        81
           6     0.9080    0.9080    0.9080        87
           7     0.9827    0.9784    0.9806       696

    accuracy                         0.9721      2189
   macro avg     0.9456    0.9136    0.9263      2189
weighted avg     0.9728    0.9721    0.9719      2189

Macro average Test Precision, Recall and F1-Score...
(0.9456210317907663, 0.9136280935294274, 0.9262501519632625, None)
Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)
embeddings:
7688 5485 2189
[[ 0.19789656  0.23600681  0.17949854 ...  0.29403934  0.05002359
   0.17582491]
 [ 0.06866873  0.20281205  0.04232956 ...  0.15999717  0.17487839
   0.0284369 ]
 [ 0.17765842 -0.05778508  0.05833988 ...  0.03746951  0.44583556
   0.03641287]
 ...
 [ 0.2562859   0.06702458  0.13683194 ...  0.05775624  0.3937477
   0.09863197]
 [ 0.04019861  0.26353905  0.02446789 ...  0.18979159  0.22491279
   0.03284492]
 [ 0.18791151  0.01330293  0.07801496 ... -0.00447337  0.33916035
   0.09332703]]

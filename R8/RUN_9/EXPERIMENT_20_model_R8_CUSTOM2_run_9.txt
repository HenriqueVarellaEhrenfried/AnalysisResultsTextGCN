(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07943 train_acc= 0.10431 val_loss= 1.58866 val_acc= 0.72993 time= 0.41391
Epoch: 0002 train_loss= 1.57066 train_acc= 0.73932 val_loss= 1.25608 val_acc= 0.71350 time= 0.13197
Epoch: 0003 train_loss= 1.20754 train_acc= 0.72534 val_loss= 1.04833 val_acc= 0.65876 time= 0.12700
Epoch: 0004 train_loss= 1.00251 train_acc= 0.67025 val_loss= 0.80687 val_acc= 0.75547 time= 0.12600
Epoch: 0005 train_loss= 0.76623 train_acc= 0.78226 val_loss= 0.69287 val_acc= 0.75365 time= 0.12311
Epoch: 0006 train_loss= 0.65145 train_acc= 0.77496 val_loss= 0.63925 val_acc= 0.75547 time= 0.12396
Epoch: 0007 train_loss= 0.58917 train_acc= 0.78590 val_loss= 0.60256 val_acc= 0.77007 time= 0.15395
Epoch: 0008 train_loss= 0.54447 train_acc= 0.80271 val_loss= 0.56828 val_acc= 0.79927 time= 0.12404
Epoch: 0009 train_loss= 0.50357 train_acc= 0.82844 val_loss= 0.53586 val_acc= 0.82117 time= 0.12496
Epoch: 0010 train_loss= 0.46483 train_acc= 0.85376 val_loss= 0.50438 val_acc= 0.84489 time= 0.12700
Epoch: 0011 train_loss= 0.43267 train_acc= 0.86672 val_loss= 0.47211 val_acc= 0.85949 time= 0.12700
Epoch: 0012 train_loss= 0.39101 train_acc= 0.88556 val_loss= 0.43991 val_acc= 0.86314 time= 0.12605
Epoch: 0013 train_loss= 0.34925 train_acc= 0.89305 val_loss= 0.40978 val_acc= 0.87956 time= 0.12300
Epoch: 0014 train_loss= 0.31837 train_acc= 0.90257 val_loss= 0.38140 val_acc= 0.89234 time= 0.12599
Epoch: 0015 train_loss= 0.28301 train_acc= 0.91817 val_loss= 0.35669 val_acc= 0.91241 time= 0.16301
Epoch: 0016 train_loss= 0.25344 train_acc= 0.93154 val_loss= 0.33693 val_acc= 0.92153 time= 0.12301
Epoch: 0017 train_loss= 0.22423 train_acc= 0.94450 val_loss= 0.31917 val_acc= 0.92336 time= 0.12306
Epoch: 0018 train_loss= 0.19565 train_acc= 0.94875 val_loss= 0.30108 val_acc= 0.93066 time= 0.12600
Epoch: 0019 train_loss= 0.16899 train_acc= 0.95179 val_loss= 0.28214 val_acc= 0.93431 time= 0.12307
Epoch: 0020 train_loss= 0.14976 train_acc= 0.95422 val_loss= 0.26385 val_acc= 0.93613 time= 0.12600
Epoch: 0021 train_loss= 0.13551 train_acc= 0.95949 val_loss= 0.25354 val_acc= 0.94161 time= 0.12700
Epoch: 0022 train_loss= 0.12003 train_acc= 0.96455 val_loss= 0.24781 val_acc= 0.93978 time= 0.17053
Epoch: 0023 train_loss= 0.10489 train_acc= 0.96800 val_loss= 0.24936 val_acc= 0.93613 time= 0.12200
Epoch: 0024 train_loss= 0.09281 train_acc= 0.96921 val_loss= 0.24991 val_acc= 0.93796 time= 0.12400
Epoch: 0025 train_loss= 0.08595 train_acc= 0.97205 val_loss= 0.24728 val_acc= 0.93796 time= 0.12300
Epoch: 0026 train_loss= 0.07615 train_acc= 0.97488 val_loss= 0.24044 val_acc= 0.94343 time= 0.12500
Epoch: 0027 train_loss= 0.06574 train_acc= 0.97752 val_loss= 0.23383 val_acc= 0.94526 time= 0.12411
Epoch: 0028 train_loss= 0.06278 train_acc= 0.97873 val_loss= 0.23093 val_acc= 0.94708 time= 0.12300
Epoch: 0029 train_loss= 0.05793 train_acc= 0.98197 val_loss= 0.23038 val_acc= 0.94708 time= 0.12400
Epoch: 0030 train_loss= 0.04897 train_acc= 0.98542 val_loss= 0.23248 val_acc= 0.95073 time= 0.15700
Epoch: 0031 train_loss= 0.04709 train_acc= 0.98562 val_loss= 0.23399 val_acc= 0.95255 time= 0.12523
Early stopping...
Optimization Finished!
Test set results: cost= 0.15844 accuracy= 0.96300 time= 0.05600
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9200    0.9504    0.9350       121
           1     0.8675    0.9600    0.9114        75
           2     0.9799    0.9917    0.9858      1083
           3     0.4545    0.5000    0.4762        10
           4     0.9200    0.6389    0.7541        36
           5     0.9189    0.8395    0.8774        81
           6     0.8681    0.9080    0.8876        87
           7     0.9825    0.9655    0.9739       696

    accuracy                         0.9630      2189
   macro avg     0.8639    0.8443    0.8502      2189
weighted avg     0.9635    0.9630    0.9626      2189

Macro average Test Precision, Recall and F1-Score...
(0.8639311585955577, 0.844257656744027, 0.8501733412198004, None)
Micro average Test Precision, Recall and F1-Score...
(0.962996802192782, 0.962996802192782, 0.962996802192782, None)
embeddings:
7688 5485 2189
[[-0.02291172  0.46608773  0.02600309 ...  0.13011685  0.05949409
   0.71918976]
 [-0.01335468 -0.00632657  0.33179337 ... -0.14242165 -0.1573077
   0.25725356]
 [ 0.5958236  -0.08744693  0.05251807 ...  0.18448718 -0.22669302
   0.07112736]
 ...
 [ 0.62934035  0.03600892  0.16599846 ...  0.28488904 -0.02979344
   0.39054593]
 [-0.12233787 -0.05710892  0.60604614 ... -0.30688825 -0.32899553
   0.29247025]
 [ 0.43414155 -0.01312566  0.1944413  ...  0.25036156 -0.02817523
   0.00713495]]

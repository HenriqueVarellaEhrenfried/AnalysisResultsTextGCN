(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07954 train_acc= 0.05712 val_loss= 1.65192 val_acc= 0.71168 time= 0.39459
Epoch: 0002 train_loss= 1.63887 train_acc= 0.71926 val_loss= 1.30006 val_acc= 0.75547 time= 0.15513
Epoch: 0003 train_loss= 1.24988 train_acc= 0.78043 val_loss= 1.10053 val_acc= 0.55292 time= 0.12483
Epoch: 0004 train_loss= 1.05077 train_acc= 0.58598 val_loss= 0.83603 val_acc= 0.76460 time= 0.12301
Epoch: 0005 train_loss= 0.79247 train_acc= 0.78509 val_loss= 0.70299 val_acc= 0.75182 time= 0.12297
Epoch: 0006 train_loss= 0.66389 train_acc= 0.77051 val_loss= 0.63776 val_acc= 0.76642 time= 0.12500
Epoch: 0007 train_loss= 0.59585 train_acc= 0.78570 val_loss= 0.58566 val_acc= 0.81569 time= 0.12604
Epoch: 0008 train_loss= 0.53665 train_acc= 0.83512 val_loss= 0.53876 val_acc= 0.84307 time= 0.12505
Epoch: 0009 train_loss= 0.48650 train_acc= 0.86348 val_loss= 0.49580 val_acc= 0.84854 time= 0.12700
Epoch: 0010 train_loss= 0.43857 train_acc= 0.87462 val_loss= 0.45839 val_acc= 0.86496 time= 0.16803
Epoch: 0011 train_loss= 0.39705 train_acc= 0.87948 val_loss= 0.42631 val_acc= 0.87591 time= 0.12397
Epoch: 0012 train_loss= 0.35692 train_acc= 0.88475 val_loss= 0.40037 val_acc= 0.87774 time= 0.12195
Epoch: 0013 train_loss= 0.32108 train_acc= 0.89244 val_loss= 0.37746 val_acc= 0.89234 time= 0.12300
Epoch: 0014 train_loss= 0.28880 train_acc= 0.90500 val_loss= 0.35405 val_acc= 0.91058 time= 0.12303
Epoch: 0015 train_loss= 0.26404 train_acc= 0.92364 val_loss= 0.33289 val_acc= 0.91971 time= 0.12200
Epoch: 0016 train_loss= 0.23248 train_acc= 0.93539 val_loss= 0.31260 val_acc= 0.92336 time= 0.12415
Epoch: 0017 train_loss= 0.20543 train_acc= 0.94531 val_loss= 0.29079 val_acc= 0.92883 time= 0.12797
Epoch: 0018 train_loss= 0.18520 train_acc= 0.94875 val_loss= 0.27224 val_acc= 0.93066 time= 0.16192
Epoch: 0019 train_loss= 0.16353 train_acc= 0.95686 val_loss= 0.26033 val_acc= 0.93248 time= 0.12400
Epoch: 0020 train_loss= 0.14471 train_acc= 0.95989 val_loss= 0.25098 val_acc= 0.93431 time= 0.12200
Epoch: 0021 train_loss= 0.12580 train_acc= 0.96293 val_loss= 0.24447 val_acc= 0.93978 time= 0.12200
Epoch: 0022 train_loss= 0.11522 train_acc= 0.96293 val_loss= 0.24156 val_acc= 0.93978 time= 0.12300
Epoch: 0023 train_loss= 0.09606 train_acc= 0.96921 val_loss= 0.23831 val_acc= 0.93796 time= 0.12200
Epoch: 0024 train_loss= 0.08770 train_acc= 0.97043 val_loss= 0.23294 val_acc= 0.94161 time= 0.12400
Epoch: 0025 train_loss= 0.08020 train_acc= 0.97043 val_loss= 0.22805 val_acc= 0.94708 time= 0.12100
Epoch: 0026 train_loss= 0.07389 train_acc= 0.97407 val_loss= 0.22556 val_acc= 0.94891 time= 0.17400
Epoch: 0027 train_loss= 0.06508 train_acc= 0.97833 val_loss= 0.22497 val_acc= 0.95073 time= 0.12600
Epoch: 0028 train_loss= 0.05629 train_acc= 0.98339 val_loss= 0.22701 val_acc= 0.94708 time= 0.12400
Epoch: 0029 train_loss= 0.05652 train_acc= 0.98137 val_loss= 0.22895 val_acc= 0.95073 time= 0.12284
Early stopping...
Optimization Finished!
Test set results: cost= 0.15233 accuracy= 0.96619 time= 0.05604
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9134    0.9587    0.9355       121
           1     0.8765    0.9467    0.9103        75
           2     0.9835    0.9908    0.9871      1083
           3     0.5455    0.6000    0.5714        10
           4     0.8929    0.6944    0.7812        36
           5     0.9315    0.8395    0.8831        81
           6     0.8723    0.9425    0.9061        87
           7     0.9854    0.9684    0.9768       696

    accuracy                         0.9662      2189
   macro avg     0.8751    0.8676    0.8689      2189
weighted avg     0.9667    0.9662    0.9660      2189

Macro average Test Precision, Recall and F1-Score...
(0.8751211864564199, 0.867622612473659, 0.8689431491522746, None)
Micro average Test Precision, Recall and F1-Score...
(0.9661946094106898, 0.9661946094106898, 0.9661946094106898, None)
embeddings:
7688 5485 2189
[[-0.02888899 -0.02428854  0.10868146 ...  0.07272384  0.21905547
   0.41284364]
 [ 0.19198152 -0.00497232 -0.15954219 ... -0.15516151 -0.13252647
   0.53305805]
 [-0.0744681   0.6329401   0.13986121 ...  0.08534231  0.03002781
   0.32027295]
 ...
 [-0.11247833  0.58270824  0.2779143  ...  0.23562744  0.18861824
   0.29877436]
 [ 0.27020323 -0.05482831 -0.29483923 ... -0.28799134 -0.28862762
   0.69585216]
 [-0.03084692  0.48057783  0.2069188  ...  0.15297206  0.17075562
   0.11936282]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07936 train_acc= 0.16690 val_loss= 1.60357 val_acc= 0.74088 time= 0.39195
Epoch: 0002 train_loss= 1.58182 train_acc= 0.74884 val_loss= 1.25024 val_acc= 0.72993 time= 0.13297
Epoch: 0003 train_loss= 1.20222 train_acc= 0.73547 val_loss= 1.01620 val_acc= 0.72628 time= 0.12903
Epoch: 0004 train_loss= 0.96467 train_acc= 0.73769 val_loss= 0.78420 val_acc= 0.75365 time= 0.12778
Epoch: 0005 train_loss= 0.74045 train_acc= 0.78246 val_loss= 0.66859 val_acc= 0.75912 time= 0.16000
Epoch: 0006 train_loss= 0.62657 train_acc= 0.77942 val_loss= 0.61570 val_acc= 0.78650 time= 0.12400
Epoch: 0007 train_loss= 0.56688 train_acc= 0.81406 val_loss= 0.57454 val_acc= 0.83394 time= 0.12303
Epoch: 0008 train_loss= 0.51536 train_acc= 0.85396 val_loss= 0.53154 val_acc= 0.84307 time= 0.12397
Epoch: 0009 train_loss= 0.46802 train_acc= 0.86692 val_loss= 0.49286 val_acc= 0.85766 time= 0.12300
Epoch: 0010 train_loss= 0.42330 train_acc= 0.87381 val_loss= 0.45987 val_acc= 0.86496 time= 0.12200
Epoch: 0011 train_loss= 0.38348 train_acc= 0.88090 val_loss= 0.42886 val_acc= 0.87956 time= 0.12600
Epoch: 0012 train_loss= 0.34557 train_acc= 0.89427 val_loss= 0.39903 val_acc= 0.88869 time= 0.12900
Epoch: 0013 train_loss= 0.30965 train_acc= 0.90986 val_loss= 0.37146 val_acc= 0.90693 time= 0.15900
Epoch: 0014 train_loss= 0.27631 train_acc= 0.92344 val_loss= 0.34833 val_acc= 0.91971 time= 0.12300
Epoch: 0015 train_loss= 0.24743 train_acc= 0.93680 val_loss= 0.32749 val_acc= 0.92153 time= 0.12405
Epoch: 0016 train_loss= 0.21720 train_acc= 0.94410 val_loss= 0.30915 val_acc= 0.92883 time= 0.12310
Epoch: 0017 train_loss= 0.19256 train_acc= 0.95240 val_loss= 0.29411 val_acc= 0.92701 time= 0.12404
Epoch: 0018 train_loss= 0.16571 train_acc= 0.95382 val_loss= 0.28325 val_acc= 0.93248 time= 0.12300
Epoch: 0019 train_loss= 0.15799 train_acc= 0.95665 val_loss= 0.26793 val_acc= 0.93613 time= 0.12249
Epoch: 0020 train_loss= 0.13510 train_acc= 0.96516 val_loss= 0.25529 val_acc= 0.93978 time= 0.16496
Epoch: 0021 train_loss= 0.12110 train_acc= 0.96354 val_loss= 0.25101 val_acc= 0.93796 time= 0.12703
Epoch: 0022 train_loss= 0.10593 train_acc= 0.96820 val_loss= 0.24849 val_acc= 0.93978 time= 0.12401
Epoch: 0023 train_loss= 0.09206 train_acc= 0.97164 val_loss= 0.24633 val_acc= 0.94161 time= 0.12300
Epoch: 0024 train_loss= 0.09080 train_acc= 0.96881 val_loss= 0.24479 val_acc= 0.94343 time= 0.12300
Epoch: 0025 train_loss= 0.07582 train_acc= 0.97509 val_loss= 0.24288 val_acc= 0.94526 time= 0.12200
Epoch: 0026 train_loss= 0.06842 train_acc= 0.97833 val_loss= 0.23996 val_acc= 0.94708 time= 0.12200
Epoch: 0027 train_loss= 0.05908 train_acc= 0.98278 val_loss= 0.23788 val_acc= 0.94526 time= 0.12313
Epoch: 0028 train_loss= 0.05640 train_acc= 0.98481 val_loss= 0.23903 val_acc= 0.94708 time= 0.16097
Epoch: 0029 train_loss= 0.05450 train_acc= 0.98177 val_loss= 0.24091 val_acc= 0.94891 time= 0.12603
Epoch: 0030 train_loss= 0.04831 train_acc= 0.98319 val_loss= 0.24565 val_acc= 0.94891 time= 0.12700
Early stopping...
Optimization Finished!
Test set results: cost= 0.15703 accuracy= 0.96482 time= 0.05496
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9200    0.9504    0.9350       121
           1     0.8690    0.9733    0.9182        75
           2     0.9808    0.9908    0.9858      1083
           3     0.7143    0.5000    0.5882        10
           4     0.9000    0.7500    0.8182        36
           5     0.8452    0.8765    0.8606        81
           6     0.9070    0.8966    0.9017        87
           7     0.9867    0.9626    0.9745       696

    accuracy                         0.9648      2189
   macro avg     0.8904    0.8625    0.8728      2189
weighted avg     0.9650    0.9648    0.9645      2189

Macro average Test Precision, Recall and F1-Score...
(0.890387221734421, 0.8625314447884473, 0.8727826619110399, None)
Micro average Test Precision, Recall and F1-Score...
(0.964824120603015, 0.964824120603015, 0.964824120603015, None)
embeddings:
7688 5485 2189
[[ 0.16944815  0.6883278  -0.05516009 ...  0.03630082  0.88127553
   0.13427803]
 [ 0.27346727  0.35278952  0.05901317 ... -0.13709785  0.5080365
  -0.15509407]
 [ 0.6851243   0.21961786  0.5061353  ...  0.31677192  0.50855947
  -0.25952032]
 ...
 [ 0.7008255   0.34313518  0.44907787 ...  0.48483992  0.60815483
  -0.07843563]
 [ 0.25546664  0.44321132  0.17176992 ... -0.20419703  0.5830877
  -0.279488  ]
 [ 0.64016813  0.10691223  0.42224115 ...  0.37750608  0.257649
  -0.05012812]]

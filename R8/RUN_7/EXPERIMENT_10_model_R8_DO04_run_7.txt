(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07939 train_acc= 0.24711 val_loss= 2.02013 val_acc= 0.54927 time= 0.39400
Epoch: 0002 train_loss= 2.01850 train_acc= 0.58416 val_loss= 1.92463 val_acc= 0.53102 time= 0.17403
Epoch: 0003 train_loss= 1.92007 train_acc= 0.56309 val_loss= 1.79596 val_acc= 0.52190 time= 0.12700
Epoch: 0004 train_loss= 1.78797 train_acc= 0.54547 val_loss= 1.64698 val_acc= 0.52007 time= 0.12800
Epoch: 0005 train_loss= 1.62772 train_acc= 0.54385 val_loss= 1.49937 val_acc= 0.52190 time= 0.12400
Epoch: 0006 train_loss= 1.47629 train_acc= 0.55195 val_loss= 1.37479 val_acc= 0.53467 time= 0.12400
Epoch: 0007 train_loss= 1.33895 train_acc= 0.56634 val_loss= 1.28133 val_acc= 0.60401 time= 0.12400
Epoch: 0008 train_loss= 1.24259 train_acc= 0.62933 val_loss= 1.21147 val_acc= 0.66241 time= 0.12400
Epoch: 0009 train_loss= 1.16415 train_acc= 0.68787 val_loss= 1.15164 val_acc= 0.72445 time= 0.17100
Epoch: 0010 train_loss= 1.10315 train_acc= 0.73547 val_loss= 1.09181 val_acc= 0.74635 time= 0.12597
Epoch: 0011 train_loss= 1.05268 train_acc= 0.76220 val_loss= 1.02849 val_acc= 0.75730 time= 0.12703
Epoch: 0012 train_loss= 0.98071 train_acc= 0.77537 val_loss= 0.96273 val_acc= 0.75912 time= 0.12500
Epoch: 0013 train_loss= 0.91450 train_acc= 0.78388 val_loss= 0.89765 val_acc= 0.75365 time= 0.12400
Epoch: 0014 train_loss= 0.85791 train_acc= 0.78631 val_loss= 0.83691 val_acc= 0.75547 time= 0.12600
Epoch: 0015 train_loss= 0.79103 train_acc= 0.78631 val_loss= 0.78351 val_acc= 0.75730 time= 0.12400
Epoch: 0016 train_loss= 0.74357 train_acc= 0.78590 val_loss= 0.73904 val_acc= 0.76095 time= 0.12300
Epoch: 0017 train_loss= 0.70098 train_acc= 0.78631 val_loss= 0.70322 val_acc= 0.76642 time= 0.15500
Epoch: 0018 train_loss= 0.66454 train_acc= 0.79664 val_loss= 0.67420 val_acc= 0.78285 time= 0.12400
Epoch: 0019 train_loss= 0.63816 train_acc= 0.80798 val_loss= 0.64944 val_acc= 0.80109 time= 0.12300
Epoch: 0020 train_loss= 0.61120 train_acc= 0.82459 val_loss= 0.62653 val_acc= 0.81752 time= 0.12597
Epoch: 0021 train_loss= 0.58464 train_acc= 0.84079 val_loss= 0.60382 val_acc= 0.82847 time= 0.12726
Epoch: 0022 train_loss= 0.56393 train_acc= 0.85254 val_loss= 0.58072 val_acc= 0.83577 time= 0.12400
Epoch: 0023 train_loss= 0.53835 train_acc= 0.86287 val_loss= 0.55749 val_acc= 0.84672 time= 0.12297
Epoch: 0024 train_loss= 0.51107 train_acc= 0.87624 val_loss= 0.53485 val_acc= 0.85219 time= 0.12703
Epoch: 0025 train_loss= 0.48745 train_acc= 0.88698 val_loss= 0.51338 val_acc= 0.85584 time= 0.16800
Epoch: 0026 train_loss= 0.46582 train_acc= 0.89163 val_loss= 0.49332 val_acc= 0.85766 time= 0.12600
Epoch: 0027 train_loss= 0.44421 train_acc= 0.89366 val_loss= 0.47468 val_acc= 0.86679 time= 0.12301
Epoch: 0028 train_loss= 0.42464 train_acc= 0.89670 val_loss= 0.45725 val_acc= 0.87774 time= 0.12399
Epoch: 0029 train_loss= 0.40535 train_acc= 0.89893 val_loss= 0.44067 val_acc= 0.88139 time= 0.12713
Epoch: 0030 train_loss= 0.38638 train_acc= 0.90419 val_loss= 0.42464 val_acc= 0.88869 time= 0.12700
Epoch: 0031 train_loss= 0.37179 train_acc= 0.90581 val_loss= 0.40894 val_acc= 0.90146 time= 0.12503
Epoch: 0032 train_loss= 0.35307 train_acc= 0.91169 val_loss= 0.39358 val_acc= 0.90146 time= 0.16800
Epoch: 0033 train_loss= 0.33696 train_acc= 0.91371 val_loss= 0.37869 val_acc= 0.90328 time= 0.12399
Epoch: 0034 train_loss= 0.32488 train_acc= 0.91959 val_loss= 0.36452 val_acc= 0.90693 time= 0.12600
Epoch: 0035 train_loss= 0.30800 train_acc= 0.92506 val_loss= 0.35115 val_acc= 0.90876 time= 0.12400
Epoch: 0036 train_loss= 0.29234 train_acc= 0.92830 val_loss= 0.33851 val_acc= 0.91241 time= 0.12301
Epoch: 0037 train_loss= 0.28001 train_acc= 0.93012 val_loss= 0.32654 val_acc= 0.91606 time= 0.12399
Epoch: 0038 train_loss= 0.26532 train_acc= 0.93721 val_loss= 0.31505 val_acc= 0.91423 time= 0.12300
Epoch: 0039 train_loss= 0.25316 train_acc= 0.93964 val_loss= 0.30401 val_acc= 0.92153 time= 0.12597
Epoch: 0040 train_loss= 0.23852 train_acc= 0.94430 val_loss= 0.29329 val_acc= 0.92701 time= 0.15703
Epoch: 0041 train_loss= 0.23182 train_acc= 0.94713 val_loss= 0.28286 val_acc= 0.92701 time= 0.12400
Epoch: 0042 train_loss= 0.22015 train_acc= 0.95058 val_loss= 0.27299 val_acc= 0.92701 time= 0.12600
Epoch: 0043 train_loss= 0.20777 train_acc= 0.95422 val_loss= 0.26378 val_acc= 0.92883 time= 0.12400
Epoch: 0044 train_loss= 0.19577 train_acc= 0.95524 val_loss= 0.25515 val_acc= 0.93248 time= 0.12501
Epoch: 0045 train_loss= 0.18516 train_acc= 0.95969 val_loss= 0.24724 val_acc= 0.93431 time= 0.12299
Epoch: 0046 train_loss= 0.17677 train_acc= 0.96050 val_loss= 0.23994 val_acc= 0.93613 time= 0.12303
Epoch: 0047 train_loss= 0.16871 train_acc= 0.96111 val_loss= 0.23319 val_acc= 0.93796 time= 0.12304
Epoch: 0048 train_loss= 0.16097 train_acc= 0.96314 val_loss= 0.22656 val_acc= 0.93978 time= 0.16997
Epoch: 0049 train_loss= 0.15593 train_acc= 0.96293 val_loss= 0.22023 val_acc= 0.94161 time= 0.12700
Epoch: 0050 train_loss= 0.14613 train_acc= 0.96516 val_loss= 0.21386 val_acc= 0.93978 time= 0.12600
Epoch: 0051 train_loss= 0.13761 train_acc= 0.96557 val_loss= 0.20777 val_acc= 0.93978 time= 0.12504
Epoch: 0052 train_loss= 0.12994 train_acc= 0.96962 val_loss= 0.20197 val_acc= 0.93978 time= 0.12301
Epoch: 0053 train_loss= 0.12641 train_acc= 0.97002 val_loss= 0.19658 val_acc= 0.93978 time= 0.12300
Epoch: 0054 train_loss= 0.12062 train_acc= 0.97103 val_loss= 0.19167 val_acc= 0.93796 time= 0.12304
Epoch: 0055 train_loss= 0.11502 train_acc= 0.96941 val_loss= 0.18731 val_acc= 0.93978 time= 0.15299
Epoch: 0056 train_loss= 0.11130 train_acc= 0.97367 val_loss= 0.18325 val_acc= 0.94343 time= 0.13105
Epoch: 0057 train_loss= 0.10747 train_acc= 0.97326 val_loss= 0.17926 val_acc= 0.94708 time= 0.12305
Epoch: 0058 train_loss= 0.09970 train_acc= 0.97630 val_loss= 0.17568 val_acc= 0.94891 time= 0.12900
Epoch: 0059 train_loss= 0.09474 train_acc= 0.97893 val_loss= 0.17237 val_acc= 0.94891 time= 0.12900
Epoch: 0060 train_loss= 0.09286 train_acc= 0.97974 val_loss= 0.16937 val_acc= 0.94891 time= 0.12300
Epoch: 0061 train_loss= 0.09038 train_acc= 0.97853 val_loss= 0.16677 val_acc= 0.94891 time= 0.12414
Epoch: 0062 train_loss= 0.08410 train_acc= 0.98096 val_loss= 0.16436 val_acc= 0.95073 time= 0.12397
Epoch: 0063 train_loss= 0.08201 train_acc= 0.98137 val_loss= 0.16235 val_acc= 0.95073 time= 0.15600
Epoch: 0064 train_loss= 0.08060 train_acc= 0.98177 val_loss= 0.16040 val_acc= 0.95073 time= 0.12300
Epoch: 0065 train_loss= 0.07554 train_acc= 0.98299 val_loss= 0.15861 val_acc= 0.95255 time= 0.12379
Epoch: 0066 train_loss= 0.07416 train_acc= 0.98440 val_loss= 0.15741 val_acc= 0.95255 time= 0.12600
Epoch: 0067 train_loss= 0.07012 train_acc= 0.98521 val_loss= 0.15618 val_acc= 0.95255 time= 0.12899
Epoch: 0068 train_loss= 0.06865 train_acc= 0.98299 val_loss= 0.15481 val_acc= 0.95255 time= 0.12697
Epoch: 0069 train_loss= 0.06596 train_acc= 0.98521 val_loss= 0.15355 val_acc= 0.95255 time= 0.12303
Epoch: 0070 train_loss= 0.06372 train_acc= 0.98501 val_loss= 0.15196 val_acc= 0.95255 time= 0.12200
Epoch: 0071 train_loss= 0.06265 train_acc= 0.98501 val_loss= 0.15037 val_acc= 0.95438 time= 0.15100
Epoch: 0072 train_loss= 0.05902 train_acc= 0.98623 val_loss= 0.14911 val_acc= 0.95620 time= 0.12200
Epoch: 0073 train_loss= 0.05774 train_acc= 0.98704 val_loss= 0.14777 val_acc= 0.95620 time= 0.12300
Epoch: 0074 train_loss= 0.05382 train_acc= 0.98785 val_loss= 0.14645 val_acc= 0.95620 time= 0.12400
Epoch: 0075 train_loss= 0.05464 train_acc= 0.98704 val_loss= 0.14537 val_acc= 0.95620 time= 0.12397
Epoch: 0076 train_loss= 0.05205 train_acc= 0.98886 val_loss= 0.14466 val_acc= 0.95620 time= 0.12600
Epoch: 0077 train_loss= 0.05149 train_acc= 0.98825 val_loss= 0.14462 val_acc= 0.95620 time= 0.12600
Epoch: 0078 train_loss= 0.04849 train_acc= 0.98947 val_loss= 0.14486 val_acc= 0.95803 time= 0.12503
Epoch: 0079 train_loss= 0.04552 train_acc= 0.99109 val_loss= 0.14486 val_acc= 0.95803 time= 0.17000
Epoch: 0080 train_loss= 0.04534 train_acc= 0.99149 val_loss= 0.14489 val_acc= 0.95803 time= 0.12401
Epoch: 0081 train_loss= 0.04323 train_acc= 0.99149 val_loss= 0.14474 val_acc= 0.95803 time= 0.12399
Epoch: 0082 train_loss= 0.04308 train_acc= 0.99190 val_loss= 0.14429 val_acc= 0.95803 time= 0.12301
Epoch: 0083 train_loss= 0.04190 train_acc= 0.99007 val_loss= 0.14328 val_acc= 0.95438 time= 0.12299
Epoch: 0084 train_loss= 0.04107 train_acc= 0.99129 val_loss= 0.14254 val_acc= 0.95438 time= 0.12599
Epoch: 0085 train_loss= 0.03945 train_acc= 0.99210 val_loss= 0.14209 val_acc= 0.95438 time= 0.12200
Epoch: 0086 train_loss= 0.03919 train_acc= 0.99271 val_loss= 0.14183 val_acc= 0.95438 time= 0.16297
Epoch: 0087 train_loss= 0.03644 train_acc= 0.99271 val_loss= 0.14217 val_acc= 0.95438 time= 0.12700
Epoch: 0088 train_loss= 0.03564 train_acc= 0.99392 val_loss= 0.14257 val_acc= 0.95438 time= 0.12303
Epoch: 0089 train_loss= 0.03524 train_acc= 0.99352 val_loss= 0.14309 val_acc= 0.95620 time= 0.12400
Epoch: 0090 train_loss= 0.03325 train_acc= 0.99473 val_loss= 0.14317 val_acc= 0.95438 time= 0.12200
Early stopping...
Optimization Finished!
Test set results: cost= 0.10951 accuracy= 0.97168 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9286    0.9669    0.9474       121
           1     0.8916    0.9867    0.9367        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.6667    0.8000        36
           5     0.9012    0.9012    0.9012        81
           6     0.9186    0.9080    0.9133        87
           7     0.9840    0.9713    0.9776       696

    accuracy                         0.9717      2189
   macro avg     0.9509    0.9241    0.9330      2189
weighted avg     0.9722    0.9717    0.9713      2189

Macro average Test Precision, Recall and F1-Score...
(0.9509352189224466, 0.9240637681893756, 0.9329722268161442, None)
Micro average Test Precision, Recall and F1-Score...
(0.9716765646413887, 0.9716765646413887, 0.9716765646413887, None)
embeddings:
7688 5485 2189
[[ 3.0604050e-01  9.6930094e-02  3.1624738e-02 ...  8.3237618e-02
   6.8104014e-02  1.8180738e-01]
 [ 1.8315180e-01  2.2417624e-01  1.6630328e-01 ...  1.9960578e-01
   1.5741178e-01  3.5014823e-02]
 [-2.9756012e-04  9.8213032e-02  4.7749838e-01 ...  5.7852960e-01
   4.3138826e-01  1.2851565e-01]
 ...
 [ 9.4818413e-02  1.3068552e-01  4.1624057e-01 ...  4.4998938e-01
   4.1126060e-01  1.9956973e-01]
 [ 3.4358943e-01  2.7223545e-01  3.3678263e-01 ...  2.4046175e-01
   1.7999345e-01  1.3786417e-02]
 [ 3.9326616e-02  1.4050636e-01  4.1264394e-01 ...  4.0434286e-01
   3.3389279e-01  1.4663240e-01]]

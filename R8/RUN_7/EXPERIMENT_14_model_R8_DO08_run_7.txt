(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07948 train_acc= 0.12092 val_loss= 2.03110 val_acc= 0.74635 time= 0.40105
Epoch: 0002 train_loss= 2.03085 train_acc= 0.76443 val_loss= 1.94481 val_acc= 0.74453 time= 0.12900
Epoch: 0003 train_loss= 1.94183 train_acc= 0.76909 val_loss= 1.82819 val_acc= 0.74635 time= 0.12610
Epoch: 0004 train_loss= 1.82448 train_acc= 0.76504 val_loss= 1.69359 val_acc= 0.75000 time= 0.12696
Epoch: 0005 train_loss= 1.68289 train_acc= 0.76828 val_loss= 1.56043 val_acc= 0.75547 time= 0.12600
Epoch: 0006 train_loss= 1.56096 train_acc= 0.77172 val_loss= 1.44753 val_acc= 0.75365 time= 0.12404
Epoch: 0007 train_loss= 1.40420 train_acc= 0.77598 val_loss= 1.36023 val_acc= 0.76277 time= 0.12300
Epoch: 0008 train_loss= 1.33193 train_acc= 0.76727 val_loss= 1.29077 val_acc= 0.74453 time= 0.16500
Epoch: 0009 train_loss= 1.26386 train_acc= 0.76990 val_loss= 1.22906 val_acc= 0.69526 time= 0.12500
Epoch: 0010 train_loss= 1.18330 train_acc= 0.69739 val_loss= 1.16797 val_acc= 0.66606 time= 0.12200
Epoch: 0011 train_loss= 1.12586 train_acc= 0.67774 val_loss= 1.10382 val_acc= 0.67153 time= 0.12296
Epoch: 0012 train_loss= 1.05476 train_acc= 0.69030 val_loss= 1.03543 val_acc= 0.70438 time= 0.12404
Epoch: 0013 train_loss= 1.01309 train_acc= 0.71298 val_loss= 0.96578 val_acc= 0.73358 time= 0.12496
Epoch: 0014 train_loss= 0.94243 train_acc= 0.74195 val_loss= 0.89885 val_acc= 0.76095 time= 0.12500
Epoch: 0015 train_loss= 0.86253 train_acc= 0.77233 val_loss= 0.83893 val_acc= 0.75730 time= 0.15100
Epoch: 0016 train_loss= 0.81163 train_acc= 0.78509 val_loss= 0.78894 val_acc= 0.76095 time= 0.13305
Epoch: 0017 train_loss= 0.75669 train_acc= 0.78550 val_loss= 0.74869 val_acc= 0.76095 time= 0.12500
Epoch: 0018 train_loss= 0.71944 train_acc= 0.78529 val_loss= 0.71602 val_acc= 0.77007 time= 0.12300
Epoch: 0019 train_loss= 0.68129 train_acc= 0.79218 val_loss= 0.68840 val_acc= 0.77737 time= 0.12301
Epoch: 0020 train_loss= 0.65687 train_acc= 0.79907 val_loss= 0.66351 val_acc= 0.80292 time= 0.12400
Epoch: 0021 train_loss= 0.62996 train_acc= 0.82256 val_loss= 0.64007 val_acc= 0.81934 time= 0.12295
Epoch: 0022 train_loss= 0.61299 train_acc= 0.83350 val_loss= 0.61728 val_acc= 0.83212 time= 0.12405
Epoch: 0023 train_loss= 0.58863 train_acc= 0.84768 val_loss= 0.59491 val_acc= 0.84124 time= 0.17295
Epoch: 0024 train_loss= 0.56117 train_acc= 0.85983 val_loss= 0.57331 val_acc= 0.84672 time= 0.12604
Epoch: 0025 train_loss= 0.53646 train_acc= 0.86956 val_loss= 0.55265 val_acc= 0.85219 time= 0.12496
Epoch: 0026 train_loss= 0.51529 train_acc= 0.87705 val_loss= 0.53327 val_acc= 0.85584 time= 0.12500
Epoch: 0027 train_loss= 0.49244 train_acc= 0.88171 val_loss= 0.51530 val_acc= 0.85766 time= 0.12300
Epoch: 0028 train_loss= 0.47696 train_acc= 0.88313 val_loss= 0.49866 val_acc= 0.86131 time= 0.12300
Epoch: 0029 train_loss= 0.45666 train_acc= 0.88617 val_loss= 0.48318 val_acc= 0.86679 time= 0.12400
Epoch: 0030 train_loss= 0.44158 train_acc= 0.88677 val_loss= 0.46848 val_acc= 0.87409 time= 0.12309
Epoch: 0031 train_loss= 0.41856 train_acc= 0.89163 val_loss= 0.45433 val_acc= 0.88686 time= 0.15100
Epoch: 0032 train_loss= 0.40792 train_acc= 0.89771 val_loss= 0.44044 val_acc= 0.88686 time= 0.12400
Epoch: 0033 train_loss= 0.38701 train_acc= 0.90237 val_loss= 0.42673 val_acc= 0.88869 time= 0.12700
Epoch: 0034 train_loss= 0.37572 train_acc= 0.90034 val_loss= 0.41319 val_acc= 0.89051 time= 0.12700
Epoch: 0035 train_loss= 0.36246 train_acc= 0.90561 val_loss= 0.39988 val_acc= 0.89781 time= 0.12300
Epoch: 0036 train_loss= 0.35259 train_acc= 0.90723 val_loss= 0.38660 val_acc= 0.90876 time= 0.12399
Epoch: 0037 train_loss= 0.33329 train_acc= 0.91756 val_loss= 0.37367 val_acc= 0.91241 time= 0.12401
Epoch: 0038 train_loss= 0.31718 train_acc= 0.92222 val_loss= 0.36134 val_acc= 0.91606 time= 0.12299
Epoch: 0039 train_loss= 0.30944 train_acc= 0.91837 val_loss= 0.34972 val_acc= 0.91788 time= 0.16700
Epoch: 0040 train_loss= 0.30065 train_acc= 0.93012 val_loss= 0.33882 val_acc= 0.91788 time= 0.12308
Epoch: 0041 train_loss= 0.28266 train_acc= 0.93194 val_loss= 0.32872 val_acc= 0.92153 time= 0.12507
Epoch: 0042 train_loss= 0.27166 train_acc= 0.93275 val_loss= 0.31914 val_acc= 0.92336 time= 0.12827
Epoch: 0043 train_loss= 0.26782 train_acc= 0.93154 val_loss= 0.30939 val_acc= 0.92701 time= 0.12503
Epoch: 0044 train_loss= 0.25521 train_acc= 0.94166 val_loss= 0.29968 val_acc= 0.93066 time= 0.12401
Epoch: 0045 train_loss= 0.25168 train_acc= 0.93640 val_loss= 0.28996 val_acc= 0.92883 time= 0.12300
Epoch: 0046 train_loss= 0.23497 train_acc= 0.94693 val_loss= 0.28062 val_acc= 0.92883 time= 0.15799
Epoch: 0047 train_loss= 0.21976 train_acc= 0.95037 val_loss= 0.27201 val_acc= 0.93066 time= 0.12400
Epoch: 0048 train_loss= 0.20428 train_acc= 0.95605 val_loss= 0.26388 val_acc= 0.93248 time= 0.12307
Epoch: 0049 train_loss= 0.19820 train_acc= 0.95544 val_loss= 0.25614 val_acc= 0.93248 time= 0.12200
Epoch: 0050 train_loss= 0.19523 train_acc= 0.95422 val_loss= 0.24882 val_acc= 0.93248 time= 0.12296
Epoch: 0051 train_loss= 0.18426 train_acc= 0.95584 val_loss= 0.24175 val_acc= 0.93248 time= 0.12678
Epoch: 0052 train_loss= 0.17845 train_acc= 0.95706 val_loss= 0.23517 val_acc= 0.93248 time= 0.12700
Epoch: 0053 train_loss= 0.17400 train_acc= 0.95665 val_loss= 0.22936 val_acc= 0.93431 time= 0.12304
Epoch: 0054 train_loss= 0.16435 train_acc= 0.95827 val_loss= 0.22402 val_acc= 0.93613 time= 0.16001
Epoch: 0055 train_loss= 0.15325 train_acc= 0.96152 val_loss= 0.21879 val_acc= 0.93613 time= 0.12200
Epoch: 0056 train_loss= 0.14900 train_acc= 0.96131 val_loss= 0.21366 val_acc= 0.93796 time= 0.12300
Epoch: 0057 train_loss= 0.14118 train_acc= 0.96293 val_loss= 0.20886 val_acc= 0.93978 time= 0.12196
Epoch: 0058 train_loss= 0.15023 train_acc= 0.95929 val_loss= 0.20397 val_acc= 0.94161 time= 0.12205
Epoch: 0059 train_loss= 0.13325 train_acc= 0.96374 val_loss= 0.19921 val_acc= 0.94343 time= 0.12595
Epoch: 0060 train_loss= 0.12985 train_acc= 0.96779 val_loss= 0.19508 val_acc= 0.94343 time= 0.12355
Epoch: 0061 train_loss= 0.12457 train_acc= 0.97043 val_loss= 0.19170 val_acc= 0.94343 time= 0.12700
Epoch: 0062 train_loss= 0.12340 train_acc= 0.96759 val_loss= 0.18860 val_acc= 0.95073 time= 0.15475
Epoch: 0063 train_loss= 0.11928 train_acc= 0.96638 val_loss= 0.18580 val_acc= 0.94708 time= 0.12300
Epoch: 0064 train_loss= 0.11156 train_acc= 0.96982 val_loss= 0.18356 val_acc= 0.94708 time= 0.12310
Epoch: 0065 train_loss= 0.11683 train_acc= 0.97002 val_loss= 0.18132 val_acc= 0.94708 time= 0.12197
Epoch: 0066 train_loss= 0.10540 train_acc= 0.97225 val_loss= 0.17922 val_acc= 0.95073 time= 0.12417
Epoch: 0067 train_loss= 0.10388 train_acc= 0.97326 val_loss= 0.17682 val_acc= 0.94891 time= 0.12400
Epoch: 0068 train_loss= 0.09661 train_acc= 0.97367 val_loss= 0.17467 val_acc= 0.94891 time= 0.12703
Epoch: 0069 train_loss= 0.09915 train_acc= 0.97630 val_loss= 0.17224 val_acc= 0.94891 time= 0.12201
Epoch: 0070 train_loss= 0.09210 train_acc= 0.97630 val_loss= 0.16950 val_acc= 0.95255 time= 0.16900
Epoch: 0071 train_loss= 0.08988 train_acc= 0.97731 val_loss= 0.16711 val_acc= 0.95073 time= 0.12542
Epoch: 0072 train_loss= 0.09103 train_acc= 0.97752 val_loss= 0.16508 val_acc= 0.95073 time= 0.12600
Epoch: 0073 train_loss= 0.08424 train_acc= 0.98055 val_loss= 0.16375 val_acc= 0.94891 time= 0.12300
Epoch: 0074 train_loss= 0.08846 train_acc= 0.97792 val_loss= 0.16311 val_acc= 0.95255 time= 0.12300
Epoch: 0075 train_loss= 0.07978 train_acc= 0.98076 val_loss= 0.16247 val_acc= 0.95255 time= 0.12300
Epoch: 0076 train_loss= 0.07691 train_acc= 0.98197 val_loss= 0.16178 val_acc= 0.95255 time= 0.12599
Epoch: 0077 train_loss= 0.08023 train_acc= 0.97974 val_loss= 0.16136 val_acc= 0.95255 time= 0.14507
Epoch: 0078 train_loss= 0.07445 train_acc= 0.98055 val_loss= 0.16107 val_acc= 0.95255 time= 0.13801
Epoch: 0079 train_loss= 0.07505 train_acc= 0.98076 val_loss= 0.16027 val_acc= 0.95255 time= 0.12296
Epoch: 0080 train_loss= 0.07178 train_acc= 0.98116 val_loss= 0.15907 val_acc= 0.95620 time= 0.12700
Epoch: 0081 train_loss= 0.07623 train_acc= 0.97974 val_loss= 0.15703 val_acc= 0.95620 time= 0.12603
Epoch: 0082 train_loss= 0.06679 train_acc= 0.98359 val_loss= 0.15483 val_acc= 0.95620 time= 0.12300
Epoch: 0083 train_loss= 0.06533 train_acc= 0.98278 val_loss= 0.15290 val_acc= 0.95620 time= 0.12300
Epoch: 0084 train_loss= 0.06607 train_acc= 0.98137 val_loss= 0.15109 val_acc= 0.95620 time= 0.12605
Epoch: 0085 train_loss= 0.05958 train_acc= 0.98643 val_loss= 0.14981 val_acc= 0.95803 time= 0.16476
Epoch: 0086 train_loss= 0.06428 train_acc= 0.98461 val_loss= 0.14931 val_acc= 0.95620 time= 0.12301
Epoch: 0087 train_loss= 0.06117 train_acc= 0.98339 val_loss= 0.14943 val_acc= 0.95438 time= 0.12200
Epoch: 0088 train_loss= 0.05863 train_acc= 0.98825 val_loss= 0.14997 val_acc= 0.95438 time= 0.12401
Epoch: 0089 train_loss= 0.05894 train_acc= 0.98440 val_loss= 0.15143 val_acc= 0.95438 time= 0.12599
Epoch: 0090 train_loss= 0.05876 train_acc= 0.98420 val_loss= 0.15350 val_acc= 0.95438 time= 0.12553
Early stopping...
Optimization Finished!
Test set results: cost= 0.11026 accuracy= 0.97122 time= 0.05600
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9431    0.9587    0.9508       121
           1     0.8889    0.9600    0.9231        75
           2     0.9853    0.9898    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     0.9643    0.7500    0.8437        36
           5     0.9565    0.8148    0.8800        81
           6     0.8542    0.9425    0.8962        87
           7     0.9813    0.9784    0.9799       696

    accuracy                         0.9712      2189
   macro avg     0.9467    0.9243    0.9327      2189
weighted avg     0.9718    0.9712    0.9710      2189

Macro average Test Precision, Recall and F1-Score...
(0.9466893211300562, 0.9242890676104591, 0.9326551135715858, None)
Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)
embeddings:
7688 5485 2189
[[ 0.1028534   0.1977591   0.1575866  ... -0.03372053  0.02542571
   0.15505016]
 [ 0.16561194  0.08174509  0.05193211 ... -0.00144619  0.12034903
   0.19283696]
 [ 0.4470031   0.18077175  0.2095786  ... -0.02510576  0.4093363
  -0.0171381 ]
 ...
 [ 0.42002702  0.25114018  0.2722513  ... -0.05586515  0.3942973
   0.25532958]
 [ 0.20720246  0.06960527  0.04419075 ... -0.02928922  0.26350448
   0.2820642 ]
 [ 0.3493405   0.17502706  0.2055414  ... -0.03614313  0.3509192
   0.11116876]]

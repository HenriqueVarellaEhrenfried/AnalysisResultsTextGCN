(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07929 train_acc= 0.16245 val_loss= 2.03576 val_acc= 0.74088 time= 0.40400
Epoch: 0002 train_loss= 2.03476 train_acc= 0.76261 val_loss= 1.95485 val_acc= 0.75365 time= 0.12800
Epoch: 0003 train_loss= 1.94946 train_acc= 0.76808 val_loss= 1.84491 val_acc= 0.75730 time= 0.12368
Epoch: 0004 train_loss= 1.84085 train_acc= 0.77496 val_loss= 1.71648 val_acc= 0.76460 time= 0.12600
Epoch: 0005 train_loss= 1.71537 train_acc= 0.76038 val_loss= 1.58609 val_acc= 0.75547 time= 0.12497
Epoch: 0006 train_loss= 1.59149 train_acc= 0.75856 val_loss= 1.47195 val_acc= 0.74635 time= 0.12700
Epoch: 0007 train_loss= 1.44151 train_acc= 0.73506 val_loss= 1.38277 val_acc= 0.72445 time= 0.12400
Epoch: 0008 train_loss= 1.32903 train_acc= 0.72514 val_loss= 1.31248 val_acc= 0.69526 time= 0.15518
Epoch: 0009 train_loss= 1.28901 train_acc= 0.69982 val_loss= 1.25172 val_acc= 0.67336 time= 0.13000
Epoch: 0010 train_loss= 1.21694 train_acc= 0.69334 val_loss= 1.19286 val_acc= 0.65693 time= 0.12300
Epoch: 0011 train_loss= 1.16079 train_acc= 0.67713 val_loss= 1.13180 val_acc= 0.67883 time= 0.12400
Epoch: 0012 train_loss= 1.10633 train_acc= 0.68098 val_loss= 1.06750 val_acc= 0.71350 time= 0.12504
Epoch: 0013 train_loss= 1.03438 train_acc= 0.71703 val_loss= 1.00197 val_acc= 0.73723 time= 0.12405
Epoch: 0014 train_loss= 0.97867 train_acc= 0.74580 val_loss= 0.93863 val_acc= 0.75912 time= 0.12500
Epoch: 0015 train_loss= 0.91187 train_acc= 0.76403 val_loss= 0.88038 val_acc= 0.76095 time= 0.12797
Epoch: 0016 train_loss= 0.84866 train_acc= 0.77740 val_loss= 0.82945 val_acc= 0.75912 time= 0.16328
Epoch: 0017 train_loss= 0.80961 train_acc= 0.77881 val_loss= 0.78661 val_acc= 0.75730 time= 0.12300
Epoch: 0018 train_loss= 0.76189 train_acc= 0.78246 val_loss= 0.75101 val_acc= 0.75730 time= 0.12408
Epoch: 0019 train_loss= 0.72396 train_acc= 0.78165 val_loss= 0.72107 val_acc= 0.76095 time= 0.12500
Epoch: 0020 train_loss= 0.69422 train_acc= 0.78692 val_loss= 0.69500 val_acc= 0.76825 time= 0.12500
Epoch: 0021 train_loss= 0.67304 train_acc= 0.78813 val_loss= 0.67142 val_acc= 0.77920 time= 0.12501
Epoch: 0022 train_loss= 0.64462 train_acc= 0.79745 val_loss= 0.64925 val_acc= 0.79197 time= 0.12503
Epoch: 0023 train_loss= 0.62797 train_acc= 0.81568 val_loss= 0.62777 val_acc= 0.80292 time= 0.12500
Epoch: 0024 train_loss= 0.59481 train_acc= 0.83411 val_loss= 0.60687 val_acc= 0.82299 time= 0.15700
Epoch: 0025 train_loss= 0.57701 train_acc= 0.84302 val_loss= 0.58640 val_acc= 0.83577 time= 0.12700
Epoch: 0026 train_loss= 0.55292 train_acc= 0.84890 val_loss= 0.56666 val_acc= 0.83942 time= 0.12362
Epoch: 0027 train_loss= 0.53417 train_acc= 0.84707 val_loss= 0.54798 val_acc= 0.84854 time= 0.12410
Epoch: 0028 train_loss= 0.51520 train_acc= 0.86125 val_loss= 0.53042 val_acc= 0.85036 time= 0.12377
Epoch: 0029 train_loss= 0.48609 train_acc= 0.86652 val_loss= 0.51398 val_acc= 0.85584 time= 0.12705
Epoch: 0030 train_loss= 0.48061 train_acc= 0.86388 val_loss= 0.49852 val_acc= 0.85766 time= 0.12400
Epoch: 0031 train_loss= 0.46012 train_acc= 0.87118 val_loss= 0.48391 val_acc= 0.86496 time= 0.12313
Epoch: 0032 train_loss= 0.44795 train_acc= 0.88049 val_loss= 0.46991 val_acc= 0.86861 time= 0.17000
Epoch: 0033 train_loss= 0.42111 train_acc= 0.87806 val_loss= 0.45630 val_acc= 0.86861 time= 0.12497
Epoch: 0034 train_loss= 0.41519 train_acc= 0.88313 val_loss= 0.44315 val_acc= 0.87044 time= 0.12514
Epoch: 0035 train_loss= 0.39775 train_acc= 0.88779 val_loss= 0.43038 val_acc= 0.87591 time= 0.12508
Epoch: 0036 train_loss= 0.38068 train_acc= 0.89204 val_loss= 0.41785 val_acc= 0.88139 time= 0.12304
Epoch: 0037 train_loss= 0.37226 train_acc= 0.89184 val_loss= 0.40542 val_acc= 0.88504 time= 0.12700
Epoch: 0038 train_loss= 0.34359 train_acc= 0.89974 val_loss= 0.39339 val_acc= 0.88869 time= 0.12356
Epoch: 0039 train_loss= 0.34208 train_acc= 0.90055 val_loss= 0.38144 val_acc= 0.89599 time= 0.15707
Epoch: 0040 train_loss= 0.33928 train_acc= 0.90703 val_loss= 0.37019 val_acc= 0.89964 time= 0.12497
Epoch: 0041 train_loss= 0.31934 train_acc= 0.90804 val_loss= 0.35946 val_acc= 0.90328 time= 0.12403
Epoch: 0042 train_loss= 0.31713 train_acc= 0.90804 val_loss= 0.34914 val_acc= 0.90876 time= 0.12400
Epoch: 0043 train_loss= 0.30066 train_acc= 0.91635 val_loss= 0.33905 val_acc= 0.91058 time= 0.12697
Epoch: 0044 train_loss= 0.30177 train_acc= 0.91837 val_loss= 0.32946 val_acc= 0.91788 time= 0.12700
Epoch: 0045 train_loss= 0.27914 train_acc= 0.92546 val_loss= 0.32038 val_acc= 0.91971 time= 0.12503
Epoch: 0046 train_loss= 0.27717 train_acc= 0.91736 val_loss= 0.31204 val_acc= 0.92153 time= 0.12797
Epoch: 0047 train_loss= 0.25704 train_acc= 0.93377 val_loss= 0.30379 val_acc= 0.92518 time= 0.15538
Epoch: 0048 train_loss= 0.25319 train_acc= 0.92951 val_loss= 0.29553 val_acc= 0.92883 time= 0.12300
Epoch: 0049 train_loss= 0.24969 train_acc= 0.93235 val_loss= 0.28724 val_acc= 0.92518 time= 0.12200
Epoch: 0050 train_loss= 0.23862 train_acc= 0.93154 val_loss= 0.27938 val_acc= 0.92701 time= 0.12201
Epoch: 0051 train_loss= 0.23299 train_acc= 0.93741 val_loss= 0.27176 val_acc= 0.92701 time= 0.12396
Epoch: 0052 train_loss= 0.22419 train_acc= 0.93660 val_loss= 0.26483 val_acc= 0.92701 time= 0.12600
Epoch: 0053 train_loss= 0.21679 train_acc= 0.93802 val_loss= 0.25842 val_acc= 0.92883 time= 0.12565
Epoch: 0054 train_loss= 0.21018 train_acc= 0.94085 val_loss= 0.25286 val_acc= 0.93248 time= 0.12678
Epoch: 0055 train_loss= 0.20584 train_acc= 0.94268 val_loss= 0.24814 val_acc= 0.93248 time= 0.15000
Epoch: 0056 train_loss= 0.19865 train_acc= 0.94531 val_loss= 0.24424 val_acc= 0.93613 time= 0.12300
Epoch: 0057 train_loss= 0.17658 train_acc= 0.95098 val_loss= 0.23951 val_acc= 0.93796 time= 0.12451
Epoch: 0058 train_loss= 0.17427 train_acc= 0.94774 val_loss= 0.23406 val_acc= 0.93796 time= 0.12400
Epoch: 0059 train_loss= 0.18186 train_acc= 0.94997 val_loss= 0.22832 val_acc= 0.94161 time= 0.12400
Epoch: 0060 train_loss= 0.17420 train_acc= 0.95281 val_loss= 0.22272 val_acc= 0.93796 time= 0.12400
Epoch: 0061 train_loss= 0.17185 train_acc= 0.94815 val_loss= 0.21648 val_acc= 0.94161 time= 0.12400
Epoch: 0062 train_loss= 0.16114 train_acc= 0.95726 val_loss= 0.21085 val_acc= 0.93796 time= 0.13025
Epoch: 0063 train_loss= 0.15720 train_acc= 0.95888 val_loss= 0.20572 val_acc= 0.93978 time= 0.17723
Epoch: 0064 train_loss= 0.15064 train_acc= 0.95969 val_loss= 0.20115 val_acc= 0.94161 time= 0.12300
Epoch: 0065 train_loss= 0.14347 train_acc= 0.95787 val_loss= 0.19715 val_acc= 0.94161 time= 0.12300
Epoch: 0066 train_loss= 0.14419 train_acc= 0.95848 val_loss= 0.19350 val_acc= 0.94891 time= 0.12438
Epoch: 0067 train_loss= 0.13879 train_acc= 0.96111 val_loss= 0.19079 val_acc= 0.94891 time= 0.12297
Epoch: 0068 train_loss= 0.14699 train_acc= 0.95969 val_loss= 0.18831 val_acc= 0.94526 time= 0.12400
Epoch: 0069 train_loss= 0.13566 train_acc= 0.96516 val_loss= 0.18637 val_acc= 0.95073 time= 0.12503
Epoch: 0070 train_loss= 0.13776 train_acc= 0.96314 val_loss= 0.18450 val_acc= 0.95073 time= 0.17200
Epoch: 0071 train_loss= 0.12064 train_acc= 0.96759 val_loss= 0.18289 val_acc= 0.94891 time= 0.12456
Epoch: 0072 train_loss= 0.12953 train_acc= 0.96354 val_loss= 0.18079 val_acc= 0.94891 time= 0.12568
Epoch: 0073 train_loss= 0.12376 train_acc= 0.96779 val_loss= 0.17856 val_acc= 0.94891 time= 0.12503
Epoch: 0074 train_loss= 0.12138 train_acc= 0.96698 val_loss= 0.17603 val_acc= 0.94891 time= 0.12355
Epoch: 0075 train_loss= 0.11771 train_acc= 0.96557 val_loss= 0.17335 val_acc= 0.94891 time= 0.12396
Epoch: 0076 train_loss= 0.11830 train_acc= 0.96638 val_loss= 0.17045 val_acc= 0.95073 time= 0.12300
Epoch: 0077 train_loss= 0.11422 train_acc= 0.96476 val_loss= 0.16814 val_acc= 0.94891 time= 0.12404
Epoch: 0078 train_loss= 0.10897 train_acc= 0.97124 val_loss= 0.16616 val_acc= 0.94891 time= 0.14995
Epoch: 0079 train_loss= 0.09829 train_acc= 0.97306 val_loss= 0.16459 val_acc= 0.94708 time= 0.12700
Epoch: 0080 train_loss= 0.10256 train_acc= 0.97286 val_loss= 0.16317 val_acc= 0.94891 time= 0.12400
Epoch: 0081 train_loss= 0.10249 train_acc= 0.97043 val_loss= 0.16264 val_acc= 0.94891 time= 0.12700
Epoch: 0082 train_loss= 0.10042 train_acc= 0.97144 val_loss= 0.16209 val_acc= 0.95073 time= 0.12504
Epoch: 0083 train_loss= 0.09739 train_acc= 0.97549 val_loss= 0.16147 val_acc= 0.95073 time= 0.12319
Epoch: 0084 train_loss= 0.09980 train_acc= 0.97205 val_loss= 0.16056 val_acc= 0.94891 time= 0.12400
Epoch: 0085 train_loss= 0.08914 train_acc= 0.97347 val_loss= 0.16013 val_acc= 0.94891 time= 0.12300
Epoch: 0086 train_loss= 0.09606 train_acc= 0.97407 val_loss= 0.15967 val_acc= 0.94891 time= 0.16100
Epoch: 0087 train_loss= 0.08788 train_acc= 0.97711 val_loss= 0.15863 val_acc= 0.94891 time= 0.12600
Epoch: 0088 train_loss= 0.08850 train_acc= 0.97428 val_loss= 0.15690 val_acc= 0.94891 time= 0.12292
Epoch: 0089 train_loss= 0.09243 train_acc= 0.97185 val_loss= 0.15490 val_acc= 0.95073 time= 0.12300
Epoch: 0090 train_loss= 0.08588 train_acc= 0.97853 val_loss= 0.15298 val_acc= 0.95255 time= 0.12396
Epoch: 0091 train_loss= 0.08687 train_acc= 0.97812 val_loss= 0.15155 val_acc= 0.95438 time= 0.12500
Epoch: 0092 train_loss= 0.08142 train_acc= 0.97731 val_loss= 0.14995 val_acc= 0.95438 time= 0.12304
Epoch: 0093 train_loss= 0.07273 train_acc= 0.97995 val_loss= 0.14876 val_acc= 0.95255 time= 0.12699
Epoch: 0094 train_loss= 0.08593 train_acc= 0.97711 val_loss= 0.14784 val_acc= 0.95438 time= 0.15599
Epoch: 0095 train_loss= 0.08086 train_acc= 0.97873 val_loss= 0.14740 val_acc= 0.95255 time= 0.12299
Epoch: 0096 train_loss= 0.06788 train_acc= 0.98299 val_loss= 0.14714 val_acc= 0.95255 time= 0.12597
Epoch: 0097 train_loss= 0.07268 train_acc= 0.97914 val_loss= 0.14717 val_acc= 0.95255 time= 0.12300
Epoch: 0098 train_loss= 0.07340 train_acc= 0.98116 val_loss= 0.14761 val_acc= 0.95255 time= 0.12400
Epoch: 0099 train_loss= 0.08077 train_acc= 0.97772 val_loss= 0.14914 val_acc= 0.95255 time= 0.12500
Epoch: 0100 train_loss= 0.07362 train_acc= 0.97995 val_loss= 0.14964 val_acc= 0.95620 time= 0.12670
Early stopping...
Optimization Finished!
Test set results: cost= 0.11204 accuracy= 0.96802 time= 0.05600
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9219    0.9752    0.9478       121
           1     0.9000    0.9600    0.9290        75
           2     0.9862    0.9898    0.9880      1083
           3     1.0000    0.9000    0.9474        10
           4     0.9600    0.6667    0.7869        36
           5     0.9677    0.7407    0.8392        81
           6     0.8058    0.9540    0.8737        87
           7     0.9799    0.9784    0.9792       696

    accuracy                         0.9680      2189
   macro avg     0.9402    0.8956    0.9114      2189
weighted avg     0.9695    0.9680    0.9674      2189

Macro average Test Precision, Recall and F1-Score...
(0.940187355661019, 0.8956160389962079, 0.9113865327470454, None)
Micro average Test Precision, Recall and F1-Score...
(0.9680219278209228, 0.9680219278209228, 0.9680219278209228, None)
embeddings:
7688 5485 2189
[[ 0.25950837  0.3174209   0.25629863 ...  0.14985283  0.05915833
   0.08556415]
 [ 0.08230322  0.15740648  0.09827193 ...  0.1262328   0.1424785
   0.08083396]
 [-0.03604739  0.01952726 -0.05288921 ...  0.07171915  0.42199486
   0.03022127]
 ...
 [ 0.09633426  0.08761846  0.0567343  ...  0.1928626   0.37518117
   0.04351364]
 [ 0.07466328  0.17972156  0.1214119  ...  0.09732432  0.17512104
   0.05267043]
 [ 0.02417804 -0.03120649  0.00246721 ...  0.04894076  0.3212796
  -0.00237723]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07961 train_acc= 0.03990 val_loss= 2.01043 val_acc= 0.73723 time= 0.44835
Epoch: 0002 train_loss= 2.00704 train_acc= 0.75370 val_loss= 1.88683 val_acc= 0.73175 time= 0.16700
Epoch: 0003 train_loss= 1.88093 train_acc= 0.74134 val_loss= 1.71952 val_acc= 0.71168 time= 0.15701
Epoch: 0004 train_loss= 1.70595 train_acc= 0.72068 val_loss= 1.54149 val_acc= 0.68613 time= 0.16200
Epoch: 0005 train_loss= 1.52081 train_acc= 0.70529 val_loss= 1.39425 val_acc= 0.68978 time= 0.17499
Epoch: 0006 train_loss= 1.36317 train_acc= 0.70630 val_loss= 1.29168 val_acc= 0.68431 time= 0.15700
Epoch: 0007 train_loss= 1.24776 train_acc= 0.70468 val_loss= 1.21522 val_acc= 0.68248 time= 0.15717
Epoch: 0008 train_loss= 1.16875 train_acc= 0.70569 val_loss= 1.14302 val_acc= 0.69343 time= 0.15929
Epoch: 0009 train_loss= 1.09681 train_acc= 0.71035 val_loss= 1.06415 val_acc= 0.73358 time= 0.15735
Epoch: 0010 train_loss= 1.01507 train_acc= 0.73891 val_loss= 0.97882 val_acc= 0.75730 time= 0.15601
Epoch: 0011 train_loss= 0.93719 train_acc= 0.76950 val_loss= 0.89458 val_acc= 0.76095 time= 0.19300
Epoch: 0012 train_loss= 0.85431 train_acc= 0.78469 val_loss= 0.81892 val_acc= 0.75912 time= 0.16015
Epoch: 0013 train_loss= 0.78300 train_acc= 0.78732 val_loss= 0.75618 val_acc= 0.75912 time= 0.15697
Epoch: 0014 train_loss= 0.72005 train_acc= 0.78590 val_loss= 0.70720 val_acc= 0.76642 time= 0.15804
Epoch: 0015 train_loss= 0.67212 train_acc= 0.78732 val_loss= 0.66956 val_acc= 0.78102 time= 0.16101
Epoch: 0016 train_loss= 0.63426 train_acc= 0.80393 val_loss= 0.63940 val_acc= 0.80292 time= 0.15607
Epoch: 0017 train_loss= 0.60136 train_acc= 0.82479 val_loss= 0.61276 val_acc= 0.83759 time= 0.16500
Epoch: 0018 train_loss= 0.57634 train_acc= 0.84869 val_loss= 0.58698 val_acc= 0.83942 time= 0.15652
Epoch: 0019 train_loss= 0.54812 train_acc= 0.86895 val_loss= 0.56086 val_acc= 0.85401 time= 0.15995
Epoch: 0020 train_loss= 0.51863 train_acc= 0.87806 val_loss= 0.53457 val_acc= 0.85766 time= 0.15803
Epoch: 0021 train_loss= 0.49238 train_acc= 0.88515 val_loss= 0.50900 val_acc= 0.86131 time= 0.15900
Epoch: 0022 train_loss= 0.46398 train_acc= 0.89184 val_loss= 0.48495 val_acc= 0.86679 time= 0.15915
Epoch: 0023 train_loss= 0.43827 train_acc= 0.89467 val_loss= 0.46269 val_acc= 0.87226 time= 0.18405
Epoch: 0024 train_loss= 0.41342 train_acc= 0.90055 val_loss= 0.44219 val_acc= 0.88686 time= 0.15795
Epoch: 0025 train_loss= 0.38879 train_acc= 0.90277 val_loss= 0.42309 val_acc= 0.89051 time= 0.15800
Epoch: 0026 train_loss= 0.37023 train_acc= 0.90602 val_loss= 0.40505 val_acc= 0.89599 time= 0.16100
Epoch: 0027 train_loss= 0.35117 train_acc= 0.90865 val_loss= 0.38780 val_acc= 0.89964 time= 0.15979
Epoch: 0028 train_loss= 0.33125 train_acc= 0.91432 val_loss= 0.37115 val_acc= 0.90511 time= 0.15800
Epoch: 0029 train_loss= 0.31509 train_acc= 0.92242 val_loss= 0.35510 val_acc= 0.90876 time= 0.19304
Epoch: 0030 train_loss= 0.29457 train_acc= 0.92688 val_loss= 0.33972 val_acc= 0.91241 time= 0.15696
Epoch: 0031 train_loss= 0.28054 train_acc= 0.93133 val_loss= 0.32517 val_acc= 0.91606 time= 0.15622
Epoch: 0032 train_loss= 0.26413 train_acc= 0.93214 val_loss= 0.31161 val_acc= 0.91971 time= 0.15708
Epoch: 0033 train_loss= 0.24535 train_acc= 0.93842 val_loss= 0.29890 val_acc= 0.92153 time= 0.15697
Epoch: 0034 train_loss= 0.23527 train_acc= 0.94349 val_loss= 0.28693 val_acc= 0.92336 time= 0.16000
Epoch: 0035 train_loss= 0.22177 train_acc= 0.94592 val_loss= 0.27576 val_acc= 0.92336 time= 0.18003
Epoch: 0036 train_loss= 0.21032 train_acc= 0.95037 val_loss= 0.26505 val_acc= 0.92518 time= 0.15800
Epoch: 0037 train_loss= 0.19476 train_acc= 0.95301 val_loss= 0.25485 val_acc= 0.92883 time= 0.15805
Epoch: 0038 train_loss= 0.18373 train_acc= 0.95564 val_loss= 0.24505 val_acc= 0.93066 time= 0.15606
Epoch: 0039 train_loss= 0.17517 train_acc= 0.95645 val_loss= 0.23589 val_acc= 0.93248 time= 0.15900
Epoch: 0040 train_loss= 0.16168 train_acc= 0.95969 val_loss= 0.22730 val_acc= 0.93248 time= 0.15600
Epoch: 0041 train_loss= 0.15402 train_acc= 0.96172 val_loss= 0.21951 val_acc= 0.93248 time= 0.16297
Epoch: 0042 train_loss= 0.14376 train_acc= 0.96172 val_loss= 0.21238 val_acc= 0.93796 time= 0.18900
Epoch: 0043 train_loss= 0.13400 train_acc= 0.96334 val_loss= 0.20605 val_acc= 0.93796 time= 0.15800
Epoch: 0044 train_loss= 0.12530 train_acc= 0.96800 val_loss= 0.20032 val_acc= 0.93796 time= 0.15500
Epoch: 0045 train_loss= 0.11833 train_acc= 0.96941 val_loss= 0.19507 val_acc= 0.93978 time= 0.15803
Epoch: 0046 train_loss= 0.11198 train_acc= 0.97043 val_loss= 0.19029 val_acc= 0.94161 time= 0.15800
Epoch: 0047 train_loss= 0.10771 train_acc= 0.97063 val_loss= 0.18602 val_acc= 0.94161 time= 0.15897
Epoch: 0048 train_loss= 0.10261 train_acc= 0.97286 val_loss= 0.18214 val_acc= 0.94343 time= 0.19700
Epoch: 0049 train_loss= 0.09687 train_acc= 0.97509 val_loss= 0.17839 val_acc= 0.94526 time= 0.15915
Epoch: 0050 train_loss= 0.09162 train_acc= 0.97711 val_loss= 0.17490 val_acc= 0.94526 time= 0.15797
Epoch: 0051 train_loss= 0.08717 train_acc= 0.97792 val_loss= 0.17170 val_acc= 0.94708 time= 0.15800
Epoch: 0052 train_loss= 0.08205 train_acc= 0.97934 val_loss= 0.16869 val_acc= 0.94891 time= 0.15633
Epoch: 0053 train_loss= 0.07945 train_acc= 0.97772 val_loss= 0.16590 val_acc= 0.94708 time= 0.15734
Epoch: 0054 train_loss= 0.07329 train_acc= 0.98157 val_loss= 0.16333 val_acc= 0.94891 time= 0.18900
Epoch: 0055 train_loss= 0.07167 train_acc= 0.98177 val_loss= 0.16121 val_acc= 0.95073 time= 0.15997
Epoch: 0056 train_loss= 0.06628 train_acc= 0.98278 val_loss= 0.15952 val_acc= 0.95073 time= 0.16069
Epoch: 0057 train_loss= 0.06551 train_acc= 0.98339 val_loss= 0.15838 val_acc= 0.95073 time= 0.15600
Epoch: 0058 train_loss= 0.06112 train_acc= 0.98461 val_loss= 0.15746 val_acc= 0.95255 time= 0.15805
Epoch: 0059 train_loss= 0.05847 train_acc= 0.98602 val_loss= 0.15694 val_acc= 0.95255 time= 0.15699
Epoch: 0060 train_loss= 0.05569 train_acc= 0.98663 val_loss= 0.15584 val_acc= 0.95255 time= 0.15953
Epoch: 0061 train_loss= 0.05273 train_acc= 0.98764 val_loss= 0.15447 val_acc= 0.95255 time= 0.15900
Epoch: 0062 train_loss= 0.05085 train_acc= 0.98785 val_loss= 0.15273 val_acc= 0.95438 time= 0.15800
Epoch: 0063 train_loss= 0.05053 train_acc= 0.98764 val_loss= 0.15142 val_acc= 0.95438 time= 0.16000
Epoch: 0064 train_loss= 0.04656 train_acc= 0.98866 val_loss= 0.15056 val_acc= 0.95438 time= 0.15701
Epoch: 0065 train_loss= 0.04590 train_acc= 0.98866 val_loss= 0.14970 val_acc= 0.95438 time= 0.15700
Epoch: 0066 train_loss= 0.04285 train_acc= 0.99007 val_loss= 0.14925 val_acc= 0.95255 time= 0.19500
Epoch: 0067 train_loss= 0.04060 train_acc= 0.99028 val_loss= 0.14914 val_acc= 0.95255 time= 0.15899
Epoch: 0068 train_loss= 0.04045 train_acc= 0.99028 val_loss= 0.14950 val_acc= 0.95255 time= 0.15701
Epoch: 0069 train_loss= 0.03837 train_acc= 0.99109 val_loss= 0.14995 val_acc= 0.95438 time= 0.15767
Epoch: 0070 train_loss= 0.03651 train_acc= 0.99109 val_loss= 0.15034 val_acc= 0.95620 time= 0.16000
Epoch: 0071 train_loss= 0.03624 train_acc= 0.99271 val_loss= 0.15001 val_acc= 0.95438 time= 0.16000
Epoch: 0072 train_loss= 0.03267 train_acc= 0.99332 val_loss= 0.14898 val_acc= 0.95438 time= 0.19200
Epoch: 0073 train_loss= 0.03257 train_acc= 0.99291 val_loss= 0.14803 val_acc= 0.95438 time= 0.15605
Epoch: 0074 train_loss= 0.03212 train_acc= 0.99230 val_loss= 0.14810 val_acc= 0.95438 time= 0.15995
Epoch: 0075 train_loss= 0.03225 train_acc= 0.99271 val_loss= 0.14886 val_acc= 0.95438 time= 0.15604
Epoch: 0076 train_loss= 0.02816 train_acc= 0.99514 val_loss= 0.14988 val_acc= 0.95255 time= 0.15796
Early stopping...
Optimization Finished!
Test set results: cost= 0.10861 accuracy= 0.97305 time= 0.06700
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9444    0.9835    0.9636       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     0.9000    0.9000    0.9000        10
           4     1.0000    0.7500    0.8571        36
           5     0.8902    0.9012    0.8957        81
           6     0.9186    0.9080    0.9133        87
           7     0.9854    0.9713    0.9783       696

    accuracy                         0.9730      2189
   macro avg     0.9418    0.9224    0.9297      2189
weighted avg     0.9734    0.9730    0.9728      2189

Macro average Test Precision, Recall and F1-Score...
(0.9418415277609404, 0.9223798838918551, 0.9296899937579566, None)
Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)
embeddings:
7688 5485 2189
[[ 0.14407215  0.13942471  0.19723271 ...  0.16390997  0.13940704
   0.15120512]
 [ 0.02960352  0.11370141  0.01352296 ...  0.03277731  0.04408517
   0.02874117]
 [ 0.12024891  0.09413502  0.00992804 ...  0.14323655  0.15550655
   0.11293425]
 ...
 [ 0.20110798  0.0374834   0.08934268 ...  0.19261438  0.20624816
   0.17010693]
 [ 0.00466245  0.09851141 -0.00933486 ...  0.01511361  0.02425707
   0.01162459]
 [ 0.1347633   0.04310986  0.02985663 ...  0.14694946  0.17787024
   0.13314155]]

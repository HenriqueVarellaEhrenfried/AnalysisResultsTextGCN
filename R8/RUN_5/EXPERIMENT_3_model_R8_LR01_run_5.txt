(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07948 train_acc= 0.08750 val_loss= 1.63558 val_acc= 0.76460 time= 0.39157
Epoch: 0002 train_loss= 1.62138 train_acc= 0.78307 val_loss= 1.28879 val_acc= 0.76095 time= 0.14700
Epoch: 0003 train_loss= 1.24503 train_acc= 0.78023 val_loss= 1.07825 val_acc= 0.62956 time= 0.12300
Epoch: 0004 train_loss= 1.02384 train_acc= 0.64918 val_loss= 0.81653 val_acc= 0.75730 time= 0.12300
Epoch: 0005 train_loss= 0.77510 train_acc= 0.78610 val_loss= 0.69470 val_acc= 0.75182 time= 0.12297
Epoch: 0006 train_loss= 0.65308 train_acc= 0.77233 val_loss= 0.63316 val_acc= 0.76825 time= 0.12800
Epoch: 0007 train_loss= 0.58371 train_acc= 0.79117 val_loss= 0.58749 val_acc= 0.79380 time= 0.12303
Epoch: 0008 train_loss= 0.53104 train_acc= 0.82824 val_loss= 0.54274 val_acc= 0.83212 time= 0.12397
Epoch: 0009 train_loss= 0.47952 train_acc= 0.86652 val_loss= 0.49992 val_acc= 0.84672 time= 0.12445
Epoch: 0010 train_loss= 0.43264 train_acc= 0.88171 val_loss= 0.46143 val_acc= 0.85949 time= 0.15699
Epoch: 0011 train_loss= 0.38042 train_acc= 0.89508 val_loss= 0.42729 val_acc= 0.88139 time= 0.12500
Epoch: 0012 train_loss= 0.34590 train_acc= 0.89913 val_loss= 0.39621 val_acc= 0.89599 time= 0.12415
Epoch: 0013 train_loss= 0.30996 train_acc= 0.90784 val_loss= 0.36732 val_acc= 0.89781 time= 0.12315
Epoch: 0014 train_loss= 0.27016 train_acc= 0.91878 val_loss= 0.34183 val_acc= 0.91606 time= 0.12370
Epoch: 0015 train_loss= 0.24922 train_acc= 0.93397 val_loss= 0.32245 val_acc= 0.92336 time= 0.12600
Epoch: 0016 train_loss= 0.21285 train_acc= 0.94794 val_loss= 0.30686 val_acc= 0.92153 time= 0.12400
Epoch: 0017 train_loss= 0.18700 train_acc= 0.95118 val_loss= 0.29344 val_acc= 0.92701 time= 0.12500
Epoch: 0018 train_loss= 0.16228 train_acc= 0.95483 val_loss= 0.28106 val_acc= 0.93248 time= 0.16229
Epoch: 0019 train_loss= 0.14758 train_acc= 0.95584 val_loss= 0.26728 val_acc= 0.93796 time= 0.12197
Epoch: 0020 train_loss= 0.13054 train_acc= 0.96476 val_loss= 0.25200 val_acc= 0.93978 time= 0.12203
Epoch: 0021 train_loss= 0.11901 train_acc= 0.96293 val_loss= 0.24167 val_acc= 0.94526 time= 0.12400
Epoch: 0022 train_loss= 0.10725 train_acc= 0.96881 val_loss= 0.23842 val_acc= 0.94343 time= 0.12300
Epoch: 0023 train_loss= 0.09532 train_acc= 0.97185 val_loss= 0.24208 val_acc= 0.94343 time= 0.12397
Epoch: 0024 train_loss= 0.08905 train_acc= 0.97063 val_loss= 0.24802 val_acc= 0.94161 time= 0.12600
Epoch: 0025 train_loss= 0.07675 train_acc= 0.97711 val_loss= 0.24906 val_acc= 0.94708 time= 0.12500
Epoch: 0026 train_loss= 0.06925 train_acc= 0.97731 val_loss= 0.24680 val_acc= 0.94891 time= 0.16302
Epoch: 0027 train_loss= 0.06501 train_acc= 0.98157 val_loss= 0.24263 val_acc= 0.95255 time= 0.12313
Epoch: 0028 train_loss= 0.05918 train_acc= 0.98238 val_loss= 0.23765 val_acc= 0.94708 time= 0.12208
Epoch: 0029 train_loss= 0.05125 train_acc= 0.98724 val_loss= 0.23555 val_acc= 0.95438 time= 0.12197
Epoch: 0030 train_loss= 0.04512 train_acc= 0.98663 val_loss= 0.23360 val_acc= 0.95255 time= 0.12300
Epoch: 0031 train_loss= 0.04341 train_acc= 0.98663 val_loss= 0.23299 val_acc= 0.95073 time= 0.12400
Epoch: 0032 train_loss= 0.04059 train_acc= 0.98744 val_loss= 0.24108 val_acc= 0.95073 time= 0.12498
Early stopping...
Optimization Finished!
Test set results: cost= 0.15508 accuracy= 0.96528 time= 0.05700
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9200    0.9504    0.9350       121
           1     0.8506    0.9867    0.9136        75
           2     0.9808    0.9908    0.9858      1083
           3     0.7500    0.6000    0.6667        10
           4     0.9310    0.7500    0.8308        36
           5     0.8315    0.9136    0.8706        81
           6     0.9487    0.8506    0.8970        87
           7     0.9867    0.9626    0.9745       696

    accuracy                         0.9653      2189
   macro avg     0.8999    0.8756    0.8842      2189
weighted avg     0.9662    0.9653    0.9651      2189

Macro average Test Precision, Recall and F1-Score...
(0.8999171774244306, 0.8755806146479621, 0.884229887654894, None)
Micro average Test Precision, Recall and F1-Score...
(0.9652809502055734, 0.9652809502055734, 0.9652809502055734, None)
embeddings:
7688 5485 2189
[[ 0.12501441 -0.06559166  0.751881   ... -0.03897511  0.09522495
   0.03520275]
 [-0.1360294  -0.14441715  0.43026772 ... -0.20998219  0.12872356
  -0.15170622]
 [ 0.07317169  0.40254304  0.16700791 ...  0.3621151   0.66231245
   0.21178514]
 ...
 [ 0.30342406  0.47792763  0.43664697 ...  0.4155118   0.68044484
   0.28845838]
 [-0.26964945 -0.26772475  0.52128935 ... -0.28708446  0.10589338
  -0.30264258]
 [ 0.17893918  0.37789866  0.16356649 ...  0.3399712   0.60197717
   0.21627083]]

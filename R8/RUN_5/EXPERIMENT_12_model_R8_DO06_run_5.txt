(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07956 train_acc= 0.09439 val_loss= 2.02926 val_acc= 0.75182 time= 0.38803
Epoch: 0002 train_loss= 2.02707 train_acc= 0.77618 val_loss= 1.94525 val_acc= 0.75000 time= 0.12700
Epoch: 0003 train_loss= 1.94175 train_acc= 0.77091 val_loss= 1.83127 val_acc= 0.75000 time= 0.12400
Epoch: 0004 train_loss= 1.82559 train_acc= 0.76990 val_loss= 1.69766 val_acc= 0.75365 time= 0.14700
Epoch: 0005 train_loss= 1.69376 train_acc= 0.77638 val_loss= 1.56245 val_acc= 0.75730 time= 0.12200
Epoch: 0006 train_loss= 1.54544 train_acc= 0.78266 val_loss= 1.44440 val_acc= 0.75912 time= 0.12497
Epoch: 0007 train_loss= 1.41194 train_acc= 0.78286 val_loss= 1.35136 val_acc= 0.75730 time= 0.12607
Epoch: 0008 train_loss= 1.31536 train_acc= 0.76585 val_loss= 1.27850 val_acc= 0.74088 time= 0.12403
Epoch: 0009 train_loss= 1.24081 train_acc= 0.75917 val_loss= 1.21520 val_acc= 0.72628 time= 0.12600
Epoch: 0010 train_loss= 1.17156 train_acc= 0.72797 val_loss= 1.15362 val_acc= 0.71533 time= 0.12385
Epoch: 0011 train_loss= 1.10254 train_acc= 0.72331 val_loss= 1.08908 val_acc= 0.72810 time= 0.12400
Epoch: 0012 train_loss= 1.04635 train_acc= 0.74053 val_loss= 1.02116 val_acc= 0.74088 time= 0.17100
Epoch: 0013 train_loss= 0.98935 train_acc= 0.75633 val_loss= 0.95214 val_acc= 0.75547 time= 0.12406
Epoch: 0014 train_loss= 0.91391 train_acc= 0.77679 val_loss= 0.88569 val_acc= 0.76277 time= 0.12400
Epoch: 0015 train_loss= 0.84652 train_acc= 0.78246 val_loss= 0.82564 val_acc= 0.76095 time= 0.12497
Epoch: 0016 train_loss= 0.79081 train_acc= 0.78995 val_loss= 0.77446 val_acc= 0.75547 time= 0.12560
Epoch: 0017 train_loss= 0.73631 train_acc= 0.78914 val_loss= 0.73228 val_acc= 0.76642 time= 0.12403
Epoch: 0018 train_loss= 0.69471 train_acc= 0.78914 val_loss= 0.69764 val_acc= 0.76825 time= 0.12397
Epoch: 0019 train_loss= 0.66051 train_acc= 0.79542 val_loss= 0.66850 val_acc= 0.78650 time= 0.15403
Epoch: 0020 train_loss= 0.63082 train_acc= 0.80859 val_loss= 0.64263 val_acc= 0.79927 time= 0.13000
Epoch: 0021 train_loss= 0.60363 train_acc= 0.82216 val_loss= 0.61843 val_acc= 0.82847 time= 0.12299
Epoch: 0022 train_loss= 0.58189 train_acc= 0.84687 val_loss= 0.59493 val_acc= 0.83759 time= 0.12447
Epoch: 0023 train_loss= 0.55256 train_acc= 0.86105 val_loss= 0.57169 val_acc= 0.84672 time= 0.12313
Epoch: 0024 train_loss= 0.53332 train_acc= 0.86733 val_loss= 0.54889 val_acc= 0.85219 time= 0.12500
Epoch: 0025 train_loss= 0.50576 train_acc= 0.88049 val_loss= 0.52688 val_acc= 0.86314 time= 0.12600
Epoch: 0026 train_loss= 0.48524 train_acc= 0.88961 val_loss= 0.50593 val_acc= 0.85949 time= 0.12500
Epoch: 0027 train_loss= 0.46235 train_acc= 0.89204 val_loss= 0.48631 val_acc= 0.86131 time= 0.16300
Epoch: 0028 train_loss= 0.43899 train_acc= 0.89650 val_loss= 0.46789 val_acc= 0.86496 time= 0.12403
Epoch: 0029 train_loss= 0.41778 train_acc= 0.89731 val_loss= 0.45063 val_acc= 0.87409 time= 0.12201
Epoch: 0030 train_loss= 0.40249 train_acc= 0.90196 val_loss= 0.43422 val_acc= 0.88321 time= 0.12301
Epoch: 0031 train_loss= 0.37889 train_acc= 0.90500 val_loss= 0.41843 val_acc= 0.88869 time= 0.12295
Epoch: 0032 train_loss= 0.37042 train_acc= 0.90865 val_loss= 0.40302 val_acc= 0.89599 time= 0.12413
Epoch: 0033 train_loss= 0.34749 train_acc= 0.91716 val_loss= 0.38807 val_acc= 0.90146 time= 0.12297
Epoch: 0034 train_loss= 0.33484 train_acc= 0.91574 val_loss= 0.37356 val_acc= 0.90511 time= 0.12800
Epoch: 0035 train_loss= 0.31615 train_acc= 0.92445 val_loss= 0.35952 val_acc= 0.91423 time= 0.15104
Epoch: 0036 train_loss= 0.30250 train_acc= 0.92708 val_loss= 0.34595 val_acc= 0.91423 time= 0.12196
Epoch: 0037 train_loss= 0.28789 train_acc= 0.92951 val_loss= 0.33279 val_acc= 0.91606 time= 0.12504
Epoch: 0038 train_loss= 0.27364 train_acc= 0.93863 val_loss= 0.32033 val_acc= 0.92153 time= 0.12299
Epoch: 0039 train_loss= 0.25734 train_acc= 0.94045 val_loss= 0.30853 val_acc= 0.92701 time= 0.12407
Epoch: 0040 train_loss= 0.24956 train_acc= 0.94207 val_loss= 0.29720 val_acc= 0.92701 time= 0.12300
Epoch: 0041 train_loss= 0.24070 train_acc= 0.94470 val_loss= 0.28664 val_acc= 0.92518 time= 0.12400
Epoch: 0042 train_loss= 0.22427 train_acc= 0.94835 val_loss= 0.27665 val_acc= 0.93066 time= 0.12500
Epoch: 0043 train_loss= 0.21499 train_acc= 0.95260 val_loss= 0.26724 val_acc= 0.92883 time= 0.17297
Epoch: 0044 train_loss= 0.20355 train_acc= 0.95260 val_loss= 0.25831 val_acc= 0.93066 time= 0.12403
Epoch: 0045 train_loss= 0.19012 train_acc= 0.95402 val_loss= 0.24990 val_acc= 0.92883 time= 0.12400
Epoch: 0046 train_loss= 0.18527 train_acc= 0.95665 val_loss= 0.24190 val_acc= 0.93066 time= 0.12307
Epoch: 0047 train_loss= 0.17593 train_acc= 0.95686 val_loss= 0.23428 val_acc= 0.93066 time= 0.12303
Epoch: 0048 train_loss= 0.16532 train_acc= 0.95908 val_loss= 0.22718 val_acc= 0.93066 time= 0.12200
Epoch: 0049 train_loss= 0.15840 train_acc= 0.95949 val_loss= 0.22044 val_acc= 0.93248 time= 0.12415
Epoch: 0050 train_loss= 0.15164 train_acc= 0.96273 val_loss= 0.21414 val_acc= 0.93431 time= 0.14995
Epoch: 0051 train_loss= 0.14304 train_acc= 0.96334 val_loss= 0.20825 val_acc= 0.93431 time= 0.14000
Epoch: 0052 train_loss= 0.13749 train_acc= 0.96800 val_loss= 0.20282 val_acc= 0.93796 time= 0.12559
Epoch: 0053 train_loss= 0.13096 train_acc= 0.96597 val_loss= 0.19776 val_acc= 0.93796 time= 0.12504
Epoch: 0054 train_loss= 0.12511 train_acc= 0.96516 val_loss= 0.19257 val_acc= 0.94161 time= 0.12401
Epoch: 0055 train_loss= 0.12019 train_acc= 0.96962 val_loss= 0.18797 val_acc= 0.94526 time= 0.12300
Epoch: 0056 train_loss= 0.11475 train_acc= 0.97043 val_loss= 0.18410 val_acc= 0.94708 time= 0.12295
Epoch: 0057 train_loss= 0.10996 train_acc= 0.97266 val_loss= 0.18069 val_acc= 0.94526 time= 0.12400
Epoch: 0058 train_loss= 0.10305 train_acc= 0.97569 val_loss= 0.17773 val_acc= 0.94708 time= 0.16900
Epoch: 0059 train_loss= 0.10349 train_acc= 0.97407 val_loss= 0.17503 val_acc= 0.94526 time= 0.12429
Epoch: 0060 train_loss= 0.09781 train_acc= 0.97569 val_loss= 0.17252 val_acc= 0.94708 time= 0.12399
Epoch: 0061 train_loss= 0.09238 train_acc= 0.97873 val_loss= 0.17016 val_acc= 0.94891 time= 0.12697
Epoch: 0062 train_loss= 0.08695 train_acc= 0.98055 val_loss= 0.16781 val_acc= 0.94891 time= 0.12503
Epoch: 0063 train_loss= 0.08409 train_acc= 0.98035 val_loss= 0.16539 val_acc= 0.95438 time= 0.12300
Epoch: 0064 train_loss= 0.08183 train_acc= 0.97995 val_loss= 0.16299 val_acc= 0.95438 time= 0.12329
Epoch: 0065 train_loss= 0.07891 train_acc= 0.98116 val_loss= 0.16075 val_acc= 0.95438 time= 0.12297
Epoch: 0066 train_loss= 0.07451 train_acc= 0.98015 val_loss= 0.15857 val_acc= 0.95255 time= 0.15103
Epoch: 0067 train_loss= 0.07449 train_acc= 0.98258 val_loss= 0.15698 val_acc= 0.95255 time= 0.12601
Epoch: 0068 train_loss= 0.07079 train_acc= 0.98218 val_loss= 0.15555 val_acc= 0.95438 time= 0.12496
Epoch: 0069 train_loss= 0.06914 train_acc= 0.98339 val_loss= 0.15433 val_acc= 0.95620 time= 0.12408
Epoch: 0070 train_loss= 0.06667 train_acc= 0.98582 val_loss= 0.15308 val_acc= 0.95620 time= 0.12697
Epoch: 0071 train_loss= 0.06480 train_acc= 0.98440 val_loss= 0.15219 val_acc= 0.95438 time= 0.12500
Epoch: 0072 train_loss= 0.06083 train_acc= 0.98380 val_loss= 0.15120 val_acc= 0.95255 time= 0.12500
Epoch: 0073 train_loss= 0.06087 train_acc= 0.98602 val_loss= 0.15020 val_acc= 0.95255 time= 0.12603
Epoch: 0074 train_loss= 0.06061 train_acc= 0.98643 val_loss= 0.14923 val_acc= 0.95438 time= 0.16600
Epoch: 0075 train_loss= 0.05719 train_acc= 0.98643 val_loss= 0.14813 val_acc= 0.95438 time= 0.12411
Epoch: 0076 train_loss= 0.05656 train_acc= 0.98764 val_loss= 0.14714 val_acc= 0.95438 time= 0.12400
Epoch: 0077 train_loss= 0.05226 train_acc= 0.98785 val_loss= 0.14631 val_acc= 0.95438 time= 0.12299
Epoch: 0078 train_loss= 0.05116 train_acc= 0.98886 val_loss= 0.14514 val_acc= 0.95438 time= 0.12300
Epoch: 0079 train_loss= 0.05071 train_acc= 0.99028 val_loss= 0.14443 val_acc= 0.95438 time= 0.12588
Epoch: 0080 train_loss= 0.04938 train_acc= 0.98987 val_loss= 0.14402 val_acc= 0.95438 time= 0.12504
Epoch: 0081 train_loss= 0.04634 train_acc= 0.98845 val_loss= 0.14349 val_acc= 0.95438 time= 0.16700
Epoch: 0082 train_loss= 0.04743 train_acc= 0.98866 val_loss= 0.14338 val_acc= 0.95438 time= 0.12401
Epoch: 0083 train_loss= 0.04432 train_acc= 0.99028 val_loss= 0.14315 val_acc= 0.95438 time= 0.12499
Epoch: 0084 train_loss= 0.04407 train_acc= 0.99048 val_loss= 0.14291 val_acc= 0.95620 time= 0.12400
Epoch: 0085 train_loss= 0.04382 train_acc= 0.99028 val_loss= 0.14286 val_acc= 0.95438 time= 0.12400
Epoch: 0086 train_loss= 0.04011 train_acc= 0.99210 val_loss= 0.14294 val_acc= 0.95438 time= 0.12513
Epoch: 0087 train_loss= 0.04045 train_acc= 0.99109 val_loss= 0.14326 val_acc= 0.95620 time= 0.12300
Epoch: 0088 train_loss= 0.03931 train_acc= 0.99190 val_loss= 0.14344 val_acc= 0.95803 time= 0.12700
Epoch: 0089 train_loss= 0.03920 train_acc= 0.99129 val_loss= 0.14350 val_acc= 0.95803 time= 0.15200
Early stopping...
Optimization Finished!
Test set results: cost= 0.10853 accuracy= 0.97305 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9370    0.9835    0.9597       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.7500    0.8571        36
           5     0.8902    0.9012    0.8957        81
           6     0.9176    0.8966    0.9070        87
           7     0.9854    0.9713    0.9783       696

    accuracy                         0.9730      2189
   macro avg     0.9533    0.9334    0.9409      2189
weighted avg     0.9734    0.9730    0.9728      2189

Macro average Test Precision, Recall and F1-Score...
(0.9532922574149458, 0.9334431022826596, 0.9409145703600299, None)
Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)
embeddings:
7688 5485 2189
[[0.05265747 0.21926151 0.0760401  ... 0.10503292 0.07392295 0.05881701]
 [0.15274617 0.06085065 0.07626145 ... 0.05915811 0.21398236 0.14889835]
 [0.43028677 0.18375409 0.3759038  ... 0.27519017 0.49407783 0.47425774]
 ...
 [0.38950914 0.24867344 0.35334465 ... 0.3223623  0.41848385 0.39740652]
 [0.23220478 0.0332621  0.10718364 ... 0.07605563 0.2149403  0.26137474]
 [0.33861426 0.19285627 0.28093508 ... 0.24872282 0.3766799  0.36399883]]

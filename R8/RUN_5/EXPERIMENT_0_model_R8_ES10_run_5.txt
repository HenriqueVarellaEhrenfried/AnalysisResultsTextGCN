(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07915 train_acc= 0.45270 val_loss= 2.01439 val_acc= 0.66241 time= 0.38798
Epoch: 0002 train_loss= 2.01173 train_acc= 0.68463 val_loss= 1.91096 val_acc= 0.61679 time= 0.13103
Epoch: 0003 train_loss= 1.90423 train_acc= 0.64027 val_loss= 1.77513 val_acc= 0.58212 time= 0.12707
Epoch: 0004 train_loss= 1.76265 train_acc= 0.60948 val_loss= 1.62320 val_acc= 0.55474 time= 0.15400
Epoch: 0005 train_loss= 1.60154 train_acc= 0.59874 val_loss= 1.47983 val_acc= 0.56022 time= 0.12300
Epoch: 0006 train_loss= 1.45912 train_acc= 0.59206 val_loss= 1.36567 val_acc= 0.58212 time= 0.12327
Epoch: 0007 train_loss= 1.33065 train_acc= 0.61110 val_loss= 1.28303 val_acc= 0.61861 time= 0.12300
Epoch: 0008 train_loss= 1.23919 train_acc= 0.63338 val_loss= 1.21896 val_acc= 0.63504 time= 0.12300
Epoch: 0009 train_loss= 1.17397 train_acc= 0.65161 val_loss= 1.15884 val_acc= 0.66241 time= 0.12397
Epoch: 0010 train_loss= 1.11987 train_acc= 0.68706 val_loss= 1.09514 val_acc= 0.70620 time= 0.12703
Epoch: 0011 train_loss= 1.04853 train_acc= 0.71683 val_loss= 1.02659 val_acc= 0.73175 time= 0.14400
Epoch: 0012 train_loss= 0.98724 train_acc= 0.75430 val_loss= 0.95663 val_acc= 0.75730 time= 0.14100
Epoch: 0013 train_loss= 0.90937 train_acc= 0.77679 val_loss= 0.89005 val_acc= 0.75365 time= 0.12465
Epoch: 0014 train_loss= 0.84721 train_acc= 0.78529 val_loss= 0.83092 val_acc= 0.75365 time= 0.12300
Epoch: 0015 train_loss= 0.78991 train_acc= 0.78610 val_loss= 0.78146 val_acc= 0.75547 time= 0.12212
Epoch: 0016 train_loss= 0.74830 train_acc= 0.78246 val_loss= 0.74168 val_acc= 0.75730 time= 0.12508
Epoch: 0017 train_loss= 0.70972 train_acc= 0.78064 val_loss= 0.70987 val_acc= 0.76460 time= 0.12197
Epoch: 0018 train_loss= 0.67189 train_acc= 0.78651 val_loss= 0.68339 val_acc= 0.77007 time= 0.12803
Epoch: 0019 train_loss= 0.64570 train_acc= 0.79725 val_loss= 0.65975 val_acc= 0.78832 time= 0.17197
Epoch: 0020 train_loss= 0.62209 train_acc= 0.81203 val_loss= 0.63713 val_acc= 0.81022 time= 0.12400
Epoch: 0021 train_loss= 0.59743 train_acc= 0.83148 val_loss= 0.61465 val_acc= 0.83029 time= 0.12403
Epoch: 0022 train_loss= 0.57025 train_acc= 0.85011 val_loss= 0.59223 val_acc= 0.83942 time= 0.12400
Epoch: 0023 train_loss= 0.55265 train_acc= 0.85963 val_loss= 0.57014 val_acc= 0.84307 time= 0.12297
Epoch: 0024 train_loss= 0.52629 train_acc= 0.86976 val_loss= 0.54889 val_acc= 0.84489 time= 0.12203
Epoch: 0025 train_loss= 0.50433 train_acc= 0.87725 val_loss= 0.52880 val_acc= 0.84854 time= 0.12416
Epoch: 0026 train_loss= 0.48356 train_acc= 0.88110 val_loss= 0.50996 val_acc= 0.85766 time= 0.12495
Epoch: 0027 train_loss= 0.46069 train_acc= 0.88677 val_loss= 0.49235 val_acc= 0.86679 time= 0.15104
Epoch: 0028 train_loss= 0.44196 train_acc= 0.89163 val_loss= 0.47569 val_acc= 0.87956 time= 0.12521
Epoch: 0029 train_loss= 0.42502 train_acc= 0.89852 val_loss= 0.45962 val_acc= 0.88686 time= 0.12500
Epoch: 0030 train_loss= 0.40529 train_acc= 0.89953 val_loss= 0.44384 val_acc= 0.88869 time= 0.12342
Epoch: 0031 train_loss= 0.39274 train_acc= 0.90277 val_loss= 0.42821 val_acc= 0.89416 time= 0.12405
Epoch: 0032 train_loss= 0.36923 train_acc= 0.90946 val_loss= 0.41282 val_acc= 0.89964 time= 0.12265
Epoch: 0033 train_loss= 0.35892 train_acc= 0.91189 val_loss= 0.39773 val_acc= 0.90693 time= 0.12417
Epoch: 0034 train_loss= 0.34169 train_acc= 0.91594 val_loss= 0.38306 val_acc= 0.90876 time= 0.12397
Epoch: 0035 train_loss= 0.33009 train_acc= 0.92080 val_loss= 0.36891 val_acc= 0.91058 time= 0.16903
Epoch: 0036 train_loss= 0.31432 train_acc= 0.92546 val_loss= 0.35529 val_acc= 0.91423 time= 0.12300
Epoch: 0037 train_loss= 0.29893 train_acc= 0.93174 val_loss= 0.34230 val_acc= 0.91788 time= 0.12580
Epoch: 0038 train_loss= 0.27914 train_acc= 0.93478 val_loss= 0.32992 val_acc= 0.91788 time= 0.12500
Epoch: 0039 train_loss= 0.27350 train_acc= 0.93822 val_loss= 0.31819 val_acc= 0.92336 time= 0.12400
Epoch: 0040 train_loss= 0.25777 train_acc= 0.94450 val_loss= 0.30695 val_acc= 0.92518 time= 0.12200
Epoch: 0041 train_loss= 0.24521 train_acc= 0.94592 val_loss= 0.29631 val_acc= 0.92883 time= 0.12200
Epoch: 0042 train_loss= 0.23184 train_acc= 0.94916 val_loss= 0.28599 val_acc= 0.92883 time= 0.13797
Epoch: 0043 train_loss= 0.22254 train_acc= 0.95118 val_loss= 0.27618 val_acc= 0.92883 time= 0.14703
Epoch: 0044 train_loss= 0.21189 train_acc= 0.95402 val_loss= 0.26694 val_acc= 0.92883 time= 0.12300
Epoch: 0045 train_loss= 0.19950 train_acc= 0.95503 val_loss= 0.25837 val_acc= 0.93066 time= 0.12200
Epoch: 0046 train_loss= 0.18971 train_acc= 0.95827 val_loss= 0.25034 val_acc= 0.93248 time= 0.12500
Epoch: 0047 train_loss= 0.18215 train_acc= 0.95807 val_loss= 0.24293 val_acc= 0.93248 time= 0.12500
Epoch: 0048 train_loss= 0.16765 train_acc= 0.96172 val_loss= 0.23586 val_acc= 0.93248 time= 0.12399
Epoch: 0049 train_loss= 0.16285 train_acc= 0.96131 val_loss= 0.22902 val_acc= 0.93066 time= 0.12500
Epoch: 0050 train_loss= 0.15746 train_acc= 0.96273 val_loss= 0.22245 val_acc= 0.93613 time= 0.16900
Epoch: 0051 train_loss= 0.14612 train_acc= 0.96395 val_loss= 0.21613 val_acc= 0.93613 time= 0.12608
Epoch: 0052 train_loss= 0.14395 train_acc= 0.96435 val_loss= 0.20979 val_acc= 0.93978 time= 0.12324
Epoch: 0053 train_loss= 0.13625 train_acc= 0.96941 val_loss= 0.20393 val_acc= 0.94161 time= 0.12300
Epoch: 0054 train_loss= 0.13202 train_acc= 0.96820 val_loss= 0.19862 val_acc= 0.94161 time= 0.12521
Epoch: 0055 train_loss= 0.12345 train_acc= 0.96455 val_loss= 0.19369 val_acc= 0.94343 time= 0.12617
Epoch: 0056 train_loss= 0.11998 train_acc= 0.97185 val_loss= 0.18922 val_acc= 0.93978 time= 0.12469
Epoch: 0057 train_loss= 0.11396 train_acc= 0.96921 val_loss= 0.18525 val_acc= 0.94891 time= 0.12458
Epoch: 0058 train_loss= 0.11046 train_acc= 0.97205 val_loss= 0.18170 val_acc= 0.94708 time= 0.15000
Epoch: 0059 train_loss= 0.10230 train_acc= 0.97610 val_loss= 0.17862 val_acc= 0.94526 time= 0.12500
Epoch: 0060 train_loss= 0.09986 train_acc= 0.97590 val_loss= 0.17582 val_acc= 0.94343 time= 0.12400
Epoch: 0061 train_loss= 0.09487 train_acc= 0.97812 val_loss= 0.17301 val_acc= 0.94526 time= 0.12300
Epoch: 0062 train_loss= 0.09236 train_acc= 0.97630 val_loss= 0.17042 val_acc= 0.94708 time= 0.12230
Epoch: 0063 train_loss= 0.08811 train_acc= 0.98035 val_loss= 0.16807 val_acc= 0.94708 time= 0.12401
Epoch: 0064 train_loss= 0.08595 train_acc= 0.98035 val_loss= 0.16590 val_acc= 0.94708 time= 0.12599
Epoch: 0065 train_loss= 0.08072 train_acc= 0.98116 val_loss= 0.16371 val_acc= 0.94708 time= 0.12500
Epoch: 0066 train_loss= 0.07881 train_acc= 0.98157 val_loss= 0.16164 val_acc= 0.95255 time= 0.16800
Epoch: 0067 train_loss= 0.07635 train_acc= 0.98238 val_loss= 0.15950 val_acc= 0.95438 time= 0.12204
Epoch: 0068 train_loss= 0.07499 train_acc= 0.98380 val_loss= 0.15722 val_acc= 0.95438 time= 0.12603
Epoch: 0069 train_loss= 0.07197 train_acc= 0.98380 val_loss= 0.15530 val_acc= 0.95438 time= 0.12301
Epoch: 0070 train_loss= 0.06963 train_acc= 0.98501 val_loss= 0.15369 val_acc= 0.95438 time= 0.12299
Epoch: 0071 train_loss= 0.06567 train_acc= 0.98562 val_loss= 0.15248 val_acc= 0.95438 time= 0.12308
Epoch: 0072 train_loss= 0.06599 train_acc= 0.98562 val_loss= 0.15152 val_acc= 0.95438 time= 0.12200
Epoch: 0073 train_loss= 0.06157 train_acc= 0.98643 val_loss= 0.15053 val_acc= 0.95438 time= 0.14752
Epoch: 0074 train_loss= 0.05942 train_acc= 0.98825 val_loss= 0.14992 val_acc= 0.95803 time= 0.14403
Epoch: 0075 train_loss= 0.05802 train_acc= 0.98866 val_loss= 0.14960 val_acc= 0.95438 time= 0.12300
Epoch: 0076 train_loss= 0.05646 train_acc= 0.98724 val_loss= 0.14921 val_acc= 0.95438 time= 0.12605
Epoch: 0077 train_loss= 0.05591 train_acc= 0.98805 val_loss= 0.14849 val_acc= 0.95438 time= 0.12311
Epoch: 0078 train_loss= 0.05435 train_acc= 0.98886 val_loss= 0.14811 val_acc= 0.95438 time= 0.12204
Epoch: 0079 train_loss= 0.05389 train_acc= 0.98785 val_loss= 0.14767 val_acc= 0.95438 time= 0.12396
Epoch: 0080 train_loss= 0.05105 train_acc= 0.98886 val_loss= 0.14666 val_acc= 0.95438 time= 0.12314
Epoch: 0081 train_loss= 0.05006 train_acc= 0.98906 val_loss= 0.14569 val_acc= 0.95620 time= 0.16412
Epoch: 0082 train_loss= 0.04806 train_acc= 0.99048 val_loss= 0.14461 val_acc= 0.95255 time= 0.12699
Epoch: 0083 train_loss= 0.04634 train_acc= 0.99149 val_loss= 0.14390 val_acc= 0.95255 time= 0.12500
Epoch: 0084 train_loss= 0.04462 train_acc= 0.99129 val_loss= 0.14346 val_acc= 0.95803 time= 0.12500
Epoch: 0085 train_loss= 0.04525 train_acc= 0.99028 val_loss= 0.14317 val_acc= 0.95438 time= 0.12500
Epoch: 0086 train_loss= 0.04102 train_acc= 0.99230 val_loss= 0.14307 val_acc= 0.95255 time= 0.12307
Epoch: 0087 train_loss= 0.04063 train_acc= 0.99068 val_loss= 0.14347 val_acc= 0.95255 time= 0.12254
Epoch: 0088 train_loss= 0.04026 train_acc= 0.99109 val_loss= 0.14412 val_acc= 0.95620 time= 0.12397
Epoch: 0089 train_loss= 0.04057 train_acc= 0.99129 val_loss= 0.14411 val_acc= 0.95620 time= 0.15103
Epoch: 0090 train_loss= 0.03767 train_acc= 0.99251 val_loss= 0.14377 val_acc= 0.95620 time= 0.12300
Epoch: 0091 train_loss= 0.03833 train_acc= 0.99251 val_loss= 0.14380 val_acc= 0.95438 time= 0.12574
Epoch: 0092 train_loss= 0.03577 train_acc= 0.99251 val_loss= 0.14343 val_acc= 0.95255 time= 0.12635
Epoch: 0093 train_loss= 0.03632 train_acc= 0.99271 val_loss= 0.14243 val_acc= 0.95438 time= 0.12400
Epoch: 0094 train_loss= 0.03472 train_acc= 0.99311 val_loss= 0.14188 val_acc= 0.95620 time= 0.12413
Epoch: 0095 train_loss= 0.03453 train_acc= 0.99271 val_loss= 0.14144 val_acc= 0.95620 time= 0.12317
Epoch: 0096 train_loss= 0.03135 train_acc= 0.99392 val_loss= 0.14121 val_acc= 0.95620 time= 0.12206
Epoch: 0097 train_loss= 0.03375 train_acc= 0.99251 val_loss= 0.14130 val_acc= 0.95620 time= 0.16800
Epoch: 0098 train_loss= 0.03225 train_acc= 0.99291 val_loss= 0.14173 val_acc= 0.95438 time= 0.12305
Epoch: 0099 train_loss= 0.03114 train_acc= 0.99311 val_loss= 0.14319 val_acc= 0.95803 time= 0.12405
Early stopping...
Optimization Finished!
Test set results: cost= 0.10756 accuracy= 0.97213 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9435    0.9669    0.9551       121
           1     0.9000    0.9600    0.9290        75
           2     0.9826    0.9917    0.9871      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9643    0.7500    0.8437        36
           5     0.9333    0.8642    0.8974        81
           6     0.9000    0.9310    0.9153        87
           7     0.9840    0.9727    0.9783       696

    accuracy                         0.9721      2189
   macro avg     0.9396    0.9296    0.9323      2189
weighted avg     0.9723    0.9721    0.9719      2189

Macro average Test Precision, Recall and F1-Score...
(0.9396108278914779, 0.9295706328126196, 0.9323014297936212, None)
Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)
embeddings:
7688 5485 2189
[[ 0.10988618  0.15702862  0.21782483 ... -0.05686492 -0.04738566
   0.1539379 ]
 [ 0.1005858   0.0131369   0.08291363 ... -0.04924718  0.0161869
   0.02788669]
 [ 0.37730417  0.08566988  0.2550212  ... -0.0627597  -0.04725546
   0.16168673]
 ...
 [ 0.38281015  0.15325391  0.29507133 ... -0.06109769 -0.05348876
   0.22532777]
 [ 0.09962839 -0.001199    0.08840985 ... -0.05976393  0.02050701
   0.02014431]
 [ 0.3016583   0.12447491  0.2367124  ... -0.05078851 -0.02023764
   0.16843985]]

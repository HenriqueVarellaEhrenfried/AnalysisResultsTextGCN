(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07952 train_acc= 0.05246 val_loss= 2.02974 val_acc= 0.72628 time= 0.40007
Epoch: 0002 train_loss= 2.02813 train_acc= 0.74175 val_loss= 1.94663 val_acc= 0.72810 time= 0.13102
Epoch: 0003 train_loss= 1.94360 train_acc= 0.74073 val_loss= 1.83222 val_acc= 0.72993 time= 0.13035
Epoch: 0004 train_loss= 1.82584 train_acc= 0.74559 val_loss= 1.69688 val_acc= 0.73358 time= 0.12503
Epoch: 0005 train_loss= 1.68814 train_acc= 0.75268 val_loss= 1.55910 val_acc= 0.74088 time= 0.12300
Epoch: 0006 train_loss= 1.53978 train_acc= 0.76544 val_loss= 1.43831 val_acc= 0.75365 time= 0.12300
Epoch: 0007 train_loss= 1.41402 train_acc= 0.77355 val_loss= 1.34202 val_acc= 0.75730 time= 0.12397
Epoch: 0008 train_loss= 1.31634 train_acc= 0.78529 val_loss= 1.26438 val_acc= 0.76095 time= 0.15911
Epoch: 0009 train_loss= 1.23050 train_acc= 0.78185 val_loss= 1.19630 val_acc= 0.74818 time= 0.12301
Epoch: 0010 train_loss= 1.15201 train_acc= 0.76241 val_loss= 1.13003 val_acc= 0.73723 time= 0.12510
Epoch: 0011 train_loss= 1.08991 train_acc= 0.75248 val_loss= 1.06125 val_acc= 0.73905 time= 0.12495
Epoch: 0012 train_loss= 1.01804 train_acc= 0.75329 val_loss= 0.98914 val_acc= 0.75182 time= 0.12797
Epoch: 0013 train_loss= 0.94839 train_acc= 0.76767 val_loss= 0.91654 val_acc= 0.76277 time= 0.12503
Epoch: 0014 train_loss= 0.87337 train_acc= 0.78286 val_loss= 0.84802 val_acc= 0.76825 time= 0.12197
Epoch: 0015 train_loss= 0.81042 train_acc= 0.78894 val_loss= 0.78793 val_acc= 0.76825 time= 0.12306
Epoch: 0016 train_loss= 0.75341 train_acc= 0.79157 val_loss= 0.73833 val_acc= 0.76277 time= 0.15094
Epoch: 0017 train_loss= 0.70674 train_acc= 0.79299 val_loss= 0.69860 val_acc= 0.77555 time= 0.12300
Epoch: 0018 train_loss= 0.66604 train_acc= 0.80575 val_loss= 0.66642 val_acc= 0.79745 time= 0.12300
Epoch: 0019 train_loss= 0.63707 train_acc= 0.82155 val_loss= 0.63892 val_acc= 0.81934 time= 0.12210
Epoch: 0020 train_loss= 0.60988 train_acc= 0.83836 val_loss= 0.61372 val_acc= 0.82482 time= 0.12410
Epoch: 0021 train_loss= 0.57965 train_acc= 0.85578 val_loss= 0.58948 val_acc= 0.83942 time= 0.12700
Epoch: 0022 train_loss= 0.55512 train_acc= 0.85882 val_loss= 0.56566 val_acc= 0.84854 time= 0.12500
Epoch: 0023 train_loss= 0.52924 train_acc= 0.87077 val_loss= 0.54242 val_acc= 0.85584 time= 0.12408
Epoch: 0024 train_loss= 0.50588 train_acc= 0.87482 val_loss= 0.52028 val_acc= 0.86131 time= 0.17004
Epoch: 0025 train_loss= 0.47931 train_acc= 0.87827 val_loss= 0.49959 val_acc= 0.86679 time= 0.12207
Epoch: 0026 train_loss= 0.45978 train_acc= 0.88211 val_loss= 0.48049 val_acc= 0.87226 time= 0.12297
Epoch: 0027 train_loss= 0.43665 train_acc= 0.88860 val_loss= 0.46286 val_acc= 0.88504 time= 0.12317
Epoch: 0028 train_loss= 0.42008 train_acc= 0.89143 val_loss= 0.44643 val_acc= 0.88686 time= 0.12395
Epoch: 0029 train_loss= 0.39995 train_acc= 0.89670 val_loss= 0.43085 val_acc= 0.89051 time= 0.12600
Epoch: 0030 train_loss= 0.38185 train_acc= 0.89994 val_loss= 0.41582 val_acc= 0.89234 time= 0.12600
Epoch: 0031 train_loss= 0.36433 train_acc= 0.90521 val_loss= 0.40117 val_acc= 0.89599 time= 0.15800
Epoch: 0032 train_loss= 0.34968 train_acc= 0.90723 val_loss= 0.38679 val_acc= 0.90146 time= 0.12503
Epoch: 0033 train_loss= 0.33377 train_acc= 0.91594 val_loss= 0.37269 val_acc= 0.90511 time= 0.12400
Epoch: 0034 train_loss= 0.31661 train_acc= 0.92100 val_loss= 0.35899 val_acc= 0.90876 time= 0.12301
Epoch: 0035 train_loss= 0.30277 train_acc= 0.92404 val_loss= 0.34578 val_acc= 0.91423 time= 0.12399
Epoch: 0036 train_loss= 0.28799 train_acc= 0.92971 val_loss= 0.33314 val_acc= 0.91423 time= 0.12502
Epoch: 0037 train_loss= 0.27210 train_acc= 0.93417 val_loss= 0.32108 val_acc= 0.91423 time= 0.12598
Epoch: 0038 train_loss= 0.26071 train_acc= 0.93782 val_loss= 0.30957 val_acc= 0.91788 time= 0.12314
Epoch: 0039 train_loss= 0.24727 train_acc= 0.94166 val_loss= 0.29854 val_acc= 0.91971 time= 0.17000
Epoch: 0040 train_loss= 0.23670 train_acc= 0.94308 val_loss= 0.28801 val_acc= 0.92153 time= 0.12400
Epoch: 0041 train_loss= 0.22562 train_acc= 0.94794 val_loss= 0.27796 val_acc= 0.92701 time= 0.12328
Epoch: 0042 train_loss= 0.21379 train_acc= 0.95058 val_loss= 0.26847 val_acc= 0.92701 time= 0.12314
Epoch: 0043 train_loss= 0.20108 train_acc= 0.95362 val_loss= 0.25955 val_acc= 0.93066 time= 0.12404
Epoch: 0044 train_loss= 0.19006 train_acc= 0.95807 val_loss= 0.25100 val_acc= 0.93066 time= 0.12201
Epoch: 0045 train_loss= 0.18171 train_acc= 0.95746 val_loss= 0.24286 val_acc= 0.93066 time= 0.12595
Epoch: 0046 train_loss= 0.17325 train_acc= 0.96030 val_loss= 0.23514 val_acc= 0.93248 time= 0.12400
Epoch: 0047 train_loss= 0.16382 train_acc= 0.96111 val_loss= 0.22784 val_acc= 0.93431 time= 0.15100
Epoch: 0048 train_loss= 0.15734 train_acc= 0.96293 val_loss= 0.22085 val_acc= 0.93613 time= 0.12508
Epoch: 0049 train_loss= 0.14730 train_acc= 0.96536 val_loss= 0.21428 val_acc= 0.93978 time= 0.12405
Epoch: 0050 train_loss= 0.13948 train_acc= 0.96739 val_loss= 0.20817 val_acc= 0.94343 time= 0.12399
Epoch: 0051 train_loss= 0.13435 train_acc= 0.96962 val_loss= 0.20242 val_acc= 0.94343 time= 0.12225
Epoch: 0052 train_loss= 0.12708 train_acc= 0.97022 val_loss= 0.19718 val_acc= 0.94526 time= 0.12300
Epoch: 0053 train_loss= 0.12035 train_acc= 0.97185 val_loss= 0.19250 val_acc= 0.94343 time= 0.12597
Epoch: 0054 train_loss= 0.11587 train_acc= 0.97185 val_loss= 0.18817 val_acc= 0.94343 time= 0.12406
Epoch: 0055 train_loss= 0.10982 train_acc= 0.97630 val_loss= 0.18413 val_acc= 0.94343 time= 0.17100
Epoch: 0056 train_loss= 0.10415 train_acc= 0.97549 val_loss= 0.18040 val_acc= 0.94526 time= 0.12300
Epoch: 0057 train_loss= 0.09990 train_acc= 0.97691 val_loss= 0.17696 val_acc= 0.94708 time= 0.12649
Epoch: 0058 train_loss= 0.09551 train_acc= 0.97853 val_loss= 0.17392 val_acc= 0.94526 time= 0.12503
Epoch: 0059 train_loss= 0.09060 train_acc= 0.97893 val_loss= 0.17126 val_acc= 0.94526 time= 0.12407
Epoch: 0060 train_loss= 0.08664 train_acc= 0.97893 val_loss= 0.16869 val_acc= 0.94891 time= 0.12453
Epoch: 0061 train_loss= 0.08304 train_acc= 0.98055 val_loss= 0.16636 val_acc= 0.94891 time= 0.12402
Epoch: 0062 train_loss= 0.07887 train_acc= 0.98197 val_loss= 0.16419 val_acc= 0.94891 time= 0.14500
Epoch: 0063 train_loss= 0.07673 train_acc= 0.98461 val_loss= 0.16225 val_acc= 0.95073 time= 0.14097
Epoch: 0064 train_loss= 0.07398 train_acc= 0.98359 val_loss= 0.16065 val_acc= 0.94891 time= 0.12403
Epoch: 0065 train_loss= 0.07073 train_acc= 0.98339 val_loss= 0.15929 val_acc= 0.94891 time= 0.12300
Epoch: 0066 train_loss= 0.06914 train_acc= 0.98400 val_loss= 0.15804 val_acc= 0.95073 time= 0.12532
Epoch: 0067 train_loss= 0.06557 train_acc= 0.98481 val_loss= 0.15694 val_acc= 0.95073 time= 0.12403
Epoch: 0068 train_loss= 0.06392 train_acc= 0.98501 val_loss= 0.15567 val_acc= 0.95073 time= 0.12401
Epoch: 0069 train_loss= 0.06077 train_acc= 0.98643 val_loss= 0.15449 val_acc= 0.95073 time= 0.12300
Epoch: 0070 train_loss= 0.05819 train_acc= 0.98623 val_loss= 0.15322 val_acc= 0.95073 time= 0.16799
Epoch: 0071 train_loss= 0.05750 train_acc= 0.98724 val_loss= 0.15221 val_acc= 0.95255 time= 0.12500
Epoch: 0072 train_loss= 0.05457 train_acc= 0.98704 val_loss= 0.15154 val_acc= 0.95255 time= 0.12336
Epoch: 0073 train_loss= 0.05295 train_acc= 0.99028 val_loss= 0.15092 val_acc= 0.95255 time= 0.12311
Epoch: 0074 train_loss= 0.05000 train_acc= 0.98926 val_loss= 0.15040 val_acc= 0.95255 time= 0.12407
Epoch: 0075 train_loss= 0.04910 train_acc= 0.98926 val_loss= 0.15004 val_acc= 0.95255 time= 0.12606
Epoch: 0076 train_loss= 0.04670 train_acc= 0.99129 val_loss= 0.14979 val_acc= 0.95620 time= 0.12400
Epoch: 0077 train_loss= 0.04571 train_acc= 0.99068 val_loss= 0.14928 val_acc= 0.95438 time= 0.12403
Epoch: 0078 train_loss= 0.04434 train_acc= 0.99089 val_loss= 0.14902 val_acc= 0.95438 time= 0.15500
Epoch: 0079 train_loss= 0.04301 train_acc= 0.99109 val_loss= 0.14912 val_acc= 0.95620 time= 0.12400
Epoch: 0080 train_loss= 0.04266 train_acc= 0.99048 val_loss= 0.14959 val_acc= 0.95438 time= 0.12414
Epoch: 0081 train_loss= 0.04156 train_acc= 0.99048 val_loss= 0.14976 val_acc= 0.95620 time= 0.12301
Epoch: 0082 train_loss= 0.03913 train_acc= 0.99170 val_loss= 0.15002 val_acc= 0.95620 time= 0.12400
Early stopping...
Optimization Finished!
Test set results: cost= 0.10642 accuracy= 0.97396 time= 0.05399
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9512    0.9669    0.9590       121
           1     0.9125    0.9733    0.9419        75
           2     0.9844    0.9917    0.9880      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9310    0.7500    0.8308        36
           5     0.9136    0.9136    0.9136        81
           6     0.9186    0.9080    0.9133        87
           7     0.9855    0.9741    0.9798       696

    accuracy                         0.9740      2189
   macro avg     0.9382    0.9347    0.9348      2189
weighted avg     0.9740    0.9740    0.9738      2189

Macro average Test Precision, Recall and F1-Score...
(0.9382391104462078, 0.9347161734682177, 0.934848296196656, None)
Micro average Test Precision, Recall and F1-Score...
(0.9739607126541799, 0.9739607126541799, 0.9739607126541799, None)
embeddings:
7688 5485 2189
[[ 0.33537704  0.05523583  0.19159466 ...  0.12601337  0.17339472
   0.05884092]
 [ 0.22788365  0.07335265  0.04577288 ...  0.15281755  0.05095999
   0.0791015 ]
 [-0.01214153  0.34051207  0.13025136 ...  0.28419638  0.1608188
   0.38188353]
 ...
 [ 0.16377728  0.32920533  0.19838926 ...  0.04882967  0.24079296
   0.35688913]
 [ 0.29201785  0.09673458  0.06399412 ...  0.28803506  0.03092322
   0.09071089]
 [ 0.00049228  0.27154058  0.15320365 ...  0.06508426  0.16585714
   0.29193944]]

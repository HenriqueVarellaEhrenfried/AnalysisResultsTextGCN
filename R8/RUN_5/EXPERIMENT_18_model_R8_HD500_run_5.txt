(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07942 train_acc= 0.12518 val_loss= 1.99059 val_acc= 0.74818 time= 0.50801
Epoch: 0002 train_loss= 1.98584 train_acc= 0.76443 val_loss= 1.82504 val_acc= 0.74453 time= 0.20200
Epoch: 0003 train_loss= 1.81714 train_acc= 0.75836 val_loss= 1.61110 val_acc= 0.72628 time= 0.19504
Epoch: 0004 train_loss= 1.59470 train_acc= 0.73202 val_loss= 1.41561 val_acc= 0.70255 time= 0.19396
Epoch: 0005 train_loss= 1.39096 train_acc= 0.71703 val_loss= 1.28468 val_acc= 0.70438 time= 0.19105
Epoch: 0006 train_loss= 1.24637 train_acc= 0.71886 val_loss= 1.19799 val_acc= 0.69526 time= 0.19295
Epoch: 0007 train_loss= 1.14941 train_acc= 0.71136 val_loss= 1.11729 val_acc= 0.71898 time= 0.19800
Epoch: 0008 train_loss= 1.07119 train_acc= 0.73648 val_loss= 1.02627 val_acc= 0.74453 time= 0.20997
Epoch: 0009 train_loss= 0.97671 train_acc= 0.76261 val_loss= 0.92975 val_acc= 0.76277 time= 0.19300
Epoch: 0010 train_loss= 0.88627 train_acc= 0.78367 val_loss= 0.84008 val_acc= 0.75730 time= 0.19200
Epoch: 0011 train_loss= 0.80228 train_acc= 0.78854 val_loss= 0.76576 val_acc= 0.75730 time= 0.19103
Epoch: 0012 train_loss= 0.72423 train_acc= 0.78428 val_loss= 0.70965 val_acc= 0.75912 time= 0.19397
Epoch: 0013 train_loss= 0.67375 train_acc= 0.78165 val_loss= 0.66893 val_acc= 0.76642 time= 0.22166
Epoch: 0014 train_loss= 0.63173 train_acc= 0.78975 val_loss= 0.63815 val_acc= 0.78650 time= 0.19372
Epoch: 0015 train_loss= 0.59673 train_acc= 0.81466 val_loss= 0.61195 val_acc= 0.81752 time= 0.19197
Epoch: 0016 train_loss= 0.57317 train_acc= 0.84262 val_loss= 0.58665 val_acc= 0.83759 time= 0.19200
Epoch: 0017 train_loss= 0.54399 train_acc= 0.86166 val_loss= 0.56102 val_acc= 0.85401 time= 0.19200
Epoch: 0018 train_loss= 0.51688 train_acc= 0.86956 val_loss= 0.53543 val_acc= 0.85401 time= 0.22303
Epoch: 0019 train_loss= 0.48763 train_acc= 0.88009 val_loss= 0.51070 val_acc= 0.85949 time= 0.19477
Epoch: 0020 train_loss= 0.45995 train_acc= 0.88272 val_loss= 0.48742 val_acc= 0.86314 time= 0.19404
Epoch: 0021 train_loss= 0.43237 train_acc= 0.88819 val_loss= 0.46587 val_acc= 0.87409 time= 0.19221
Epoch: 0022 train_loss= 0.40974 train_acc= 0.89204 val_loss= 0.44567 val_acc= 0.88139 time= 0.19400
Epoch: 0023 train_loss= 0.38587 train_acc= 0.89771 val_loss= 0.42653 val_acc= 0.89051 time= 0.22900
Epoch: 0024 train_loss= 0.36648 train_acc= 0.90257 val_loss= 0.40812 val_acc= 0.89964 time= 0.19100
Epoch: 0025 train_loss= 0.34695 train_acc= 0.90764 val_loss= 0.39026 val_acc= 0.90328 time= 0.19400
Epoch: 0026 train_loss= 0.32644 train_acc= 0.91655 val_loss= 0.37292 val_acc= 0.90511 time= 0.19304
Epoch: 0027 train_loss= 0.30644 train_acc= 0.92040 val_loss= 0.35622 val_acc= 0.90511 time= 0.19101
Epoch: 0028 train_loss= 0.28849 train_acc= 0.92587 val_loss= 0.34041 val_acc= 0.91241 time= 0.22295
Epoch: 0029 train_loss= 0.27194 train_acc= 0.92870 val_loss= 0.32557 val_acc= 0.91971 time= 0.19100
Epoch: 0030 train_loss= 0.25259 train_acc= 0.93741 val_loss= 0.31179 val_acc= 0.92153 time= 0.19104
Epoch: 0031 train_loss= 0.23518 train_acc= 0.94369 val_loss= 0.29884 val_acc= 0.92701 time= 0.19396
Epoch: 0032 train_loss= 0.22221 train_acc= 0.94875 val_loss= 0.28656 val_acc= 0.92518 time= 0.19268
Epoch: 0033 train_loss= 0.20772 train_acc= 0.95017 val_loss= 0.27491 val_acc= 0.92518 time= 0.22500
Epoch: 0034 train_loss= 0.19276 train_acc= 0.95544 val_loss= 0.26365 val_acc= 0.92883 time= 0.19498
Epoch: 0035 train_loss= 0.18071 train_acc= 0.95665 val_loss= 0.25295 val_acc= 0.93248 time= 0.19402
Epoch: 0036 train_loss= 0.16880 train_acc= 0.95807 val_loss= 0.24276 val_acc= 0.93613 time= 0.19498
Epoch: 0037 train_loss= 0.15703 train_acc= 0.95989 val_loss= 0.23310 val_acc= 0.93796 time= 0.19578
Epoch: 0038 train_loss= 0.14292 train_acc= 0.96273 val_loss= 0.22438 val_acc= 0.93796 time= 0.22101
Epoch: 0039 train_loss= 0.13523 train_acc= 0.96536 val_loss= 0.21657 val_acc= 0.93978 time= 0.19396
Epoch: 0040 train_loss= 0.12593 train_acc= 0.96820 val_loss= 0.20951 val_acc= 0.94161 time= 0.19205
Epoch: 0041 train_loss= 0.11811 train_acc= 0.96860 val_loss= 0.20353 val_acc= 0.94343 time= 0.19296
Epoch: 0042 train_loss= 0.11159 train_acc= 0.97144 val_loss= 0.19829 val_acc= 0.93978 time= 0.19300
Epoch: 0043 train_loss= 0.10437 train_acc= 0.97428 val_loss= 0.19364 val_acc= 0.94343 time= 0.22999
Epoch: 0044 train_loss= 0.09467 train_acc= 0.97509 val_loss= 0.18990 val_acc= 0.94891 time= 0.19300
Epoch: 0045 train_loss= 0.09052 train_acc= 0.97549 val_loss= 0.18638 val_acc= 0.94891 time= 0.19196
Epoch: 0046 train_loss= 0.08445 train_acc= 0.97853 val_loss= 0.18280 val_acc= 0.94891 time= 0.19205
Epoch: 0047 train_loss= 0.07980 train_acc= 0.97873 val_loss= 0.17955 val_acc= 0.94891 time= 0.19300
Epoch: 0048 train_loss= 0.07500 train_acc= 0.98015 val_loss= 0.17634 val_acc= 0.94891 time= 0.22495
Epoch: 0049 train_loss= 0.06905 train_acc= 0.98218 val_loss= 0.17358 val_acc= 0.94891 time= 0.19500
Epoch: 0050 train_loss= 0.06551 train_acc= 0.98197 val_loss= 0.17113 val_acc= 0.94891 time= 0.19204
Epoch: 0051 train_loss= 0.06266 train_acc= 0.98359 val_loss= 0.16898 val_acc= 0.95073 time= 0.19300
Epoch: 0052 train_loss= 0.06156 train_acc= 0.98481 val_loss= 0.16758 val_acc= 0.95255 time= 0.19299
Epoch: 0053 train_loss= 0.05667 train_acc= 0.98764 val_loss= 0.16683 val_acc= 0.95255 time= 0.21701
Epoch: 0054 train_loss= 0.05335 train_acc= 0.98501 val_loss= 0.16653 val_acc= 0.95620 time= 0.19195
Epoch: 0055 train_loss= 0.05165 train_acc= 0.98724 val_loss= 0.16649 val_acc= 0.95438 time= 0.19600
Epoch: 0056 train_loss= 0.04835 train_acc= 0.98866 val_loss= 0.16645 val_acc= 0.95620 time= 0.19304
Epoch: 0057 train_loss= 0.04560 train_acc= 0.98724 val_loss= 0.16614 val_acc= 0.95620 time= 0.19300
Epoch: 0058 train_loss= 0.04342 train_acc= 0.98906 val_loss= 0.16549 val_acc= 0.95438 time= 0.20295
Epoch: 0059 train_loss= 0.04236 train_acc= 0.99007 val_loss= 0.16419 val_acc= 0.95438 time= 0.19005
Epoch: 0060 train_loss= 0.03981 train_acc= 0.99028 val_loss= 0.16252 val_acc= 0.95620 time= 0.19695
Epoch: 0061 train_loss= 0.03818 train_acc= 0.99048 val_loss= 0.16089 val_acc= 0.95620 time= 0.19400
Epoch: 0062 train_loss= 0.03645 train_acc= 0.98987 val_loss= 0.16023 val_acc= 0.95255 time= 0.19100
Epoch: 0063 train_loss= 0.03513 train_acc= 0.99048 val_loss= 0.15993 val_acc= 0.95255 time= 0.19500
Epoch: 0064 train_loss= 0.03196 train_acc= 0.99271 val_loss= 0.16040 val_acc= 0.95438 time= 0.19400
Epoch: 0065 train_loss= 0.03129 train_acc= 0.99089 val_loss= 0.16141 val_acc= 0.95438 time= 0.19500
Epoch: 0066 train_loss= 0.02971 train_acc= 0.99251 val_loss= 0.16279 val_acc= 0.95438 time= 0.19400
Early stopping...
Optimization Finished!
Test set results: cost= 0.11170 accuracy= 0.97122 time= 0.07900
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9435    0.9669    0.9551       121
           1     0.9012    0.9733    0.9359        75
           2     0.9826    0.9917    0.9871      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9643    0.7500    0.8437        36
           5     0.9200    0.8519    0.8846        81
           6     0.8889    0.9195    0.9040        87
           7     0.9840    0.9713    0.9776       696

    accuracy                         0.9712      2189
   macro avg     0.9367    0.9281    0.9301      2189
weighted avg     0.9714    0.9712    0.9710      2189

Macro average Test Precision, Recall and F1-Score...
(0.9367066842311289, 0.9280777102923983, 0.930052241142818, None)
Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)
embeddings:
7688 5485 2189
[[ 0.10034756  0.08497071  0.07191875 ...  0.09931437  0.08701224
   0.06649491]
 [-0.00827901  0.1189376   0.02112036 ...  0.00624796  0.01060865
   0.10383936]
 [ 0.04458696  0.1676849   0.17396598 ...  0.09661246  0.1489131
   0.02296154]
 ...
 [ 0.10365517  0.04416583  0.21872279 ...  0.1547282   0.17867726
   0.04409872]
 [-0.03476513  0.23063838  0.01078889 ... -0.01597578 -0.01376192
   0.08013094]
 [ 0.07449524  0.14298698  0.17252591 ...  0.11071585  0.13249896
   0.03893155]]

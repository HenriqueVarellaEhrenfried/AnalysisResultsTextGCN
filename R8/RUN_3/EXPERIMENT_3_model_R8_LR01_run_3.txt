(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07944 train_acc= 0.05489 val_loss= 1.59506 val_acc= 0.75182 time= 0.40579
Epoch: 0002 train_loss= 1.56966 train_acc= 0.77314 val_loss= 1.27716 val_acc= 0.72993 time= 0.13100
Epoch: 0003 train_loss= 1.22378 train_acc= 0.73263 val_loss= 1.03816 val_acc= 0.68066 time= 0.12600
Epoch: 0004 train_loss= 0.98782 train_acc= 0.69617 val_loss= 0.79431 val_acc= 0.75365 time= 0.12301
Epoch: 0005 train_loss= 0.74980 train_acc= 0.78145 val_loss= 0.68401 val_acc= 0.75365 time= 0.12700
Epoch: 0006 train_loss= 0.64280 train_acc= 0.77415 val_loss= 0.62989 val_acc= 0.75730 time= 0.12306
Epoch: 0007 train_loss= 0.58276 train_acc= 0.79218 val_loss= 0.58892 val_acc= 0.81022 time= 0.16400
Epoch: 0008 train_loss= 0.53663 train_acc= 0.83492 val_loss= 0.54686 val_acc= 0.83029 time= 0.12600
Epoch: 0009 train_loss= 0.48655 train_acc= 0.85335 val_loss= 0.50726 val_acc= 0.84489 time= 0.12501
Epoch: 0010 train_loss= 0.44101 train_acc= 0.86753 val_loss= 0.47243 val_acc= 0.85584 time= 0.12396
Epoch: 0011 train_loss= 0.40067 train_acc= 0.87705 val_loss= 0.44085 val_acc= 0.87044 time= 0.12303
Epoch: 0012 train_loss= 0.36134 train_acc= 0.88495 val_loss= 0.41191 val_acc= 0.88321 time= 0.12300
Epoch: 0013 train_loss= 0.32451 train_acc= 0.89407 val_loss= 0.38497 val_acc= 0.89781 time= 0.12200
Epoch: 0014 train_loss= 0.29543 train_acc= 0.91270 val_loss= 0.35897 val_acc= 0.90693 time= 0.12299
Epoch: 0015 train_loss= 0.26052 train_acc= 0.93377 val_loss= 0.33535 val_acc= 0.92336 time= 0.14900
Epoch: 0016 train_loss= 0.23148 train_acc= 0.94369 val_loss= 0.31507 val_acc= 0.92518 time= 0.12200
Epoch: 0017 train_loss= 0.20321 train_acc= 0.94875 val_loss= 0.29762 val_acc= 0.92883 time= 0.12500
Epoch: 0018 train_loss= 0.17672 train_acc= 0.95260 val_loss= 0.28364 val_acc= 0.93066 time= 0.12297
Epoch: 0019 train_loss= 0.16021 train_acc= 0.95341 val_loss= 0.27275 val_acc= 0.93796 time= 0.12403
Epoch: 0020 train_loss= 0.14230 train_acc= 0.96070 val_loss= 0.26367 val_acc= 0.94161 time= 0.12300
Epoch: 0021 train_loss= 0.12268 train_acc= 0.96536 val_loss= 0.25473 val_acc= 0.94526 time= 0.12309
Epoch: 0022 train_loss= 0.11382 train_acc= 0.96678 val_loss= 0.24775 val_acc= 0.94526 time= 0.12498
Epoch: 0023 train_loss= 0.09905 train_acc= 0.96820 val_loss= 0.24263 val_acc= 0.94161 time= 0.15001
Epoch: 0024 train_loss= 0.08849 train_acc= 0.97144 val_loss= 0.24000 val_acc= 0.94161 time= 0.12202
Epoch: 0025 train_loss= 0.08217 train_acc= 0.97367 val_loss= 0.24024 val_acc= 0.94526 time= 0.12498
Epoch: 0026 train_loss= 0.07207 train_acc= 0.97772 val_loss= 0.24279 val_acc= 0.94526 time= 0.12400
Epoch: 0027 train_loss= 0.06605 train_acc= 0.97711 val_loss= 0.24512 val_acc= 0.94526 time= 0.12400
Epoch: 0028 train_loss= 0.05987 train_acc= 0.98096 val_loss= 0.24537 val_acc= 0.94526 time= 0.12200
Epoch: 0029 train_loss= 0.05699 train_acc= 0.98137 val_loss= 0.24262 val_acc= 0.94708 time= 0.12403
Epoch: 0030 train_loss= 0.05198 train_acc= 0.98197 val_loss= 0.24039 val_acc= 0.94891 time= 0.12399
Epoch: 0031 train_loss= 0.04419 train_acc= 0.98704 val_loss= 0.24087 val_acc= 0.94891 time= 0.16701
Epoch: 0032 train_loss= 0.04070 train_acc= 0.98785 val_loss= 0.24387 val_acc= 0.95255 time= 0.12299
Early stopping...
Optimization Finished!
Test set results: cost= 0.15181 accuracy= 0.96117 time= 0.05397
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9200    0.9504    0.9350       121
           1     0.9103    0.9467    0.9281        75
           2     0.9772    0.9908    0.9840      1083
           3     0.3846    0.5000    0.4348        10
           4     0.9286    0.7222    0.8125        36
           5     0.8816    0.8272    0.8535        81
           6     0.8495    0.9080    0.8778        87
           7     0.9853    0.9598    0.9723       696

    accuracy                         0.9612      2189
   macro avg     0.8546    0.8506    0.8497      2189
weighted avg     0.9622    0.9612    0.9613      2189

Macro average Test Precision, Recall and F1-Score...
(0.8546208254445644, 0.8506306359336155, 0.849740416744649, None)
Micro average Test Precision, Recall and F1-Score...
(0.9611694837825491, 0.9611694837825491, 0.9611694837825491, None)
embeddings:
7688 5485 2189
[[-0.01264187  0.27545562  0.2873499  ...  0.14826699  0.1397405
   0.21051705]
 [ 0.26945606 -0.13340363  0.36445144 ... -0.1241824  -0.15209758
  -0.13105625]
 [ 0.6845355  -0.26896554  0.30324495 ... -0.15959173  0.08936312
  -0.25297126]
 ...
 [ 0.55265725 -0.12792005  0.01644303 ...  0.03873066  0.23903355
  -0.16427073]
 [ 0.5515301  -0.2558989   0.54223543 ... -0.2730192  -0.2940575
  -0.26258013]
 [ 0.6574932  -0.10230125  0.28198552 ... -0.02366392  0.20603977
  -0.10385813]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07948 train_acc= 0.07494 val_loss= 1.62855 val_acc= 0.74088 time= 0.39180
Epoch: 0002 train_loss= 1.60842 train_acc= 0.74782 val_loss= 1.29683 val_acc= 0.67336 time= 0.12907
Epoch: 0003 train_loss= 1.25276 train_acc= 0.69556 val_loss= 1.09388 val_acc= 0.58759 time= 0.15799
Epoch: 0004 train_loss= 1.04256 train_acc= 0.61495 val_loss= 0.84456 val_acc= 0.75547 time= 0.12400
Epoch: 0005 train_loss= 0.80136 train_acc= 0.78408 val_loss= 0.71527 val_acc= 0.75365 time= 0.12400
Epoch: 0006 train_loss= 0.67443 train_acc= 0.76970 val_loss= 0.64683 val_acc= 0.76277 time= 0.12698
Epoch: 0007 train_loss= 0.59808 train_acc= 0.78793 val_loss= 0.60026 val_acc= 0.78832 time= 0.12400
Epoch: 0008 train_loss= 0.54677 train_acc= 0.82054 val_loss= 0.55404 val_acc= 0.82664 time= 0.12505
Epoch: 0009 train_loss= 0.49636 train_acc= 0.85781 val_loss= 0.50642 val_acc= 0.85219 time= 0.12301
Epoch: 0010 train_loss= 0.44095 train_acc= 0.88333 val_loss= 0.46207 val_acc= 0.86496 time= 0.12300
Epoch: 0011 train_loss= 0.39199 train_acc= 0.89082 val_loss= 0.42379 val_acc= 0.87409 time= 0.16600
Epoch: 0012 train_loss= 0.35216 train_acc= 0.89710 val_loss= 0.39146 val_acc= 0.88139 time= 0.12132
Epoch: 0013 train_loss= 0.31150 train_acc= 0.90561 val_loss= 0.36163 val_acc= 0.88869 time= 0.12300
Epoch: 0014 train_loss= 0.27553 train_acc= 0.91716 val_loss= 0.33651 val_acc= 0.90146 time= 0.12340
Epoch: 0015 train_loss= 0.23958 train_acc= 0.93154 val_loss= 0.31990 val_acc= 0.91788 time= 0.12797
Epoch: 0016 train_loss= 0.21970 train_acc= 0.94004 val_loss= 0.30617 val_acc= 0.91788 time= 0.12403
Epoch: 0017 train_loss= 0.19711 train_acc= 0.94551 val_loss= 0.29106 val_acc= 0.92336 time= 0.12500
Epoch: 0018 train_loss= 0.17033 train_acc= 0.95281 val_loss= 0.27991 val_acc= 0.92336 time= 0.12297
Epoch: 0019 train_loss= 0.14788 train_acc= 0.95483 val_loss= 0.26915 val_acc= 0.93248 time= 0.15004
Epoch: 0020 train_loss= 0.12953 train_acc= 0.96192 val_loss= 0.25552 val_acc= 0.93431 time= 0.12399
Epoch: 0021 train_loss= 0.12059 train_acc= 0.96496 val_loss= 0.24141 val_acc= 0.93978 time= 0.12397
Epoch: 0022 train_loss= 0.10884 train_acc= 0.96698 val_loss= 0.23456 val_acc= 0.94343 time= 0.12304
Epoch: 0023 train_loss= 0.09414 train_acc= 0.97205 val_loss= 0.23216 val_acc= 0.94343 time= 0.12396
Epoch: 0024 train_loss= 0.08104 train_acc= 0.97691 val_loss= 0.23499 val_acc= 0.93978 time= 0.12700
Epoch: 0025 train_loss= 0.07429 train_acc= 0.97671 val_loss= 0.23874 val_acc= 0.93796 time= 0.12403
Epoch: 0026 train_loss= 0.06504 train_acc= 0.97914 val_loss= 0.23964 val_acc= 0.94161 time= 0.12197
Early stopping...
Optimization Finished!
Test set results: cost= 0.14527 accuracy= 0.96665 time= 0.09800
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9280    0.9587    0.9431       121
           1     0.8690    0.9733    0.9182        75
           2     0.9844    0.9898    0.9871      1083
           3     1.0000    0.1000    0.1818        10
           4     0.7568    0.7778    0.7671        36
           5     0.9444    0.8395    0.8889        81
           6     0.8710    0.9310    0.9000        87
           7     0.9840    0.9727    0.9783       696

    accuracy                         0.9667      2189
   macro avg     0.9172    0.8179    0.8206      2189
weighted avg     0.9675    0.9667    0.9651      2189

Macro average Test Precision, Recall and F1-Score...
(0.9172021922646241, 0.8178592038386414, 0.8205738922527765, None)
Micro average Test Precision, Recall and F1-Score...
(0.966651439013248, 0.966651439013248, 0.966651439013248, None)
embeddings:
7688 5485 2189
[[ 8.4708229e-02  7.1184224e-01  2.1991313e-01 ... -1.7568074e-02
  -3.9182808e-02 -3.4110153e-01]
 [ 2.0629629e-01  3.4584829e-01  2.5069690e-01 ...  1.6424198e-02
   7.7870205e-02 -2.9599205e-01]
 [-2.1873664e-02  1.6229659e-01  8.3799410e-01 ...  1.6368684e-01
   5.4123539e-01 -3.5974231e-01]
 ...
 [ 1.3248716e-04  3.4909621e-01  8.4731424e-01 ... -2.9985410e-01
   5.1611513e-01 -3.5665327e-01]
 [ 2.8268507e-01  3.5763294e-01  2.0015211e-01 ...  1.5417276e-01
   1.3004392e-01 -4.0816572e-01]
 [-2.0515012e-02  1.1630623e-01  7.2410274e-01 ...  2.1044549e-02
   4.0729114e-01 -2.7756029e-01]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07934 train_acc= 0.20802 val_loss= 2.02313 val_acc= 0.75182 time= 0.40886
Epoch: 0002 train_loss= 2.02235 train_acc= 0.78003 val_loss= 1.93071 val_acc= 0.75730 time= 0.12807
Epoch: 0003 train_loss= 1.92852 train_acc= 0.78509 val_loss= 1.80607 val_acc= 0.75730 time= 0.12700
Epoch: 0004 train_loss= 1.79771 train_acc= 0.78367 val_loss= 1.66240 val_acc= 0.75730 time= 0.12300
Epoch: 0005 train_loss= 1.64575 train_acc= 0.78124 val_loss= 1.52159 val_acc= 0.75730 time= 0.12900
Epoch: 0006 train_loss= 1.50626 train_acc= 0.78064 val_loss= 1.40431 val_acc= 0.75730 time= 0.12300
Epoch: 0007 train_loss= 1.37133 train_acc= 0.77841 val_loss= 1.31548 val_acc= 0.74453 time= 0.12400
Epoch: 0008 train_loss= 1.28750 train_acc= 0.76099 val_loss= 1.24608 val_acc= 0.72993 time= 0.12305
Epoch: 0009 train_loss= 1.20558 train_acc= 0.73364 val_loss= 1.18427 val_acc= 0.71533 time= 0.15301
Epoch: 0010 train_loss= 1.14245 train_acc= 0.72210 val_loss= 1.12219 val_acc= 0.71715 time= 0.12300
Epoch: 0011 train_loss= 1.07665 train_acc= 0.72817 val_loss= 1.05674 val_acc= 0.72810 time= 0.12423
Epoch: 0012 train_loss= 1.00912 train_acc= 0.74256 val_loss= 0.98869 val_acc= 0.75182 time= 0.12300
Epoch: 0013 train_loss= 0.94468 train_acc= 0.76484 val_loss= 0.92147 val_acc= 0.76277 time= 0.12800
Epoch: 0014 train_loss= 0.88186 train_acc= 0.77962 val_loss= 0.85921 val_acc= 0.76277 time= 0.12403
Epoch: 0015 train_loss= 0.81850 train_acc= 0.78874 val_loss= 0.80507 val_acc= 0.76095 time= 0.12397
Epoch: 0016 train_loss= 0.77043 train_acc= 0.78813 val_loss= 0.76025 val_acc= 0.75730 time= 0.12503
Epoch: 0017 train_loss= 0.72251 train_acc= 0.78590 val_loss= 0.72379 val_acc= 0.76460 time= 0.15697
Epoch: 0018 train_loss= 0.68562 train_acc= 0.78874 val_loss= 0.69376 val_acc= 0.77372 time= 0.12303
Epoch: 0019 train_loss= 0.65833 train_acc= 0.79542 val_loss= 0.66782 val_acc= 0.78467 time= 0.12511
Epoch: 0020 train_loss= 0.63325 train_acc= 0.80393 val_loss= 0.64407 val_acc= 0.79745 time= 0.12280
Epoch: 0021 train_loss= 0.60473 train_acc= 0.81689 val_loss= 0.62137 val_acc= 0.80657 time= 0.12552
Epoch: 0022 train_loss= 0.58181 train_acc= 0.82824 val_loss= 0.59919 val_acc= 0.81934 time= 0.12508
Epoch: 0023 train_loss= 0.56393 train_acc= 0.84181 val_loss= 0.57731 val_acc= 0.83759 time= 0.12423
Epoch: 0024 train_loss= 0.53270 train_acc= 0.86125 val_loss= 0.55593 val_acc= 0.85219 time= 0.12536
Epoch: 0025 train_loss= 0.51730 train_acc= 0.86388 val_loss= 0.53534 val_acc= 0.86131 time= 0.16200
Epoch: 0026 train_loss= 0.49218 train_acc= 0.88110 val_loss= 0.51573 val_acc= 0.86861 time= 0.12400
Epoch: 0027 train_loss= 0.47035 train_acc= 0.89204 val_loss= 0.49709 val_acc= 0.87774 time= 0.12300
Epoch: 0028 train_loss= 0.45071 train_acc= 0.89325 val_loss= 0.47936 val_acc= 0.88321 time= 0.12400
Epoch: 0029 train_loss= 0.43294 train_acc= 0.89872 val_loss= 0.46244 val_acc= 0.88686 time= 0.12310
Epoch: 0030 train_loss= 0.41266 train_acc= 0.90338 val_loss= 0.44625 val_acc= 0.89234 time= 0.12797
Epoch: 0031 train_loss= 0.39490 train_acc= 0.90662 val_loss= 0.43061 val_acc= 0.89599 time= 0.12569
Epoch: 0032 train_loss= 0.37484 train_acc= 0.90926 val_loss= 0.41545 val_acc= 0.89781 time= 0.16897
Epoch: 0033 train_loss= 0.36359 train_acc= 0.91189 val_loss= 0.40080 val_acc= 0.90328 time= 0.12325
Epoch: 0034 train_loss= 0.34721 train_acc= 0.91736 val_loss= 0.38663 val_acc= 0.90511 time= 0.12300
Epoch: 0035 train_loss= 0.32778 train_acc= 0.91999 val_loss= 0.37300 val_acc= 0.91241 time= 0.12204
Epoch: 0036 train_loss= 0.31822 train_acc= 0.92100 val_loss= 0.35999 val_acc= 0.91241 time= 0.12501
Epoch: 0037 train_loss= 0.30450 train_acc= 0.92728 val_loss= 0.34748 val_acc= 0.91606 time= 0.12299
Epoch: 0038 train_loss= 0.28767 train_acc= 0.93417 val_loss= 0.33556 val_acc= 0.91971 time= 0.12605
Epoch: 0039 train_loss= 0.27651 train_acc= 0.93640 val_loss= 0.32408 val_acc= 0.92153 time= 0.12397
Epoch: 0040 train_loss= 0.26156 train_acc= 0.94349 val_loss= 0.31294 val_acc= 0.92153 time= 0.16403
Epoch: 0041 train_loss= 0.25216 train_acc= 0.94632 val_loss= 0.30224 val_acc= 0.92518 time= 0.12300
Epoch: 0042 train_loss= 0.23740 train_acc= 0.94916 val_loss= 0.29195 val_acc= 0.92883 time= 0.12364
Epoch: 0043 train_loss= 0.22705 train_acc= 0.95200 val_loss= 0.28207 val_acc= 0.92701 time= 0.12463
Epoch: 0044 train_loss= 0.21236 train_acc= 0.95443 val_loss= 0.27256 val_acc= 0.92883 time= 0.12497
Epoch: 0045 train_loss= 0.20773 train_acc= 0.95260 val_loss= 0.26347 val_acc= 0.92883 time= 0.12400
Epoch: 0046 train_loss= 0.19770 train_acc= 0.95969 val_loss= 0.25501 val_acc= 0.93066 time= 0.12510
Epoch: 0047 train_loss= 0.18969 train_acc= 0.95848 val_loss= 0.24718 val_acc= 0.93248 time= 0.12301
Epoch: 0048 train_loss= 0.17586 train_acc= 0.96091 val_loss= 0.23994 val_acc= 0.93431 time= 0.15703
Epoch: 0049 train_loss= 0.17085 train_acc= 0.96030 val_loss= 0.23325 val_acc= 0.93431 time= 0.12300
Epoch: 0050 train_loss= 0.16189 train_acc= 0.95929 val_loss= 0.22701 val_acc= 0.93796 time= 0.12215
Epoch: 0051 train_loss= 0.15244 train_acc= 0.96476 val_loss= 0.22107 val_acc= 0.93978 time= 0.12497
Epoch: 0052 train_loss= 0.14295 train_acc= 0.96557 val_loss= 0.21535 val_acc= 0.94343 time= 0.12400
Epoch: 0053 train_loss= 0.14604 train_acc= 0.96374 val_loss= 0.20993 val_acc= 0.94343 time= 0.12404
Epoch: 0054 train_loss= 0.13211 train_acc= 0.96840 val_loss= 0.20455 val_acc= 0.94343 time= 0.12396
Epoch: 0055 train_loss= 0.12878 train_acc= 0.96881 val_loss= 0.19961 val_acc= 0.94526 time= 0.12503
Epoch: 0056 train_loss= 0.12121 train_acc= 0.96921 val_loss= 0.19530 val_acc= 0.94526 time= 0.16010
Epoch: 0057 train_loss= 0.11801 train_acc= 0.96982 val_loss= 0.19121 val_acc= 0.94708 time= 0.12603
Epoch: 0058 train_loss= 0.11385 train_acc= 0.97103 val_loss= 0.18742 val_acc= 0.94708 time= 0.12297
Epoch: 0059 train_loss= 0.10766 train_acc= 0.97266 val_loss= 0.18405 val_acc= 0.94708 time= 0.12352
Epoch: 0060 train_loss= 0.10592 train_acc= 0.97650 val_loss= 0.18092 val_acc= 0.94891 time= 0.12314
Epoch: 0061 train_loss= 0.10052 train_acc= 0.97752 val_loss= 0.17796 val_acc= 0.94891 time= 0.12304
Epoch: 0062 train_loss= 0.09554 train_acc= 0.97630 val_loss= 0.17532 val_acc= 0.94708 time= 0.12405
Epoch: 0063 train_loss= 0.09259 train_acc= 0.97893 val_loss= 0.17280 val_acc= 0.95255 time= 0.12803
Epoch: 0064 train_loss= 0.08758 train_acc= 0.97812 val_loss= 0.17034 val_acc= 0.95438 time= 0.15900
Epoch: 0065 train_loss= 0.08697 train_acc= 0.97752 val_loss= 0.16778 val_acc= 0.95255 time= 0.12797
Epoch: 0066 train_loss= 0.08262 train_acc= 0.98076 val_loss= 0.16534 val_acc= 0.95255 time= 0.12503
Epoch: 0067 train_loss= 0.07864 train_acc= 0.98177 val_loss= 0.16341 val_acc= 0.95255 time= 0.12303
Epoch: 0068 train_loss= 0.07607 train_acc= 0.98197 val_loss= 0.16178 val_acc= 0.95255 time= 0.12400
Epoch: 0069 train_loss= 0.07223 train_acc= 0.98278 val_loss= 0.16060 val_acc= 0.95438 time= 0.12400
Epoch: 0070 train_loss= 0.07143 train_acc= 0.98521 val_loss= 0.15973 val_acc= 0.95438 time= 0.12400
Epoch: 0071 train_loss= 0.06837 train_acc= 0.98400 val_loss= 0.15896 val_acc= 0.95438 time= 0.17000
Epoch: 0072 train_loss= 0.06633 train_acc= 0.98521 val_loss= 0.15796 val_acc= 0.95255 time= 0.12317
Epoch: 0073 train_loss= 0.06172 train_acc= 0.98623 val_loss= 0.15682 val_acc= 0.95255 time= 0.12709
Epoch: 0074 train_loss= 0.06290 train_acc= 0.98501 val_loss= 0.15576 val_acc= 0.95255 time= 0.12498
Epoch: 0075 train_loss= 0.05933 train_acc= 0.98582 val_loss= 0.15458 val_acc= 0.95073 time= 0.12347
Epoch: 0076 train_loss= 0.05605 train_acc= 0.98744 val_loss= 0.15364 val_acc= 0.95073 time= 0.12300
Epoch: 0077 train_loss= 0.05881 train_acc= 0.98704 val_loss= 0.15238 val_acc= 0.95073 time= 0.12300
Epoch: 0078 train_loss= 0.05504 train_acc= 0.98643 val_loss= 0.15147 val_acc= 0.95255 time= 0.12300
Epoch: 0079 train_loss= 0.05310 train_acc= 0.98886 val_loss= 0.15088 val_acc= 0.95255 time= 0.15300
Epoch: 0080 train_loss= 0.04923 train_acc= 0.98805 val_loss= 0.15035 val_acc= 0.95438 time= 0.12300
Epoch: 0081 train_loss= 0.05054 train_acc= 0.98906 val_loss= 0.14963 val_acc= 0.95255 time= 0.12300
Epoch: 0082 train_loss= 0.04737 train_acc= 0.99007 val_loss= 0.14931 val_acc= 0.95255 time= 0.12531
Epoch: 0083 train_loss= 0.04689 train_acc= 0.99007 val_loss= 0.14899 val_acc= 0.95255 time= 0.12500
Epoch: 0084 train_loss= 0.04481 train_acc= 0.99190 val_loss= 0.14916 val_acc= 0.95073 time= 0.12400
Epoch: 0085 train_loss= 0.04280 train_acc= 0.98947 val_loss= 0.14964 val_acc= 0.95073 time= 0.12300
Epoch: 0086 train_loss= 0.04398 train_acc= 0.99129 val_loss= 0.14943 val_acc= 0.95073 time= 0.12300
Epoch: 0087 train_loss= 0.04168 train_acc= 0.99007 val_loss= 0.14908 val_acc= 0.95255 time= 0.15700
Epoch: 0088 train_loss= 0.04133 train_acc= 0.98967 val_loss= 0.14879 val_acc= 0.95255 time= 0.12400
Epoch: 0089 train_loss= 0.03866 train_acc= 0.99190 val_loss= 0.14842 val_acc= 0.95255 time= 0.12400
Epoch: 0090 train_loss= 0.03745 train_acc= 0.99230 val_loss= 0.14746 val_acc= 0.95255 time= 0.12300
Epoch: 0091 train_loss= 0.03804 train_acc= 0.99190 val_loss= 0.14645 val_acc= 0.95255 time= 0.12534
Epoch: 0092 train_loss= 0.03659 train_acc= 0.99251 val_loss= 0.14570 val_acc= 0.95255 time= 0.12458
Epoch: 0093 train_loss= 0.03594 train_acc= 0.99291 val_loss= 0.14543 val_acc= 0.95438 time= 0.12300
Epoch: 0094 train_loss= 0.03456 train_acc= 0.99311 val_loss= 0.14547 val_acc= 0.95438 time= 0.12600
Epoch: 0095 train_loss= 0.03569 train_acc= 0.99271 val_loss= 0.14608 val_acc= 0.95438 time= 0.16597
Epoch: 0096 train_loss= 0.03245 train_acc= 0.99372 val_loss= 0.14694 val_acc= 0.95438 time= 0.12703
Epoch: 0097 train_loss= 0.03277 train_acc= 0.99291 val_loss= 0.14813 val_acc= 0.95438 time= 0.12400
Early stopping...
Optimization Finished!
Test set results: cost= 0.10906 accuracy= 0.97213 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9297    0.9835    0.9558       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     0.9000    0.9000    0.9000        10
           4     1.0000    0.6944    0.8197        36
           5     0.9000    0.8889    0.8944        81
           6     0.9080    0.9080    0.9080        87
           7     0.9854    0.9727    0.9790       696

    accuracy                         0.9721      2189
   macro avg     0.9399    0.9141    0.9233      2189
weighted avg     0.9725    0.9721    0.9718      2189

Macro average Test Precision, Recall and F1-Score...
(0.939899239971383, 0.9140718272720167, 0.9233130152271001, None)
Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)
embeddings:
7688 5485 2189
[[ 0.20435268  0.3538622   0.15349755 ...  0.08347525  0.24036941
   0.13632435]
 [ 0.19486026  0.16554028  0.04682016 ...  0.18985446  0.2685478
   0.05131164]
 [ 0.24887244  0.02245373  0.22006759 ...  0.5188834  -0.03291714
   0.20969006]
 ...
 [ 0.11432657  0.11452189  0.2707271  ...  0.4435939   0.14891818
   0.27220684]
 [ 0.23507129  0.19973095  0.01904395 ...  0.22671285  0.35577726
   0.03728962]
 [ 0.02418658  0.00435024  0.20723091 ...  0.40504742  0.0456774
   0.20042983]]

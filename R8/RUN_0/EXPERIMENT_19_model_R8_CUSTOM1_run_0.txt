(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07930 train_acc= 0.18493 val_loss= 2.05904 val_acc= 0.75365 time= 0.40487
Epoch: 0002 train_loss= 2.05938 train_acc= 0.76666 val_loss= 2.02589 val_acc= 0.75365 time= 0.13000
Epoch: 0003 train_loss= 2.02458 train_acc= 0.76281 val_loss= 1.98270 val_acc= 0.75547 time= 0.13101
Epoch: 0004 train_loss= 1.98472 train_acc= 0.77091 val_loss= 1.93050 val_acc= 0.75547 time= 0.12700
Epoch: 0005 train_loss= 1.92409 train_acc= 0.76788 val_loss= 1.87022 val_acc= 0.75730 time= 0.12402
Epoch: 0006 train_loss= 1.86036 train_acc= 0.77476 val_loss= 1.80356 val_acc= 0.76095 time= 0.12300
Epoch: 0007 train_loss= 1.79637 train_acc= 0.76828 val_loss= 1.73260 val_acc= 0.76277 time= 0.12304
Epoch: 0008 train_loss= 1.73709 train_acc= 0.77679 val_loss= 1.66022 val_acc= 0.76277 time= 0.14899
Epoch: 0009 train_loss= 1.64440 train_acc= 0.77112 val_loss= 1.58927 val_acc= 0.76460 time= 0.13501
Epoch: 0010 train_loss= 1.56823 train_acc= 0.76585 val_loss= 1.52262 val_acc= 0.76460 time= 0.12395
Epoch: 0011 train_loss= 1.51301 train_acc= 0.75208 val_loss= 1.46210 val_acc= 0.76095 time= 0.12500
Epoch: 0012 train_loss= 1.41856 train_acc= 0.76220 val_loss= 1.40806 val_acc= 0.75730 time= 0.12600
Epoch: 0013 train_loss= 1.39339 train_acc= 0.75228 val_loss= 1.36033 val_acc= 0.75182 time= 0.12304
Epoch: 0014 train_loss= 1.32801 train_acc= 0.74418 val_loss= 1.31730 val_acc= 0.74453 time= 0.12296
Epoch: 0015 train_loss= 1.31640 train_acc= 0.75127 val_loss= 1.27736 val_acc= 0.74088 time= 0.12404
Epoch: 0016 train_loss= 1.25114 train_acc= 0.74033 val_loss= 1.23890 val_acc= 0.72810 time= 0.16500
Epoch: 0017 train_loss= 1.21525 train_acc= 0.72777 val_loss= 1.20086 val_acc= 0.72445 time= 0.12200
Epoch: 0018 train_loss= 1.17079 train_acc= 0.73243 val_loss= 1.16259 val_acc= 0.71898 time= 0.12315
Epoch: 0019 train_loss= 1.08835 train_acc= 0.73830 val_loss= 1.12384 val_acc= 0.72080 time= 0.12510
Epoch: 0020 train_loss= 1.09021 train_acc= 0.72554 val_loss= 1.08477 val_acc= 0.72810 time= 0.12910
Epoch: 0021 train_loss= 1.05842 train_acc= 0.73223 val_loss= 1.04579 val_acc= 0.73358 time= 0.12306
Epoch: 0022 train_loss= 1.01655 train_acc= 0.72757 val_loss= 1.00726 val_acc= 0.74088 time= 0.12303
Epoch: 0023 train_loss= 0.97638 train_acc= 0.73769 val_loss= 0.96979 val_acc= 0.75000 time= 0.12297
Epoch: 0024 train_loss= 0.94346 train_acc= 0.75775 val_loss= 0.93393 val_acc= 0.75912 time= 0.15204
Epoch: 0025 train_loss= 0.89354 train_acc= 0.77132 val_loss= 0.90018 val_acc= 0.76095 time= 0.12300
Epoch: 0026 train_loss= 0.87224 train_acc= 0.78205 val_loss= 0.86891 val_acc= 0.76095 time= 0.12196
Epoch: 0027 train_loss= 0.83060 train_acc= 0.77679 val_loss= 0.84029 val_acc= 0.76460 time= 0.12307
Epoch: 0028 train_loss= 0.80550 train_acc= 0.77881 val_loss= 0.81428 val_acc= 0.76642 time= 0.12803
Epoch: 0029 train_loss= 0.77267 train_acc= 0.78854 val_loss= 0.79072 val_acc= 0.76825 time= 0.12300
Epoch: 0030 train_loss= 0.77340 train_acc= 0.78935 val_loss= 0.76916 val_acc= 0.76825 time= 0.12308
Epoch: 0031 train_loss= 0.74764 train_acc= 0.79076 val_loss= 0.74914 val_acc= 0.77372 time= 0.12399
Epoch: 0032 train_loss= 0.73057 train_acc= 0.78509 val_loss= 0.73026 val_acc= 0.78102 time= 0.16823
Epoch: 0033 train_loss= 0.70212 train_acc= 0.80373 val_loss= 0.71220 val_acc= 0.78467 time= 0.12500
Epoch: 0034 train_loss= 0.68061 train_acc= 0.80980 val_loss= 0.69470 val_acc= 0.79380 time= 0.12315
Epoch: 0035 train_loss= 0.68276 train_acc= 0.81264 val_loss= 0.67756 val_acc= 0.80474 time= 0.12400
Epoch: 0036 train_loss= 0.65233 train_acc= 0.82175 val_loss= 0.66092 val_acc= 0.81569 time= 0.12699
Epoch: 0037 train_loss= 0.63417 train_acc= 0.82763 val_loss= 0.64479 val_acc= 0.82482 time= 0.12501
Epoch: 0038 train_loss= 0.61775 train_acc= 0.84606 val_loss= 0.62909 val_acc= 0.83029 time= 0.12199
Epoch: 0039 train_loss= 0.60866 train_acc= 0.84748 val_loss= 0.61395 val_acc= 0.83212 time= 0.14600
Epoch: 0040 train_loss= 0.58648 train_acc= 0.84849 val_loss= 0.59947 val_acc= 0.83759 time= 0.13800
Epoch: 0041 train_loss= 0.57120 train_acc= 0.85092 val_loss= 0.58569 val_acc= 0.83759 time= 0.12305
Epoch: 0042 train_loss= 0.56889 train_acc= 0.85416 val_loss= 0.57255 val_acc= 0.84124 time= 0.12299
Epoch: 0043 train_loss= 0.54147 train_acc= 0.86024 val_loss= 0.55994 val_acc= 0.84672 time= 0.12404
Epoch: 0044 train_loss= 0.53495 train_acc= 0.86834 val_loss= 0.54784 val_acc= 0.84854 time= 0.12697
Epoch: 0045 train_loss= 0.51460 train_acc= 0.86004 val_loss= 0.53628 val_acc= 0.85036 time= 0.12611
Epoch: 0046 train_loss= 0.49497 train_acc= 0.87340 val_loss= 0.52522 val_acc= 0.85219 time= 0.12392
Epoch: 0047 train_loss= 0.48615 train_acc= 0.86854 val_loss= 0.51454 val_acc= 0.85766 time= 0.16499
Epoch: 0048 train_loss= 0.48204 train_acc= 0.86328 val_loss= 0.50420 val_acc= 0.86131 time= 0.12405
Epoch: 0049 train_loss= 0.46724 train_acc= 0.87138 val_loss= 0.49412 val_acc= 0.86679 time= 0.12399
Epoch: 0050 train_loss= 0.46521 train_acc= 0.87219 val_loss= 0.48427 val_acc= 0.86861 time= 0.12214
Epoch: 0051 train_loss= 0.44833 train_acc= 0.87786 val_loss= 0.47462 val_acc= 0.87409 time= 0.12397
Epoch: 0052 train_loss= 0.44016 train_acc= 0.87725 val_loss= 0.46524 val_acc= 0.87409 time= 0.12503
Epoch: 0053 train_loss= 0.42604 train_acc= 0.88313 val_loss= 0.45617 val_acc= 0.87591 time= 0.12597
Epoch: 0054 train_loss= 0.42622 train_acc= 0.88232 val_loss= 0.44733 val_acc= 0.88139 time= 0.12403
Epoch: 0055 train_loss= 0.41883 train_acc= 0.88738 val_loss= 0.43872 val_acc= 0.88504 time= 0.15501
Epoch: 0056 train_loss= 0.41913 train_acc= 0.89103 val_loss= 0.43029 val_acc= 0.88504 time= 0.12306
Epoch: 0057 train_loss= 0.39163 train_acc= 0.89184 val_loss= 0.42202 val_acc= 0.88686 time= 0.12499
Epoch: 0058 train_loss= 0.38830 train_acc= 0.89407 val_loss= 0.41393 val_acc= 0.89051 time= 0.12500
Epoch: 0059 train_loss= 0.38898 train_acc= 0.88839 val_loss= 0.40599 val_acc= 0.89416 time= 0.12296
Epoch: 0060 train_loss= 0.37539 train_acc= 0.89204 val_loss= 0.39829 val_acc= 0.89599 time= 0.12503
Epoch: 0061 train_loss= 0.35460 train_acc= 0.90541 val_loss= 0.39090 val_acc= 0.90328 time= 0.12710
Epoch: 0062 train_loss= 0.35578 train_acc= 0.90622 val_loss= 0.38379 val_acc= 0.90693 time= 0.12300
Epoch: 0063 train_loss= 0.34544 train_acc= 0.90318 val_loss= 0.37688 val_acc= 0.90876 time= 0.15500
Epoch: 0064 train_loss= 0.32900 train_acc= 0.91736 val_loss= 0.37016 val_acc= 0.91058 time= 0.12404
Epoch: 0065 train_loss= 0.33237 train_acc= 0.91513 val_loss= 0.36371 val_acc= 0.90876 time= 0.12200
Epoch: 0066 train_loss= 0.32130 train_acc= 0.91999 val_loss= 0.35749 val_acc= 0.91058 time= 0.12307
Epoch: 0067 train_loss= 0.32207 train_acc= 0.92141 val_loss= 0.35153 val_acc= 0.91423 time= 0.12207
Epoch: 0068 train_loss= 0.31297 train_acc= 0.92546 val_loss= 0.34576 val_acc= 0.91971 time= 0.12600
Epoch: 0069 train_loss= 0.29753 train_acc= 0.93073 val_loss= 0.34039 val_acc= 0.92336 time= 0.12497
Epoch: 0070 train_loss= 0.29528 train_acc= 0.93032 val_loss= 0.33499 val_acc= 0.92518 time= 0.12807
Epoch: 0071 train_loss= 0.29323 train_acc= 0.93356 val_loss= 0.32942 val_acc= 0.92518 time= 0.16603
Epoch: 0072 train_loss= 0.28953 train_acc= 0.93296 val_loss= 0.32391 val_acc= 0.92518 time= 0.12400
Epoch: 0073 train_loss= 0.27506 train_acc= 0.93903 val_loss= 0.31836 val_acc= 0.92701 time= 0.12400
Epoch: 0074 train_loss= 0.28369 train_acc= 0.92911 val_loss= 0.31246 val_acc= 0.92701 time= 0.12297
Epoch: 0075 train_loss= 0.26764 train_acc= 0.93761 val_loss= 0.30665 val_acc= 0.92883 time= 0.12303
Epoch: 0076 train_loss= 0.27098 train_acc= 0.93356 val_loss= 0.30095 val_acc= 0.92701 time= 0.12505
Epoch: 0077 train_loss= 0.26630 train_acc= 0.93478 val_loss= 0.29552 val_acc= 0.92518 time= 0.12404
Epoch: 0078 train_loss= 0.25673 train_acc= 0.94592 val_loss= 0.29027 val_acc= 0.92518 time= 0.16600
Epoch: 0079 train_loss= 0.24854 train_acc= 0.93984 val_loss= 0.28515 val_acc= 0.92883 time= 0.12301
Epoch: 0080 train_loss= 0.23501 train_acc= 0.94308 val_loss= 0.28030 val_acc= 0.92883 time= 0.12424
Epoch: 0081 train_loss= 0.23749 train_acc= 0.94531 val_loss= 0.27571 val_acc= 0.93248 time= 0.12500
Epoch: 0082 train_loss= 0.22798 train_acc= 0.95260 val_loss= 0.27134 val_acc= 0.93431 time= 0.12400
Epoch: 0083 train_loss= 0.22608 train_acc= 0.95159 val_loss= 0.26731 val_acc= 0.93613 time= 0.12300
Epoch: 0084 train_loss= 0.23004 train_acc= 0.94430 val_loss= 0.26348 val_acc= 0.93796 time= 0.12613
Epoch: 0085 train_loss= 0.22790 train_acc= 0.94653 val_loss= 0.25994 val_acc= 0.93613 time= 0.12385
Epoch: 0086 train_loss= 0.20268 train_acc= 0.95321 val_loss= 0.25642 val_acc= 0.93613 time= 0.16000
Epoch: 0087 train_loss= 0.21426 train_acc= 0.95220 val_loss= 0.25282 val_acc= 0.93613 time= 0.12303
Epoch: 0088 train_loss= 0.21068 train_acc= 0.95058 val_loss= 0.24950 val_acc= 0.93613 time= 0.12306
Epoch: 0089 train_loss= 0.20186 train_acc= 0.95625 val_loss= 0.24623 val_acc= 0.93613 time= 0.12399
Epoch: 0090 train_loss= 0.20104 train_acc= 0.95382 val_loss= 0.24285 val_acc= 0.93978 time= 0.12301
Epoch: 0091 train_loss= 0.19879 train_acc= 0.95200 val_loss= 0.23925 val_acc= 0.94161 time= 0.12310
Epoch: 0092 train_loss= 0.19165 train_acc= 0.95382 val_loss= 0.23583 val_acc= 0.94161 time= 0.12491
Epoch: 0093 train_loss= 0.18653 train_acc= 0.95240 val_loss= 0.23219 val_acc= 0.94343 time= 0.12500
Epoch: 0094 train_loss= 0.17392 train_acc= 0.95989 val_loss= 0.22878 val_acc= 0.94526 time= 0.15400
Epoch: 0095 train_loss= 0.18969 train_acc= 0.95665 val_loss= 0.22528 val_acc= 0.94343 time= 0.12600
Epoch: 0096 train_loss= 0.18361 train_acc= 0.96010 val_loss= 0.22211 val_acc= 0.94343 time= 0.12501
Epoch: 0097 train_loss= 0.18491 train_acc= 0.95929 val_loss= 0.21898 val_acc= 0.94343 time= 0.12306
Epoch: 0098 train_loss= 0.16567 train_acc= 0.96212 val_loss= 0.21603 val_acc= 0.94343 time= 0.12298
Epoch: 0099 train_loss= 0.17790 train_acc= 0.95483 val_loss= 0.21316 val_acc= 0.94526 time= 0.12300
Epoch: 0100 train_loss= 0.17266 train_acc= 0.96212 val_loss= 0.21071 val_acc= 0.94526 time= 0.12701
Epoch: 0101 train_loss= 0.16264 train_acc= 0.96435 val_loss= 0.20831 val_acc= 0.94526 time= 0.12699
Epoch: 0102 train_loss= 0.16659 train_acc= 0.96091 val_loss= 0.20604 val_acc= 0.94526 time= 0.16797
Epoch: 0103 train_loss= 0.16034 train_acc= 0.96253 val_loss= 0.20375 val_acc= 0.94708 time= 0.12604
Epoch: 0104 train_loss= 0.16318 train_acc= 0.96233 val_loss= 0.20163 val_acc= 0.94891 time= 0.12500
Epoch: 0105 train_loss= 0.15764 train_acc= 0.95969 val_loss= 0.19934 val_acc= 0.95073 time= 0.12353
Epoch: 0106 train_loss= 0.14787 train_acc= 0.96617 val_loss= 0.19694 val_acc= 0.95255 time= 0.12325
Epoch: 0107 train_loss= 0.15335 train_acc= 0.96233 val_loss= 0.19503 val_acc= 0.95255 time= 0.12200
Epoch: 0108 train_loss= 0.14290 train_acc= 0.96719 val_loss= 0.19319 val_acc= 0.95255 time= 0.12508
Epoch: 0109 train_loss= 0.14686 train_acc= 0.96617 val_loss= 0.19138 val_acc= 0.95255 time= 0.16700
Epoch: 0110 train_loss= 0.14448 train_acc= 0.96455 val_loss= 0.18949 val_acc= 0.95255 time= 0.12597
Epoch: 0111 train_loss= 0.14386 train_acc= 0.96273 val_loss= 0.18765 val_acc= 0.95255 time= 0.12803
Epoch: 0112 train_loss= 0.13737 train_acc= 0.96820 val_loss= 0.18591 val_acc= 0.95438 time= 0.12300
Epoch: 0113 train_loss= 0.13722 train_acc= 0.96617 val_loss= 0.18422 val_acc= 0.95438 time= 0.12400
Epoch: 0114 train_loss= 0.13159 train_acc= 0.96779 val_loss= 0.18253 val_acc= 0.95438 time= 0.12300
Epoch: 0115 train_loss= 0.13230 train_acc= 0.96860 val_loss= 0.18113 val_acc= 0.95438 time= 0.12301
Epoch: 0116 train_loss= 0.13475 train_acc= 0.96779 val_loss= 0.17966 val_acc= 0.95438 time= 0.12607
Epoch: 0117 train_loss= 0.13131 train_acc= 0.96719 val_loss= 0.17826 val_acc= 0.95438 time= 0.14997
Epoch: 0118 train_loss= 0.13172 train_acc= 0.96901 val_loss= 0.17696 val_acc= 0.95438 time= 0.12411
Epoch: 0119 train_loss= 0.12127 train_acc= 0.97205 val_loss= 0.17579 val_acc= 0.95438 time= 0.12603
Epoch: 0120 train_loss= 0.13037 train_acc= 0.96860 val_loss= 0.17465 val_acc= 0.95438 time= 0.12300
Epoch: 0121 train_loss= 0.13152 train_acc= 0.96577 val_loss= 0.17351 val_acc= 0.95438 time= 0.12401
Epoch: 0122 train_loss= 0.12326 train_acc= 0.96800 val_loss= 0.17223 val_acc= 0.95438 time= 0.12400
Epoch: 0123 train_loss= 0.11664 train_acc= 0.97185 val_loss= 0.17099 val_acc= 0.95438 time= 0.12407
Epoch: 0124 train_loss= 0.11439 train_acc= 0.97103 val_loss= 0.16976 val_acc= 0.95438 time= 0.12658
Epoch: 0125 train_loss= 0.11849 train_acc= 0.97002 val_loss= 0.16856 val_acc= 0.95255 time= 0.16000
Epoch: 0126 train_loss= 0.11358 train_acc= 0.97164 val_loss= 0.16723 val_acc= 0.95073 time= 0.12300
Epoch: 0127 train_loss= 0.11336 train_acc= 0.97347 val_loss= 0.16595 val_acc= 0.95073 time= 0.12608
Epoch: 0128 train_loss= 0.11421 train_acc= 0.97509 val_loss= 0.16479 val_acc= 0.95073 time= 0.12512
Epoch: 0129 train_loss= 0.11861 train_acc= 0.97407 val_loss= 0.16375 val_acc= 0.95073 time= 0.12300
Epoch: 0130 train_loss= 0.11291 train_acc= 0.96962 val_loss= 0.16325 val_acc= 0.95073 time= 0.12513
Epoch: 0131 train_loss= 0.10406 train_acc= 0.97772 val_loss= 0.16293 val_acc= 0.95255 time= 0.12300
Epoch: 0132 train_loss= 0.10683 train_acc= 0.97549 val_loss= 0.16237 val_acc= 0.95255 time= 0.12896
Epoch: 0133 train_loss= 0.10658 train_acc= 0.97245 val_loss= 0.16144 val_acc= 0.95073 time= 0.15822
Epoch: 0134 train_loss= 0.10557 train_acc= 0.97428 val_loss= 0.16020 val_acc= 0.95073 time= 0.12306
Epoch: 0135 train_loss= 0.11415 train_acc= 0.97266 val_loss= 0.15857 val_acc= 0.95073 time= 0.12500
Epoch: 0136 train_loss= 0.10270 train_acc= 0.97428 val_loss= 0.15692 val_acc= 0.95073 time= 0.12796
Epoch: 0137 train_loss= 0.10405 train_acc= 0.97407 val_loss= 0.15577 val_acc= 0.95073 time= 0.12400
Epoch: 0138 train_loss= 0.09769 train_acc= 0.97792 val_loss= 0.15483 val_acc= 0.94891 time= 0.12403
Epoch: 0139 train_loss= 0.10110 train_acc= 0.97347 val_loss= 0.15412 val_acc= 0.95073 time= 0.12364
Epoch: 0140 train_loss= 0.10039 train_acc= 0.97691 val_loss= 0.15374 val_acc= 0.95073 time= 0.17700
Epoch: 0141 train_loss= 0.10127 train_acc= 0.97488 val_loss= 0.15349 val_acc= 0.95073 time= 0.12300
Epoch: 0142 train_loss= 0.09490 train_acc= 0.97833 val_loss= 0.15322 val_acc= 0.95073 time= 0.12200
Epoch: 0143 train_loss= 0.09443 train_acc= 0.97873 val_loss= 0.15299 val_acc= 0.95073 time= 0.12500
Epoch: 0144 train_loss= 0.09151 train_acc= 0.97833 val_loss= 0.15288 val_acc= 0.95073 time= 0.12697
Epoch: 0145 train_loss= 0.09041 train_acc= 0.97590 val_loss= 0.15270 val_acc= 0.95073 time= 0.12503
Epoch: 0146 train_loss= 0.09567 train_acc= 0.97853 val_loss= 0.15245 val_acc= 0.95438 time= 0.12300
Epoch: 0147 train_loss= 0.09253 train_acc= 0.97509 val_loss= 0.15205 val_acc= 0.95438 time= 0.12318
Epoch: 0148 train_loss= 0.09837 train_acc= 0.97549 val_loss= 0.15153 val_acc= 0.95438 time= 0.15900
Epoch: 0149 train_loss= 0.08511 train_acc= 0.97853 val_loss= 0.15111 val_acc= 0.95438 time= 0.12590
Epoch: 0150 train_loss= 0.08708 train_acc= 0.97711 val_loss= 0.15047 val_acc= 0.95438 time= 0.12400
Epoch: 0151 train_loss= 0.08430 train_acc= 0.97934 val_loss= 0.14952 val_acc= 0.95255 time= 0.12300
Epoch: 0152 train_loss= 0.09521 train_acc= 0.97671 val_loss= 0.14876 val_acc= 0.95255 time= 0.12700
Epoch: 0153 train_loss= 0.08671 train_acc= 0.97934 val_loss= 0.14824 val_acc= 0.95255 time= 0.12301
Epoch: 0154 train_loss= 0.08906 train_acc= 0.97914 val_loss= 0.14737 val_acc= 0.95438 time= 0.12399
Epoch: 0155 train_loss= 0.09290 train_acc= 0.97752 val_loss= 0.14661 val_acc= 0.95438 time= 0.12304
Epoch: 0156 train_loss= 0.08612 train_acc= 0.98015 val_loss= 0.14593 val_acc= 0.95438 time= 0.16700
Epoch: 0157 train_loss= 0.09049 train_acc= 0.97731 val_loss= 0.14546 val_acc= 0.95438 time= 0.12402
Epoch: 0158 train_loss= 0.07746 train_acc= 0.98177 val_loss= 0.14495 val_acc= 0.95438 time= 0.12495
Epoch: 0159 train_loss= 0.07997 train_acc= 0.98157 val_loss= 0.14473 val_acc= 0.95438 time= 0.12403
Epoch: 0160 train_loss= 0.07305 train_acc= 0.98440 val_loss= 0.14456 val_acc= 0.95438 time= 0.12497
Epoch: 0161 train_loss= 0.07555 train_acc= 0.98035 val_loss= 0.14465 val_acc= 0.95438 time= 0.12600
Epoch: 0162 train_loss= 0.08113 train_acc= 0.98157 val_loss= 0.14490 val_acc= 0.95620 time= 0.12301
Early stopping...
Optimization Finished!
Test set results: cost= 0.11582 accuracy= 0.97350 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9508    0.9587    0.9547       121
           1     0.8605    0.9867    0.9193        75
           2     0.9862    0.9908    0.9885      1083
           3     1.0000    1.0000    1.0000        10
           4     0.9630    0.7222    0.8254        36
           5     0.9722    0.8642    0.9150        81
           6     0.9000    0.9310    0.9153        87
           7     0.9813    0.9784    0.9799       696

    accuracy                         0.9735      2189
   macro avg     0.9517    0.9290    0.9373      2189
weighted avg     0.9741    0.9735    0.9732      2189

Macro average Test Precision, Recall and F1-Score...
(0.9517439025521122, 0.9290016567478182, 0.9372514494409361, None)
Micro average Test Precision, Recall and F1-Score...
(0.9735038830516217, 0.9735038830516217, 0.9735038830516217, None)
embeddings:
7688 5485 2189
[[ 0.06671894  0.27851254  0.17414576 ...  0.15903798  0.1810348
   0.23717596]
 [ 0.07719596  0.16923994  0.11570347 ...  0.0348472   0.0541608
   0.10115906]
 [ 0.295257   -0.04568993  0.22677566 ... -0.03018001 -0.02973359
   0.00093129]
 ...
 [ 0.26898322  0.09577225  0.27130908 ...  0.04892296  0.04457576
   0.0467527 ]
 [ 0.11354633  0.20911472  0.10047585 ...  0.03877953  0.06648478
   0.11058138]
 [ 0.24152572 -0.00615759  0.20427036 ...  0.03572435  0.00572594
  -0.03264483]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07948 train_acc= 0.07616 val_loss= 1.60988 val_acc= 0.75912 time= 0.38603
Epoch: 0002 train_loss= 1.58671 train_acc= 0.78367 val_loss= 1.27287 val_acc= 0.70438 time= 0.12900
Epoch: 0003 train_loss= 1.22869 train_acc= 0.72817 val_loss= 1.05174 val_acc= 0.66058 time= 0.12400
Epoch: 0004 train_loss= 1.00245 train_acc= 0.66721 val_loss= 0.80465 val_acc= 0.75365 time= 0.12200
Epoch: 0005 train_loss= 0.76147 train_acc= 0.78165 val_loss= 0.68811 val_acc= 0.75365 time= 0.14200
Epoch: 0006 train_loss= 0.64658 train_acc= 0.77375 val_loss= 0.63269 val_acc= 0.76825 time= 0.12201
Epoch: 0007 train_loss= 0.58194 train_acc= 0.79664 val_loss= 0.59532 val_acc= 0.80839 time= 0.12399
Epoch: 0008 train_loss= 0.53857 train_acc= 0.83188 val_loss= 0.55755 val_acc= 0.82847 time= 0.12234
Epoch: 0009 train_loss= 0.49619 train_acc= 0.85436 val_loss= 0.52194 val_acc= 0.83942 time= 0.12506
Epoch: 0010 train_loss= 0.45330 train_acc= 0.86166 val_loss= 0.49103 val_acc= 0.84854 time= 0.12400
Epoch: 0011 train_loss= 0.41257 train_acc= 0.86834 val_loss= 0.46042 val_acc= 0.86679 time= 0.12300
Epoch: 0012 train_loss= 0.37857 train_acc= 0.88110 val_loss= 0.42679 val_acc= 0.88321 time= 0.12201
Epoch: 0013 train_loss= 0.34326 train_acc= 0.89548 val_loss= 0.39411 val_acc= 0.89416 time= 0.14797
Epoch: 0014 train_loss= 0.30342 train_acc= 0.91290 val_loss= 0.36506 val_acc= 0.90511 time= 0.12204
Epoch: 0015 train_loss= 0.27899 train_acc= 0.92364 val_loss= 0.33982 val_acc= 0.91606 time= 0.12338
Epoch: 0016 train_loss= 0.24152 train_acc= 0.93701 val_loss= 0.31586 val_acc= 0.92336 time= 0.12300
Epoch: 0017 train_loss= 0.21226 train_acc= 0.94248 val_loss= 0.29473 val_acc= 0.93066 time= 0.12299
Epoch: 0018 train_loss= 0.18534 train_acc= 0.94936 val_loss= 0.27906 val_acc= 0.93431 time= 0.12200
Epoch: 0019 train_loss= 0.16054 train_acc= 0.95362 val_loss= 0.26678 val_acc= 0.93431 time= 0.12304
Epoch: 0020 train_loss= 0.14114 train_acc= 0.96111 val_loss= 0.25525 val_acc= 0.93613 time= 0.12307
Epoch: 0021 train_loss= 0.12400 train_acc= 0.96374 val_loss= 0.24451 val_acc= 0.94161 time= 0.15001
Epoch: 0022 train_loss= 0.10753 train_acc= 0.96860 val_loss= 0.23661 val_acc= 0.94526 time= 0.12200
Epoch: 0023 train_loss= 0.09638 train_acc= 0.97002 val_loss= 0.23472 val_acc= 0.94526 time= 0.12202
Epoch: 0024 train_loss= 0.08272 train_acc= 0.97468 val_loss= 0.23922 val_acc= 0.94161 time= 0.12203
Epoch: 0025 train_loss= 0.08406 train_acc= 0.96962 val_loss= 0.24416 val_acc= 0.93978 time= 0.12297
Epoch: 0026 train_loss= 0.07017 train_acc= 0.97549 val_loss= 0.24039 val_acc= 0.94343 time= 0.12208
Epoch: 0027 train_loss= 0.06323 train_acc= 0.97731 val_loss= 0.23564 val_acc= 0.95255 time= 0.12300
Epoch: 0028 train_loss= 0.05614 train_acc= 0.98238 val_loss= 0.23323 val_acc= 0.95438 time= 0.12300
Epoch: 0029 train_loss= 0.05223 train_acc= 0.98501 val_loss= 0.23183 val_acc= 0.95073 time= 0.15596
Epoch: 0030 train_loss= 0.04946 train_acc= 0.98440 val_loss= 0.23342 val_acc= 0.95255 time= 0.12203
Epoch: 0031 train_loss= 0.04149 train_acc= 0.98683 val_loss= 0.23879 val_acc= 0.95438 time= 0.12397
Early stopping...
Optimization Finished!
Test set results: cost= 0.15346 accuracy= 0.96574 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9062    0.9587    0.9317       121
           1     0.8902    0.9733    0.9299        75
           2     0.9790    0.9917    0.9853      1083
           3     0.6250    0.5000    0.5556        10
           4     0.9600    0.6667    0.7869        36
           5     0.8889    0.8889    0.8889        81
           6     0.9080    0.9080    0.9080        87
           7     0.9853    0.9641    0.9746       696

    accuracy                         0.9657      2189
   macro avg     0.8928    0.8564    0.8701      2189
weighted avg     0.9659    0.9657    0.9652      2189

Macro average Test Precision, Recall and F1-Score...
(0.8928472761096764, 0.856422845289179, 0.8701178009001043, None)
Micro average Test Precision, Recall and F1-Score...
(0.9657377798081316, 0.9657377798081316, 0.9657377798081316, None)
embeddings:
7688 5485 2189
[[ 0.09422265  0.1591835  -0.351789   ...  0.6910619   0.11455464
   0.08133058]
 [ 0.17372282  0.22159004 -0.3239747  ...  0.26357886 -0.16193822
   0.24733905]
 [ 0.5924559   0.17716885 -0.40303895 ...  0.12301475 -0.02629639
   0.91747326]
 ...
 [ 0.5650798   0.14389403 -0.40596893 ...  0.18737519  0.21087484
   0.46666282]
 [ 0.20495784  0.3196083  -0.47472087 ...  0.3098664  -0.30107117
   0.42031705]
 [ 0.6554243  -0.01092978 -0.29945678 ...  0.06222763  0.07700573
   0.6703828 ]]

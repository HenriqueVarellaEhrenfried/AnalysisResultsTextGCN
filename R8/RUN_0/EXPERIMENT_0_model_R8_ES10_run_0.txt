(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07926 train_acc= 0.31234 val_loss= 2.02002 val_acc= 0.74818 time= 1.71349
Epoch: 0002 train_loss= 2.01750 train_acc= 0.76180 val_loss= 1.92179 val_acc= 0.73723 time= 0.13041
Epoch: 0003 train_loss= 1.91491 train_acc= 0.74377 val_loss= 1.78912 val_acc= 0.69526 time= 0.12501
Epoch: 0004 train_loss= 1.78101 train_acc= 0.70711 val_loss= 1.63651 val_acc= 0.65146 time= 0.12507
Epoch: 0005 train_loss= 1.62448 train_acc= 0.67774 val_loss= 1.48823 val_acc= 0.62044 time= 0.13800
Epoch: 0006 train_loss= 1.46857 train_acc= 0.64837 val_loss= 1.36671 val_acc= 0.62044 time= 0.12309
Epoch: 0007 train_loss= 1.33184 train_acc= 0.64067 val_loss= 1.27734 val_acc= 0.64051 time= 0.12367
Epoch: 0008 train_loss= 1.23051 train_acc= 0.66336 val_loss= 1.20980 val_acc= 0.67701 time= 0.12296
Epoch: 0009 train_loss= 1.17130 train_acc= 0.69131 val_loss= 1.14934 val_acc= 0.71168 time= 0.12304
Epoch: 0010 train_loss= 1.11027 train_acc= 0.72352 val_loss= 1.08701 val_acc= 0.73540 time= 0.12301
Epoch: 0011 train_loss= 1.03957 train_acc= 0.75127 val_loss= 1.02016 val_acc= 0.75182 time= 0.12399
Epoch: 0012 train_loss= 0.97142 train_acc= 0.77537 val_loss= 0.95081 val_acc= 0.76095 time= 0.12497
Epoch: 0013 train_loss= 0.90981 train_acc= 0.78205 val_loss= 0.88294 val_acc= 0.76095 time= 0.15804
Epoch: 0014 train_loss= 0.84192 train_acc= 0.78833 val_loss= 0.82053 val_acc= 0.75730 time= 0.12300
Epoch: 0015 train_loss= 0.78555 train_acc= 0.78651 val_loss= 0.76644 val_acc= 0.75547 time= 0.12300
Epoch: 0016 train_loss= 0.73058 train_acc= 0.78894 val_loss= 0.72188 val_acc= 0.76642 time= 0.12299
Epoch: 0017 train_loss= 0.68869 train_acc= 0.79076 val_loss= 0.68600 val_acc= 0.77190 time= 0.12401
Epoch: 0018 train_loss= 0.65421 train_acc= 0.80413 val_loss= 0.65672 val_acc= 0.79380 time= 0.12300
Epoch: 0019 train_loss= 0.62302 train_acc= 0.81892 val_loss= 0.63137 val_acc= 0.82117 time= 0.12400
Epoch: 0020 train_loss= 0.59583 train_acc= 0.84424 val_loss= 0.60772 val_acc= 0.83029 time= 0.15607
Epoch: 0021 train_loss= 0.57525 train_acc= 0.85923 val_loss= 0.58431 val_acc= 0.85401 time= 0.12600
Epoch: 0022 train_loss= 0.54954 train_acc= 0.86996 val_loss= 0.56087 val_acc= 0.86131 time= 0.12300
Epoch: 0023 train_loss= 0.52233 train_acc= 0.87624 val_loss= 0.53780 val_acc= 0.86314 time= 0.12400
Epoch: 0024 train_loss= 0.49808 train_acc= 0.87928 val_loss= 0.51563 val_acc= 0.86861 time= 0.12401
Epoch: 0025 train_loss= 0.47410 train_acc= 0.88515 val_loss= 0.49486 val_acc= 0.87409 time= 0.12503
Epoch: 0026 train_loss= 0.45224 train_acc= 0.88637 val_loss= 0.47572 val_acc= 0.87409 time= 0.12300
Epoch: 0027 train_loss= 0.43261 train_acc= 0.89022 val_loss= 0.45804 val_acc= 0.88686 time= 0.12407
Epoch: 0028 train_loss= 0.41121 train_acc= 0.89791 val_loss= 0.44161 val_acc= 0.88869 time= 0.16200
Epoch: 0029 train_loss= 0.39204 train_acc= 0.89974 val_loss= 0.42607 val_acc= 0.89599 time= 0.12211
Epoch: 0030 train_loss= 0.37344 train_acc= 0.90521 val_loss= 0.41107 val_acc= 0.89964 time= 0.12357
Epoch: 0031 train_loss= 0.35731 train_acc= 0.90905 val_loss= 0.39652 val_acc= 0.90146 time= 0.12300
Epoch: 0032 train_loss= 0.34522 train_acc= 0.91088 val_loss= 0.38224 val_acc= 0.90146 time= 0.12300
Epoch: 0033 train_loss= 0.32597 train_acc= 0.91371 val_loss= 0.36821 val_acc= 0.90693 time= 0.12205
Epoch: 0034 train_loss= 0.31559 train_acc= 0.92060 val_loss= 0.35459 val_acc= 0.91241 time= 0.12400
Epoch: 0035 train_loss= 0.29931 train_acc= 0.92546 val_loss= 0.34139 val_acc= 0.91241 time= 0.12304
Epoch: 0036 train_loss= 0.28432 train_acc= 0.92890 val_loss= 0.32872 val_acc= 0.91423 time= 0.14796
Epoch: 0037 train_loss= 0.27355 train_acc= 0.93235 val_loss= 0.31663 val_acc= 0.91788 time= 0.12300
Epoch: 0038 train_loss= 0.26118 train_acc= 0.93761 val_loss= 0.30530 val_acc= 0.91788 time= 0.12304
Epoch: 0039 train_loss= 0.24584 train_acc= 0.94166 val_loss= 0.29471 val_acc= 0.92153 time= 0.12200
Epoch: 0040 train_loss= 0.23894 train_acc= 0.94612 val_loss= 0.28484 val_acc= 0.92518 time= 0.12417
Epoch: 0041 train_loss= 0.22156 train_acc= 0.95118 val_loss= 0.27538 val_acc= 0.92701 time= 0.12300
Epoch: 0042 train_loss= 0.21335 train_acc= 0.95564 val_loss= 0.26634 val_acc= 0.93066 time= 0.12296
Epoch: 0043 train_loss= 0.19902 train_acc= 0.95827 val_loss= 0.25754 val_acc= 0.93248 time= 0.12302
Epoch: 0044 train_loss= 0.19541 train_acc= 0.95544 val_loss= 0.24912 val_acc= 0.93613 time= 0.14998
Epoch: 0045 train_loss= 0.18321 train_acc= 0.95767 val_loss= 0.24109 val_acc= 0.93613 time= 0.12300
Epoch: 0046 train_loss= 0.17239 train_acc= 0.95969 val_loss= 0.23345 val_acc= 0.93613 time= 0.12211
Epoch: 0047 train_loss= 0.16530 train_acc= 0.96192 val_loss= 0.22630 val_acc= 0.93796 time= 0.12327
Epoch: 0048 train_loss= 0.15499 train_acc= 0.96435 val_loss= 0.21951 val_acc= 0.93978 time= 0.12407
Epoch: 0049 train_loss= 0.14825 train_acc= 0.96557 val_loss= 0.21336 val_acc= 0.93978 time= 0.12307
Epoch: 0050 train_loss= 0.14196 train_acc= 0.96455 val_loss= 0.20761 val_acc= 0.93978 time= 0.12300
Epoch: 0051 train_loss= 0.13901 train_acc= 0.96678 val_loss= 0.20226 val_acc= 0.93613 time= 0.12320
Epoch: 0052 train_loss= 0.13348 train_acc= 0.96658 val_loss= 0.19728 val_acc= 0.93796 time= 0.16306
Epoch: 0053 train_loss= 0.12181 train_acc= 0.96840 val_loss= 0.19269 val_acc= 0.93796 time= 0.12300
Epoch: 0054 train_loss= 0.11922 train_acc= 0.96982 val_loss= 0.18855 val_acc= 0.94161 time= 0.12500
Epoch: 0055 train_loss= 0.11436 train_acc= 0.96962 val_loss= 0.18477 val_acc= 0.94708 time= 0.12300
Epoch: 0056 train_loss= 0.10910 train_acc= 0.97185 val_loss= 0.18136 val_acc= 0.94891 time= 0.12396
Epoch: 0057 train_loss= 0.10785 train_acc= 0.97245 val_loss= 0.17785 val_acc= 0.94891 time= 0.12400
Epoch: 0058 train_loss= 0.10143 train_acc= 0.97509 val_loss= 0.17434 val_acc= 0.95073 time= 0.12400
Epoch: 0059 train_loss= 0.09705 train_acc= 0.97752 val_loss= 0.17121 val_acc= 0.95073 time= 0.12522
Epoch: 0060 train_loss= 0.09471 train_acc= 0.97691 val_loss= 0.16856 val_acc= 0.95073 time= 0.15600
Epoch: 0061 train_loss= 0.08994 train_acc= 0.97812 val_loss= 0.16621 val_acc= 0.95255 time= 0.12314
Epoch: 0062 train_loss= 0.08479 train_acc= 0.98218 val_loss= 0.16394 val_acc= 0.95255 time= 0.12399
Epoch: 0063 train_loss= 0.08294 train_acc= 0.98197 val_loss= 0.16179 val_acc= 0.95255 time= 0.12220
Epoch: 0064 train_loss= 0.08046 train_acc= 0.98238 val_loss= 0.15985 val_acc= 0.95255 time= 0.12304
Epoch: 0065 train_loss= 0.07961 train_acc= 0.98197 val_loss= 0.15835 val_acc= 0.95438 time= 0.12406
Epoch: 0066 train_loss= 0.07636 train_acc= 0.98359 val_loss= 0.15711 val_acc= 0.95255 time= 0.12204
Epoch: 0067 train_loss= 0.07358 train_acc= 0.98177 val_loss= 0.15584 val_acc= 0.95255 time= 0.15400
Epoch: 0068 train_loss= 0.06973 train_acc= 0.98420 val_loss= 0.15436 val_acc= 0.95255 time= 0.12700
Epoch: 0069 train_loss= 0.06877 train_acc= 0.98380 val_loss= 0.15233 val_acc= 0.95255 time= 0.12201
Epoch: 0070 train_loss= 0.06611 train_acc= 0.98420 val_loss= 0.15068 val_acc= 0.95255 time= 0.12358
Epoch: 0071 train_loss= 0.06173 train_acc= 0.98562 val_loss= 0.14929 val_acc= 0.95255 time= 0.12300
Epoch: 0072 train_loss= 0.06164 train_acc= 0.98643 val_loss= 0.14801 val_acc= 0.95255 time= 0.12405
Epoch: 0073 train_loss= 0.06037 train_acc= 0.98724 val_loss= 0.14678 val_acc= 0.95438 time= 0.12274
Epoch: 0074 train_loss= 0.05847 train_acc= 0.98562 val_loss= 0.14562 val_acc= 0.95620 time= 0.12500
Epoch: 0075 train_loss= 0.05806 train_acc= 0.98602 val_loss= 0.14471 val_acc= 0.95438 time= 0.16700
Epoch: 0076 train_loss= 0.05301 train_acc= 0.98906 val_loss= 0.14411 val_acc= 0.95438 time= 0.12300
Epoch: 0077 train_loss= 0.05151 train_acc= 0.98906 val_loss= 0.14390 val_acc= 0.95438 time= 0.12500
Epoch: 0078 train_loss= 0.05100 train_acc= 0.98866 val_loss= 0.14382 val_acc= 0.95438 time= 0.12301
Epoch: 0079 train_loss= 0.05155 train_acc= 0.98825 val_loss= 0.14389 val_acc= 0.95803 time= 0.12400
Epoch: 0080 train_loss= 0.04880 train_acc= 0.98926 val_loss= 0.14348 val_acc= 0.95620 time= 0.12497
Epoch: 0081 train_loss= 0.04684 train_acc= 0.98926 val_loss= 0.14293 val_acc= 0.95620 time= 0.12903
Epoch: 0082 train_loss= 0.04484 train_acc= 0.99170 val_loss= 0.14189 val_acc= 0.95620 time= 0.12600
Epoch: 0083 train_loss= 0.04376 train_acc= 0.98967 val_loss= 0.14087 val_acc= 0.95438 time= 0.15500
Epoch: 0084 train_loss= 0.04236 train_acc= 0.99149 val_loss= 0.14026 val_acc= 0.95438 time= 0.12500
Epoch: 0085 train_loss= 0.04290 train_acc= 0.99068 val_loss= 0.13990 val_acc= 0.95438 time= 0.12596
Epoch: 0086 train_loss= 0.04205 train_acc= 0.99170 val_loss= 0.13989 val_acc= 0.95620 time= 0.12603
Epoch: 0087 train_loss= 0.04034 train_acc= 0.99170 val_loss= 0.14016 val_acc= 0.95620 time= 0.12610
Epoch: 0088 train_loss= 0.03952 train_acc= 0.99028 val_loss= 0.14058 val_acc= 0.95438 time= 0.12596
Epoch: 0089 train_loss= 0.03832 train_acc= 0.99230 val_loss= 0.14062 val_acc= 0.95438 time= 0.12800
Epoch: 0090 train_loss= 0.03791 train_acc= 0.99190 val_loss= 0.14045 val_acc= 0.95073 time= 0.12900
Epoch: 0091 train_loss= 0.03627 train_acc= 0.99291 val_loss= 0.14072 val_acc= 0.95073 time= 0.15800
Epoch: 0092 train_loss= 0.03524 train_acc= 0.99332 val_loss= 0.14056 val_acc= 0.95255 time= 0.12310
Early stopping...
Optimization Finished!
Test set results: cost= 0.10850 accuracy= 0.97305 time= 0.05499
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9291    0.9752    0.9516       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.7222    0.8387        36
           5     0.9231    0.8889    0.9057        81
           6     0.9091    0.9195    0.9143        87
           7     0.9840    0.9727    0.9783       696

    accuracy                         0.9730      2189
   macro avg     0.9552    0.9304    0.9398      2189
weighted avg     0.9734    0.9730    0.9728      2189

Macro average Test Precision, Recall and F1-Score...
(0.955166225232376, 0.9304477732521947, 0.9397642578098564, None)
Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)
embeddings:
7688 5485 2189
[[ 0.17776823  0.34900343  0.07123087 ...  0.06587125  0.26875502
   0.1814312 ]
 [ 0.03247578  0.18175736  0.0950015  ...  0.23491219  0.23126431
   0.02602284]
 [ 0.10032906 -0.00907107  0.34809384 ...  0.64378774  0.08987942
   0.09218528]
 ...
 [ 0.17060606  0.14252955  0.33774382 ...  0.3590218   0.261237
   0.18276261]
 [ 0.0056107   0.22155687  0.14608479 ...  0.39901662  0.23964615
  -0.00377803]
 [ 0.12800874 -0.01319501  0.26957798 ...  0.4214927   0.03430463
   0.13553995]]

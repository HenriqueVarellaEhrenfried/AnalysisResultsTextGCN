(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07927 train_acc= 0.31234 val_loss= 2.00095 val_acc= 0.75912 time= 0.45213
Epoch: 0002 train_loss= 1.99858 train_acc= 0.78408 val_loss= 1.86363 val_acc= 0.76277 time= 0.16281
Epoch: 0003 train_loss= 1.85477 train_acc= 0.77760 val_loss= 1.68335 val_acc= 0.74635 time= 0.16128
Epoch: 0004 train_loss= 1.66694 train_acc= 0.75390 val_loss= 1.50008 val_acc= 0.71898 time= 0.18803
Epoch: 0005 train_loss= 1.47775 train_acc= 0.73830 val_loss= 1.35680 val_acc= 0.70803 time= 0.15697
Epoch: 0006 train_loss= 1.32165 train_acc= 0.72453 val_loss= 1.26000 val_acc= 0.69161 time= 0.15800
Epoch: 0007 train_loss= 1.21418 train_acc= 0.70731 val_loss= 1.18573 val_acc= 0.67883 time= 0.15703
Epoch: 0008 train_loss= 1.13649 train_acc= 0.70103 val_loss= 1.11177 val_acc= 0.70255 time= 0.15596
Epoch: 0009 train_loss= 1.06679 train_acc= 0.71886 val_loss= 1.02946 val_acc= 0.73358 time= 0.16300
Epoch: 0010 train_loss= 0.98602 train_acc= 0.75673 val_loss= 0.94244 val_acc= 0.76277 time= 0.19200
Epoch: 0011 train_loss= 0.90184 train_acc= 0.78084 val_loss= 0.85906 val_acc= 0.76095 time= 0.15700
Epoch: 0012 train_loss= 0.81999 train_acc= 0.78793 val_loss= 0.78687 val_acc= 0.75912 time= 0.15962
Epoch: 0013 train_loss= 0.74997 train_acc= 0.78813 val_loss= 0.72933 val_acc= 0.76460 time= 0.15607
Epoch: 0014 train_loss= 0.69317 train_acc= 0.78671 val_loss= 0.68559 val_acc= 0.76825 time= 0.15635
Epoch: 0015 train_loss= 0.65201 train_acc= 0.79603 val_loss= 0.65190 val_acc= 0.79380 time= 0.15997
Epoch: 0016 train_loss= 0.61873 train_acc= 0.81588 val_loss= 0.62377 val_acc= 0.82299 time= 0.16704
Epoch: 0017 train_loss= 0.59095 train_acc= 0.84059 val_loss= 0.59759 val_acc= 0.82847 time= 0.15696
Epoch: 0018 train_loss= 0.55809 train_acc= 0.85882 val_loss= 0.57162 val_acc= 0.84672 time= 0.15603
Epoch: 0019 train_loss= 0.53274 train_acc= 0.86753 val_loss= 0.54551 val_acc= 0.85401 time= 0.16070
Epoch: 0020 train_loss= 0.50580 train_acc= 0.87381 val_loss= 0.51990 val_acc= 0.85766 time= 0.15598
Epoch: 0021 train_loss= 0.47481 train_acc= 0.87685 val_loss= 0.49564 val_acc= 0.86131 time= 0.15700
Epoch: 0022 train_loss= 0.44821 train_acc= 0.88090 val_loss= 0.47326 val_acc= 0.86496 time= 0.19200
Epoch: 0023 train_loss= 0.42656 train_acc= 0.88353 val_loss= 0.45283 val_acc= 0.87409 time= 0.15700
Epoch: 0024 train_loss= 0.40298 train_acc= 0.89244 val_loss= 0.43410 val_acc= 0.88686 time= 0.15613
Epoch: 0025 train_loss= 0.37899 train_acc= 0.89447 val_loss= 0.41654 val_acc= 0.89234 time= 0.15713
Epoch: 0026 train_loss= 0.36221 train_acc= 0.90176 val_loss= 0.39970 val_acc= 0.89964 time= 0.15600
Epoch: 0027 train_loss= 0.34375 train_acc= 0.90946 val_loss= 0.38334 val_acc= 0.90146 time= 0.15600
Epoch: 0028 train_loss= 0.32131 train_acc= 0.91776 val_loss= 0.36742 val_acc= 0.90693 time= 0.19297
Epoch: 0029 train_loss= 0.30671 train_acc= 0.92080 val_loss= 0.35188 val_acc= 0.90876 time= 0.16003
Epoch: 0030 train_loss= 0.28762 train_acc= 0.92749 val_loss= 0.33694 val_acc= 0.91241 time= 0.15700
Epoch: 0031 train_loss= 0.27355 train_acc= 0.93255 val_loss= 0.32286 val_acc= 0.91606 time= 0.15700
Epoch: 0032 train_loss= 0.25907 train_acc= 0.93620 val_loss= 0.30971 val_acc= 0.92153 time= 0.15804
Epoch: 0033 train_loss= 0.24517 train_acc= 0.94126 val_loss= 0.29749 val_acc= 0.92518 time= 0.15801
Epoch: 0034 train_loss= 0.22903 train_acc= 0.94592 val_loss= 0.28607 val_acc= 0.92518 time= 0.17200
Epoch: 0035 train_loss= 0.21480 train_acc= 0.94936 val_loss= 0.27553 val_acc= 0.92518 time= 0.16403
Epoch: 0036 train_loss= 0.20725 train_acc= 0.95078 val_loss= 0.26551 val_acc= 0.92883 time= 0.15660
Epoch: 0037 train_loss= 0.19359 train_acc= 0.95463 val_loss= 0.25596 val_acc= 0.93248 time= 0.15700
Epoch: 0038 train_loss= 0.17967 train_acc= 0.95503 val_loss= 0.24668 val_acc= 0.93248 time= 0.15800
Epoch: 0039 train_loss= 0.17293 train_acc= 0.95665 val_loss= 0.23771 val_acc= 0.93248 time= 0.15806
Epoch: 0040 train_loss= 0.16102 train_acc= 0.96091 val_loss= 0.22903 val_acc= 0.93613 time= 0.15800
Epoch: 0041 train_loss= 0.15504 train_acc= 0.96010 val_loss= 0.22087 val_acc= 0.93613 time= 0.18800
Epoch: 0042 train_loss= 0.14211 train_acc= 0.96334 val_loss= 0.21356 val_acc= 0.93796 time= 0.16100
Epoch: 0043 train_loss= 0.13520 train_acc= 0.96617 val_loss= 0.20713 val_acc= 0.93978 time= 0.15705
Epoch: 0044 train_loss= 0.12473 train_acc= 0.96658 val_loss= 0.20143 val_acc= 0.93978 time= 0.15700
Epoch: 0045 train_loss= 0.12113 train_acc= 0.97124 val_loss= 0.19629 val_acc= 0.94343 time= 0.15699
Epoch: 0046 train_loss= 0.11343 train_acc= 0.97347 val_loss= 0.19157 val_acc= 0.94526 time= 0.15601
Epoch: 0047 train_loss= 0.10623 train_acc= 0.97529 val_loss= 0.18751 val_acc= 0.94891 time= 0.18700
Epoch: 0048 train_loss= 0.10507 train_acc= 0.97205 val_loss= 0.18385 val_acc= 0.94891 time= 0.15999
Epoch: 0049 train_loss= 0.09624 train_acc= 0.97549 val_loss= 0.18040 val_acc= 0.94891 time= 0.15801
Epoch: 0050 train_loss= 0.09375 train_acc= 0.97691 val_loss= 0.17692 val_acc= 0.95073 time= 0.15786
Epoch: 0051 train_loss= 0.08688 train_acc= 0.97893 val_loss= 0.17371 val_acc= 0.94891 time= 0.15600
Epoch: 0052 train_loss= 0.08315 train_acc= 0.97934 val_loss= 0.17067 val_acc= 0.94891 time= 0.15700
Epoch: 0053 train_loss= 0.07918 train_acc= 0.98076 val_loss= 0.16783 val_acc= 0.94891 time= 0.16301
Epoch: 0054 train_loss= 0.07764 train_acc= 0.98137 val_loss= 0.16557 val_acc= 0.94891 time= 0.15699
Epoch: 0055 train_loss= 0.07110 train_acc= 0.98278 val_loss= 0.16338 val_acc= 0.95073 time= 0.16000
Epoch: 0056 train_loss= 0.07014 train_acc= 0.98238 val_loss= 0.16146 val_acc= 0.95073 time= 0.15708
Epoch: 0057 train_loss= 0.06576 train_acc= 0.98440 val_loss= 0.15970 val_acc= 0.95255 time= 0.15799
Epoch: 0058 train_loss= 0.06360 train_acc= 0.98481 val_loss= 0.15840 val_acc= 0.95438 time= 0.15600
Epoch: 0059 train_loss= 0.06053 train_acc= 0.98481 val_loss= 0.15742 val_acc= 0.95255 time= 0.19200
Epoch: 0060 train_loss= 0.05800 train_acc= 0.98521 val_loss= 0.15647 val_acc= 0.95255 time= 0.15600
Epoch: 0061 train_loss= 0.05440 train_acc= 0.98724 val_loss= 0.15599 val_acc= 0.95255 time= 0.15800
Epoch: 0062 train_loss= 0.05391 train_acc= 0.98663 val_loss= 0.15542 val_acc= 0.95438 time= 0.16202
Epoch: 0063 train_loss= 0.05177 train_acc= 0.98825 val_loss= 0.15481 val_acc= 0.95438 time= 0.15799
Epoch: 0064 train_loss= 0.04923 train_acc= 0.98906 val_loss= 0.15407 val_acc= 0.95438 time= 0.15700
Epoch: 0065 train_loss= 0.04715 train_acc= 0.98886 val_loss= 0.15357 val_acc= 0.95438 time= 0.19100
Epoch: 0066 train_loss= 0.04614 train_acc= 0.98906 val_loss= 0.15312 val_acc= 0.95438 time= 0.15600
Epoch: 0067 train_loss= 0.04168 train_acc= 0.98947 val_loss= 0.15221 val_acc= 0.95255 time= 0.15900
Epoch: 0068 train_loss= 0.04122 train_acc= 0.99028 val_loss= 0.15167 val_acc= 0.95255 time= 0.16200
Epoch: 0069 train_loss= 0.04147 train_acc= 0.99109 val_loss= 0.15117 val_acc= 0.95255 time= 0.15700
Epoch: 0070 train_loss= 0.03962 train_acc= 0.99028 val_loss= 0.15102 val_acc= 0.95255 time= 0.15797
Epoch: 0071 train_loss= 0.03763 train_acc= 0.99109 val_loss= 0.15183 val_acc= 0.95255 time= 0.16603
Epoch: 0072 train_loss= 0.03641 train_acc= 0.99149 val_loss= 0.15271 val_acc= 0.95438 time= 0.16807
Epoch: 0073 train_loss= 0.03432 train_acc= 0.99251 val_loss= 0.15317 val_acc= 0.95438 time= 0.15600
Early stopping...
Optimization Finished!
Test set results: cost= 0.10778 accuracy= 0.97259 time= 0.06700
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9435    0.9669    0.9551       121
           1     0.9012    0.9733    0.9359        75
           2     0.9835    0.9917    0.9876      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9310    0.7500    0.8308        36
           5     0.8916    0.9136    0.9024        81
           6     0.9398    0.8966    0.9176        87
           7     0.9854    0.9713    0.9783       696

    accuracy                         0.9726      2189
   macro avg     0.9356    0.9329    0.9325      2189
weighted avg     0.9727    0.9726    0.9724      2189

Macro average Test Precision, Recall and F1-Score...
(0.9356466090117028, 0.9329201964567235, 0.9325142849913569, None)
Micro average Test Precision, Recall and F1-Score...
(0.9725902238465053, 0.9725902238465053, 0.9725902238465053, None)
embeddings:
7688 5485 2189
[[ 0.06783631  0.32270768  0.13404012 ...  0.15754394  0.14105478
   0.1083447 ]
 [ 0.0416274   0.19611749  0.05575734 ...  0.03553728  0.02184897
   0.10351252]
 [ 0.2502781   0.06232109  0.19724856 ...  0.14057422  0.1236403
   0.34872636]
 ...
 [ 0.27256328  0.1446328   0.24984393 ...  0.20202218  0.18031953
   0.3400644 ]
 [ 0.0485094   0.1959826   0.02816212 ...  0.0112536  -0.00377519
   0.10859103]
 [ 0.22717136  0.01555514  0.19322282 ...  0.1616981   0.12408921
   0.27128243]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07937 train_acc= 0.14685 val_loss= 2.02352 val_acc= 0.74088 time= 0.39013
Epoch: 0002 train_loss= 2.02084 train_acc= 0.74944 val_loss= 1.93006 val_acc= 0.73723 time= 0.13410
Epoch: 0003 train_loss= 1.92600 train_acc= 0.75005 val_loss= 1.80197 val_acc= 0.71898 time= 0.16109
Epoch: 0004 train_loss= 1.79326 train_acc= 0.73101 val_loss= 1.65237 val_acc= 0.69526 time= 0.12700
Epoch: 0005 train_loss= 1.63462 train_acc= 0.71319 val_loss= 1.50391 val_acc= 0.68066 time= 0.12200
Epoch: 0006 train_loss= 1.47890 train_acc= 0.69556 val_loss= 1.37905 val_acc= 0.68613 time= 0.12200
Epoch: 0007 train_loss= 1.34605 train_acc= 0.70022 val_loss= 1.28487 val_acc= 0.71715 time= 0.12404
Epoch: 0008 train_loss= 1.24427 train_acc= 0.73182 val_loss= 1.21250 val_acc= 0.72993 time= 0.12307
Epoch: 0009 train_loss= 1.17163 train_acc= 0.74256 val_loss= 1.14836 val_acc= 0.73723 time= 0.12605
Epoch: 0010 train_loss= 1.10378 train_acc= 0.75390 val_loss= 1.08366 val_acc= 0.74818 time= 0.17400
Epoch: 0011 train_loss= 1.04178 train_acc= 0.76605 val_loss= 1.01581 val_acc= 0.75912 time= 0.12400
Epoch: 0012 train_loss= 0.97441 train_acc= 0.77557 val_loss= 0.94643 val_acc= 0.75912 time= 0.12299
Epoch: 0013 train_loss= 0.90968 train_acc= 0.78205 val_loss= 0.87905 val_acc= 0.76095 time= 0.12700
Epoch: 0014 train_loss= 0.84134 train_acc= 0.78833 val_loss= 0.81766 val_acc= 0.76095 time= 0.12300
Epoch: 0015 train_loss= 0.77941 train_acc= 0.78773 val_loss= 0.76536 val_acc= 0.76460 time= 0.12401
Epoch: 0016 train_loss= 0.73099 train_acc= 0.78914 val_loss= 0.72311 val_acc= 0.76825 time= 0.12361
Epoch: 0017 train_loss= 0.69096 train_acc= 0.79684 val_loss= 0.68939 val_acc= 0.78285 time= 0.12501
Epoch: 0018 train_loss= 0.65651 train_acc= 0.81082 val_loss= 0.66134 val_acc= 0.80109 time= 0.15101
Epoch: 0019 train_loss= 0.62533 train_acc= 0.82844 val_loss= 0.63622 val_acc= 0.82299 time= 0.12399
Epoch: 0020 train_loss= 0.60280 train_acc= 0.84707 val_loss= 0.61206 val_acc= 0.83577 time= 0.12300
Epoch: 0021 train_loss= 0.57507 train_acc= 0.86105 val_loss= 0.58814 val_acc= 0.85036 time= 0.12500
Epoch: 0022 train_loss= 0.55025 train_acc= 0.86976 val_loss= 0.56461 val_acc= 0.85584 time= 0.12300
Epoch: 0023 train_loss= 0.52620 train_acc= 0.87097 val_loss= 0.54193 val_acc= 0.85584 time= 0.12300
Epoch: 0024 train_loss= 0.49705 train_acc= 0.87543 val_loss= 0.52058 val_acc= 0.85584 time= 0.12200
Epoch: 0025 train_loss= 0.47655 train_acc= 0.87948 val_loss= 0.50073 val_acc= 0.85766 time= 0.12400
Epoch: 0026 train_loss= 0.45528 train_acc= 0.88414 val_loss= 0.48239 val_acc= 0.86131 time= 0.16804
Epoch: 0027 train_loss= 0.43484 train_acc= 0.88880 val_loss= 0.46537 val_acc= 0.87409 time= 0.12503
Epoch: 0028 train_loss= 0.41543 train_acc= 0.89204 val_loss= 0.44936 val_acc= 0.88139 time= 0.12400
Epoch: 0029 train_loss= 0.39879 train_acc= 0.89791 val_loss= 0.43404 val_acc= 0.89051 time= 0.12300
Epoch: 0030 train_loss= 0.38368 train_acc= 0.90217 val_loss= 0.41911 val_acc= 0.89599 time= 0.12301
Epoch: 0031 train_loss= 0.36432 train_acc= 0.90723 val_loss= 0.40441 val_acc= 0.89599 time= 0.12429
Epoch: 0032 train_loss= 0.35112 train_acc= 0.91007 val_loss= 0.38993 val_acc= 0.89781 time= 0.12300
Epoch: 0033 train_loss= 0.33724 train_acc= 0.91594 val_loss= 0.37583 val_acc= 0.89964 time= 0.13797
Epoch: 0034 train_loss= 0.31950 train_acc= 0.92100 val_loss= 0.36227 val_acc= 0.90693 time= 0.14503
Epoch: 0035 train_loss= 0.30857 train_acc= 0.92404 val_loss= 0.34928 val_acc= 0.91058 time= 0.12601
Epoch: 0036 train_loss= 0.29266 train_acc= 0.92911 val_loss= 0.33686 val_acc= 0.91241 time= 0.12399
Epoch: 0037 train_loss= 0.27964 train_acc= 0.93133 val_loss= 0.32496 val_acc= 0.91241 time= 0.12210
Epoch: 0038 train_loss= 0.26398 train_acc= 0.93741 val_loss= 0.31350 val_acc= 0.92153 time= 0.12300
Epoch: 0039 train_loss= 0.25263 train_acc= 0.93822 val_loss= 0.30246 val_acc= 0.92336 time= 0.12352
Epoch: 0040 train_loss= 0.24180 train_acc= 0.94329 val_loss= 0.29176 val_acc= 0.92701 time= 0.12309
Epoch: 0041 train_loss= 0.23124 train_acc= 0.94774 val_loss= 0.28155 val_acc= 0.92883 time= 0.17300
Epoch: 0042 train_loss= 0.21812 train_acc= 0.95321 val_loss= 0.27168 val_acc= 0.93066 time= 0.12401
Epoch: 0043 train_loss= 0.20730 train_acc= 0.95544 val_loss= 0.26235 val_acc= 0.93248 time= 0.12500
Epoch: 0044 train_loss= 0.19686 train_acc= 0.95706 val_loss= 0.25344 val_acc= 0.93613 time= 0.12600
Epoch: 0045 train_loss= 0.18703 train_acc= 0.95888 val_loss= 0.24493 val_acc= 0.93613 time= 0.12299
Epoch: 0046 train_loss= 0.17771 train_acc= 0.95949 val_loss= 0.23684 val_acc= 0.93613 time= 0.12320
Epoch: 0047 train_loss= 0.16910 train_acc= 0.96010 val_loss= 0.22933 val_acc= 0.93613 time= 0.12306
Epoch: 0048 train_loss= 0.15912 train_acc= 0.96293 val_loss= 0.22243 val_acc= 0.93796 time= 0.12100
Epoch: 0049 train_loss= 0.15429 train_acc= 0.96131 val_loss= 0.21603 val_acc= 0.93978 time= 0.15297
Epoch: 0050 train_loss= 0.14411 train_acc= 0.96739 val_loss= 0.20988 val_acc= 0.93978 time= 0.12303
Epoch: 0051 train_loss= 0.13816 train_acc= 0.96800 val_loss= 0.20401 val_acc= 0.94161 time= 0.12501
Epoch: 0052 train_loss= 0.13085 train_acc= 0.96881 val_loss= 0.19843 val_acc= 0.94343 time= 0.12399
Epoch: 0053 train_loss= 0.12410 train_acc= 0.97144 val_loss= 0.19323 val_acc= 0.94343 time= 0.12400
Epoch: 0054 train_loss= 0.11832 train_acc= 0.97185 val_loss= 0.18828 val_acc= 0.94526 time= 0.12300
Epoch: 0055 train_loss= 0.11184 train_acc= 0.97367 val_loss= 0.18371 val_acc= 0.94526 time= 0.12400
Epoch: 0056 train_loss= 0.10583 train_acc= 0.97569 val_loss= 0.17949 val_acc= 0.94526 time= 0.12309
Epoch: 0057 train_loss= 0.10389 train_acc= 0.97509 val_loss= 0.17570 val_acc= 0.94708 time= 0.16997
Epoch: 0058 train_loss= 0.10016 train_acc= 0.97812 val_loss= 0.17218 val_acc= 0.94708 time= 0.12304
Epoch: 0059 train_loss= 0.09655 train_acc= 0.97731 val_loss= 0.16913 val_acc= 0.94708 time= 0.12397
Epoch: 0060 train_loss= 0.09253 train_acc= 0.97873 val_loss= 0.16650 val_acc= 0.95073 time= 0.12662
Epoch: 0061 train_loss= 0.08809 train_acc= 0.98177 val_loss= 0.16429 val_acc= 0.95073 time= 0.12400
Epoch: 0062 train_loss= 0.08511 train_acc= 0.98076 val_loss= 0.16242 val_acc= 0.95073 time= 0.12307
Epoch: 0063 train_loss= 0.08125 train_acc= 0.98197 val_loss= 0.16059 val_acc= 0.95073 time= 0.12307
Epoch: 0064 train_loss= 0.07753 train_acc= 0.98258 val_loss= 0.15849 val_acc= 0.95438 time= 0.14096
Epoch: 0065 train_loss= 0.07378 train_acc= 0.98319 val_loss= 0.15639 val_acc= 0.95438 time= 0.14500
Epoch: 0066 train_loss= 0.07165 train_acc= 0.98420 val_loss= 0.15431 val_acc= 0.95255 time= 0.12500
Epoch: 0067 train_loss= 0.06970 train_acc= 0.98359 val_loss= 0.15249 val_acc= 0.95255 time= 0.12320
Epoch: 0068 train_loss= 0.06607 train_acc= 0.98643 val_loss= 0.15095 val_acc= 0.95255 time= 0.12596
Epoch: 0069 train_loss= 0.06374 train_acc= 0.98623 val_loss= 0.14949 val_acc= 0.95255 time= 0.12504
Epoch: 0070 train_loss= 0.06113 train_acc= 0.98744 val_loss= 0.14846 val_acc= 0.95255 time= 0.12196
Epoch: 0071 train_loss= 0.05880 train_acc= 0.98582 val_loss= 0.14771 val_acc= 0.95255 time= 0.12400
Epoch: 0072 train_loss= 0.05833 train_acc= 0.98683 val_loss= 0.14711 val_acc= 0.95438 time= 0.16605
Epoch: 0073 train_loss= 0.05573 train_acc= 0.98683 val_loss= 0.14643 val_acc= 0.95438 time= 0.12295
Epoch: 0074 train_loss= 0.05341 train_acc= 0.98724 val_loss= 0.14603 val_acc= 0.95438 time= 0.12604
Epoch: 0075 train_loss= 0.05200 train_acc= 0.98764 val_loss= 0.14561 val_acc= 0.95438 time= 0.12306
Epoch: 0076 train_loss= 0.05083 train_acc= 0.98825 val_loss= 0.14521 val_acc= 0.95438 time= 0.12496
Epoch: 0077 train_loss= 0.04879 train_acc= 0.98906 val_loss= 0.14481 val_acc= 0.95255 time= 0.12400
Epoch: 0078 train_loss= 0.04790 train_acc= 0.98906 val_loss= 0.14427 val_acc= 0.95255 time= 0.12300
Epoch: 0079 train_loss= 0.04577 train_acc= 0.99048 val_loss= 0.14362 val_acc= 0.95255 time= 0.12300
Epoch: 0080 train_loss= 0.04446 train_acc= 0.98987 val_loss= 0.14327 val_acc= 0.95255 time= 0.15004
Epoch: 0081 train_loss= 0.04342 train_acc= 0.99089 val_loss= 0.14311 val_acc= 0.95255 time= 0.12294
Epoch: 0082 train_loss= 0.04189 train_acc= 0.99109 val_loss= 0.14292 val_acc= 0.95438 time= 0.12504
Epoch: 0083 train_loss= 0.04095 train_acc= 0.98987 val_loss= 0.14277 val_acc= 0.95620 time= 0.12399
Epoch: 0084 train_loss= 0.03994 train_acc= 0.99048 val_loss= 0.14191 val_acc= 0.95438 time= 0.12400
Epoch: 0085 train_loss= 0.03866 train_acc= 0.99170 val_loss= 0.14133 val_acc= 0.95255 time= 0.12601
Epoch: 0086 train_loss= 0.03809 train_acc= 0.99149 val_loss= 0.14103 val_acc= 0.95438 time= 0.12396
Epoch: 0087 train_loss= 0.03636 train_acc= 0.99291 val_loss= 0.14121 val_acc= 0.95255 time= 0.12313
Epoch: 0088 train_loss= 0.03526 train_acc= 0.99251 val_loss= 0.14178 val_acc= 0.95255 time= 0.16500
Epoch: 0089 train_loss= 0.03458 train_acc= 0.99332 val_loss= 0.14261 val_acc= 0.95255 time= 0.12420
Early stopping...
Optimization Finished!
Test set results: cost= 0.10891 accuracy= 0.97122 time= 0.05734
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9219    0.9752    0.9478       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.6944    0.8197        36
           5     0.9211    0.8642    0.8917        81
           6     0.8791    0.9195    0.8989        87
           7     0.9840    0.9713    0.9776       696

    accuracy                         0.9712      2189
   macro avg     0.9503    0.9237    0.9331      2189
weighted avg     0.9717    0.9712    0.9709      2189

Macro average Test Precision, Recall and F1-Score...
(0.950256668672963, 0.9237095335757367, 0.9331457620652985, None)
Micro average Test Precision, Recall and F1-Score...
(0.9712197350388305, 0.9712197350388305, 0.9712197350388305, None)
embeddings:
7688 5485 2189
[[ 0.07037072  0.3635231   0.04973729 ...  0.16205524  0.3638079
   0.19922918]
 [ 0.2603566   0.17942533  0.10124162 ...  0.06453751  0.1859926
   0.01765808]
 [ 0.5135183   0.06120408  0.3943565  ...  0.24522734  0.06949732
   0.02662678]
 ...
 [ 0.44623026  0.11914352  0.3587804  ...  0.30588242  0.1059427
   0.12392836]
 [ 0.33136594  0.20462102  0.17795403 ...  0.06252906  0.17885834
   0.00429066]
 [ 0.41454914 -0.01735203  0.3278492  ...  0.23529492 -0.00307447
   0.05534046]]

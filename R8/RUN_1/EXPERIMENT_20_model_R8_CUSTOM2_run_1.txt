(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07957 train_acc= 0.03909 val_loss= 1.64808 val_acc= 0.75730 time= 0.38641
Epoch: 0002 train_loss= 1.63711 train_acc= 0.78286 val_loss= 1.27696 val_acc= 0.76642 time= 0.12901
Epoch: 0003 train_loss= 1.23647 train_acc= 0.78590 val_loss= 1.06462 val_acc= 0.67336 time= 0.13004
Epoch: 0004 train_loss= 1.00797 train_acc= 0.68989 val_loss= 0.81680 val_acc= 0.75912 time= 0.12499
Epoch: 0005 train_loss= 0.77437 train_acc= 0.78671 val_loss= 0.68714 val_acc= 0.75547 time= 0.16700
Epoch: 0006 train_loss= 0.64424 train_acc= 0.77537 val_loss= 0.62453 val_acc= 0.76460 time= 0.12323
Epoch: 0007 train_loss= 0.57360 train_acc= 0.79400 val_loss= 0.58319 val_acc= 0.80839 time= 0.12399
Epoch: 0008 train_loss= 0.52337 train_acc= 0.83857 val_loss= 0.54137 val_acc= 0.83394 time= 0.12199
Epoch: 0009 train_loss= 0.47312 train_acc= 0.86044 val_loss= 0.49999 val_acc= 0.84489 time= 0.12300
Epoch: 0010 train_loss= 0.42685 train_acc= 0.86814 val_loss= 0.46289 val_acc= 0.85949 time= 0.12401
Epoch: 0011 train_loss= 0.38269 train_acc= 0.87847 val_loss= 0.43000 val_acc= 0.87774 time= 0.12400
Epoch: 0012 train_loss= 0.34485 train_acc= 0.89224 val_loss= 0.39971 val_acc= 0.88504 time= 0.12900
Epoch: 0013 train_loss= 0.30409 train_acc= 0.90764 val_loss= 0.37062 val_acc= 0.89416 time= 0.15300
Epoch: 0014 train_loss= 0.26397 train_acc= 0.92404 val_loss= 0.34523 val_acc= 0.91423 time= 0.12300
Epoch: 0015 train_loss= 0.23486 train_acc= 0.93863 val_loss= 0.32334 val_acc= 0.92336 time= 0.12201
Epoch: 0016 train_loss= 0.20754 train_acc= 0.94653 val_loss= 0.30313 val_acc= 0.92701 time= 0.12508
Epoch: 0017 train_loss= 0.18507 train_acc= 0.94855 val_loss= 0.28544 val_acc= 0.93431 time= 0.12310
Epoch: 0018 train_loss= 0.15935 train_acc= 0.95301 val_loss= 0.27605 val_acc= 0.93796 time= 0.12240
Epoch: 0019 train_loss= 0.13794 train_acc= 0.95706 val_loss= 0.26907 val_acc= 0.93796 time= 0.12501
Epoch: 0020 train_loss= 0.12367 train_acc= 0.96415 val_loss= 0.25778 val_acc= 0.94343 time= 0.16799
Epoch: 0021 train_loss= 0.10788 train_acc= 0.96678 val_loss= 0.24661 val_acc= 0.94526 time= 0.12506
Epoch: 0022 train_loss= 0.09796 train_acc= 0.97022 val_loss= 0.24413 val_acc= 0.94161 time= 0.12525
Epoch: 0023 train_loss= 0.09116 train_acc= 0.97002 val_loss= 0.24480 val_acc= 0.94343 time= 0.12400
Epoch: 0024 train_loss= 0.07850 train_acc= 0.97185 val_loss= 0.24679 val_acc= 0.94161 time= 0.12200
Epoch: 0025 train_loss= 0.07343 train_acc= 0.97590 val_loss= 0.24967 val_acc= 0.94708 time= 0.12500
Early stopping...
Optimization Finished!
Test set results: cost= 0.16125 accuracy= 0.96391 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8947    0.9835    0.9370       121
           1     0.8690    0.9733    0.9182        75
           2     0.9844    0.9917    0.9880      1083
           3     1.0000    0.3000    0.4615        10
           4     0.9630    0.7222    0.8254        36
           5     0.9219    0.7284    0.8138        81
           6     0.7900    0.9080    0.8449        87
           7     0.9854    0.9727    0.9790       696

    accuracy                         0.9639      2189
   macro avg     0.9261    0.8225    0.8460      2189
weighted avg     0.9655    0.9639    0.9627      2189

Macro average Test Precision, Recall and F1-Score...
(0.9260605435660625, 0.8224823210991773, 0.8459958268020601, None)
Micro average Test Precision, Recall and F1-Score...
(0.9639104613978986, 0.9639104613978986, 0.9639104613978986, None)
embeddings:
7688 5485 2189
[[-0.33432785 -0.27435324  0.15056986 ... -0.02253229  0.07574823
   0.13872048]
 [-0.31252244 -0.26126155 -0.11776097 ... -0.11757506 -0.12635984
  -0.14335638]
 [-0.3777774  -0.32683286  0.07718922 ...  0.41690227  0.3373344
   0.04988682]
 ...
 [-0.39055395 -0.34013176  0.25326887 ...  0.3433742   0.4084177
   0.22970009]
 [-0.45182058 -0.39570364 -0.2473506  ... -0.15646748 -0.25015664
  -0.29204696]
 [-0.27976832 -0.25654873  0.16885148 ...  0.40134078  0.363826
   0.13552149]]

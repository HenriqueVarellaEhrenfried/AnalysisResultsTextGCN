(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07943 train_acc= 0.08933 val_loss= 2.02634 val_acc= 0.56934 time= 0.39080
Epoch: 0002 train_loss= 2.02417 train_acc= 0.59672 val_loss= 1.93485 val_acc= 0.54380 time= 0.13100
Epoch: 0003 train_loss= 1.93208 train_acc= 0.59247 val_loss= 1.81103 val_acc= 0.53650 time= 0.12401
Epoch: 0004 train_loss= 1.79911 train_acc= 0.57444 val_loss= 1.66843 val_acc= 0.53102 time= 0.12334
Epoch: 0005 train_loss= 1.65447 train_acc= 0.56654 val_loss= 1.52814 val_acc= 0.53102 time= 0.12396
Epoch: 0006 train_loss= 1.49430 train_acc= 0.56573 val_loss= 1.41107 val_acc= 0.54562 time= 0.12312
Epoch: 0007 train_loss= 1.36723 train_acc= 0.58031 val_loss= 1.32339 val_acc= 0.60401 time= 0.12712
Epoch: 0008 train_loss= 1.29402 train_acc= 0.62609 val_loss= 1.25684 val_acc= 0.62956 time= 0.12199
Epoch: 0009 train_loss= 1.22594 train_acc= 0.64391 val_loss= 1.19826 val_acc= 0.64964 time= 0.15400
Epoch: 0010 train_loss= 1.15809 train_acc= 0.68807 val_loss= 1.13861 val_acc= 0.67883 time= 0.12207
Epoch: 0011 train_loss= 1.09422 train_acc= 0.68787 val_loss= 1.07436 val_acc= 0.72263 time= 0.12509
Epoch: 0012 train_loss= 1.02565 train_acc= 0.71521 val_loss= 1.00584 val_acc= 0.73723 time= 0.12300
Epoch: 0013 train_loss= 0.97131 train_acc= 0.75511 val_loss= 0.93719 val_acc= 0.75912 time= 0.12403
Epoch: 0014 train_loss= 0.89277 train_acc= 0.77375 val_loss= 0.87285 val_acc= 0.75182 time= 0.12400
Epoch: 0015 train_loss= 0.83382 train_acc= 0.78428 val_loss= 0.81648 val_acc= 0.75730 time= 0.12497
Epoch: 0016 train_loss= 0.78066 train_acc= 0.78307 val_loss= 0.76991 val_acc= 0.75547 time= 0.12503
Epoch: 0017 train_loss= 0.73600 train_acc= 0.78023 val_loss= 0.73264 val_acc= 0.76277 time= 0.16900
Epoch: 0018 train_loss= 0.69884 train_acc= 0.78347 val_loss= 0.70264 val_acc= 0.77007 time= 0.12300
Epoch: 0019 train_loss= 0.66648 train_acc= 0.79319 val_loss= 0.67717 val_acc= 0.79197 time= 0.12700
Epoch: 0020 train_loss= 0.63821 train_acc= 0.81365 val_loss= 0.65368 val_acc= 0.81934 time= 0.12400
Epoch: 0021 train_loss= 0.61782 train_acc= 0.82884 val_loss= 0.63047 val_acc= 0.83212 time= 0.12400
Epoch: 0022 train_loss= 0.59434 train_acc= 0.85173 val_loss= 0.60680 val_acc= 0.84489 time= 0.12400
Epoch: 0023 train_loss= 0.56560 train_acc= 0.86024 val_loss= 0.58316 val_acc= 0.84672 time= 0.12297
Epoch: 0024 train_loss= 0.54407 train_acc= 0.86388 val_loss= 0.56025 val_acc= 0.85219 time= 0.16403
Epoch: 0025 train_loss= 0.52105 train_acc= 0.87219 val_loss= 0.53856 val_acc= 0.85401 time= 0.12200
Epoch: 0026 train_loss= 0.49573 train_acc= 0.87482 val_loss= 0.51849 val_acc= 0.85401 time= 0.12400
Epoch: 0027 train_loss= 0.47986 train_acc= 0.87806 val_loss= 0.50010 val_acc= 0.85949 time= 0.12500
Epoch: 0028 train_loss= 0.45987 train_acc= 0.88049 val_loss= 0.48308 val_acc= 0.86496 time= 0.12297
Epoch: 0029 train_loss= 0.43356 train_acc= 0.88981 val_loss= 0.46706 val_acc= 0.87226 time= 0.12303
Epoch: 0030 train_loss= 0.41761 train_acc= 0.89184 val_loss= 0.45166 val_acc= 0.88139 time= 0.12297
Epoch: 0031 train_loss= 0.40040 train_acc= 0.89346 val_loss= 0.43674 val_acc= 0.88686 time= 0.12303
Epoch: 0032 train_loss= 0.38330 train_acc= 0.89670 val_loss= 0.42209 val_acc= 0.89051 time= 0.16397
Epoch: 0033 train_loss= 0.36619 train_acc= 0.90257 val_loss= 0.40782 val_acc= 0.89234 time= 0.12303
Epoch: 0034 train_loss= 0.35957 train_acc= 0.90419 val_loss= 0.39397 val_acc= 0.89416 time= 0.12300
Epoch: 0035 train_loss= 0.33742 train_acc= 0.90642 val_loss= 0.38074 val_acc= 0.89964 time= 0.12497
Epoch: 0036 train_loss= 0.32236 train_acc= 0.91209 val_loss= 0.36798 val_acc= 0.90146 time= 0.12507
Epoch: 0037 train_loss= 0.31774 train_acc= 0.91574 val_loss= 0.35568 val_acc= 0.90876 time= 0.12400
Epoch: 0038 train_loss= 0.29120 train_acc= 0.92040 val_loss= 0.34376 val_acc= 0.91423 time= 0.12300
Epoch: 0039 train_loss= 0.28553 train_acc= 0.92992 val_loss= 0.33219 val_acc= 0.91423 time= 0.12306
Epoch: 0040 train_loss= 0.27230 train_acc= 0.93133 val_loss= 0.32104 val_acc= 0.92336 time= 0.15699
Epoch: 0041 train_loss= 0.26202 train_acc= 0.93539 val_loss= 0.31031 val_acc= 0.92336 time= 0.12197
Epoch: 0042 train_loss= 0.25060 train_acc= 0.94025 val_loss= 0.30000 val_acc= 0.92518 time= 0.12300
Epoch: 0043 train_loss= 0.24096 train_acc= 0.94632 val_loss= 0.29040 val_acc= 0.92701 time= 0.12402
Epoch: 0044 train_loss= 0.22610 train_acc= 0.94734 val_loss= 0.28128 val_acc= 0.92701 time= 0.12598
Epoch: 0045 train_loss= 0.21555 train_acc= 0.94936 val_loss= 0.27262 val_acc= 0.92701 time= 0.12400
Epoch: 0046 train_loss= 0.20833 train_acc= 0.95301 val_loss= 0.26432 val_acc= 0.92883 time= 0.12300
Epoch: 0047 train_loss= 0.20135 train_acc= 0.95321 val_loss= 0.25639 val_acc= 0.93066 time= 0.12303
Epoch: 0048 train_loss= 0.18938 train_acc= 0.95382 val_loss= 0.24873 val_acc= 0.93613 time= 0.17000
Epoch: 0049 train_loss= 0.17280 train_acc= 0.95929 val_loss= 0.24144 val_acc= 0.93796 time= 0.12401
Epoch: 0050 train_loss= 0.17109 train_acc= 0.95665 val_loss= 0.23421 val_acc= 0.93796 time= 0.12296
Epoch: 0051 train_loss= 0.16574 train_acc= 0.96030 val_loss= 0.22731 val_acc= 0.93796 time= 0.12303
Epoch: 0052 train_loss= 0.15763 train_acc= 0.96192 val_loss= 0.22027 val_acc= 0.93796 time= 0.12600
Epoch: 0053 train_loss= 0.14570 train_acc= 0.96455 val_loss= 0.21375 val_acc= 0.93978 time= 0.12300
Epoch: 0054 train_loss= 0.14611 train_acc= 0.96334 val_loss= 0.20765 val_acc= 0.93796 time= 0.12296
Epoch: 0055 train_loss= 0.13689 train_acc= 0.96557 val_loss= 0.20191 val_acc= 0.93796 time= 0.14900
Epoch: 0056 train_loss= 0.13316 train_acc= 0.96779 val_loss= 0.19645 val_acc= 0.93978 time= 0.13628
Epoch: 0057 train_loss= 0.12324 train_acc= 0.96941 val_loss= 0.19166 val_acc= 0.93978 time= 0.12397
Epoch: 0058 train_loss= 0.12031 train_acc= 0.96941 val_loss= 0.18747 val_acc= 0.94343 time= 0.12300
Epoch: 0059 train_loss= 0.11644 train_acc= 0.97083 val_loss= 0.18389 val_acc= 0.94343 time= 0.12317
Epoch: 0060 train_loss= 0.11225 train_acc= 0.97205 val_loss= 0.18071 val_acc= 0.94161 time= 0.12505
Epoch: 0061 train_loss= 0.11012 train_acc= 0.97286 val_loss= 0.17775 val_acc= 0.94343 time= 0.12206
Epoch: 0062 train_loss= 0.10142 train_acc= 0.97468 val_loss= 0.17574 val_acc= 0.94526 time= 0.12237
Epoch: 0063 train_loss= 0.10001 train_acc= 0.97630 val_loss= 0.17325 val_acc= 0.94526 time= 0.16600
Epoch: 0064 train_loss= 0.09576 train_acc= 0.97812 val_loss= 0.17085 val_acc= 0.94343 time= 0.12400
Epoch: 0065 train_loss= 0.09256 train_acc= 0.97772 val_loss= 0.16800 val_acc= 0.95073 time= 0.12505
Epoch: 0066 train_loss= 0.08975 train_acc= 0.97772 val_loss= 0.16523 val_acc= 0.95255 time= 0.12207
Epoch: 0067 train_loss= 0.08246 train_acc= 0.98137 val_loss= 0.16255 val_acc= 0.95255 time= 0.12500
Epoch: 0068 train_loss= 0.08429 train_acc= 0.97914 val_loss= 0.16039 val_acc= 0.95255 time= 0.12300
Epoch: 0069 train_loss= 0.08045 train_acc= 0.98076 val_loss= 0.15861 val_acc= 0.95255 time= 0.12500
Epoch: 0070 train_loss= 0.08085 train_acc= 0.97974 val_loss= 0.15714 val_acc= 0.95073 time= 0.12400
Epoch: 0071 train_loss= 0.07574 train_acc= 0.98055 val_loss= 0.15607 val_acc= 0.95255 time= 0.15000
Epoch: 0072 train_loss= 0.07386 train_acc= 0.98339 val_loss= 0.15572 val_acc= 0.95255 time= 0.12697
Epoch: 0073 train_loss= 0.07168 train_acc= 0.98197 val_loss= 0.15556 val_acc= 0.95255 time= 0.12700
Epoch: 0074 train_loss= 0.06940 train_acc= 0.98299 val_loss= 0.15444 val_acc= 0.95255 time= 0.12310
Epoch: 0075 train_loss= 0.07089 train_acc= 0.98339 val_loss= 0.15234 val_acc= 0.95255 time= 0.12195
Epoch: 0076 train_loss= 0.06429 train_acc= 0.98521 val_loss= 0.15054 val_acc= 0.95255 time= 0.12309
Epoch: 0077 train_loss= 0.06540 train_acc= 0.98278 val_loss= 0.14854 val_acc= 0.95438 time= 0.12599
Epoch: 0078 train_loss= 0.05879 train_acc= 0.98562 val_loss= 0.14690 val_acc= 0.95620 time= 0.12301
Epoch: 0079 train_loss= 0.05904 train_acc= 0.98724 val_loss= 0.14538 val_acc= 0.95803 time= 0.16699
Epoch: 0080 train_loss= 0.06111 train_acc= 0.98521 val_loss= 0.14354 val_acc= 0.95620 time= 0.12508
Epoch: 0081 train_loss= 0.05880 train_acc= 0.98582 val_loss= 0.14209 val_acc= 0.95438 time= 0.12700
Epoch: 0082 train_loss= 0.05666 train_acc= 0.98643 val_loss= 0.14155 val_acc= 0.95438 time= 0.12597
Epoch: 0083 train_loss= 0.05012 train_acc= 0.98926 val_loss= 0.14155 val_acc= 0.95803 time= 0.12203
Epoch: 0084 train_loss= 0.05422 train_acc= 0.98683 val_loss= 0.14192 val_acc= 0.95985 time= 0.12362
Epoch: 0085 train_loss= 0.05367 train_acc= 0.98704 val_loss= 0.14239 val_acc= 0.95803 time= 0.12600
Epoch: 0086 train_loss= 0.05191 train_acc= 0.98845 val_loss= 0.14236 val_acc= 0.95620 time= 0.12501
Epoch: 0087 train_loss= 0.04904 train_acc= 0.98926 val_loss= 0.14194 val_acc= 0.95438 time= 0.15799
Epoch: 0088 train_loss= 0.04740 train_acc= 0.99007 val_loss= 0.14171 val_acc= 0.95438 time= 0.12300
Epoch: 0089 train_loss= 0.04672 train_acc= 0.98947 val_loss= 0.14145 val_acc= 0.95255 time= 0.12500
Epoch: 0090 train_loss= 0.04453 train_acc= 0.99028 val_loss= 0.14118 val_acc= 0.95255 time= 0.12401
Epoch: 0091 train_loss= 0.04444 train_acc= 0.99028 val_loss= 0.14105 val_acc= 0.95255 time= 0.12399
Epoch: 0092 train_loss= 0.04217 train_acc= 0.99068 val_loss= 0.14096 val_acc= 0.95438 time= 0.12501
Epoch: 0093 train_loss= 0.04293 train_acc= 0.99089 val_loss= 0.14049 val_acc= 0.95438 time= 0.12600
Epoch: 0094 train_loss= 0.03903 train_acc= 0.99048 val_loss= 0.14035 val_acc= 0.95620 time= 0.17004
Epoch: 0095 train_loss= 0.04051 train_acc= 0.99109 val_loss= 0.14039 val_acc= 0.95620 time= 0.12396
Epoch: 0096 train_loss= 0.04075 train_acc= 0.99109 val_loss= 0.14032 val_acc= 0.95803 time= 0.12315
Epoch: 0097 train_loss= 0.03863 train_acc= 0.99068 val_loss= 0.14003 val_acc= 0.95803 time= 0.12513
Epoch: 0098 train_loss= 0.03755 train_acc= 0.99149 val_loss= 0.13975 val_acc= 0.95438 time= 0.12299
Epoch: 0099 train_loss= 0.03670 train_acc= 0.99170 val_loss= 0.13940 val_acc= 0.95438 time= 0.12301
Epoch: 0100 train_loss= 0.03838 train_acc= 0.99251 val_loss= 0.13943 val_acc= 0.95438 time= 0.12399
Epoch: 0101 train_loss= 0.03698 train_acc= 0.99251 val_loss= 0.13999 val_acc= 0.95438 time= 0.12197
Epoch: 0102 train_loss= 0.03567 train_acc= 0.99291 val_loss= 0.14061 val_acc= 0.95438 time= 0.15304
Early stopping...
Optimization Finished!
Test set results: cost= 0.10737 accuracy= 0.97168 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9370    0.9835    0.9597       121
           1     0.9125    0.9733    0.9419        75
           2     0.9844    0.9908    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.7222    0.8387        36
           5     0.9559    0.8025    0.8725        81
           6     0.8367    0.9425    0.8865        87
           7     0.9841    0.9756    0.9798       696

    accuracy                         0.9717      2189
   macro avg     0.9513    0.9238    0.9333      2189
weighted avg     0.9726    0.9717    0.9713      2189

Macro average Test Precision, Recall and F1-Score...
(0.9513233201967174, 0.9237957004590511, 0.9333331312267912, None)
Micro average Test Precision, Recall and F1-Score...
(0.9716765646413887, 0.9716765646413887, 0.9716765646413887, None)
embeddings:
7688 5485 2189
[[ 0.16141042  0.01801257  0.17856255 ...  0.38525838  0.09853372
   0.17966598]
 [ 0.03885842  0.13367926  0.02560158 ...  0.19147201  0.24356492
   0.0017499 ]
 [ 0.02329693  0.4691599   0.01427826 ...  0.00860198  0.10532794
  -0.02038363]
 ...
 [ 0.12048699  0.40181518  0.09854109 ...  0.13012286  0.25406072
   0.07631129]
 [ 0.04282707  0.2510232   0.01715389 ...  0.18817821  0.3724647
  -0.01489849]
 [ 0.07563962  0.3400295   0.04942082 ... -0.02510684  0.33480793
   0.05022929]]

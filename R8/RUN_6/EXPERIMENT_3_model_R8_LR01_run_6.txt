(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07935 train_acc= 0.15921 val_loss= 1.61431 val_acc= 0.75000 time= 0.39480
Epoch: 0002 train_loss= 1.59899 train_acc= 0.77334 val_loss= 1.25794 val_acc= 0.76095 time= 0.14503
Epoch: 0003 train_loss= 1.20394 train_acc= 0.78671 val_loss= 1.03117 val_acc= 0.70073 time= 0.12706
Epoch: 0004 train_loss= 0.97937 train_acc= 0.71643 val_loss= 0.78771 val_acc= 0.75912 time= 0.12304
Epoch: 0005 train_loss= 0.74297 train_acc= 0.78752 val_loss= 0.67930 val_acc= 0.75547 time= 0.12196
Epoch: 0006 train_loss= 0.63359 train_acc= 0.77841 val_loss= 0.63145 val_acc= 0.75547 time= 0.12300
Epoch: 0007 train_loss= 0.57952 train_acc= 0.78388 val_loss= 0.60025 val_acc= 0.78467 time= 0.12500
Epoch: 0008 train_loss= 0.53860 train_acc= 0.80271 val_loss= 0.57281 val_acc= 0.79745 time= 0.12764
Epoch: 0009 train_loss= 0.50542 train_acc= 0.81912 val_loss= 0.54705 val_acc= 0.81204 time= 0.14203
Epoch: 0010 train_loss= 0.47149 train_acc= 0.83472 val_loss= 0.52253 val_acc= 0.82117 time= 0.13997
Epoch: 0011 train_loss= 0.44049 train_acc= 0.84748 val_loss= 0.49627 val_acc= 0.84124 time= 0.12200
Epoch: 0012 train_loss= 0.40827 train_acc= 0.86206 val_loss= 0.46473 val_acc= 0.86679 time= 0.12400
Epoch: 0013 train_loss= 0.36854 train_acc= 0.88333 val_loss= 0.43062 val_acc= 0.87956 time= 0.12404
Epoch: 0014 train_loss= 0.33675 train_acc= 0.90217 val_loss= 0.39851 val_acc= 0.89781 time= 0.12301
Epoch: 0015 train_loss= 0.30606 train_acc= 0.91209 val_loss= 0.37116 val_acc= 0.90693 time= 0.12401
Epoch: 0016 train_loss= 0.27295 train_acc= 0.92992 val_loss= 0.34788 val_acc= 0.91788 time= 0.12396
Epoch: 0017 train_loss= 0.24571 train_acc= 0.93761 val_loss= 0.32711 val_acc= 0.92336 time= 0.17502
Epoch: 0018 train_loss= 0.21256 train_acc= 0.94612 val_loss= 0.30859 val_acc= 0.92883 time= 0.12400
Epoch: 0019 train_loss= 0.18727 train_acc= 0.95281 val_loss= 0.29567 val_acc= 0.93066 time= 0.12300
Epoch: 0020 train_loss= 0.16461 train_acc= 0.95726 val_loss= 0.28670 val_acc= 0.93248 time= 0.12304
Epoch: 0021 train_loss= 0.14240 train_acc= 0.96152 val_loss= 0.27877 val_acc= 0.93613 time= 0.12400
Epoch: 0022 train_loss= 0.12728 train_acc= 0.96536 val_loss= 0.26845 val_acc= 0.93613 time= 0.12696
Epoch: 0023 train_loss= 0.11678 train_acc= 0.96476 val_loss= 0.25653 val_acc= 0.94161 time= 0.12362
Epoch: 0024 train_loss= 0.10268 train_acc= 0.96982 val_loss= 0.24790 val_acc= 0.94343 time= 0.12500
Epoch: 0025 train_loss= 0.08741 train_acc= 0.97428 val_loss= 0.24657 val_acc= 0.94161 time= 0.15099
Epoch: 0026 train_loss= 0.08491 train_acc= 0.97245 val_loss= 0.24862 val_acc= 0.93978 time= 0.12645
Epoch: 0027 train_loss= 0.07265 train_acc= 0.97914 val_loss= 0.25312 val_acc= 0.94161 time= 0.12505
Epoch: 0028 train_loss= 0.07276 train_acc= 0.97569 val_loss= 0.25674 val_acc= 0.94161 time= 0.12300
Epoch: 0029 train_loss= 0.06020 train_acc= 0.97995 val_loss= 0.26027 val_acc= 0.94526 time= 0.12299
Epoch: 0030 train_loss= 0.05725 train_acc= 0.97995 val_loss= 0.25871 val_acc= 0.95073 time= 0.12237
Epoch: 0031 train_loss= 0.04965 train_acc= 0.98380 val_loss= 0.25816 val_acc= 0.95255 time= 0.12301
Early stopping...
Optimization Finished!
Test set results: cost= 0.15321 accuracy= 0.96711 time= 0.05400
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9426    0.9504    0.9465       121
           1     0.9091    0.9333    0.9211        75
           2     0.9764    0.9917    0.9840      1083
           3     0.5000    0.5000    0.5000        10
           4     0.9000    0.7500    0.8182        36
           5     0.9146    0.9259    0.9202        81
           6     0.9310    0.9310    0.9310        87
           7     0.9838    0.9626    0.9731       696

    accuracy                         0.9671      2189
   macro avg     0.8822    0.8681    0.8743      2189
weighted avg     0.9671    0.9671    0.9669      2189

Macro average Test Precision, Recall and F1-Score...
(0.8821991760976321, 0.868130049251477, 0.87426417493859, None)
Micro average Test Precision, Recall and F1-Score...
(0.9671082686158063, 0.9671082686158063, 0.9671082686158063, None)
embeddings:
7688 5485 2189
[[-0.35404462  0.02099609  0.10161594 ... -0.20428208  0.28916088
  -0.01808537]
 [-0.30512226 -0.18245395 -0.16834202 ... -0.19870138 -0.08560673
  -0.1768911 ]
 [-0.40022042  0.02412163  0.02821787 ... -0.16181155 -0.20524879
   0.3984112 ]
 ...
 [-0.41426885  0.249291    0.16378888 ... -0.10722059 -0.08079258
   0.38482335]
 [-0.41781726 -0.31628114 -0.35426393 ... -0.28892297 -0.20108682
  -0.25740537]
 [-0.30627567  0.14780429  0.12159362 ... -0.1428298  -0.03754062
   0.37648526]]

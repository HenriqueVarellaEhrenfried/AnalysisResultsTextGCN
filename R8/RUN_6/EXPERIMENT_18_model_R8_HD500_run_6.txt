(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07958 train_acc= 0.02917 val_loss= 2.00140 val_acc= 0.75365 time= 0.53900
Epoch: 0002 train_loss= 1.99819 train_acc= 0.77375 val_loss= 1.84805 val_acc= 0.73723 time= 0.19106
Epoch: 0003 train_loss= 1.84064 train_acc= 0.74640 val_loss= 1.64228 val_acc= 0.68248 time= 0.19295
Epoch: 0004 train_loss= 1.62870 train_acc= 0.69597 val_loss= 1.44441 val_acc= 0.63869 time= 0.19600
Epoch: 0005 train_loss= 1.41421 train_acc= 0.66073 val_loss= 1.30656 val_acc= 0.63504 time= 0.21900
Epoch: 0006 train_loss= 1.26388 train_acc= 0.66214 val_loss= 1.21639 val_acc= 0.65511 time= 0.19705
Epoch: 0007 train_loss= 1.17137 train_acc= 0.67126 val_loss= 1.13610 val_acc= 0.69708 time= 0.19100
Epoch: 0008 train_loss= 1.08653 train_acc= 0.71481 val_loss= 1.04765 val_acc= 0.73358 time= 0.19100
Epoch: 0009 train_loss= 0.99944 train_acc= 0.75775 val_loss= 0.95329 val_acc= 0.75912 time= 0.19500
Epoch: 0010 train_loss= 0.90953 train_acc= 0.77922 val_loss= 0.86338 val_acc= 0.75547 time= 0.22300
Epoch: 0011 train_loss= 0.82090 train_acc= 0.78610 val_loss= 0.78709 val_acc= 0.75547 time= 0.19200
Epoch: 0012 train_loss= 0.74717 train_acc= 0.78448 val_loss= 0.72756 val_acc= 0.75547 time= 0.19605
Epoch: 0013 train_loss= 0.68781 train_acc= 0.78286 val_loss= 0.68290 val_acc= 0.76460 time= 0.19005
Epoch: 0014 train_loss= 0.64346 train_acc= 0.78590 val_loss= 0.64840 val_acc= 0.77372 time= 0.19304
Epoch: 0015 train_loss= 0.61047 train_acc= 0.80251 val_loss= 0.61917 val_acc= 0.79927 time= 0.22603
Epoch: 0016 train_loss= 0.57825 train_acc= 0.82560 val_loss= 0.59174 val_acc= 0.83029 time= 0.19097
Epoch: 0017 train_loss= 0.55119 train_acc= 0.85092 val_loss= 0.56444 val_acc= 0.84489 time= 0.19307
Epoch: 0018 train_loss= 0.52340 train_acc= 0.86915 val_loss= 0.53703 val_acc= 0.85401 time= 0.19501
Epoch: 0019 train_loss= 0.49278 train_acc= 0.87867 val_loss= 0.51014 val_acc= 0.85766 time= 0.19300
Epoch: 0020 train_loss= 0.46211 train_acc= 0.88617 val_loss= 0.48446 val_acc= 0.86496 time= 0.22200
Epoch: 0021 train_loss= 0.43267 train_acc= 0.89244 val_loss= 0.46026 val_acc= 0.86679 time= 0.19100
Epoch: 0022 train_loss= 0.40515 train_acc= 0.89812 val_loss= 0.43750 val_acc= 0.87774 time= 0.19007
Epoch: 0023 train_loss= 0.38090 train_acc= 0.89852 val_loss= 0.41602 val_acc= 0.88686 time= 0.19200
Epoch: 0024 train_loss= 0.35780 train_acc= 0.90602 val_loss= 0.39557 val_acc= 0.89599 time= 0.19579
Epoch: 0025 train_loss= 0.33400 train_acc= 0.91108 val_loss= 0.37612 val_acc= 0.90146 time= 0.22803
Epoch: 0026 train_loss= 0.31366 train_acc= 0.91918 val_loss= 0.35770 val_acc= 0.90511 time= 0.19197
Epoch: 0027 train_loss= 0.29397 train_acc= 0.92506 val_loss= 0.34030 val_acc= 0.90693 time= 0.19203
Epoch: 0028 train_loss= 0.27580 train_acc= 0.92931 val_loss= 0.32405 val_acc= 0.91058 time= 0.19200
Epoch: 0029 train_loss= 0.25874 train_acc= 0.93214 val_loss= 0.30897 val_acc= 0.91606 time= 0.19297
Epoch: 0030 train_loss= 0.24149 train_acc= 0.93539 val_loss= 0.29503 val_acc= 0.91788 time= 0.22812
Epoch: 0031 train_loss= 0.22638 train_acc= 0.94389 val_loss= 0.28219 val_acc= 0.92336 time= 0.19400
Epoch: 0032 train_loss= 0.20811 train_acc= 0.94977 val_loss= 0.27025 val_acc= 0.92518 time= 0.19200
Epoch: 0033 train_loss= 0.19560 train_acc= 0.95281 val_loss= 0.25905 val_acc= 0.92701 time= 0.19200
Epoch: 0034 train_loss= 0.17883 train_acc= 0.95625 val_loss= 0.24845 val_acc= 0.93066 time= 0.19200
Epoch: 0035 train_loss= 0.16927 train_acc= 0.95686 val_loss= 0.23853 val_acc= 0.93066 time= 0.20800
Epoch: 0036 train_loss= 0.15492 train_acc= 0.95929 val_loss= 0.22891 val_acc= 0.93431 time= 0.19800
Epoch: 0037 train_loss= 0.14904 train_acc= 0.95989 val_loss= 0.21977 val_acc= 0.93613 time= 0.19300
Epoch: 0038 train_loss= 0.13562 train_acc= 0.96476 val_loss= 0.21153 val_acc= 0.93796 time= 0.19100
Epoch: 0039 train_loss= 0.12849 train_acc= 0.96800 val_loss= 0.20429 val_acc= 0.93978 time= 0.19100
Epoch: 0040 train_loss= 0.11618 train_acc= 0.96921 val_loss= 0.19818 val_acc= 0.93978 time= 0.19400
Epoch: 0041 train_loss= 0.11035 train_acc= 0.96860 val_loss= 0.19290 val_acc= 0.93978 time= 0.19900
Epoch: 0042 train_loss= 0.10225 train_acc= 0.97205 val_loss= 0.18842 val_acc= 0.94161 time= 0.19600
Epoch: 0043 train_loss= 0.09572 train_acc= 0.97590 val_loss= 0.18470 val_acc= 0.94708 time= 0.19200
Epoch: 0044 train_loss= 0.09027 train_acc= 0.97549 val_loss= 0.18126 val_acc= 0.94708 time= 0.19100
Epoch: 0045 train_loss= 0.08461 train_acc= 0.97812 val_loss= 0.17785 val_acc= 0.94708 time= 0.19400
Epoch: 0046 train_loss= 0.07971 train_acc= 0.97914 val_loss= 0.17460 val_acc= 0.94526 time= 0.21600
Epoch: 0047 train_loss= 0.07669 train_acc= 0.97954 val_loss= 0.17151 val_acc= 0.94526 time= 0.19303
Epoch: 0048 train_loss= 0.07089 train_acc= 0.98197 val_loss= 0.16888 val_acc= 0.94708 time= 0.19475
Epoch: 0049 train_loss= 0.06655 train_acc= 0.98278 val_loss= 0.16640 val_acc= 0.94708 time= 0.19200
Epoch: 0050 train_loss= 0.06449 train_acc= 0.98299 val_loss= 0.16419 val_acc= 0.95073 time= 0.19200
Epoch: 0051 train_loss= 0.05878 train_acc= 0.98602 val_loss= 0.16269 val_acc= 0.95073 time= 0.22303
Epoch: 0052 train_loss= 0.05683 train_acc= 0.98542 val_loss= 0.16169 val_acc= 0.95255 time= 0.19297
Epoch: 0053 train_loss= 0.05244 train_acc= 0.98764 val_loss= 0.16088 val_acc= 0.95255 time= 0.19200
Epoch: 0054 train_loss= 0.05169 train_acc= 0.98764 val_loss= 0.16028 val_acc= 0.95255 time= 0.19501
Epoch: 0055 train_loss= 0.04759 train_acc= 0.98926 val_loss= 0.15941 val_acc= 0.95255 time= 0.19200
Epoch: 0056 train_loss= 0.04629 train_acc= 0.98906 val_loss= 0.15860 val_acc= 0.95438 time= 0.22103
Epoch: 0057 train_loss= 0.04258 train_acc= 0.98987 val_loss= 0.15806 val_acc= 0.95620 time= 0.19200
Epoch: 0058 train_loss= 0.04117 train_acc= 0.98987 val_loss= 0.15777 val_acc= 0.95803 time= 0.19500
Epoch: 0059 train_loss= 0.03873 train_acc= 0.99089 val_loss= 0.15796 val_acc= 0.95803 time= 0.19304
Epoch: 0060 train_loss= 0.03782 train_acc= 0.99109 val_loss= 0.15780 val_acc= 0.95803 time= 0.19540
Epoch: 0061 train_loss= 0.03532 train_acc= 0.99129 val_loss= 0.15742 val_acc= 0.95985 time= 0.22303
Epoch: 0062 train_loss= 0.03403 train_acc= 0.99271 val_loss= 0.15730 val_acc= 0.95803 time= 0.19200
Epoch: 0063 train_loss= 0.03367 train_acc= 0.99230 val_loss= 0.15669 val_acc= 0.95620 time= 0.19400
Epoch: 0064 train_loss= 0.03124 train_acc= 0.99230 val_loss= 0.15579 val_acc= 0.95438 time= 0.19197
Epoch: 0065 train_loss= 0.02949 train_acc= 0.99230 val_loss= 0.15545 val_acc= 0.95620 time= 0.19300
Epoch: 0066 train_loss= 0.02907 train_acc= 0.99311 val_loss= 0.15558 val_acc= 0.95803 time= 0.22900
Epoch: 0067 train_loss= 0.02683 train_acc= 0.99413 val_loss= 0.15499 val_acc= 0.95620 time= 0.19103
Epoch: 0068 train_loss= 0.02596 train_acc= 0.99392 val_loss= 0.15433 val_acc= 0.95620 time= 0.19199
Epoch: 0069 train_loss= 0.02481 train_acc= 0.99473 val_loss= 0.15407 val_acc= 0.95620 time= 0.19300
Epoch: 0070 train_loss= 0.02396 train_acc= 0.99453 val_loss= 0.15441 val_acc= 0.95620 time= 0.19301
Epoch: 0071 train_loss= 0.02314 train_acc= 0.99473 val_loss= 0.15502 val_acc= 0.95620 time= 0.22296
Epoch: 0072 train_loss= 0.02255 train_acc= 0.99433 val_loss= 0.15640 val_acc= 0.95803 time= 0.19500
Early stopping...
Optimization Finished!
Test set results: cost= 0.11268 accuracy= 0.96985 time= 0.07800
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9375    0.9917    0.9639       121
           1     0.9125    0.9733    0.9419        75
           2     0.9826    0.9917    0.9871      1083
           3     0.9091    1.0000    0.9524        10
           4     1.0000    0.7222    0.8387        36
           5     0.9054    0.8272    0.8645        81
           6     0.8495    0.9080    0.8778        87
           7     0.9854    0.9684    0.9768       696

    accuracy                         0.9698      2189
   macro avg     0.9352    0.9228    0.9254      2189
weighted avg     0.9704    0.9698    0.9695      2189

Macro average Test Precision, Recall and F1-Score...
(0.9352444310581115, 0.9228222648593143, 0.9253899236640166, None)
Micro average Test Precision, Recall and F1-Score...
(0.9698492462311558, 0.9698492462311558, 0.9698492462311558, None)
embeddings:
7688 5485 2189
[[ 0.14351186  0.12236725  0.11757404 ... -0.0006831   0.08094969
   0.11329724]
 [ 0.01599136  0.00441537  0.12908016 ...  0.17638834  0.13299546
  -0.00668576]
 [-0.05477874  0.09319737  0.0617834  ...  0.28456154  0.0255862
   0.06337814]
 ...
 [ 0.01088158  0.15477252  0.11790206 ...  0.21642488  0.07593413
   0.12451605]
 [ 0.03179696 -0.02120104  0.09634075 ...  0.1547312   0.19481874
  -0.03396674]
 [ 0.00049159  0.09990648  0.01653087 ...  0.23322155  0.1079259
   0.09016025]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07934 train_acc= 0.20559 val_loss= 2.02753 val_acc= 0.75182 time= 0.38405
Epoch: 0002 train_loss= 2.02591 train_acc= 0.77800 val_loss= 1.93717 val_acc= 0.76095 time= 0.15595
Epoch: 0003 train_loss= 1.92689 train_acc= 0.78084 val_loss= 1.81557 val_acc= 0.75000 time= 0.13260
Epoch: 0004 train_loss= 1.80903 train_acc= 0.75998 val_loss= 1.67485 val_acc= 0.71350 time= 0.12704
Epoch: 0005 train_loss= 1.66559 train_acc= 0.71764 val_loss= 1.53474 val_acc= 0.67153 time= 0.12299
Epoch: 0006 train_loss= 1.51349 train_acc= 0.69475 val_loss= 1.41460 val_acc= 0.66241 time= 0.12296
Epoch: 0007 train_loss= 1.37496 train_acc= 0.68969 val_loss= 1.32215 val_acc= 0.65876 time= 0.12313
Epoch: 0008 train_loss= 1.27728 train_acc= 0.67349 val_loss= 1.25042 val_acc= 0.66971 time= 0.12299
Epoch: 0009 train_loss= 1.21930 train_acc= 0.67652 val_loss= 1.18777 val_acc= 0.68978 time= 0.14400
Epoch: 0010 train_loss= 1.13147 train_acc= 0.69637 val_loss= 1.12514 val_acc= 0.72080 time= 0.13997
Epoch: 0011 train_loss= 1.07194 train_acc= 0.72736 val_loss= 1.05899 val_acc= 0.74088 time= 0.12603
Epoch: 0012 train_loss= 1.02224 train_acc= 0.75268 val_loss= 0.99041 val_acc= 0.75547 time= 0.12617
Epoch: 0013 train_loss= 0.94283 train_acc= 0.77112 val_loss= 0.92277 val_acc= 0.75547 time= 0.12600
Epoch: 0014 train_loss= 0.87816 train_acc= 0.78205 val_loss= 0.85987 val_acc= 0.75730 time= 0.12411
Epoch: 0015 train_loss= 0.82641 train_acc= 0.78631 val_loss= 0.80441 val_acc= 0.75730 time= 0.12399
Epoch: 0016 train_loss= 0.77579 train_acc= 0.78104 val_loss= 0.75777 val_acc= 0.75912 time= 0.12309
Epoch: 0017 train_loss= 0.72873 train_acc= 0.78509 val_loss= 0.71978 val_acc= 0.76642 time= 0.16501
Epoch: 0018 train_loss= 0.69847 train_acc= 0.79137 val_loss= 0.68872 val_acc= 0.78467 time= 0.12300
Epoch: 0019 train_loss= 0.65517 train_acc= 0.81001 val_loss= 0.66219 val_acc= 0.80474 time= 0.12500
Epoch: 0020 train_loss= 0.62976 train_acc= 0.83107 val_loss= 0.63790 val_acc= 0.81569 time= 0.12300
Epoch: 0021 train_loss= 0.60924 train_acc= 0.83776 val_loss= 0.61446 val_acc= 0.82664 time= 0.12694
Epoch: 0022 train_loss= 0.58642 train_acc= 0.84728 val_loss= 0.59147 val_acc= 0.84124 time= 0.12501
Epoch: 0023 train_loss= 0.55547 train_acc= 0.86166 val_loss= 0.56910 val_acc= 0.84489 time= 0.12400
Epoch: 0024 train_loss= 0.53902 train_acc= 0.86652 val_loss= 0.54790 val_acc= 0.84672 time= 0.12399
Epoch: 0025 train_loss= 0.50907 train_acc= 0.87178 val_loss= 0.52825 val_acc= 0.84854 time= 0.15001
Epoch: 0026 train_loss= 0.48597 train_acc= 0.87259 val_loss= 0.51033 val_acc= 0.85584 time= 0.12200
Epoch: 0027 train_loss= 0.46616 train_acc= 0.87847 val_loss= 0.49388 val_acc= 0.85949 time= 0.12396
Epoch: 0028 train_loss= 0.44790 train_acc= 0.88110 val_loss= 0.47850 val_acc= 0.86314 time= 0.12403
Epoch: 0029 train_loss= 0.43154 train_acc= 0.88738 val_loss= 0.46364 val_acc= 0.87409 time= 0.12302
Epoch: 0030 train_loss= 0.42258 train_acc= 0.88799 val_loss= 0.44892 val_acc= 0.87956 time= 0.12589
Epoch: 0031 train_loss= 0.39700 train_acc= 0.90014 val_loss= 0.43437 val_acc= 0.88869 time= 0.12600
Epoch: 0032 train_loss= 0.38546 train_acc= 0.89852 val_loss= 0.42000 val_acc= 0.89599 time= 0.12304
Epoch: 0033 train_loss= 0.37245 train_acc= 0.89933 val_loss= 0.40581 val_acc= 0.89599 time= 0.16904
Epoch: 0034 train_loss= 0.36191 train_acc= 0.90034 val_loss= 0.39209 val_acc= 0.89599 time= 0.12300
Epoch: 0035 train_loss= 0.34079 train_acc= 0.91209 val_loss= 0.37900 val_acc= 0.89781 time= 0.12596
Epoch: 0036 train_loss= 0.32728 train_acc= 0.91189 val_loss= 0.36660 val_acc= 0.90328 time= 0.12503
Epoch: 0037 train_loss= 0.31368 train_acc= 0.91594 val_loss= 0.35486 val_acc= 0.90876 time= 0.12410
Epoch: 0038 train_loss= 0.30689 train_acc= 0.92019 val_loss= 0.34376 val_acc= 0.91058 time= 0.12407
Epoch: 0039 train_loss= 0.29197 train_acc= 0.92546 val_loss= 0.33308 val_acc= 0.91788 time= 0.12600
Epoch: 0040 train_loss= 0.27890 train_acc= 0.92951 val_loss= 0.32274 val_acc= 0.92518 time= 0.16200
Epoch: 0041 train_loss= 0.27236 train_acc= 0.93296 val_loss= 0.31284 val_acc= 0.92883 time= 0.12801
Epoch: 0042 train_loss= 0.25808 train_acc= 0.94025 val_loss= 0.30326 val_acc= 0.92701 time= 0.12200
Epoch: 0043 train_loss= 0.24159 train_acc= 0.93883 val_loss= 0.29417 val_acc= 0.92701 time= 0.12297
Epoch: 0044 train_loss= 0.23353 train_acc= 0.94389 val_loss= 0.28495 val_acc= 0.93248 time= 0.12637
Epoch: 0045 train_loss= 0.23056 train_acc= 0.94207 val_loss= 0.27555 val_acc= 0.93431 time= 0.12364
Epoch: 0046 train_loss= 0.21350 train_acc= 0.94774 val_loss= 0.26637 val_acc= 0.93431 time= 0.12300
Epoch: 0047 train_loss= 0.20321 train_acc= 0.95483 val_loss= 0.25759 val_acc= 0.93613 time= 0.12372
Epoch: 0048 train_loss= 0.19760 train_acc= 0.95078 val_loss= 0.24946 val_acc= 0.93613 time= 0.16111
Epoch: 0049 train_loss= 0.18688 train_acc= 0.95179 val_loss= 0.24189 val_acc= 0.93613 time= 0.12614
Epoch: 0050 train_loss= 0.18477 train_acc= 0.95544 val_loss= 0.23463 val_acc= 0.93431 time= 0.12600
Epoch: 0051 train_loss= 0.17524 train_acc= 0.95767 val_loss= 0.22776 val_acc= 0.93431 time= 0.12304
Epoch: 0052 train_loss= 0.16176 train_acc= 0.95807 val_loss= 0.22152 val_acc= 0.93796 time= 0.12500
Epoch: 0053 train_loss= 0.16387 train_acc= 0.95888 val_loss= 0.21555 val_acc= 0.93978 time= 0.12381
Epoch: 0054 train_loss= 0.14602 train_acc= 0.96293 val_loss= 0.20994 val_acc= 0.93613 time= 0.12296
Epoch: 0055 train_loss= 0.15157 train_acc= 0.95969 val_loss= 0.20512 val_acc= 0.93796 time= 0.12310
Epoch: 0056 train_loss= 0.14205 train_acc= 0.96597 val_loss= 0.20108 val_acc= 0.93796 time= 0.15099
Epoch: 0057 train_loss= 0.13279 train_acc= 0.96557 val_loss= 0.19772 val_acc= 0.94343 time= 0.12300
Epoch: 0058 train_loss= 0.13102 train_acc= 0.96536 val_loss= 0.19414 val_acc= 0.94526 time= 0.12817
Epoch: 0059 train_loss= 0.13198 train_acc= 0.96415 val_loss= 0.19040 val_acc= 0.94343 time= 0.12601
Epoch: 0060 train_loss= 0.11742 train_acc= 0.97083 val_loss= 0.18664 val_acc= 0.94343 time= 0.12396
Epoch: 0061 train_loss= 0.11762 train_acc= 0.97185 val_loss= 0.18321 val_acc= 0.94708 time= 0.12548
Epoch: 0062 train_loss= 0.11233 train_acc= 0.97164 val_loss= 0.17941 val_acc= 0.94891 time= 0.12343
Epoch: 0063 train_loss= 0.10934 train_acc= 0.97306 val_loss= 0.17554 val_acc= 0.94708 time= 0.12304
Epoch: 0064 train_loss= 0.10749 train_acc= 0.97266 val_loss= 0.17223 val_acc= 0.94891 time= 0.16801
Epoch: 0065 train_loss= 0.10421 train_acc= 0.97407 val_loss= 0.16919 val_acc= 0.94891 time= 0.12299
Epoch: 0066 train_loss= 0.10192 train_acc= 0.97205 val_loss= 0.16626 val_acc= 0.94891 time= 0.12240
Epoch: 0067 train_loss= 0.09887 train_acc= 0.97347 val_loss= 0.16398 val_acc= 0.94891 time= 0.12600
Epoch: 0068 train_loss= 0.09077 train_acc= 0.97833 val_loss= 0.16238 val_acc= 0.94891 time= 0.12689
Epoch: 0069 train_loss= 0.09134 train_acc= 0.97488 val_loss= 0.16199 val_acc= 0.95073 time= 0.12400
Epoch: 0070 train_loss= 0.08243 train_acc= 0.97711 val_loss= 0.16240 val_acc= 0.95438 time= 0.12303
Epoch: 0071 train_loss= 0.08852 train_acc= 0.97873 val_loss= 0.16213 val_acc= 0.95438 time= 0.15697
Epoch: 0072 train_loss= 0.08333 train_acc= 0.97974 val_loss= 0.16143 val_acc= 0.95438 time= 0.12503
Epoch: 0073 train_loss= 0.08464 train_acc= 0.97812 val_loss= 0.15983 val_acc= 0.95255 time= 0.12299
Epoch: 0074 train_loss= 0.07869 train_acc= 0.97873 val_loss= 0.15754 val_acc= 0.95255 time= 0.12498
Epoch: 0075 train_loss= 0.07534 train_acc= 0.98299 val_loss= 0.15514 val_acc= 0.95255 time= 0.12300
Epoch: 0076 train_loss= 0.07300 train_acc= 0.98339 val_loss= 0.15329 val_acc= 0.95255 time= 0.12500
Epoch: 0077 train_loss= 0.07069 train_acc= 0.98238 val_loss= 0.15134 val_acc= 0.95438 time= 0.12826
Epoch: 0078 train_loss= 0.07014 train_acc= 0.98137 val_loss= 0.15015 val_acc= 0.95438 time= 0.12403
Epoch: 0079 train_loss= 0.06791 train_acc= 0.98299 val_loss= 0.14952 val_acc= 0.95438 time= 0.16097
Epoch: 0080 train_loss= 0.06449 train_acc= 0.98359 val_loss= 0.14830 val_acc= 0.95438 time= 0.12300
Epoch: 0081 train_loss= 0.06208 train_acc= 0.98481 val_loss= 0.14732 val_acc= 0.95438 time= 0.12303
Epoch: 0082 train_loss= 0.06154 train_acc= 0.98643 val_loss= 0.14617 val_acc= 0.95255 time= 0.12309
Epoch: 0083 train_loss= 0.06316 train_acc= 0.98339 val_loss= 0.14546 val_acc= 0.95255 time= 0.12400
Epoch: 0084 train_loss= 0.06274 train_acc= 0.98197 val_loss= 0.14479 val_acc= 0.95255 time= 0.12300
Epoch: 0085 train_loss= 0.05968 train_acc= 0.98623 val_loss= 0.14425 val_acc= 0.95438 time= 0.12526
Epoch: 0086 train_loss= 0.05817 train_acc= 0.98562 val_loss= 0.14373 val_acc= 0.95620 time= 0.12601
Epoch: 0087 train_loss= 0.05668 train_acc= 0.98501 val_loss= 0.14325 val_acc= 0.95620 time= 0.15500
Epoch: 0088 train_loss= 0.05695 train_acc= 0.98704 val_loss= 0.14309 val_acc= 0.95620 time= 0.12300
Epoch: 0089 train_loss= 0.05629 train_acc= 0.98542 val_loss= 0.14256 val_acc= 0.95803 time= 0.12301
Epoch: 0090 train_loss= 0.05553 train_acc= 0.98521 val_loss= 0.14194 val_acc= 0.95803 time= 0.12203
Epoch: 0091 train_loss= 0.05342 train_acc= 0.98562 val_loss= 0.14123 val_acc= 0.95803 time= 0.12400
Epoch: 0092 train_loss= 0.05066 train_acc= 0.98866 val_loss= 0.14102 val_acc= 0.95438 time= 0.12401
Epoch: 0093 train_loss= 0.04871 train_acc= 0.98825 val_loss= 0.14124 val_acc= 0.95438 time= 0.12597
Epoch: 0094 train_loss= 0.05107 train_acc= 0.98825 val_loss= 0.14193 val_acc= 0.95255 time= 0.12603
Epoch: 0095 train_loss= 0.04718 train_acc= 0.98683 val_loss= 0.14259 val_acc= 0.95438 time= 0.17097
Early stopping...
Optimization Finished!
Test set results: cost= 0.10852 accuracy= 0.97396 time= 0.05600
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9365    0.9752    0.9555       121
           1     0.9125    0.9733    0.9419        75
           2     0.9844    0.9917    0.9880      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.7222    0.8387        36
           5     0.9351    0.8889    0.9114        81
           6     0.8989    0.9195    0.9091        87
           7     0.9841    0.9756    0.9798       696

    accuracy                         0.9740      2189
   macro avg     0.9564    0.9308    0.9406      2189
weighted avg     0.9743    0.9740    0.9737      2189

Macro average Test Precision, Recall and F1-Score...
(0.9564281515314144, 0.9308069686544935, 0.9405540650834878, None)
Micro average Test Precision, Recall and F1-Score...
(0.9739607126541799, 0.9739607126541799, 0.9739607126541799, None)
embeddings:
7688 5485 2189
[[ 0.25068545  0.23763476  0.10461862 ...  0.07707374  0.13214755
   0.3285211 ]
 [ 0.20153964  0.03746885  0.21657957 ...  0.26261497  0.05445625
   0.1979881 ]
 [ 0.1561095  -0.00488031  0.30426055 ...  0.31353545  0.19122459
  -0.03314584]
 ...
 [ 0.14707641  0.06002309  0.36282623 ...  0.19645533  0.27168012
   0.1397485 ]
 [ 0.13854495  0.0308172   0.2271507  ...  0.44574136  0.03514253
   0.22153057]
 [ 0.00697107  0.01107365  0.34024924 ...  0.16128647  0.19720706
  -0.02717861]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07924 train_acc= 0.26048 val_loss= 1.57500 val_acc= 0.75912 time= 0.39067
Epoch: 0002 train_loss= 1.55526 train_acc= 0.78712 val_loss= 1.26557 val_acc= 0.74088 time= 0.13100
Epoch: 0003 train_loss= 1.21996 train_acc= 0.75491 val_loss= 1.04641 val_acc= 0.62409 time= 0.13800
Epoch: 0004 train_loss= 0.99683 train_acc= 0.64027 val_loss= 0.79216 val_acc= 0.75547 time= 0.12235
Epoch: 0005 train_loss= 0.75274 train_acc= 0.78448 val_loss= 0.68819 val_acc= 0.75182 time= 0.12400
Epoch: 0006 train_loss= 0.64564 train_acc= 0.77031 val_loss= 0.63409 val_acc= 0.76642 time= 0.12494
Epoch: 0007 train_loss= 0.58442 train_acc= 0.78752 val_loss= 0.58891 val_acc= 0.80292 time= 0.12600
Epoch: 0008 train_loss= 0.53382 train_acc= 0.82277 val_loss= 0.54602 val_acc= 0.83942 time= 0.12501
Epoch: 0009 train_loss= 0.48617 train_acc= 0.85031 val_loss= 0.50635 val_acc= 0.85401 time= 0.12199
Epoch: 0010 train_loss= 0.44291 train_acc= 0.86996 val_loss= 0.46884 val_acc= 0.86496 time= 0.12404
Epoch: 0011 train_loss= 0.39881 train_acc= 0.88090 val_loss= 0.43295 val_acc= 0.88504 time= 0.14800
Epoch: 0012 train_loss= 0.35943 train_acc= 0.89609 val_loss= 0.39947 val_acc= 0.89781 time= 0.12199
Epoch: 0013 train_loss= 0.31941 train_acc= 0.90399 val_loss= 0.37012 val_acc= 0.91423 time= 0.12401
Epoch: 0014 train_loss= 0.28124 train_acc= 0.92607 val_loss= 0.34655 val_acc= 0.91606 time= 0.12300
Epoch: 0015 train_loss= 0.24834 train_acc= 0.93721 val_loss= 0.32655 val_acc= 0.92336 time= 0.12300
Epoch: 0016 train_loss= 0.22280 train_acc= 0.94227 val_loss= 0.30517 val_acc= 0.92701 time= 0.12796
Epoch: 0017 train_loss= 0.18916 train_acc= 0.95341 val_loss= 0.28526 val_acc= 0.93066 time= 0.12603
Epoch: 0018 train_loss= 0.16715 train_acc= 0.95422 val_loss= 0.27239 val_acc= 0.93248 time= 0.12307
Epoch: 0019 train_loss= 0.14322 train_acc= 0.95787 val_loss= 0.26227 val_acc= 0.93796 time= 0.15499
Epoch: 0020 train_loss= 0.12996 train_acc= 0.95949 val_loss= 0.25024 val_acc= 0.93796 time= 0.12206
Epoch: 0021 train_loss= 0.11526 train_acc= 0.96820 val_loss= 0.23996 val_acc= 0.94343 time= 0.12200
Epoch: 0022 train_loss= 0.09606 train_acc= 0.97245 val_loss= 0.23712 val_acc= 0.94526 time= 0.12200
Epoch: 0023 train_loss= 0.08670 train_acc= 0.97103 val_loss= 0.23964 val_acc= 0.94343 time= 0.12304
Epoch: 0024 train_loss= 0.07949 train_acc= 0.97428 val_loss= 0.24297 val_acc= 0.94161 time= 0.12400
Epoch: 0025 train_loss= 0.07304 train_acc= 0.97549 val_loss= 0.24470 val_acc= 0.94161 time= 0.12700
Early stopping...
Optimization Finished!
Test set results: cost= 0.15594 accuracy= 0.96345 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.8881    0.9835    0.9333       121
           1     0.8889    0.9600    0.9231        75
           2     0.9844    0.9908    0.9876      1083
           3     1.0000    0.2000    0.3333        10
           4     0.8929    0.6944    0.7812        36
           5     0.8649    0.7901    0.8258        81
           6     0.8229    0.9080    0.8634        87
           7     0.9868    0.9698    0.9783       696

    accuracy                         0.9635      2189
   macro avg     0.9161    0.8121    0.8283      2189
weighted avg     0.9643    0.9635    0.9621      2189

Macro average Test Precision, Recall and F1-Score...
(0.9161041299697537, 0.812084866061435, 0.8282529588089973, None)
Micro average Test Precision, Recall and F1-Score...
(0.9634536317953404, 0.9634536317953404, 0.9634536317953404, None)
embeddings:
7688 5485 2189
[[ 0.56132895  0.05466847  0.09712946 ...  0.14378068  0.03999926
   0.07019688]
 [ 0.22193128 -0.14333847  0.09412938 ... -0.12177622 -0.02668809
  -0.1575239 ]
 [ 0.11376535  0.04171755  0.6540188  ...  0.05651355  0.6101985
  -0.22241685]
 ...
 [ 0.22240779  0.19214372  0.65109915 ...  0.21363363  0.64930713
  -0.07752243]
 [ 0.29732656 -0.28758875  0.04940716 ... -0.2511543  -0.07756765
  -0.30646306]
 [ 0.07389551  0.15297939  0.5512907  ...  0.1529028   0.47736782
  -0.05147387]]

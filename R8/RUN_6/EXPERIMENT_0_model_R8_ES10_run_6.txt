(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07962 train_acc= 0.01803 val_loss= 2.03301 val_acc= 0.75730 time= 0.39018
Epoch: 0002 train_loss= 2.03087 train_acc= 0.77031 val_loss= 1.95239 val_acc= 0.74453 time= 0.13300
Epoch: 0003 train_loss= 1.94809 train_acc= 0.76666 val_loss= 1.83957 val_acc= 0.73723 time= 0.14800
Epoch: 0004 train_loss= 1.83002 train_acc= 0.75268 val_loss= 1.70306 val_acc= 0.72445 time= 0.12700
Epoch: 0005 train_loss= 1.68876 train_acc= 0.73547 val_loss= 1.55925 val_acc= 0.70620 time= 0.12400
Epoch: 0006 train_loss= 1.53395 train_acc= 0.70853 val_loss= 1.42832 val_acc= 0.70255 time= 0.12500
Epoch: 0007 train_loss= 1.40352 train_acc= 0.71035 val_loss= 1.32306 val_acc= 0.71350 time= 0.12300
Epoch: 0008 train_loss= 1.28343 train_acc= 0.72169 val_loss= 1.24177 val_acc= 0.72263 time= 0.12300
Epoch: 0009 train_loss= 1.20300 train_acc= 0.73425 val_loss= 1.17415 val_acc= 0.73358 time= 0.12213
Epoch: 0010 train_loss= 1.13180 train_acc= 0.74539 val_loss= 1.11024 val_acc= 0.74270 time= 0.13800
Epoch: 0011 train_loss= 1.06107 train_acc= 0.75998 val_loss= 1.04473 val_acc= 0.75547 time= 0.14400
Epoch: 0012 train_loss= 0.99801 train_acc= 0.77476 val_loss= 0.97686 val_acc= 0.76095 time= 0.12215
Epoch: 0013 train_loss= 0.93529 train_acc= 0.77983 val_loss= 0.90851 val_acc= 0.76277 time= 0.12700
Epoch: 0014 train_loss= 0.87289 train_acc= 0.78631 val_loss= 0.84306 val_acc= 0.76095 time= 0.12500
Epoch: 0015 train_loss= 0.80945 train_acc= 0.78773 val_loss= 0.78373 val_acc= 0.76095 time= 0.12275
Epoch: 0016 train_loss= 0.74720 train_acc= 0.79036 val_loss= 0.73291 val_acc= 0.76277 time= 0.12396
Epoch: 0017 train_loss= 0.69882 train_acc= 0.79117 val_loss= 0.69139 val_acc= 0.77007 time= 0.12300
Epoch: 0018 train_loss= 0.65573 train_acc= 0.80737 val_loss= 0.65796 val_acc= 0.79562 time= 0.16819
Epoch: 0019 train_loss= 0.62319 train_acc= 0.82844 val_loss= 0.63010 val_acc= 0.82847 time= 0.12200
Epoch: 0020 train_loss= 0.59558 train_acc= 0.85133 val_loss= 0.60516 val_acc= 0.84307 time= 0.12395
Epoch: 0021 train_loss= 0.56667 train_acc= 0.86530 val_loss= 0.58130 val_acc= 0.85219 time= 0.12274
Epoch: 0022 train_loss= 0.54474 train_acc= 0.87178 val_loss= 0.55766 val_acc= 0.85949 time= 0.12996
Epoch: 0023 train_loss= 0.52277 train_acc= 0.87847 val_loss= 0.53423 val_acc= 0.85949 time= 0.12503
Epoch: 0024 train_loss= 0.49740 train_acc= 0.88292 val_loss= 0.51155 val_acc= 0.86131 time= 0.12301
Epoch: 0025 train_loss= 0.46866 train_acc= 0.89062 val_loss= 0.49012 val_acc= 0.86496 time= 0.12299
Epoch: 0026 train_loss= 0.44475 train_acc= 0.89548 val_loss= 0.47038 val_acc= 0.86496 time= 0.14999
Epoch: 0027 train_loss= 0.42851 train_acc= 0.89771 val_loss= 0.45238 val_acc= 0.87409 time= 0.12309
Epoch: 0028 train_loss= 0.40103 train_acc= 0.90075 val_loss= 0.43592 val_acc= 0.88321 time= 0.12400
Epoch: 0029 train_loss= 0.38661 train_acc= 0.90480 val_loss= 0.42060 val_acc= 0.89234 time= 0.12400
Epoch: 0030 train_loss= 0.37124 train_acc= 0.90622 val_loss= 0.40595 val_acc= 0.89599 time= 0.12308
Epoch: 0031 train_loss= 0.34929 train_acc= 0.91392 val_loss= 0.39175 val_acc= 0.89781 time= 0.12786
Epoch: 0032 train_loss= 0.33390 train_acc= 0.91736 val_loss= 0.37780 val_acc= 0.90328 time= 0.12400
Epoch: 0033 train_loss= 0.32171 train_acc= 0.92121 val_loss= 0.36402 val_acc= 0.90511 time= 0.12400
Epoch: 0034 train_loss= 0.30622 train_acc= 0.92303 val_loss= 0.35058 val_acc= 0.90876 time= 0.16716
Epoch: 0035 train_loss= 0.29275 train_acc= 0.92789 val_loss= 0.33761 val_acc= 0.90876 time= 0.12300
Epoch: 0036 train_loss= 0.27662 train_acc= 0.93235 val_loss= 0.32533 val_acc= 0.91606 time= 0.12312
Epoch: 0037 train_loss= 0.26502 train_acc= 0.93539 val_loss= 0.31380 val_acc= 0.91788 time= 0.12309
Epoch: 0038 train_loss= 0.25146 train_acc= 0.93944 val_loss= 0.30300 val_acc= 0.91971 time= 0.12226
Epoch: 0039 train_loss= 0.24007 train_acc= 0.94065 val_loss= 0.29284 val_acc= 0.91971 time= 0.12413
Epoch: 0040 train_loss= 0.22643 train_acc= 0.94612 val_loss= 0.28320 val_acc= 0.92701 time= 0.12797
Epoch: 0041 train_loss= 0.22120 train_acc= 0.94531 val_loss= 0.27397 val_acc= 0.93066 time= 0.13400
Epoch: 0042 train_loss= 0.20748 train_acc= 0.94835 val_loss= 0.26532 val_acc= 0.92883 time= 0.14903
Epoch: 0043 train_loss= 0.20021 train_acc= 0.95159 val_loss= 0.25721 val_acc= 0.93248 time= 0.12218
Epoch: 0044 train_loss= 0.18911 train_acc= 0.95544 val_loss= 0.24962 val_acc= 0.93431 time= 0.12196
Epoch: 0045 train_loss= 0.18250 train_acc= 0.95686 val_loss= 0.24238 val_acc= 0.93431 time= 0.12210
Epoch: 0046 train_loss= 0.17553 train_acc= 0.95807 val_loss= 0.23527 val_acc= 0.93431 time= 0.12300
Epoch: 0047 train_loss= 0.16014 train_acc= 0.96253 val_loss= 0.22846 val_acc= 0.93796 time= 0.12401
Epoch: 0048 train_loss= 0.15811 train_acc= 0.96192 val_loss= 0.22207 val_acc= 0.93796 time= 0.12596
Epoch: 0049 train_loss= 0.14994 train_acc= 0.96435 val_loss= 0.21623 val_acc= 0.93796 time= 0.17403
Epoch: 0050 train_loss= 0.14391 train_acc= 0.96476 val_loss= 0.21091 val_acc= 0.93796 time= 0.12500
Epoch: 0051 train_loss= 0.13969 train_acc= 0.96496 val_loss= 0.20592 val_acc= 0.93978 time= 0.12200
Epoch: 0052 train_loss= 0.13159 train_acc= 0.96678 val_loss= 0.20129 val_acc= 0.93978 time= 0.12401
Epoch: 0053 train_loss= 0.12736 train_acc= 0.96698 val_loss= 0.19680 val_acc= 0.94526 time= 0.12500
Epoch: 0054 train_loss= 0.11790 train_acc= 0.97144 val_loss= 0.19270 val_acc= 0.94526 time= 0.12300
Epoch: 0055 train_loss= 0.11263 train_acc= 0.97124 val_loss= 0.18895 val_acc= 0.94161 time= 0.12399
Epoch: 0056 train_loss= 0.11071 train_acc= 0.97185 val_loss= 0.18553 val_acc= 0.94343 time= 0.12606
Epoch: 0057 train_loss= 0.10490 train_acc= 0.97488 val_loss= 0.18223 val_acc= 0.94343 time= 0.14799
Epoch: 0058 train_loss= 0.10382 train_acc= 0.97488 val_loss= 0.17888 val_acc= 0.94526 time= 0.12476
Epoch: 0059 train_loss= 0.09529 train_acc= 0.97671 val_loss= 0.17572 val_acc= 0.94526 time= 0.12603
Epoch: 0060 train_loss= 0.09288 train_acc= 0.97792 val_loss= 0.17280 val_acc= 0.94708 time= 0.12197
Epoch: 0061 train_loss= 0.08832 train_acc= 0.98157 val_loss= 0.17027 val_acc= 0.95255 time= 0.12303
Epoch: 0062 train_loss= 0.08467 train_acc= 0.98177 val_loss= 0.16816 val_acc= 0.95255 time= 0.12200
Epoch: 0063 train_loss= 0.08375 train_acc= 0.98055 val_loss= 0.16612 val_acc= 0.95255 time= 0.12397
Epoch: 0064 train_loss= 0.07973 train_acc= 0.98177 val_loss= 0.16420 val_acc= 0.95073 time= 0.12503
Epoch: 0065 train_loss= 0.07893 train_acc= 0.98177 val_loss= 0.16251 val_acc= 0.95073 time= 0.15201
Epoch: 0066 train_loss= 0.07499 train_acc= 0.98218 val_loss= 0.16133 val_acc= 0.95073 time= 0.12299
Epoch: 0067 train_loss= 0.07222 train_acc= 0.98339 val_loss= 0.16047 val_acc= 0.95255 time= 0.12597
Epoch: 0068 train_loss= 0.07085 train_acc= 0.98359 val_loss= 0.15943 val_acc= 0.95255 time= 0.12600
Epoch: 0069 train_loss= 0.06889 train_acc= 0.98461 val_loss= 0.15817 val_acc= 0.95255 time= 0.12300
Epoch: 0070 train_loss= 0.06690 train_acc= 0.98501 val_loss= 0.15649 val_acc= 0.95255 time= 0.12404
Epoch: 0071 train_loss= 0.06147 train_acc= 0.98623 val_loss= 0.15488 val_acc= 0.95255 time= 0.12299
Epoch: 0072 train_loss= 0.06348 train_acc= 0.98501 val_loss= 0.15331 val_acc= 0.95255 time= 0.12696
Epoch: 0073 train_loss= 0.06094 train_acc= 0.98643 val_loss= 0.15199 val_acc= 0.95073 time= 0.16803
Epoch: 0074 train_loss= 0.05887 train_acc= 0.98461 val_loss= 0.15137 val_acc= 0.95255 time= 0.12200
Epoch: 0075 train_loss= 0.05702 train_acc= 0.98724 val_loss= 0.15114 val_acc= 0.95073 time= 0.12297
Epoch: 0076 train_loss= 0.05515 train_acc= 0.98744 val_loss= 0.15100 val_acc= 0.95255 time= 0.12600
Epoch: 0077 train_loss= 0.05469 train_acc= 0.98764 val_loss= 0.15080 val_acc= 0.95438 time= 0.12600
Epoch: 0078 train_loss= 0.05254 train_acc= 0.98724 val_loss= 0.15027 val_acc= 0.95255 time= 0.12403
Epoch: 0079 train_loss= 0.04998 train_acc= 0.98805 val_loss= 0.14924 val_acc= 0.95255 time= 0.12297
Epoch: 0080 train_loss= 0.05014 train_acc= 0.98947 val_loss= 0.14848 val_acc= 0.95438 time= 0.16103
Epoch: 0081 train_loss= 0.04699 train_acc= 0.99048 val_loss= 0.14772 val_acc= 0.95438 time= 0.12500
Epoch: 0082 train_loss= 0.04754 train_acc= 0.98825 val_loss= 0.14760 val_acc= 0.95438 time= 0.12297
Epoch: 0083 train_loss= 0.04534 train_acc= 0.98987 val_loss= 0.14803 val_acc= 0.95438 time= 0.12300
Epoch: 0084 train_loss= 0.04269 train_acc= 0.99068 val_loss= 0.14847 val_acc= 0.95620 time= 0.12304
Epoch: 0085 train_loss= 0.04462 train_acc= 0.99089 val_loss= 0.14885 val_acc= 0.95803 time= 0.12396
Epoch: 0086 train_loss= 0.04263 train_acc= 0.98947 val_loss= 0.14894 val_acc= 0.95438 time= 0.12600
Epoch: 0087 train_loss= 0.04197 train_acc= 0.98987 val_loss= 0.14874 val_acc= 0.95438 time= 0.12500
Epoch: 0088 train_loss= 0.03998 train_acc= 0.99190 val_loss= 0.14782 val_acc= 0.95620 time= 0.16103
Epoch: 0089 train_loss= 0.03971 train_acc= 0.99089 val_loss= 0.14677 val_acc= 0.95438 time= 0.12597
Epoch: 0090 train_loss= 0.04043 train_acc= 0.99007 val_loss= 0.14589 val_acc= 0.95073 time= 0.12300
Epoch: 0091 train_loss= 0.03617 train_acc= 0.99291 val_loss= 0.14546 val_acc= 0.95073 time= 0.12456
Epoch: 0092 train_loss= 0.03715 train_acc= 0.99251 val_loss= 0.14493 val_acc= 0.95255 time= 0.12308
Epoch: 0093 train_loss= 0.03665 train_acc= 0.99210 val_loss= 0.14460 val_acc= 0.95255 time= 0.12401
Epoch: 0094 train_loss= 0.03546 train_acc= 0.99291 val_loss= 0.14484 val_acc= 0.95255 time= 0.12400
Epoch: 0095 train_loss= 0.03461 train_acc= 0.99291 val_loss= 0.14556 val_acc= 0.95255 time= 0.12699
Epoch: 0096 train_loss= 0.03252 train_acc= 0.99372 val_loss= 0.14626 val_acc= 0.95255 time= 0.15900
Epoch: 0097 train_loss= 0.03191 train_acc= 0.99372 val_loss= 0.14698 val_acc= 0.95438 time= 0.12397
Early stopping...
Optimization Finished!
Test set results: cost= 0.10944 accuracy= 0.97305 time= 0.05604
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9219    0.9752    0.9478       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.6944    0.8197        36
           5     0.9595    0.8765    0.9161        81
           6     0.8913    0.9425    0.9162        87
           7     0.9840    0.9727    0.9783       696

    accuracy                         0.9730      2189
   macro avg     0.9566    0.9283    0.9385      2189
weighted avg     0.9736    0.9730    0.9727      2189

Macro average Test Precision, Recall and F1-Score...
(0.9565833648386259, 0.9283059043718201, 0.9384548544465201, None)
Micro average Test Precision, Recall and F1-Score...
(0.9730470534490635, 0.9730470534490635, 0.9730470534490635, None)
embeddings:
7688 5485 2189
[[ 0.06700421  0.20024115  0.19947381 ...  0.18661654  0.20253685
   0.0170068 ]
 [ 0.1417561   0.20707205  0.13026068 ...  0.018458    0.05250869
   0.1129681 ]
 [ 0.44765794  0.28659883  0.34428284 ...  0.05170242 -0.00140252
   0.4265492 ]
 ...
 [ 0.40220004  0.02229815  0.3888737  ...  0.14528896  0.11838633
   0.36201325]
 [ 0.18435878  0.29417467  0.1264365  ... -0.00265716  0.0381098
   0.2280266 ]
 [ 0.35508108  0.06224617  0.2948102  ...  0.0896479   0.05862689
   0.3263893 ]]

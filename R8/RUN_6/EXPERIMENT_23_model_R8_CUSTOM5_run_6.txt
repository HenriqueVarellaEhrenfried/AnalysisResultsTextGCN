(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07947 train_acc= 0.20802 val_loss= 2.02713 val_acc= 0.75547 time= 0.40339
Epoch: 0002 train_loss= 2.02525 train_acc= 0.77942 val_loss= 1.94157 val_acc= 0.75547 time= 0.12700
Epoch: 0003 train_loss= 1.93685 train_acc= 0.78307 val_loss= 1.82472 val_acc= 0.75547 time= 0.12500
Epoch: 0004 train_loss= 1.81793 train_acc= 0.78529 val_loss= 1.68720 val_acc= 0.75730 time= 0.12548
Epoch: 0005 train_loss= 1.67594 train_acc= 0.78712 val_loss= 1.54827 val_acc= 0.76095 time= 0.12399
Epoch: 0006 train_loss= 1.52842 train_acc= 0.78307 val_loss= 1.42834 val_acc= 0.76277 time= 0.12411
Epoch: 0007 train_loss= 1.39903 train_acc= 0.78226 val_loss= 1.33463 val_acc= 0.75547 time= 0.12428
Epoch: 0008 train_loss= 1.29734 train_acc= 0.77780 val_loss= 1.26024 val_acc= 0.74270 time= 0.17300
Epoch: 0009 train_loss= 1.21610 train_acc= 0.76099 val_loss= 1.19445 val_acc= 0.73358 time= 0.12503
Epoch: 0010 train_loss= 1.15164 train_acc= 0.75005 val_loss= 1.12922 val_acc= 0.73358 time= 0.12397
Epoch: 0011 train_loss= 1.08500 train_acc= 0.74539 val_loss= 1.06059 val_acc= 0.73540 time= 0.12304
Epoch: 0012 train_loss= 1.01692 train_acc= 0.75511 val_loss= 0.98854 val_acc= 0.75000 time= 0.12696
Epoch: 0013 train_loss= 0.94419 train_acc= 0.77274 val_loss= 0.91611 val_acc= 0.76277 time= 0.12303
Epoch: 0014 train_loss= 0.87404 train_acc= 0.78509 val_loss= 0.84793 val_acc= 0.76277 time= 0.12256
Epoch: 0015 train_loss= 0.80749 train_acc= 0.79036 val_loss= 0.78790 val_acc= 0.76460 time= 0.12312
Epoch: 0016 train_loss= 0.75118 train_acc= 0.78914 val_loss= 0.73800 val_acc= 0.76277 time= 0.15000
Epoch: 0017 train_loss= 0.70160 train_acc= 0.79279 val_loss= 0.69784 val_acc= 0.77555 time= 0.12629
Epoch: 0018 train_loss= 0.66294 train_acc= 0.80130 val_loss= 0.66544 val_acc= 0.79015 time= 0.12401
Epoch: 0019 train_loss= 0.62995 train_acc= 0.81142 val_loss= 0.63812 val_acc= 0.80474 time= 0.12299
Epoch: 0020 train_loss= 0.60223 train_acc= 0.82662 val_loss= 0.61338 val_acc= 0.82664 time= 0.12597
Epoch: 0021 train_loss= 0.57739 train_acc= 0.84707 val_loss= 0.58951 val_acc= 0.83759 time= 0.12425
Epoch: 0022 train_loss= 0.55144 train_acc= 0.86166 val_loss= 0.56578 val_acc= 0.85036 time= 0.12403
Epoch: 0023 train_loss= 0.52472 train_acc= 0.87280 val_loss= 0.54229 val_acc= 0.86679 time= 0.12317
Epoch: 0024 train_loss= 0.50034 train_acc= 0.88110 val_loss= 0.51957 val_acc= 0.87226 time= 0.16901
Epoch: 0025 train_loss= 0.47576 train_acc= 0.88839 val_loss= 0.49814 val_acc= 0.87409 time= 0.12299
Epoch: 0026 train_loss= 0.45204 train_acc= 0.89407 val_loss= 0.47830 val_acc= 0.87591 time= 0.12588
Epoch: 0027 train_loss= 0.42959 train_acc= 0.90055 val_loss= 0.46005 val_acc= 0.88686 time= 0.12603
Epoch: 0028 train_loss= 0.41028 train_acc= 0.90480 val_loss= 0.44315 val_acc= 0.89416 time= 0.12497
Epoch: 0029 train_loss= 0.39124 train_acc= 0.91148 val_loss= 0.42730 val_acc= 0.90146 time= 0.12503
Epoch: 0030 train_loss= 0.37358 train_acc= 0.91290 val_loss= 0.41216 val_acc= 0.90876 time= 0.12329
Epoch: 0031 train_loss= 0.35609 train_acc= 0.92060 val_loss= 0.39752 val_acc= 0.90876 time= 0.15800
Epoch: 0032 train_loss= 0.34062 train_acc= 0.92506 val_loss= 0.38325 val_acc= 0.91241 time= 0.12407
Epoch: 0033 train_loss= 0.32460 train_acc= 0.92850 val_loss= 0.36935 val_acc= 0.91241 time= 0.12416
Epoch: 0034 train_loss= 0.31005 train_acc= 0.93214 val_loss= 0.35585 val_acc= 0.91241 time= 0.12404
Epoch: 0035 train_loss= 0.29368 train_acc= 0.93660 val_loss= 0.34281 val_acc= 0.91606 time= 0.12700
Epoch: 0036 train_loss= 0.28026 train_acc= 0.93883 val_loss= 0.33036 val_acc= 0.92153 time= 0.12776
Epoch: 0037 train_loss= 0.26609 train_acc= 0.94430 val_loss= 0.31849 val_acc= 0.92153 time= 0.12500
Epoch: 0038 train_loss= 0.25314 train_acc= 0.94835 val_loss= 0.30721 val_acc= 0.92153 time= 0.12305
Epoch: 0039 train_loss= 0.24079 train_acc= 0.95139 val_loss= 0.29652 val_acc= 0.92153 time= 0.15795
Epoch: 0040 train_loss= 0.22797 train_acc= 0.95402 val_loss= 0.28636 val_acc= 0.92336 time= 0.12647
Epoch: 0041 train_loss= 0.21665 train_acc= 0.95584 val_loss= 0.27664 val_acc= 0.92518 time= 0.12399
Epoch: 0042 train_loss= 0.20574 train_acc= 0.95706 val_loss= 0.26735 val_acc= 0.92701 time= 0.12300
Epoch: 0043 train_loss= 0.19532 train_acc= 0.95827 val_loss= 0.25843 val_acc= 0.93066 time= 0.12404
Epoch: 0044 train_loss= 0.18657 train_acc= 0.96172 val_loss= 0.24996 val_acc= 0.93066 time= 0.12296
Epoch: 0045 train_loss= 0.17566 train_acc= 0.96314 val_loss= 0.24192 val_acc= 0.93066 time= 0.12800
Epoch: 0046 train_loss= 0.16578 train_acc= 0.96395 val_loss= 0.23433 val_acc= 0.93248 time= 0.12603
Epoch: 0047 train_loss= 0.15697 train_acc= 0.96577 val_loss= 0.22725 val_acc= 0.93613 time= 0.15500
Epoch: 0048 train_loss= 0.14951 train_acc= 0.96658 val_loss= 0.22068 val_acc= 0.93796 time= 0.12336
Epoch: 0049 train_loss= 0.14183 train_acc= 0.96860 val_loss= 0.21463 val_acc= 0.93978 time= 0.12395
Epoch: 0050 train_loss= 0.13513 train_acc= 0.96881 val_loss= 0.20900 val_acc= 0.94161 time= 0.12405
Epoch: 0051 train_loss= 0.12784 train_acc= 0.96941 val_loss= 0.20395 val_acc= 0.94526 time= 0.12312
Epoch: 0052 train_loss= 0.12236 train_acc= 0.97245 val_loss= 0.19928 val_acc= 0.94708 time= 0.12385
Epoch: 0053 train_loss= 0.11779 train_acc= 0.97448 val_loss= 0.19493 val_acc= 0.94708 time= 0.12599
Epoch: 0054 train_loss= 0.11041 train_acc= 0.97529 val_loss= 0.19102 val_acc= 0.94708 time= 0.12797
Epoch: 0055 train_loss= 0.10537 train_acc= 0.97630 val_loss= 0.18740 val_acc= 0.94708 time= 0.17203
Epoch: 0056 train_loss= 0.10033 train_acc= 0.97893 val_loss= 0.18401 val_acc= 0.94891 time= 0.12301
Epoch: 0057 train_loss= 0.09673 train_acc= 0.97812 val_loss= 0.18085 val_acc= 0.94708 time= 0.12200
Epoch: 0058 train_loss= 0.09120 train_acc= 0.98035 val_loss= 0.17788 val_acc= 0.94891 time= 0.12300
Epoch: 0059 train_loss= 0.08800 train_acc= 0.98177 val_loss= 0.17517 val_acc= 0.95073 time= 0.12400
Epoch: 0060 train_loss= 0.08434 train_acc= 0.98339 val_loss= 0.17272 val_acc= 0.95073 time= 0.12321
Epoch: 0061 train_loss= 0.08062 train_acc= 0.98380 val_loss= 0.17060 val_acc= 0.95073 time= 0.12596
Epoch: 0062 train_loss= 0.07800 train_acc= 0.98380 val_loss= 0.16868 val_acc= 0.95073 time= 0.16912
Epoch: 0063 train_loss= 0.07424 train_acc= 0.98461 val_loss= 0.16705 val_acc= 0.95073 time= 0.12532
Epoch: 0064 train_loss= 0.07145 train_acc= 0.98521 val_loss= 0.16561 val_acc= 0.95255 time= 0.12600
Epoch: 0065 train_loss= 0.06829 train_acc= 0.98542 val_loss= 0.16437 val_acc= 0.95255 time= 0.12500
Epoch: 0066 train_loss= 0.06575 train_acc= 0.98623 val_loss= 0.16333 val_acc= 0.95438 time= 0.12500
Epoch: 0067 train_loss= 0.06357 train_acc= 0.98623 val_loss= 0.16230 val_acc= 0.95438 time= 0.12403
Epoch: 0068 train_loss= 0.06058 train_acc= 0.98704 val_loss= 0.16139 val_acc= 0.95438 time= 0.12297
Epoch: 0069 train_loss= 0.05906 train_acc= 0.98744 val_loss= 0.16065 val_acc= 0.95438 time= 0.12303
Epoch: 0070 train_loss= 0.05677 train_acc= 0.98785 val_loss= 0.16005 val_acc= 0.95438 time= 0.15400
Epoch: 0071 train_loss= 0.05417 train_acc= 0.98845 val_loss= 0.15955 val_acc= 0.95438 time= 0.12309
Epoch: 0072 train_loss= 0.05280 train_acc= 0.98845 val_loss= 0.15909 val_acc= 0.95438 time= 0.12500
Epoch: 0073 train_loss= 0.05068 train_acc= 0.98845 val_loss= 0.15880 val_acc= 0.95620 time= 0.12800
Epoch: 0074 train_loss= 0.04953 train_acc= 0.98906 val_loss= 0.15863 val_acc= 0.95620 time= 0.12569
Epoch: 0075 train_loss= 0.04716 train_acc= 0.98967 val_loss= 0.15848 val_acc= 0.95620 time= 0.12299
Epoch: 0076 train_loss= 0.04601 train_acc= 0.99129 val_loss= 0.15815 val_acc= 0.95620 time= 0.12300
Epoch: 0077 train_loss= 0.04411 train_acc= 0.99089 val_loss= 0.15773 val_acc= 0.95438 time= 0.12300
Epoch: 0078 train_loss= 0.04329 train_acc= 0.99210 val_loss= 0.15727 val_acc= 0.95438 time= 0.17100
Epoch: 0079 train_loss= 0.04193 train_acc= 0.99149 val_loss= 0.15716 val_acc= 0.95438 time= 0.12309
Epoch: 0080 train_loss= 0.04061 train_acc= 0.99149 val_loss= 0.15725 val_acc= 0.95438 time= 0.12500
Epoch: 0081 train_loss= 0.03960 train_acc= 0.99230 val_loss= 0.15745 val_acc= 0.95620 time= 0.12395
Epoch: 0082 train_loss= 0.03846 train_acc= 0.99251 val_loss= 0.15744 val_acc= 0.95620 time= 0.12684
Epoch: 0083 train_loss= 0.03760 train_acc= 0.99352 val_loss= 0.15731 val_acc= 0.95620 time= 0.12600
Epoch: 0084 train_loss= 0.03603 train_acc= 0.99352 val_loss= 0.15729 val_acc= 0.95620 time= 0.12404
Epoch: 0085 train_loss= 0.03509 train_acc= 0.99372 val_loss= 0.15760 val_acc= 0.95620 time= 0.15396
Early stopping...
Optimization Finished!
Test set results: cost= 0.11118 accuracy= 0.97213 time= 0.05968
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9440    0.9752    0.9593       121
           1     0.9136    0.9867    0.9487        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     0.9310    0.7500    0.8308        36
           5     0.9103    0.8765    0.8931        81
           6     0.8876    0.9080    0.8977        87
           7     0.9854    0.9698    0.9776       696

    accuracy                         0.9721      2189
   macro avg     0.9444    0.9322    0.9368      2189
weighted avg     0.9722    0.9721    0.9719      2189

Macro average Test Precision, Recall and F1-Score...
(0.944428691592164, 0.9322474752530461, 0.9368480639753682, None)
Micro average Test Precision, Recall and F1-Score...
(0.972133394243947, 0.972133394243947, 0.972133394243947, None)
embeddings:
7688 5485 2189
[[ 0.20648503  0.08767073 -0.07008413 ...  0.22781572 -0.0692638
   0.38314265]
 [ 0.03708754  0.07705984 -0.05607856 ...  0.04236089 -0.06173225
   0.20395142]
 [ 0.04550241  0.3659266  -0.08107213 ...  0.09106767 -0.07162575
   0.05784751]
 ...
 [ 0.15788524  0.3721112  -0.08311701 ...  0.16988927 -0.06820615
   0.13570498]
 [ 0.01365456  0.07399808 -0.08245306 ...  0.02597315 -0.08759605
   0.23934878]
 [ 0.08834851  0.29076365 -0.06038062 ...  0.12839435 -0.05439776
   0.01840153]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07951 train_acc= 0.05935 val_loss= 1.60531 val_acc= 0.70620 time= 0.39402
Epoch: 0002 train_loss= 1.59334 train_acc= 0.72169 val_loss= 1.25771 val_acc= 0.68066 time= 0.13004
Epoch: 0003 train_loss= 1.20360 train_acc= 0.70630 val_loss= 1.04540 val_acc= 0.68978 time= 0.12838
Epoch: 0004 train_loss= 0.98640 train_acc= 0.70954 val_loss= 0.81358 val_acc= 0.75365 time= 0.12699
Epoch: 0005 train_loss= 0.76839 train_acc= 0.77841 val_loss= 0.69778 val_acc= 0.75547 time= 0.16569
Epoch: 0006 train_loss= 0.65071 train_acc= 0.77658 val_loss= 0.64833 val_acc= 0.75730 time= 0.12300
Epoch: 0007 train_loss= 0.59684 train_acc= 0.78590 val_loss= 0.61668 val_acc= 0.76825 time= 0.12301
Epoch: 0008 train_loss= 0.55725 train_acc= 0.79765 val_loss= 0.58562 val_acc= 0.78467 time= 0.12567
Epoch: 0009 train_loss= 0.52424 train_acc= 0.81021 val_loss= 0.55698 val_acc= 0.79380 time= 0.12298
Epoch: 0010 train_loss= 0.48700 train_acc= 0.82216 val_loss= 0.53043 val_acc= 0.81387 time= 0.12401
Epoch: 0011 train_loss= 0.44984 train_acc= 0.83998 val_loss= 0.50195 val_acc= 0.83759 time= 0.12405
Epoch: 0012 train_loss= 0.41975 train_acc= 0.85801 val_loss= 0.47043 val_acc= 0.85584 time= 0.12500
Epoch: 0013 train_loss= 0.38995 train_acc= 0.87786 val_loss= 0.43838 val_acc= 0.87226 time= 0.15900
Epoch: 0014 train_loss= 0.35023 train_acc= 0.89447 val_loss= 0.40887 val_acc= 0.88504 time= 0.12593
Epoch: 0015 train_loss= 0.31091 train_acc= 0.91047 val_loss= 0.38297 val_acc= 0.89416 time= 0.12300
Epoch: 0016 train_loss= 0.28391 train_acc= 0.91837 val_loss= 0.36248 val_acc= 0.90328 time= 0.12500
Epoch: 0017 train_loss= 0.26030 train_acc= 0.92789 val_loss= 0.34594 val_acc= 0.91423 time= 0.12500
Epoch: 0018 train_loss= 0.23443 train_acc= 0.93498 val_loss= 0.33082 val_acc= 0.92518 time= 0.12400
Epoch: 0019 train_loss= 0.20410 train_acc= 0.94531 val_loss= 0.31532 val_acc= 0.92518 time= 0.12403
Epoch: 0020 train_loss= 0.18465 train_acc= 0.94997 val_loss= 0.29852 val_acc= 0.93248 time= 0.16700
Epoch: 0021 train_loss= 0.15841 train_acc= 0.95341 val_loss= 0.28301 val_acc= 0.93066 time= 0.12500
Epoch: 0022 train_loss= 0.13730 train_acc= 0.95888 val_loss= 0.27113 val_acc= 0.93248 time= 0.12500
Epoch: 0023 train_loss= 0.13134 train_acc= 0.96070 val_loss= 0.26166 val_acc= 0.93978 time= 0.12449
Epoch: 0024 train_loss= 0.11493 train_acc= 0.96415 val_loss= 0.25648 val_acc= 0.94161 time= 0.12708
Epoch: 0025 train_loss= 0.09624 train_acc= 0.97144 val_loss= 0.25494 val_acc= 0.93796 time= 0.12500
Epoch: 0026 train_loss= 0.08901 train_acc= 0.97104 val_loss= 0.25594 val_acc= 0.93431 time= 0.12305
Epoch: 0027 train_loss= 0.07711 train_acc= 0.97448 val_loss= 0.25723 val_acc= 0.93613 time= 0.12264
Epoch: 0028 train_loss= 0.07220 train_acc= 0.97610 val_loss= 0.25553 val_acc= 0.93613 time= 0.14800
Epoch: 0029 train_loss= 0.06623 train_acc= 0.97731 val_loss= 0.25261 val_acc= 0.93796 time= 0.12300
Epoch: 0030 train_loss= 0.05809 train_acc= 0.98076 val_loss= 0.25257 val_acc= 0.94708 time= 0.12515
Epoch: 0031 train_loss= 0.05153 train_acc= 0.98258 val_loss= 0.25611 val_acc= 0.94891 time= 0.12600
Early stopping...
Optimization Finished!
Test set results: cost= 0.15911 accuracy= 0.96437 time= 0.05600
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9120    0.9421    0.9268       121
           1     0.8765    0.9467    0.9103        75
           2     0.9781    0.9917    0.9849      1083
           3     0.4167    0.5000    0.4545        10
           4     0.9333    0.7778    0.8485        36
           5     0.9351    0.8889    0.9114        81
           6     0.9070    0.8966    0.9017        87
           7     0.9838    0.9612    0.9724       696

    accuracy                         0.9644      2189
   macro avg     0.8678    0.8631    0.8638      2189
weighted avg     0.9651    0.9644    0.9644      2189

Macro average Test Precision, Recall and F1-Score...
(0.8678188118802528, 0.8631163081307609, 0.8638119422020515, None)
Micro average Test Precision, Recall and F1-Score...
(0.9643672910004568, 0.9643672910004568, 0.9643672910004568, None)
embeddings:
7688 5485 2189
[[ 0.08098099  0.15820438  0.16912377 ...  0.8792684  -0.21584614
  -0.35101432]
 [ 0.14487696  0.27589226 -0.12174128 ...  0.4155845  -0.2132776
  -0.31651005]
 [ 0.783263    0.91103184  0.15729733 ...  0.37122643 -0.24443802
  -0.40056506]
 ...
 [ 0.6457281   0.70132494  0.28187612 ...  0.5731299  -0.24316408
  -0.39794236]
 [ 0.02655676  0.53981227 -0.27584764 ...  0.4724634  -0.3030737
  -0.4562582 ]
 [ 0.6056886   0.7987606   0.21512306 ...  0.06059925 -0.17874241
  -0.29470205]]

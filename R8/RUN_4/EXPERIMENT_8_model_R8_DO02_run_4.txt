(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07935 train_acc= 0.17136 val_loss= 2.02212 val_acc= 0.75547 time= 0.38303
Epoch: 0002 train_loss= 2.02020 train_acc= 0.78388 val_loss= 1.92708 val_acc= 0.75547 time= 0.12997
Epoch: 0003 train_loss= 1.92143 train_acc= 0.78509 val_loss= 1.79778 val_acc= 0.76095 time= 0.12711
Epoch: 0004 train_loss= 1.78891 train_acc= 0.78286 val_loss= 1.64940 val_acc= 0.75182 time= 0.15107
Epoch: 0005 train_loss= 1.63382 train_acc= 0.76727 val_loss= 1.50623 val_acc= 0.73540 time= 0.12404
Epoch: 0006 train_loss= 1.48413 train_acc= 0.75917 val_loss= 1.38952 val_acc= 0.73358 time= 0.12603
Epoch: 0007 train_loss= 1.35929 train_acc= 0.74235 val_loss= 1.30251 val_acc= 0.71168 time= 0.12506
Epoch: 0008 train_loss= 1.26424 train_acc= 0.72372 val_loss= 1.23400 val_acc= 0.65876 time= 0.12400
Epoch: 0009 train_loss= 1.19437 train_acc= 0.68139 val_loss= 1.17216 val_acc= 0.63686 time= 0.12212
Epoch: 0010 train_loss= 1.12886 train_acc= 0.66214 val_loss= 1.10910 val_acc= 0.64234 time= 0.12470
Epoch: 0011 train_loss= 1.07023 train_acc= 0.66133 val_loss= 1.04133 val_acc= 0.68796 time= 0.12499
Epoch: 0012 train_loss= 0.99927 train_acc= 0.70367 val_loss= 0.97024 val_acc= 0.73175 time= 0.15901
Epoch: 0013 train_loss= 0.93084 train_acc= 0.74965 val_loss= 0.90063 val_acc= 0.75730 time= 0.12300
Epoch: 0014 train_loss= 0.86190 train_acc= 0.77577 val_loss= 0.83757 val_acc= 0.75547 time= 0.12495
Epoch: 0015 train_loss= 0.80228 train_acc= 0.78570 val_loss= 0.78431 val_acc= 0.75912 time= 0.12505
Epoch: 0016 train_loss= 0.75221 train_acc= 0.78469 val_loss= 0.74107 val_acc= 0.76460 time= 0.12300
Epoch: 0017 train_loss= 0.70813 train_acc= 0.78469 val_loss= 0.70612 val_acc= 0.76642 time= 0.12396
Epoch: 0018 train_loss= 0.67303 train_acc= 0.79623 val_loss= 0.67685 val_acc= 0.78832 time= 0.12204
Epoch: 0019 train_loss= 0.64288 train_acc= 0.81446 val_loss= 0.65050 val_acc= 0.82117 time= 0.12600
Epoch: 0020 train_loss= 0.61748 train_acc= 0.83533 val_loss= 0.62509 val_acc= 0.83577 time= 0.15296
Epoch: 0021 train_loss= 0.58928 train_acc= 0.85295 val_loss= 0.59967 val_acc= 0.84672 time= 0.12305
Epoch: 0022 train_loss= 0.56108 train_acc= 0.86773 val_loss= 0.57429 val_acc= 0.85036 time= 0.12400
Epoch: 0023 train_loss= 0.53524 train_acc= 0.87361 val_loss= 0.54946 val_acc= 0.85401 time= 0.12416
Epoch: 0024 train_loss= 0.50944 train_acc= 0.88110 val_loss= 0.52569 val_acc= 0.85766 time= 0.12500
Epoch: 0025 train_loss= 0.48384 train_acc= 0.88738 val_loss= 0.50337 val_acc= 0.86679 time= 0.12305
Epoch: 0026 train_loss= 0.46067 train_acc= 0.89184 val_loss= 0.48258 val_acc= 0.87044 time= 0.12272
Epoch: 0027 train_loss= 0.43715 train_acc= 0.89609 val_loss= 0.46321 val_acc= 0.87591 time= 0.12403
Epoch: 0028 train_loss= 0.41689 train_acc= 0.89832 val_loss= 0.44502 val_acc= 0.88686 time= 0.16901
Epoch: 0029 train_loss= 0.39737 train_acc= 0.90298 val_loss= 0.42773 val_acc= 0.89416 time= 0.12207
Epoch: 0030 train_loss= 0.37830 train_acc= 0.90480 val_loss= 0.41116 val_acc= 0.89416 time= 0.12401
Epoch: 0031 train_loss= 0.36085 train_acc= 0.90804 val_loss= 0.39519 val_acc= 0.89416 time= 0.12315
Epoch: 0032 train_loss= 0.34231 train_acc= 0.91452 val_loss= 0.37979 val_acc= 0.90146 time= 0.12583
Epoch: 0033 train_loss= 0.32564 train_acc= 0.91918 val_loss= 0.36488 val_acc= 0.90876 time= 0.12404
Epoch: 0034 train_loss= 0.30997 train_acc= 0.92364 val_loss= 0.35054 val_acc= 0.91788 time= 0.12307
Epoch: 0035 train_loss= 0.29463 train_acc= 0.92688 val_loss= 0.33679 val_acc= 0.91788 time= 0.15000
Epoch: 0036 train_loss= 0.28137 train_acc= 0.92971 val_loss= 0.32366 val_acc= 0.91788 time= 0.13800
Epoch: 0037 train_loss= 0.26488 train_acc= 0.93498 val_loss= 0.31134 val_acc= 0.92153 time= 0.12301
Epoch: 0038 train_loss= 0.24969 train_acc= 0.93923 val_loss= 0.29980 val_acc= 0.92701 time= 0.12299
Epoch: 0039 train_loss= 0.23906 train_acc= 0.94349 val_loss= 0.28896 val_acc= 0.92518 time= 0.12397
Epoch: 0040 train_loss= 0.22546 train_acc= 0.94713 val_loss= 0.27867 val_acc= 0.92518 time= 0.12303
Epoch: 0041 train_loss= 0.21679 train_acc= 0.95139 val_loss= 0.26876 val_acc= 0.92518 time= 0.12595
Epoch: 0042 train_loss= 0.20538 train_acc= 0.95362 val_loss= 0.25930 val_acc= 0.92883 time= 0.12503
Epoch: 0043 train_loss= 0.19514 train_acc= 0.95584 val_loss= 0.25031 val_acc= 0.93066 time= 0.16301
Epoch: 0044 train_loss= 0.18716 train_acc= 0.95645 val_loss= 0.24182 val_acc= 0.93248 time= 0.12699
Epoch: 0045 train_loss= 0.17668 train_acc= 0.95969 val_loss= 0.23385 val_acc= 0.93431 time= 0.12401
Epoch: 0046 train_loss= 0.16572 train_acc= 0.96050 val_loss= 0.22636 val_acc= 0.93431 time= 0.12400
Epoch: 0047 train_loss= 0.15939 train_acc= 0.96233 val_loss= 0.21940 val_acc= 0.93431 time= 0.12300
Epoch: 0048 train_loss= 0.15100 train_acc= 0.96435 val_loss= 0.21286 val_acc= 0.93796 time= 0.12299
Epoch: 0049 train_loss= 0.14498 train_acc= 0.96476 val_loss= 0.20673 val_acc= 0.93978 time= 0.12297
Epoch: 0050 train_loss= 0.13821 train_acc= 0.96455 val_loss= 0.20102 val_acc= 0.93978 time= 0.12590
Epoch: 0051 train_loss= 0.13062 train_acc= 0.96759 val_loss= 0.19562 val_acc= 0.93978 time= 0.15003
Epoch: 0052 train_loss= 0.12548 train_acc= 0.96921 val_loss= 0.19052 val_acc= 0.93978 time= 0.12800
Epoch: 0053 train_loss= 0.11877 train_acc= 0.97043 val_loss= 0.18565 val_acc= 0.94343 time= 0.12400
Epoch: 0054 train_loss= 0.11410 train_acc= 0.97185 val_loss= 0.18114 val_acc= 0.94343 time= 0.12300
Epoch: 0055 train_loss= 0.11001 train_acc= 0.97286 val_loss= 0.17704 val_acc= 0.94161 time= 0.12305
Epoch: 0056 train_loss= 0.10338 train_acc= 0.97650 val_loss= 0.17330 val_acc= 0.94161 time= 0.12301
Epoch: 0057 train_loss= 0.09882 train_acc= 0.97610 val_loss= 0.16999 val_acc= 0.94343 time= 0.12296
Epoch: 0058 train_loss= 0.09511 train_acc= 0.97792 val_loss= 0.16694 val_acc= 0.94343 time= 0.12400
Epoch: 0059 train_loss= 0.09031 train_acc= 0.97974 val_loss= 0.16423 val_acc= 0.94526 time= 0.17404
Epoch: 0060 train_loss= 0.08764 train_acc= 0.98055 val_loss= 0.16182 val_acc= 0.95073 time= 0.12296
Epoch: 0061 train_loss= 0.08367 train_acc= 0.98157 val_loss= 0.15962 val_acc= 0.95438 time= 0.12400
Epoch: 0062 train_loss= 0.08213 train_acc= 0.98278 val_loss= 0.15756 val_acc= 0.95438 time= 0.12300
Epoch: 0063 train_loss= 0.07735 train_acc= 0.98299 val_loss= 0.15556 val_acc= 0.95438 time= 0.12204
Epoch: 0064 train_loss= 0.07491 train_acc= 0.98299 val_loss= 0.15367 val_acc= 0.95438 time= 0.12400
Epoch: 0065 train_loss= 0.07220 train_acc= 0.98299 val_loss= 0.15189 val_acc= 0.95438 time= 0.12300
Epoch: 0066 train_loss= 0.07005 train_acc= 0.98420 val_loss= 0.15021 val_acc= 0.95438 time= 0.14896
Epoch: 0067 train_loss= 0.06646 train_acc= 0.98461 val_loss= 0.14878 val_acc= 0.95255 time= 0.13512
Epoch: 0068 train_loss= 0.06456 train_acc= 0.98663 val_loss= 0.14753 val_acc= 0.95255 time= 0.12508
Epoch: 0069 train_loss= 0.06207 train_acc= 0.98704 val_loss= 0.14618 val_acc= 0.95255 time= 0.12497
Epoch: 0070 train_loss= 0.06114 train_acc= 0.98704 val_loss= 0.14477 val_acc= 0.95438 time= 0.12303
Epoch: 0071 train_loss= 0.05865 train_acc= 0.98643 val_loss= 0.14336 val_acc= 0.95438 time= 0.12400
Epoch: 0072 train_loss= 0.05599 train_acc= 0.98764 val_loss= 0.14221 val_acc= 0.95438 time= 0.12200
Epoch: 0073 train_loss= 0.05491 train_acc= 0.98906 val_loss= 0.14137 val_acc= 0.95255 time= 0.12300
Epoch: 0074 train_loss= 0.05374 train_acc= 0.98785 val_loss= 0.14076 val_acc= 0.95620 time= 0.16601
Epoch: 0075 train_loss= 0.05150 train_acc= 0.98926 val_loss= 0.14023 val_acc= 0.95620 time= 0.12199
Epoch: 0076 train_loss= 0.05014 train_acc= 0.98987 val_loss= 0.13955 val_acc= 0.95620 time= 0.12499
Epoch: 0077 train_loss= 0.04814 train_acc= 0.98967 val_loss= 0.13914 val_acc= 0.95620 time= 0.12700
Epoch: 0078 train_loss= 0.04699 train_acc= 0.99109 val_loss= 0.13879 val_acc= 0.95620 time= 0.12226
Epoch: 0079 train_loss= 0.04593 train_acc= 0.99109 val_loss= 0.13815 val_acc= 0.95620 time= 0.12297
Epoch: 0080 train_loss= 0.04388 train_acc= 0.99089 val_loss= 0.13752 val_acc= 0.95620 time= 0.12503
Epoch: 0081 train_loss= 0.04324 train_acc= 0.99109 val_loss= 0.13678 val_acc= 0.95620 time= 0.12400
Epoch: 0082 train_loss= 0.04257 train_acc= 0.99048 val_loss= 0.13624 val_acc= 0.95438 time= 0.15100
Epoch: 0083 train_loss= 0.04068 train_acc= 0.99129 val_loss= 0.13589 val_acc= 0.95255 time= 0.12197
Epoch: 0084 train_loss= 0.03981 train_acc= 0.99190 val_loss= 0.13569 val_acc= 0.95255 time= 0.12303
Epoch: 0085 train_loss= 0.03854 train_acc= 0.99129 val_loss= 0.13562 val_acc= 0.95255 time= 0.12505
Epoch: 0086 train_loss= 0.03761 train_acc= 0.99291 val_loss= 0.13564 val_acc= 0.95255 time= 0.12604
Epoch: 0087 train_loss= 0.03720 train_acc= 0.99311 val_loss= 0.13595 val_acc= 0.95438 time= 0.12232
Epoch: 0088 train_loss= 0.03620 train_acc= 0.99291 val_loss= 0.13626 val_acc= 0.95255 time= 0.12304
Epoch: 0089 train_loss= 0.03492 train_acc= 0.99311 val_loss= 0.13646 val_acc= 0.95255 time= 0.12501
Early stopping...
Optimization Finished!
Test set results: cost= 0.10655 accuracy= 0.97350 time= 0.09299
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9512    0.9669    0.9590       121
           1     0.9125    0.9733    0.9419        75
           2     0.9835    0.9917    0.9876      1083
           3     0.9091    1.0000    0.9524        10
           4     0.9643    0.7500    0.8437        36
           5     0.9351    0.8889    0.9114        81
           6     0.8989    0.9195    0.9091        87
           7     0.9840    0.9741    0.9791       696

    accuracy                         0.9735      2189
   macro avg     0.9423    0.9331    0.9355      2189
weighted avg     0.9736    0.9735    0.9733      2189

Macro average Test Precision, Recall and F1-Score...
(0.9423235989673728, 0.9330665353243267, 0.9355267153233063, None)
Micro average Test Precision, Recall and F1-Score...
(0.9735038830516217, 0.9735038830516217, 0.9735038830516217, None)
embeddings:
7688 5485 2189
[[0.16268128 0.0819089  0.22717407 ... 0.16793606 0.08918435 0.05106262]
 [0.02558356 0.19969049 0.14210762 ... 0.04096691 0.21243177 0.16074619]
 [0.07855231 0.5328838  0.06160687 ... 0.14952894 0.50359064 0.5410787 ]
 ...
 [0.15582469 0.44506994 0.13721852 ... 0.22408292 0.43958813 0.416186  ]
 [0.02788754 0.29293844 0.12673888 ... 0.01963634 0.38460374 0.2653184 ]
 [0.1101381  0.40431336 0.02858658 ... 0.15536508 0.3899583  0.3962347 ]]

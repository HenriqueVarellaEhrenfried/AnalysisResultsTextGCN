(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07958 train_acc= 0.05125 val_loss= 1.62957 val_acc= 0.73905 time= 0.38448
Epoch: 0002 train_loss= 1.61262 train_acc= 0.74620 val_loss= 1.25568 val_acc= 0.75365 time= 0.12797
Epoch: 0003 train_loss= 1.20787 train_acc= 0.77274 val_loss= 1.05788 val_acc= 0.66423 time= 0.12298
Epoch: 0004 train_loss= 1.00040 train_acc= 0.68564 val_loss= 0.81805 val_acc= 0.75365 time= 0.14200
Epoch: 0005 train_loss= 0.77596 train_acc= 0.78509 val_loss= 0.69030 val_acc= 0.75730 time= 0.12299
Epoch: 0006 train_loss= 0.64783 train_acc= 0.77598 val_loss= 0.62709 val_acc= 0.77007 time= 0.12497
Epoch: 0007 train_loss= 0.57782 train_acc= 0.79988 val_loss= 0.58275 val_acc= 0.82117 time= 0.12400
Epoch: 0008 train_loss= 0.53204 train_acc= 0.83472 val_loss= 0.53805 val_acc= 0.83759 time= 0.12200
Epoch: 0009 train_loss= 0.48012 train_acc= 0.85963 val_loss= 0.49595 val_acc= 0.84489 time= 0.12203
Epoch: 0010 train_loss= 0.42898 train_acc= 0.87280 val_loss= 0.45853 val_acc= 0.85949 time= 0.12252
Epoch: 0011 train_loss= 0.38403 train_acc= 0.89103 val_loss= 0.42363 val_acc= 0.87774 time= 0.12503
Epoch: 0012 train_loss= 0.34404 train_acc= 0.89953 val_loss= 0.38981 val_acc= 0.89051 time= 0.15100
Epoch: 0013 train_loss= 0.31026 train_acc= 0.91108 val_loss= 0.35973 val_acc= 0.89964 time= 0.12401
Epoch: 0014 train_loss= 0.27271 train_acc= 0.92323 val_loss= 0.33436 val_acc= 0.91058 time= 0.12400
Epoch: 0015 train_loss= 0.24491 train_acc= 0.93579 val_loss= 0.31307 val_acc= 0.92153 time= 0.13043
Epoch: 0016 train_loss= 0.21614 train_acc= 0.94551 val_loss= 0.29600 val_acc= 0.92336 time= 0.12500
Epoch: 0017 train_loss= 0.18366 train_acc= 0.95260 val_loss= 0.28341 val_acc= 0.92701 time= 0.12313
Epoch: 0018 train_loss= 0.16273 train_acc= 0.95159 val_loss= 0.27306 val_acc= 0.93248 time= 0.12201
Epoch: 0019 train_loss= 0.14288 train_acc= 0.95827 val_loss= 0.26063 val_acc= 0.93613 time= 0.12400
Epoch: 0020 train_loss= 0.12901 train_acc= 0.96152 val_loss= 0.24701 val_acc= 0.93978 time= 0.16897
Epoch: 0021 train_loss= 0.11549 train_acc= 0.96577 val_loss= 0.24066 val_acc= 0.93978 time= 0.12603
Epoch: 0022 train_loss= 0.10195 train_acc= 0.96779 val_loss= 0.24075 val_acc= 0.93796 time= 0.12300
Epoch: 0023 train_loss= 0.09163 train_acc= 0.96881 val_loss= 0.24179 val_acc= 0.93978 time= 0.12411
Epoch: 0024 train_loss= 0.08231 train_acc= 0.97367 val_loss= 0.24226 val_acc= 0.94161 time= 0.12700
Epoch: 0025 train_loss= 0.07814 train_acc= 0.97569 val_loss= 0.24080 val_acc= 0.94161 time= 0.12300
Epoch: 0026 train_loss= 0.07363 train_acc= 0.97347 val_loss= 0.23744 val_acc= 0.94526 time= 0.12302
Epoch: 0027 train_loss= 0.06440 train_acc= 0.98015 val_loss= 0.23402 val_acc= 0.95073 time= 0.16000
Epoch: 0028 train_loss= 0.05776 train_acc= 0.98400 val_loss= 0.23418 val_acc= 0.95255 time= 0.12350
Epoch: 0029 train_loss= 0.05219 train_acc= 0.98380 val_loss= 0.23852 val_acc= 0.95255 time= 0.12700
Epoch: 0030 train_loss= 0.04977 train_acc= 0.98501 val_loss= 0.24145 val_acc= 0.95255 time= 0.12200
Early stopping...
Optimization Finished!
Test set results: cost= 0.15994 accuracy= 0.96391 time= 0.05401
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9580    0.9421    0.9500       121
           1     0.8588    0.9733    0.9125        75
           2     0.9844    0.9917    0.9880      1083
           3     0.3125    0.5000    0.3846        10
           4     0.9565    0.6111    0.7458        36
           5     0.9254    0.7654    0.8378        81
           6     0.8283    0.9425    0.8817        87
           7     0.9840    0.9741    0.9791       696

    accuracy                         0.9639      2189
   macro avg     0.8510    0.8375    0.8349      2189
weighted avg     0.9666    0.9639    0.9640      2189

Macro average Test Precision, Recall and F1-Score...
(0.8509921528364629, 0.8375477151124553, 0.8349422768308971, None)
Micro average Test Precision, Recall and F1-Score...
(0.9639104613978986, 0.9639104613978986, 0.9639104613978986, None)
embeddings:
7688 5485 2189
[[-0.01216924  0.13607448 -0.05311349 ...  0.6960695  -0.02719139
   0.09633507]
 [ 0.07322522 -0.1685351  -0.08829515 ...  0.28640068  0.10660945
   0.14054362]
 [ 0.7388927  -0.07134904  0.52443844 ...  0.19717935  0.5603767
   0.5787849 ]
 ...
 [ 0.60874224  0.13252363  0.5035823  ...  0.21807025  0.41545856
   0.589716  ]
 [ 0.21313518 -0.31366798 -0.12542284 ...  0.36678478  0.29218838
   0.07851362]
 [ 0.47467455  0.08659753  0.4583943  ...  0.06834067  0.46246862
   0.52173793]]

(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07939 train_acc= 0.28216 val_loss= 2.02083 val_acc= 0.68796 time= 0.38627
Epoch: 0002 train_loss= 2.01842 train_acc= 0.69718 val_loss= 1.92482 val_acc= 0.59672 time= 0.13500
Epoch: 0003 train_loss= 1.91988 train_acc= 0.62204 val_loss= 1.79669 val_acc= 0.53285 time= 0.14400
Epoch: 0004 train_loss= 1.78614 train_acc= 0.56694 val_loss= 1.65022 val_acc= 0.52007 time= 0.12800
Epoch: 0005 train_loss= 1.63584 train_acc= 0.54750 val_loss= 1.50788 val_acc= 0.51825 time= 0.12600
Epoch: 0006 train_loss= 1.48371 train_acc= 0.54223 val_loss= 1.39081 val_acc= 0.52190 time= 0.12300
Epoch: 0007 train_loss= 1.36104 train_acc= 0.55216 val_loss= 1.30529 val_acc= 0.53285 time= 0.12300
Epoch: 0008 train_loss= 1.27905 train_acc= 0.56573 val_loss= 1.24198 val_acc= 0.56387 time= 0.12300
Epoch: 0009 train_loss= 1.20582 train_acc= 0.59165 val_loss= 1.18620 val_acc= 0.62044 time= 0.12200
Epoch: 0010 train_loss= 1.13825 train_acc= 0.63176 val_loss= 1.12843 val_acc= 0.66241 time= 0.14800
Epoch: 0011 train_loss= 1.08051 train_acc= 0.67895 val_loss= 1.06564 val_acc= 0.71715 time= 0.13700
Epoch: 0012 train_loss= 1.02700 train_acc= 0.72595 val_loss= 0.99963 val_acc= 0.74453 time= 0.12400
Epoch: 0013 train_loss= 0.95903 train_acc= 0.75896 val_loss= 0.93410 val_acc= 0.75912 time= 0.12512
Epoch: 0014 train_loss= 0.89504 train_acc= 0.77679 val_loss= 0.87288 val_acc= 0.75365 time= 0.12407
Epoch: 0015 train_loss= 0.83022 train_acc= 0.78428 val_loss= 0.81871 val_acc= 0.75730 time= 0.12304
Epoch: 0016 train_loss= 0.77893 train_acc= 0.78327 val_loss= 0.77299 val_acc= 0.76095 time= 0.12403
Epoch: 0017 train_loss= 0.73793 train_acc= 0.77962 val_loss= 0.73545 val_acc= 0.76277 time= 0.12497
Epoch: 0018 train_loss= 0.69952 train_acc= 0.78550 val_loss= 0.70438 val_acc= 0.77007 time= 0.16400
Epoch: 0019 train_loss= 0.67104 train_acc= 0.79481 val_loss= 0.67750 val_acc= 0.78650 time= 0.12600
Epoch: 0020 train_loss= 0.64223 train_acc= 0.80433 val_loss= 0.65271 val_acc= 0.79927 time= 0.12407
Epoch: 0021 train_loss= 0.61864 train_acc= 0.82216 val_loss= 0.62848 val_acc= 0.82664 time= 0.12560
Epoch: 0022 train_loss= 0.59413 train_acc= 0.83897 val_loss= 0.60417 val_acc= 0.83942 time= 0.12400
Epoch: 0023 train_loss= 0.56601 train_acc= 0.85659 val_loss= 0.57965 val_acc= 0.84307 time= 0.12405
Epoch: 0024 train_loss= 0.54159 train_acc= 0.87381 val_loss= 0.55526 val_acc= 0.85036 time= 0.12299
Epoch: 0025 train_loss= 0.51179 train_acc= 0.87968 val_loss= 0.53147 val_acc= 0.85401 time= 0.12346
Epoch: 0026 train_loss= 0.48783 train_acc= 0.88718 val_loss= 0.50861 val_acc= 0.85401 time= 0.15003
Epoch: 0027 train_loss= 0.46508 train_acc= 0.88860 val_loss= 0.48695 val_acc= 0.85949 time= 0.12597
Epoch: 0028 train_loss= 0.44478 train_acc= 0.89184 val_loss= 0.46647 val_acc= 0.86131 time= 0.12489
Epoch: 0029 train_loss= 0.41914 train_acc= 0.89670 val_loss= 0.44712 val_acc= 0.87044 time= 0.12402
Epoch: 0030 train_loss= 0.40006 train_acc= 0.89974 val_loss= 0.42878 val_acc= 0.87591 time= 0.12695
Epoch: 0031 train_loss= 0.37943 train_acc= 0.90318 val_loss= 0.41136 val_acc= 0.88504 time= 0.12503
Epoch: 0032 train_loss= 0.36179 train_acc= 0.90581 val_loss= 0.39479 val_acc= 0.89234 time= 0.12497
Epoch: 0033 train_loss= 0.34644 train_acc= 0.91128 val_loss= 0.37899 val_acc= 0.89781 time= 0.12504
Epoch: 0034 train_loss= 0.32802 train_acc= 0.91533 val_loss= 0.36389 val_acc= 0.90511 time= 0.16897
Epoch: 0035 train_loss= 0.31129 train_acc= 0.91918 val_loss= 0.34952 val_acc= 0.90693 time= 0.12300
Epoch: 0036 train_loss= 0.29731 train_acc= 0.92526 val_loss= 0.33574 val_acc= 0.91241 time= 0.12504
Epoch: 0037 train_loss= 0.28197 train_acc= 0.92951 val_loss= 0.32263 val_acc= 0.91423 time= 0.12108
Epoch: 0038 train_loss= 0.26415 train_acc= 0.93539 val_loss= 0.31013 val_acc= 0.91423 time= 0.12312
Epoch: 0039 train_loss= 0.25139 train_acc= 0.93761 val_loss= 0.29855 val_acc= 0.91788 time= 0.12605
Epoch: 0040 train_loss= 0.24055 train_acc= 0.94004 val_loss= 0.28777 val_acc= 0.92153 time= 0.12300
Epoch: 0041 train_loss= 0.22835 train_acc= 0.94612 val_loss= 0.27766 val_acc= 0.92518 time= 0.15900
Epoch: 0042 train_loss= 0.22078 train_acc= 0.94855 val_loss= 0.26821 val_acc= 0.93066 time= 0.12500
Epoch: 0043 train_loss= 0.20791 train_acc= 0.95281 val_loss= 0.25937 val_acc= 0.93066 time= 0.12315
Epoch: 0044 train_loss= 0.19844 train_acc= 0.95179 val_loss= 0.25108 val_acc= 0.93066 time= 0.12700
Epoch: 0045 train_loss= 0.18508 train_acc= 0.95625 val_loss= 0.24322 val_acc= 0.93248 time= 0.12397
Epoch: 0046 train_loss= 0.17969 train_acc= 0.95483 val_loss= 0.23549 val_acc= 0.93248 time= 0.12203
Epoch: 0047 train_loss= 0.16746 train_acc= 0.95848 val_loss= 0.22785 val_acc= 0.93431 time= 0.12500
Epoch: 0048 train_loss= 0.16193 train_acc= 0.95827 val_loss= 0.22033 val_acc= 0.93613 time= 0.12801
Epoch: 0049 train_loss= 0.15153 train_acc= 0.96152 val_loss= 0.21329 val_acc= 0.93431 time= 0.16099
Epoch: 0050 train_loss= 0.14481 train_acc= 0.96172 val_loss= 0.20680 val_acc= 0.93978 time= 0.12401
Epoch: 0051 train_loss= 0.13962 train_acc= 0.96597 val_loss= 0.20097 val_acc= 0.93978 time= 0.12304
Epoch: 0052 train_loss= 0.13365 train_acc= 0.96395 val_loss= 0.19561 val_acc= 0.94161 time= 0.12542
Epoch: 0053 train_loss= 0.13048 train_acc= 0.96800 val_loss= 0.19083 val_acc= 0.94161 time= 0.12397
Epoch: 0054 train_loss= 0.11934 train_acc= 0.97002 val_loss= 0.18649 val_acc= 0.94161 time= 0.12300
Epoch: 0055 train_loss= 0.11761 train_acc= 0.97002 val_loss= 0.18272 val_acc= 0.93978 time= 0.12300
Epoch: 0056 train_loss= 0.11098 train_acc= 0.97448 val_loss= 0.17945 val_acc= 0.95073 time= 0.12700
Epoch: 0057 train_loss= 0.10532 train_acc= 0.97428 val_loss= 0.17601 val_acc= 0.95073 time= 0.15600
Epoch: 0058 train_loss= 0.10195 train_acc= 0.97468 val_loss= 0.17253 val_acc= 0.95073 time= 0.12300
Epoch: 0059 train_loss= 0.09661 train_acc= 0.97650 val_loss= 0.16900 val_acc= 0.95073 time= 0.12410
Epoch: 0060 train_loss= 0.09317 train_acc= 0.98015 val_loss= 0.16566 val_acc= 0.95073 time= 0.12297
Epoch: 0061 train_loss= 0.09092 train_acc= 0.97691 val_loss= 0.16248 val_acc= 0.94891 time= 0.12403
Epoch: 0062 train_loss= 0.08607 train_acc= 0.97893 val_loss= 0.15999 val_acc= 0.95255 time= 0.12307
Epoch: 0063 train_loss= 0.08243 train_acc= 0.98137 val_loss= 0.15784 val_acc= 0.95438 time= 0.12400
Epoch: 0064 train_loss= 0.07923 train_acc= 0.98076 val_loss= 0.15598 val_acc= 0.95438 time= 0.12497
Epoch: 0065 train_loss= 0.07551 train_acc= 0.98319 val_loss= 0.15404 val_acc= 0.95438 time= 0.16961
Epoch: 0066 train_loss= 0.07671 train_acc= 0.98197 val_loss= 0.15227 val_acc= 0.95438 time= 0.12403
Epoch: 0067 train_loss= 0.07095 train_acc= 0.98359 val_loss= 0.15075 val_acc= 0.95438 time= 0.12297
Epoch: 0068 train_loss= 0.06882 train_acc= 0.98400 val_loss= 0.14971 val_acc= 0.95438 time= 0.12304
Epoch: 0069 train_loss= 0.06387 train_acc= 0.98582 val_loss= 0.14895 val_acc= 0.95438 time= 0.12599
Epoch: 0070 train_loss= 0.06304 train_acc= 0.98542 val_loss= 0.14790 val_acc= 0.95438 time= 0.12397
Epoch: 0071 train_loss= 0.06247 train_acc= 0.98339 val_loss= 0.14671 val_acc= 0.95620 time= 0.12303
Epoch: 0072 train_loss= 0.06037 train_acc= 0.98602 val_loss= 0.14528 val_acc= 0.95438 time= 0.15600
Epoch: 0073 train_loss= 0.05772 train_acc= 0.98704 val_loss= 0.14410 val_acc= 0.95438 time= 0.12697
Epoch: 0074 train_loss= 0.05724 train_acc= 0.98643 val_loss= 0.14277 val_acc= 0.95438 time= 0.12417
Epoch: 0075 train_loss= 0.05307 train_acc= 0.98805 val_loss= 0.14137 val_acc= 0.95438 time= 0.12403
Epoch: 0076 train_loss= 0.05144 train_acc= 0.98866 val_loss= 0.14042 val_acc= 0.95438 time= 0.12197
Epoch: 0077 train_loss= 0.04968 train_acc= 0.98825 val_loss= 0.13925 val_acc= 0.95438 time= 0.12700
Epoch: 0078 train_loss= 0.04892 train_acc= 0.98866 val_loss= 0.13848 val_acc= 0.95255 time= 0.12503
Epoch: 0079 train_loss= 0.04726 train_acc= 0.98967 val_loss= 0.13833 val_acc= 0.95255 time= 0.12433
Epoch: 0080 train_loss= 0.04862 train_acc= 0.98825 val_loss= 0.13850 val_acc= 0.95438 time= 0.16195
Epoch: 0081 train_loss= 0.04593 train_acc= 0.99048 val_loss= 0.13884 val_acc= 0.95620 time= 0.12400
Epoch: 0082 train_loss= 0.04589 train_acc= 0.99068 val_loss= 0.13847 val_acc= 0.95620 time= 0.12311
Epoch: 0083 train_loss= 0.04357 train_acc= 0.99109 val_loss= 0.13819 val_acc= 0.95803 time= 0.12500
Epoch: 0084 train_loss= 0.04252 train_acc= 0.99068 val_loss= 0.13813 val_acc= 0.95803 time= 0.12401
Epoch: 0085 train_loss= 0.04023 train_acc= 0.99251 val_loss= 0.13824 val_acc= 0.95620 time= 0.12400
Epoch: 0086 train_loss= 0.03865 train_acc= 0.99230 val_loss= 0.13802 val_acc= 0.95620 time= 0.12402
Epoch: 0087 train_loss= 0.03728 train_acc= 0.99210 val_loss= 0.13753 val_acc= 0.95620 time= 0.12430
Epoch: 0088 train_loss= 0.03742 train_acc= 0.99068 val_loss= 0.13661 val_acc= 0.95985 time= 0.15100
Epoch: 0089 train_loss= 0.03570 train_acc= 0.99271 val_loss= 0.13584 val_acc= 0.95620 time= 0.12400
Epoch: 0090 train_loss= 0.03525 train_acc= 0.99190 val_loss= 0.13565 val_acc= 0.95620 time= 0.12400
Epoch: 0091 train_loss= 0.03566 train_acc= 0.99251 val_loss= 0.13600 val_acc= 0.95620 time= 0.12400
Epoch: 0092 train_loss= 0.03440 train_acc= 0.99372 val_loss= 0.13661 val_acc= 0.95620 time= 0.12799
Epoch: 0093 train_loss= 0.03292 train_acc= 0.99392 val_loss= 0.13732 val_acc= 0.95438 time= 0.12400
Early stopping...
Optimization Finished!
Test set results: cost= 0.10743 accuracy= 0.97031 time= 0.05700
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9431    0.9587    0.9508       121
           1     0.9024    0.9867    0.9427        75
           2     0.9835    0.9917    0.9876      1083
           3     1.0000    1.0000    1.0000        10
           4     0.9643    0.7500    0.8437        36
           5     0.9296    0.8148    0.8684        81
           6     0.8421    0.9195    0.8791        87
           7     0.9840    0.9727    0.9783       696

    accuracy                         0.9703      2189
   macro avg     0.9436    0.9243    0.9313      2189
weighted avg     0.9708    0.9703    0.9700      2189

Macro average Test Precision, Recall and F1-Score...
(0.9436281261175443, 0.924261287179345, 0.9313370836797239, None)
Micro average Test Precision, Recall and F1-Score...
(0.970306075833714, 0.970306075833714, 0.970306075833714, None)
embeddings:
7688 5485 2189
[[ 0.15566811  0.1278408   0.14292103 ...  0.35006577  0.08005043
  -0.06010542]
 [ 0.00337795  0.04409229  0.04981623 ...  0.15978572  0.07670887
  -0.05775165]
 [ 0.04866753  0.19500086  0.21145888 ...  0.0942155   0.37950054
  -0.06164145]
 ...
 [ 0.13427919  0.26367134  0.27947098 ...  0.10632716  0.36026064
  -0.06747622]
 [-0.01353859  0.02722919  0.0364065  ...  0.2157327   0.09837209
  -0.08241788]
 [ 0.10522266  0.20975037  0.22217155 ...  0.01409829  0.29124317
  -0.05155846]]

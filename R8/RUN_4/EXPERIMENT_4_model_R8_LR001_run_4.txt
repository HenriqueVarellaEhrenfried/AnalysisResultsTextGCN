(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07943 train_acc= 0.06482 val_loss= 2.05648 val_acc= 0.73358 time= 0.38978
Epoch: 0002 train_loss= 2.05575 train_acc= 0.75046 val_loss= 2.02295 val_acc= 0.75182 time= 0.13009
Epoch: 0003 train_loss= 2.02079 train_acc= 0.76929 val_loss= 1.97940 val_acc= 0.75365 time= 0.15400
Epoch: 0004 train_loss= 1.97553 train_acc= 0.77638 val_loss= 1.92626 val_acc= 0.75730 time= 0.12230
Epoch: 0005 train_loss= 1.92055 train_acc= 0.78104 val_loss= 1.86426 val_acc= 0.76277 time= 0.12517
Epoch: 0006 train_loss= 1.85743 train_acc= 0.78469 val_loss= 1.79476 val_acc= 0.76460 time= 0.12400
Epoch: 0007 train_loss= 1.78044 train_acc= 0.78327 val_loss= 1.72003 val_acc= 0.76460 time= 0.12400
Epoch: 0008 train_loss= 1.70926 train_acc= 0.78023 val_loss= 1.64310 val_acc= 0.76460 time= 0.12600
Epoch: 0009 train_loss= 1.62073 train_acc= 0.77942 val_loss= 1.56752 val_acc= 0.76277 time= 0.12500
Epoch: 0010 train_loss= 1.53664 train_acc= 0.78266 val_loss= 1.49647 val_acc= 0.76095 time= 0.12400
Epoch: 0011 train_loss= 1.47928 train_acc= 0.78570 val_loss= 1.43218 val_acc= 0.75730 time= 0.15300
Epoch: 0012 train_loss= 1.40814 train_acc= 0.78145 val_loss= 1.37515 val_acc= 0.75730 time= 0.12300
Epoch: 0013 train_loss= 1.35257 train_acc= 0.77699 val_loss= 1.32454 val_acc= 0.75730 time= 0.12400
Epoch: 0014 train_loss= 1.28394 train_acc= 0.77314 val_loss= 1.27858 val_acc= 0.75182 time= 0.12400
Epoch: 0015 train_loss= 1.24028 train_acc= 0.77152 val_loss= 1.23547 val_acc= 0.74270 time= 0.12543
Epoch: 0016 train_loss= 1.20464 train_acc= 0.75856 val_loss= 1.19385 val_acc= 0.73905 time= 0.12406
Epoch: 0017 train_loss= 1.14942 train_acc= 0.75187 val_loss= 1.15254 val_acc= 0.73723 time= 0.12500
Epoch: 0018 train_loss= 1.11318 train_acc= 0.75106 val_loss= 1.11089 val_acc= 0.73905 time= 0.12303
Epoch: 0019 train_loss= 1.07041 train_acc= 0.75714 val_loss= 1.06874 val_acc= 0.74270 time= 0.15419
Epoch: 0020 train_loss= 1.03155 train_acc= 0.76038 val_loss= 1.02623 val_acc= 0.75182 time= 0.12400
Epoch: 0021 train_loss= 0.98393 train_acc= 0.76463 val_loss= 0.98378 val_acc= 0.75365 time= 0.12201
Epoch: 0022 train_loss= 0.94100 train_acc= 0.77314 val_loss= 0.94214 val_acc= 0.76460 time= 0.12298
Epoch: 0023 train_loss= 0.90181 train_acc= 0.77962 val_loss= 0.90209 val_acc= 0.76460 time= 0.12749
Epoch: 0024 train_loss= 0.86696 train_acc= 0.78509 val_loss= 0.86444 val_acc= 0.76825 time= 0.12399
Epoch: 0025 train_loss= 0.82724 train_acc= 0.78793 val_loss= 0.82969 val_acc= 0.76277 time= 0.12501
Epoch: 0026 train_loss= 0.79484 train_acc= 0.78995 val_loss= 0.79813 val_acc= 0.77007 time= 0.12500
Epoch: 0027 train_loss= 0.76074 train_acc= 0.79016 val_loss= 0.76974 val_acc= 0.76642 time= 0.16649
Epoch: 0028 train_loss= 0.73873 train_acc= 0.79542 val_loss= 0.74420 val_acc= 0.76825 time= 0.12305
Epoch: 0029 train_loss= 0.71011 train_acc= 0.79887 val_loss= 0.72105 val_acc= 0.77920 time= 0.12314
Epoch: 0030 train_loss= 0.68859 train_acc= 0.80150 val_loss= 0.69974 val_acc= 0.78650 time= 0.12347
Epoch: 0031 train_loss= 0.67030 train_acc= 0.81264 val_loss= 0.67975 val_acc= 0.80292 time= 0.12329
Epoch: 0032 train_loss= 0.64629 train_acc= 0.82439 val_loss= 0.66069 val_acc= 0.81752 time= 0.12700
Epoch: 0033 train_loss= 0.63079 train_acc= 0.83796 val_loss= 0.64226 val_acc= 0.83029 time= 0.12500
Epoch: 0034 train_loss= 0.60927 train_acc= 0.85133 val_loss= 0.62427 val_acc= 0.84307 time= 0.13603
Epoch: 0035 train_loss= 0.59518 train_acc= 0.85761 val_loss= 0.60670 val_acc= 0.85036 time= 0.14597
Epoch: 0036 train_loss= 0.57532 train_acc= 0.86348 val_loss= 0.58963 val_acc= 0.85036 time= 0.12200
Epoch: 0037 train_loss= 0.55179 train_acc= 0.87199 val_loss= 0.57320 val_acc= 0.85219 time= 0.12300
Epoch: 0038 train_loss= 0.53964 train_acc= 0.87320 val_loss= 0.55748 val_acc= 0.86131 time= 0.12218
Epoch: 0039 train_loss= 0.51815 train_acc= 0.88090 val_loss= 0.54254 val_acc= 0.86496 time= 0.12234
Epoch: 0040 train_loss= 0.50650 train_acc= 0.88191 val_loss= 0.52832 val_acc= 0.86679 time= 0.12495
Epoch: 0041 train_loss= 0.48544 train_acc= 0.88779 val_loss= 0.51480 val_acc= 0.87226 time= 0.12700
Epoch: 0042 train_loss= 0.47573 train_acc= 0.89103 val_loss= 0.50190 val_acc= 0.87591 time= 0.16700
Epoch: 0043 train_loss= 0.46403 train_acc= 0.89244 val_loss= 0.48950 val_acc= 0.87956 time= 0.12400
Epoch: 0044 train_loss= 0.44849 train_acc= 0.89508 val_loss= 0.47752 val_acc= 0.88321 time= 0.12311
Epoch: 0045 train_loss= 0.43716 train_acc= 0.89852 val_loss= 0.46587 val_acc= 0.88321 time= 0.12200
Epoch: 0046 train_loss= 0.42522 train_acc= 0.89994 val_loss= 0.45451 val_acc= 0.89051 time= 0.12300
Epoch: 0047 train_loss= 0.40966 train_acc= 0.90379 val_loss= 0.44342 val_acc= 0.89416 time= 0.12300
Epoch: 0048 train_loss= 0.39777 train_acc= 0.90662 val_loss= 0.43260 val_acc= 0.89964 time= 0.12397
Epoch: 0049 train_loss= 0.39083 train_acc= 0.90541 val_loss= 0.42205 val_acc= 0.89964 time= 0.12600
Epoch: 0050 train_loss= 0.37467 train_acc= 0.90845 val_loss= 0.41182 val_acc= 0.90328 time= 0.15603
Epoch: 0051 train_loss= 0.36428 train_acc= 0.91412 val_loss= 0.40191 val_acc= 0.90876 time= 0.12297
Epoch: 0052 train_loss= 0.35713 train_acc= 0.91614 val_loss= 0.39228 val_acc= 0.91241 time= 0.12400
Epoch: 0053 train_loss= 0.34582 train_acc= 0.91938 val_loss= 0.38296 val_acc= 0.91606 time= 0.12203
Epoch: 0054 train_loss= 0.33310 train_acc= 0.92485 val_loss= 0.37392 val_acc= 0.91606 time= 0.12300
Epoch: 0055 train_loss= 0.32674 train_acc= 0.92445 val_loss= 0.36512 val_acc= 0.91606 time= 0.12197
Epoch: 0056 train_loss= 0.31547 train_acc= 0.92951 val_loss= 0.35661 val_acc= 0.91606 time= 0.12303
Epoch: 0057 train_loss= 0.30574 train_acc= 0.93275 val_loss= 0.34827 val_acc= 0.91606 time= 0.12300
Epoch: 0058 train_loss= 0.30070 train_acc= 0.93761 val_loss= 0.34016 val_acc= 0.91971 time= 0.15807
Epoch: 0059 train_loss= 0.28504 train_acc= 0.94085 val_loss= 0.33231 val_acc= 0.92153 time= 0.12400
Epoch: 0060 train_loss= 0.28235 train_acc= 0.94308 val_loss= 0.32466 val_acc= 0.92336 time= 0.12203
Epoch: 0061 train_loss= 0.27420 train_acc= 0.94288 val_loss= 0.31719 val_acc= 0.92336 time= 0.12451
Epoch: 0062 train_loss= 0.26822 train_acc= 0.94491 val_loss= 0.30999 val_acc= 0.92701 time= 0.12397
Epoch: 0063 train_loss= 0.25419 train_acc= 0.95037 val_loss= 0.30297 val_acc= 0.93066 time= 0.12210
Epoch: 0064 train_loss= 0.24913 train_acc= 0.95139 val_loss= 0.29617 val_acc= 0.93248 time= 0.12201
Epoch: 0065 train_loss= 0.23879 train_acc= 0.95443 val_loss= 0.28960 val_acc= 0.93431 time= 0.12300
Epoch: 0066 train_loss= 0.23569 train_acc= 0.95220 val_loss= 0.28326 val_acc= 0.93613 time= 0.17000
Epoch: 0067 train_loss= 0.22861 train_acc= 0.95524 val_loss= 0.27718 val_acc= 0.93613 time= 0.12500
Epoch: 0068 train_loss= 0.22228 train_acc= 0.95645 val_loss= 0.27128 val_acc= 0.93613 time= 0.12500
Epoch: 0069 train_loss= 0.21016 train_acc= 0.95807 val_loss= 0.26568 val_acc= 0.93613 time= 0.12305
Epoch: 0070 train_loss= 0.20692 train_acc= 0.95969 val_loss= 0.26029 val_acc= 0.93613 time= 0.12301
Epoch: 0071 train_loss= 0.19990 train_acc= 0.96070 val_loss= 0.25517 val_acc= 0.93796 time= 0.12400
Epoch: 0072 train_loss= 0.19487 train_acc= 0.96172 val_loss= 0.25017 val_acc= 0.93796 time= 0.12325
Epoch: 0073 train_loss= 0.19244 train_acc= 0.96050 val_loss= 0.24534 val_acc= 0.93796 time= 0.14800
Epoch: 0074 train_loss= 0.18585 train_acc= 0.96151 val_loss= 0.24055 val_acc= 0.93796 time= 0.13700
Epoch: 0075 train_loss= 0.17798 train_acc= 0.96334 val_loss= 0.23593 val_acc= 0.93978 time= 0.12397
Epoch: 0076 train_loss= 0.17436 train_acc= 0.96496 val_loss= 0.23144 val_acc= 0.93978 time= 0.12600
Epoch: 0077 train_loss= 0.17168 train_acc= 0.96395 val_loss= 0.22705 val_acc= 0.93978 time= 0.12304
Epoch: 0078 train_loss= 0.16345 train_acc= 0.96212 val_loss= 0.22278 val_acc= 0.93978 time= 0.12404
Epoch: 0079 train_loss= 0.16283 train_acc= 0.96638 val_loss= 0.21869 val_acc= 0.94161 time= 0.12303
Epoch: 0080 train_loss= 0.15955 train_acc= 0.96435 val_loss= 0.21487 val_acc= 0.94161 time= 0.12399
Epoch: 0081 train_loss= 0.15087 train_acc= 0.96800 val_loss= 0.21129 val_acc= 0.94161 time= 0.16500
Epoch: 0082 train_loss= 0.14885 train_acc= 0.96779 val_loss= 0.20789 val_acc= 0.94526 time= 0.12301
Epoch: 0083 train_loss= 0.14313 train_acc= 0.96739 val_loss= 0.20464 val_acc= 0.94708 time= 0.12599
Epoch: 0084 train_loss= 0.14135 train_acc= 0.97063 val_loss= 0.20155 val_acc= 0.94891 time= 0.12497
Epoch: 0085 train_loss= 0.13688 train_acc= 0.97043 val_loss= 0.19854 val_acc= 0.94708 time= 0.12500
Epoch: 0086 train_loss= 0.13347 train_acc= 0.97164 val_loss= 0.19567 val_acc= 0.94708 time= 0.12404
Epoch: 0087 train_loss= 0.12925 train_acc= 0.97387 val_loss= 0.19288 val_acc= 0.94526 time= 0.12451
Epoch: 0088 train_loss= 0.12668 train_acc= 0.97367 val_loss= 0.19019 val_acc= 0.94708 time= 0.12304
Epoch: 0089 train_loss= 0.12460 train_acc= 0.97286 val_loss= 0.18762 val_acc= 0.94526 time= 0.15099
Epoch: 0090 train_loss= 0.11992 train_acc= 0.97569 val_loss= 0.18516 val_acc= 0.94708 time= 0.12201
Epoch: 0091 train_loss= 0.11905 train_acc= 0.97509 val_loss= 0.18284 val_acc= 0.94891 time= 0.12599
Epoch: 0092 train_loss= 0.11238 train_acc= 0.97995 val_loss= 0.18059 val_acc= 0.94891 time= 0.12192
Epoch: 0093 train_loss= 0.11443 train_acc= 0.97650 val_loss= 0.17848 val_acc= 0.94891 time= 0.12604
Epoch: 0094 train_loss= 0.11350 train_acc= 0.97569 val_loss= 0.17634 val_acc= 0.94891 time= 0.12697
Epoch: 0095 train_loss= 0.10457 train_acc= 0.97995 val_loss= 0.17420 val_acc= 0.94891 time= 0.12204
Epoch: 0096 train_loss= 0.10378 train_acc= 0.97954 val_loss= 0.17226 val_acc= 0.94891 time= 0.12401
Epoch: 0097 train_loss= 0.10249 train_acc= 0.98015 val_loss= 0.17048 val_acc= 0.94891 time= 0.16708
Epoch: 0098 train_loss= 0.10302 train_acc= 0.97893 val_loss= 0.16878 val_acc= 0.94891 time= 0.12337
Epoch: 0099 train_loss= 0.09960 train_acc= 0.97934 val_loss= 0.16733 val_acc= 0.95073 time= 0.12501
Epoch: 0100 train_loss= 0.09390 train_acc= 0.98157 val_loss= 0.16589 val_acc= 0.95073 time= 0.12400
Epoch: 0101 train_loss= 0.09381 train_acc= 0.98076 val_loss= 0.16439 val_acc= 0.95255 time= 0.12306
Epoch: 0102 train_loss= 0.09162 train_acc= 0.98238 val_loss= 0.16301 val_acc= 0.95073 time= 0.12500
Epoch: 0103 train_loss= 0.08903 train_acc= 0.98319 val_loss= 0.16162 val_acc= 0.95073 time= 0.12600
Epoch: 0104 train_loss= 0.09141 train_acc= 0.98055 val_loss= 0.16004 val_acc= 0.95255 time= 0.14712
Epoch: 0105 train_loss= 0.08772 train_acc= 0.98015 val_loss= 0.15856 val_acc= 0.95073 time= 0.13500
Epoch: 0106 train_loss= 0.08639 train_acc= 0.98359 val_loss= 0.15710 val_acc= 0.95073 time= 0.12209
Epoch: 0107 train_loss= 0.08360 train_acc= 0.98380 val_loss= 0.15580 val_acc= 0.95073 time= 0.12396
Epoch: 0108 train_loss= 0.08174 train_acc= 0.98501 val_loss= 0.15467 val_acc= 0.95073 time= 0.12404
Epoch: 0109 train_loss= 0.08007 train_acc= 0.98440 val_loss= 0.15363 val_acc= 0.95073 time= 0.12199
Epoch: 0110 train_loss= 0.07896 train_acc= 0.98380 val_loss= 0.15288 val_acc= 0.95255 time= 0.12301
Epoch: 0111 train_loss= 0.07748 train_acc= 0.98440 val_loss= 0.15229 val_acc= 0.95255 time= 0.12496
Epoch: 0112 train_loss= 0.07359 train_acc= 0.98724 val_loss= 0.15178 val_acc= 0.95255 time= 0.16904
Epoch: 0113 train_loss= 0.07535 train_acc= 0.98501 val_loss= 0.15114 val_acc= 0.95255 time= 0.12200
Epoch: 0114 train_loss= 0.07408 train_acc= 0.98623 val_loss= 0.15052 val_acc= 0.95255 time= 0.12299
Epoch: 0115 train_loss= 0.06988 train_acc= 0.98683 val_loss= 0.14989 val_acc= 0.95255 time= 0.12107
Epoch: 0116 train_loss= 0.07182 train_acc= 0.98562 val_loss= 0.14914 val_acc= 0.95255 time= 0.12600
Epoch: 0117 train_loss= 0.07048 train_acc= 0.98582 val_loss= 0.14824 val_acc= 0.95255 time= 0.12200
Epoch: 0118 train_loss= 0.06796 train_acc= 0.98582 val_loss= 0.14728 val_acc= 0.95255 time= 0.12304
Epoch: 0119 train_loss= 0.06589 train_acc= 0.98724 val_loss= 0.14634 val_acc= 0.95255 time= 0.12299
Epoch: 0120 train_loss= 0.06533 train_acc= 0.98562 val_loss= 0.14538 val_acc= 0.95255 time= 0.15801
Epoch: 0121 train_loss= 0.06361 train_acc= 0.98744 val_loss= 0.14452 val_acc= 0.95255 time= 0.12403
Epoch: 0122 train_loss= 0.06222 train_acc= 0.98805 val_loss= 0.14370 val_acc= 0.95438 time= 0.12201
Epoch: 0123 train_loss= 0.06239 train_acc= 0.98724 val_loss= 0.14306 val_acc= 0.95255 time= 0.12305
Epoch: 0124 train_loss= 0.06284 train_acc= 0.98845 val_loss= 0.14259 val_acc= 0.95255 time= 0.12495
Epoch: 0125 train_loss= 0.05981 train_acc= 0.98967 val_loss= 0.14219 val_acc= 0.95255 time= 0.12301
Epoch: 0126 train_loss= 0.05899 train_acc= 0.98987 val_loss= 0.14188 val_acc= 0.95255 time= 0.12300
Epoch: 0127 train_loss= 0.05716 train_acc= 0.99068 val_loss= 0.14151 val_acc= 0.95255 time= 0.12304
Epoch: 0128 train_loss= 0.05734 train_acc= 0.98967 val_loss= 0.14118 val_acc= 0.95255 time= 0.15396
Epoch: 0129 train_loss= 0.05519 train_acc= 0.98926 val_loss= 0.14087 val_acc= 0.95255 time= 0.12804
Epoch: 0130 train_loss= 0.05484 train_acc= 0.98886 val_loss= 0.14041 val_acc= 0.95255 time= 0.12308
Epoch: 0131 train_loss= 0.05459 train_acc= 0.99068 val_loss= 0.13996 val_acc= 0.95255 time= 0.12296
Epoch: 0132 train_loss= 0.05504 train_acc= 0.99028 val_loss= 0.13969 val_acc= 0.95255 time= 0.12300
Epoch: 0133 train_loss= 0.05338 train_acc= 0.99068 val_loss= 0.13962 val_acc= 0.95255 time= 0.12492
Epoch: 0134 train_loss= 0.05082 train_acc= 0.99007 val_loss= 0.13936 val_acc= 0.95255 time= 0.12207
Epoch: 0135 train_loss= 0.05097 train_acc= 0.99048 val_loss= 0.13897 val_acc= 0.95255 time= 0.12608
Epoch: 0136 train_loss= 0.05006 train_acc= 0.99089 val_loss= 0.13881 val_acc= 0.95620 time= 0.16505
Epoch: 0137 train_loss= 0.04814 train_acc= 0.99291 val_loss= 0.13882 val_acc= 0.95620 time= 0.12700
Epoch: 0138 train_loss= 0.04962 train_acc= 0.99109 val_loss= 0.13856 val_acc= 0.95438 time= 0.12400
Epoch: 0139 train_loss= 0.04693 train_acc= 0.99210 val_loss= 0.13827 val_acc= 0.95255 time= 0.12200
Epoch: 0140 train_loss= 0.04729 train_acc= 0.99089 val_loss= 0.13797 val_acc= 0.95255 time= 0.12408
Epoch: 0141 train_loss= 0.04580 train_acc= 0.99089 val_loss= 0.13783 val_acc= 0.95255 time= 0.12504
Epoch: 0142 train_loss= 0.04653 train_acc= 0.99190 val_loss= 0.13791 val_acc= 0.95255 time= 0.12306
Epoch: 0143 train_loss= 0.04360 train_acc= 0.99210 val_loss= 0.13773 val_acc= 0.95255 time= 0.16600
Epoch: 0144 train_loss= 0.04251 train_acc= 0.99352 val_loss= 0.13751 val_acc= 0.95438 time= 0.12300
Epoch: 0145 train_loss= 0.04425 train_acc= 0.99210 val_loss= 0.13729 val_acc= 0.95255 time= 0.12304
Epoch: 0146 train_loss= 0.04248 train_acc= 0.99190 val_loss= 0.13723 val_acc= 0.95255 time= 0.12500
Epoch: 0147 train_loss= 0.04113 train_acc= 0.99291 val_loss= 0.13720 val_acc= 0.95255 time= 0.12500
Epoch: 0148 train_loss= 0.04086 train_acc= 0.99332 val_loss= 0.13720 val_acc= 0.95255 time= 0.12303
Epoch: 0149 train_loss= 0.04187 train_acc= 0.99129 val_loss= 0.13729 val_acc= 0.95255 time= 0.12597
Epoch: 0150 train_loss= 0.03919 train_acc= 0.99291 val_loss= 0.13748 val_acc= 0.95255 time= 0.12300
Epoch: 0151 train_loss= 0.03916 train_acc= 0.99332 val_loss= 0.13762 val_acc= 0.95255 time= 0.15600
Early stopping...
Optimization Finished!
Test set results: cost= 0.10818 accuracy= 0.97442 time= 0.05500
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9512    0.9669    0.9590       121
           1     0.8916    0.9867    0.9367        75
           2     0.9844    0.9917    0.9880      1083
           3     1.0000    1.0000    1.0000        10
           4     1.0000    0.7222    0.8387        36
           5     0.9351    0.8889    0.9114        81
           6     0.9101    0.9310    0.9205        87
           7     0.9841    0.9756    0.9798       696

    accuracy                         0.9744      2189
   macro avg     0.9571    0.9329    0.9418      2189
weighted avg     0.9748    0.9744    0.9741      2189

Macro average Test Precision, Recall and F1-Score...
(0.9570548760068652, 0.932877359079116, 0.941765042539769, None)
Micro average Test Precision, Recall and F1-Score...
(0.9744175422567383, 0.9744175422567383, 0.9744175422567383, None)
embeddings:
7688 5485 2189
[[ 0.1859523   0.22504511  0.18205199 ...  0.1880028   0.07462759
   0.10103912]
 [ 0.06034094  0.17504011  0.07687242 ...  0.06531426  0.18314885
   0.18866494]
 [ 0.08891281  0.18197118  0.15034747 ... -0.01566475  0.15507641
   0.414046  ]
 ...
 [ 0.14581518  0.06620833  0.20545052 ...  0.07197496  0.01277687
   0.15891847]
 [ 0.08125529  0.25689584  0.07411359 ...  0.0926458   0.28946033
   0.2855257 ]
 [ 0.09852134  0.02263808  0.128911   ...  0.01942938  0.0433706
   0.23559064]]

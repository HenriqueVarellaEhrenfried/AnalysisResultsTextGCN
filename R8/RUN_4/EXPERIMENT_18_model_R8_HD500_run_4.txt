(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)
15362
  (0, 5864)	1.021520945883481
  (0, 6572)	4.818458885761806
  (0, 7052)	3.362096962025199
  (0, 7916)	1.2457508634099117
  (0, 7933)	2.0838819303261675
  (0, 8096)	0.686135075474489
  (0, 9590)	1.1548972396321597
  (0, 10449)	5.949860997252907
  (0, 10914)	2.080745492836037
  (0, 11036)	1.6330397727043
  (0, 11444)	0.09164216929228669
  (0, 11878)	3.647275904258861
  (0, 12720)	0.9536627509544204
  (1, 5523)	5.7267174459386965
  (1, 5674)	3.6274732769626814
  (1, 5758)	4.360625792136326
  (1, 5917)	7.153833801578843
  (1, 5969)	7.631389111767649
  (1, 6209)	5.161403636888637
  (1, 6216)	3.9217127499606215
  (1, 6333)	6.547697998008527
  (1, 6737)	3.989766213205637
  (1, 6888)	1.7407007606022247
  (1, 7223)	2.810028379725159
  (1, 7295)	2.0131453792343894
  :	:
  (15361, 10641)	4.975301357254776
  (15361, 10690)	3.30014637316366
  (15361, 10787)	2.680292058069188
  (15361, 10833)	2.6898432290535306
  (15361, 10951)	4.868055826901179
  (15361, 11049)	3.7091513079769487
  (15361, 11110)	3.381072863484204
  (15361, 11181)	2.7552778649537504
  (15361, 11213)	2.929436111108544
  (15361, 11216)	6.866151729127062
  (15361, 11246)	3.88934746545859
  (15361, 11252)	5.687496732785416
  (15361, 11271)	0.967968172022305
  (15361, 11444)	0.09164216929228669
  (15361, 11902)	2.548663615590751
  (15361, 11922)	3.01069907518731
  (15361, 12000)	4.093563006887281
  (15361, 12048)	7.336155358372797
  (15361, 12172)	3.1962002848986444
  (15361, 12457)	4.5267526630103
  (15361, 12548)	3.0678574890272587
  (15361, 12694)	3.775109275768746
  (15361, 12757)	47.08188589283273
  (15361, 12768)	3.9416469648614387
  (15361, 13025)	6.921592674632485
(15362, 15362)
(15362, 15362)
15362
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(None, 8), dtype=float32)
Epoch: 0001 train_loss= 2.07936 train_acc= 0.21207 val_loss= 1.98880 val_acc= 0.75365 time= 0.53788
Epoch: 0002 train_loss= 1.98497 train_acc= 0.77517 val_loss= 1.81794 val_acc= 0.73723 time= 0.19311
Epoch: 0003 train_loss= 1.81064 train_acc= 0.74661 val_loss= 1.59751 val_acc= 0.67518 time= 0.19497
Epoch: 0004 train_loss= 1.57995 train_acc= 0.69253 val_loss= 1.39857 val_acc= 0.63504 time= 0.19503
Epoch: 0005 train_loss= 1.36943 train_acc= 0.65242 val_loss= 1.26889 val_acc= 0.64964 time= 0.20204
Epoch: 0006 train_loss= 1.23048 train_acc= 0.67187 val_loss= 1.18407 val_acc= 0.67883 time= 0.19700
Epoch: 0007 train_loss= 1.13447 train_acc= 0.70043 val_loss= 1.10325 val_acc= 0.73358 time= 0.19039
Epoch: 0008 train_loss= 1.05839 train_acc= 0.74175 val_loss= 1.01272 val_acc= 0.75912 time= 0.19200
Epoch: 0009 train_loss= 0.96665 train_acc= 0.77355 val_loss= 0.91861 val_acc= 0.75730 time= 0.19830
Epoch: 0010 train_loss= 0.87337 train_acc= 0.78671 val_loss= 0.83175 val_acc= 0.75730 time= 0.22000
Epoch: 0011 train_loss= 0.78907 train_acc= 0.78610 val_loss= 0.75979 val_acc= 0.75547 time= 0.19227
Epoch: 0012 train_loss= 0.72367 train_acc= 0.78489 val_loss= 0.70539 val_acc= 0.75730 time= 0.19598
Epoch: 0013 train_loss= 0.66787 train_acc= 0.78651 val_loss= 0.66612 val_acc= 0.76642 time= 0.19201
Epoch: 0014 train_loss= 0.62865 train_acc= 0.79461 val_loss= 0.63630 val_acc= 0.78832 time= 0.19606
Epoch: 0015 train_loss= 0.59560 train_acc= 0.81811 val_loss= 0.61071 val_acc= 0.81752 time= 0.22000
Epoch: 0016 train_loss= 0.57068 train_acc= 0.83978 val_loss= 0.58627 val_acc= 0.83759 time= 0.19200
Epoch: 0017 train_loss= 0.54367 train_acc= 0.86024 val_loss= 0.56184 val_acc= 0.85036 time= 0.19400
Epoch: 0018 train_loss= 0.51634 train_acc= 0.86794 val_loss= 0.53775 val_acc= 0.85036 time= 0.19314
Epoch: 0019 train_loss= 0.48943 train_acc= 0.87340 val_loss= 0.51462 val_acc= 0.85219 time= 0.19400
Epoch: 0020 train_loss= 0.46305 train_acc= 0.87786 val_loss= 0.49279 val_acc= 0.85766 time= 0.22000
Epoch: 0021 train_loss= 0.43868 train_acc= 0.88374 val_loss= 0.47223 val_acc= 0.86496 time= 0.19610
Epoch: 0022 train_loss= 0.41944 train_acc= 0.88799 val_loss= 0.45273 val_acc= 0.87774 time= 0.19400
Epoch: 0023 train_loss= 0.39467 train_acc= 0.89224 val_loss= 0.43411 val_acc= 0.89051 time= 0.19301
Epoch: 0024 train_loss= 0.37503 train_acc= 0.89548 val_loss= 0.41601 val_acc= 0.89051 time= 0.19300
Epoch: 0025 train_loss= 0.35499 train_acc= 0.90176 val_loss= 0.39843 val_acc= 0.89416 time= 0.22001
Epoch: 0026 train_loss= 0.33294 train_acc= 0.90845 val_loss= 0.38127 val_acc= 0.89781 time= 0.19199
Epoch: 0027 train_loss= 0.31559 train_acc= 0.91371 val_loss= 0.36465 val_acc= 0.89964 time= 0.19247
Epoch: 0028 train_loss= 0.29773 train_acc= 0.92323 val_loss= 0.34874 val_acc= 0.91058 time= 0.19600
Epoch: 0029 train_loss= 0.27955 train_acc= 0.92931 val_loss= 0.33365 val_acc= 0.91971 time= 0.19200
Epoch: 0030 train_loss= 0.26351 train_acc= 0.93377 val_loss= 0.31955 val_acc= 0.92153 time= 0.22000
Epoch: 0031 train_loss= 0.24812 train_acc= 0.94025 val_loss= 0.30636 val_acc= 0.92701 time= 0.19300
Epoch: 0032 train_loss= 0.23276 train_acc= 0.94470 val_loss= 0.29396 val_acc= 0.92518 time= 0.19400
Epoch: 0033 train_loss= 0.21521 train_acc= 0.95058 val_loss= 0.28200 val_acc= 0.92518 time= 0.19518
Epoch: 0034 train_loss= 0.20282 train_acc= 0.95179 val_loss= 0.27046 val_acc= 0.92701 time= 0.19300
Epoch: 0035 train_loss= 0.18747 train_acc= 0.95686 val_loss= 0.25908 val_acc= 0.92883 time= 0.22200
Epoch: 0036 train_loss= 0.17774 train_acc= 0.95848 val_loss= 0.24792 val_acc= 0.93248 time= 0.19200
Epoch: 0037 train_loss= 0.16515 train_acc= 0.95949 val_loss= 0.23755 val_acc= 0.93431 time= 0.19200
Epoch: 0038 train_loss= 0.15363 train_acc= 0.96212 val_loss= 0.22815 val_acc= 0.93796 time= 0.19400
Epoch: 0039 train_loss= 0.14357 train_acc= 0.96334 val_loss= 0.21995 val_acc= 0.93978 time= 0.19300
Epoch: 0040 train_loss= 0.13156 train_acc= 0.96658 val_loss= 0.21282 val_acc= 0.93978 time= 0.21604
Epoch: 0041 train_loss= 0.12563 train_acc= 0.96941 val_loss= 0.20683 val_acc= 0.93978 time= 0.19001
Epoch: 0042 train_loss= 0.11610 train_acc= 0.97103 val_loss= 0.20202 val_acc= 0.94526 time= 0.19160
Epoch: 0043 train_loss= 0.10652 train_acc= 0.97306 val_loss= 0.19796 val_acc= 0.94526 time= 0.19200
Epoch: 0044 train_loss= 0.09980 train_acc= 0.97448 val_loss= 0.19436 val_acc= 0.94708 time= 0.19543
Epoch: 0045 train_loss= 0.09531 train_acc= 0.97630 val_loss= 0.19077 val_acc= 0.94526 time= 0.19500
Epoch: 0046 train_loss= 0.08880 train_acc= 0.97610 val_loss= 0.18690 val_acc= 0.94526 time= 0.19196
Epoch: 0047 train_loss= 0.08449 train_acc= 0.97914 val_loss= 0.18299 val_acc= 0.94526 time= 0.19309
Epoch: 0048 train_loss= 0.07987 train_acc= 0.97954 val_loss= 0.17894 val_acc= 0.94891 time= 0.19096
Epoch: 0049 train_loss= 0.07505 train_acc= 0.98055 val_loss= 0.17534 val_acc= 0.94891 time= 0.19500
Epoch: 0050 train_loss= 0.07000 train_acc= 0.98359 val_loss= 0.17248 val_acc= 0.94891 time= 0.19700
Epoch: 0051 train_loss= 0.06691 train_acc= 0.98319 val_loss= 0.17044 val_acc= 0.95073 time= 0.19300
Epoch: 0052 train_loss= 0.06344 train_acc= 0.98339 val_loss= 0.16892 val_acc= 0.95073 time= 0.19300
Epoch: 0053 train_loss= 0.06035 train_acc= 0.98521 val_loss= 0.16772 val_acc= 0.95255 time= 0.19565
Epoch: 0054 train_loss= 0.05726 train_acc= 0.98704 val_loss= 0.16702 val_acc= 0.95255 time= 0.19300
Epoch: 0055 train_loss= 0.05522 train_acc= 0.98643 val_loss= 0.16666 val_acc= 0.95255 time= 0.21500
Epoch: 0056 train_loss= 0.05190 train_acc= 0.98764 val_loss= 0.16602 val_acc= 0.95255 time= 0.19521
Epoch: 0057 train_loss= 0.04919 train_acc= 0.98805 val_loss= 0.16562 val_acc= 0.95255 time= 0.19200
Epoch: 0058 train_loss= 0.04490 train_acc= 0.98886 val_loss= 0.16484 val_acc= 0.95438 time= 0.19103
Epoch: 0059 train_loss= 0.04634 train_acc= 0.98785 val_loss= 0.16359 val_acc= 0.95438 time= 0.19311
Epoch: 0060 train_loss= 0.04248 train_acc= 0.98967 val_loss= 0.16248 val_acc= 0.95255 time= 0.22395
Epoch: 0061 train_loss= 0.03952 train_acc= 0.99028 val_loss= 0.16151 val_acc= 0.95255 time= 0.19300
Epoch: 0062 train_loss= 0.03901 train_acc= 0.99089 val_loss= 0.16100 val_acc= 0.95255 time= 0.19421
Epoch: 0063 train_loss= 0.03681 train_acc= 0.99311 val_loss= 0.16117 val_acc= 0.95620 time= 0.19271
Epoch: 0064 train_loss= 0.03630 train_acc= 0.99109 val_loss= 0.16141 val_acc= 0.95620 time= 0.19200
Epoch: 0065 train_loss= 0.03393 train_acc= 0.99251 val_loss= 0.16098 val_acc= 0.95620 time= 0.22200
Epoch: 0066 train_loss= 0.03311 train_acc= 0.99149 val_loss= 0.16053 val_acc= 0.95438 time= 0.19400
Epoch: 0067 train_loss= 0.03102 train_acc= 0.99271 val_loss= 0.16000 val_acc= 0.95438 time= 0.19700
Epoch: 0068 train_loss= 0.02983 train_acc= 0.99271 val_loss= 0.16007 val_acc= 0.95438 time= 0.19301
Epoch: 0069 train_loss= 0.02830 train_acc= 0.99392 val_loss= 0.16048 val_acc= 0.95620 time= 0.19299
Epoch: 0070 train_loss= 0.02860 train_acc= 0.99372 val_loss= 0.16148 val_acc= 0.95620 time= 0.22201
Early stopping...
Optimization Finished!
Test set results: cost= 0.11229 accuracy= 0.96939 time= 0.07796
15362
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.9365    0.9752    0.9555       121
           1     0.9114    0.9600    0.9351        75
           2     0.9826    0.9917    0.9871      1083
           3     0.9000    0.9000    0.9000        10
           4     1.0000    0.7500    0.8571        36
           5     0.8947    0.8395    0.8662        81
           6     0.8587    0.9080    0.8827        87
           7     0.9840    0.9698    0.9768       696

    accuracy                         0.9694      2189
   macro avg     0.9335    0.9118    0.9201      2189
weighted avg     0.9698    0.9694    0.9692      2189

Macro average Test Precision, Recall and F1-Score...
(0.9334893127307223, 0.9117845122900832, 0.9200718108261955, None)
Micro average Test Precision, Recall and F1-Score...
(0.9693924166285975, 0.9693924166285975, 0.9693924166285975, None)
embeddings:
7688 5485 2189
[[ 0.2960057   0.15623862  0.1440551  ...  0.09537818  0.11110318
   0.0865587 ]
 [ 0.16145605  0.02013984  0.03056292 ...  0.01762934 -0.01117857
   0.10753459]
 [ 0.04222016 -0.0709156   0.12416525 ...  0.14501165 -0.04782885
   0.16755769]
 ...
 [ 0.1306917  -0.00643396  0.20437478 ...  0.18387602  0.01463082
   0.06147806]
 [ 0.17107794  0.0293753   0.00466837 ... -0.00602204 -0.03458675
   0.20904365]
 [ 0.01399935  0.00328191  0.14294563 ...  0.13862528 -0.00867157
   0.05512726]]
